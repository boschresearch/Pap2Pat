# I. INTRODUCTION

Multi-object 3D shape reconstruction and 6D pose (i.e. 3D orientation and position) and size estimation from raw visual observations is crucial for robotics manipulation [1,2,3], navigation [4,5] and scene understanding [6,7]. The ability to perform pose estimation in real-time leads to fast feedback control [8] and the capability to reconstruct complete 3D shapes [9,10,11] results in fine-grained understanding of local geometry, often helpful in robotic grasping [2,12]. Recent advances in deep learning have enabled great progress in instance-level 6D pose estimation [13,14,15] where the exact 3D models of objects and their sizes are known a-priori. Unfortunately, these methods [16,17,18] do not generalize well to realistic-settings on novel object instances with unknown 3D models in the same category, often referred to as category-level settings. Despite progress in categorylevel pose estimation, this problem remains challenging even when similar object instances are provided as priors during training, due to a high variance of objects within a category.

Recent works on shape reconstruction [19,20] and category-level 6D pose and size estimation [21,22,23] use complex multi-stage pipelines. As shown in Figure 1, these approaches independently employ two stages, one for performing 2D detection [24,25,26] and another for performing our single-stage approach. The single-stage approach uses object instances as centers to jointly optimize 3D shape, 6D pose and size.

object reconstruction or 6D pose and size estimation. This pipeline is computationally expensive, not scalable, and has low performance on real-world novel object instances, due to the inability to express explicit representation of shape variations within a category. Motivated by above, we propose to reconstruct complete 3D shapes and estimate 6D pose and sizes of novel object instances within a specific category, from a single-view RGB-D in a single-shot manner.

To address these challenges, we introduce Center-based Shape reconstruction and 6D pose and size estimation (Cen-terSnap), a single-shot approach to output complete 3D information (3D shape, 6D pose and sizes of multiple objects) in a bounding-box proposal-free and per-pixel manner. Our approach is inspired by recent success in anchor-free, singleshot 2D key-point estimation and object detection [27,28,29,30]. As shown in Figure 1, we propose to learn a spatial per-pixel representation of multiple objects at their center locations using a feature pyramid backbone [3,24]. Our technique directly regresses multiple shape, pose, and size codes, which we denote as object-centric 3D parameter maps. At each object's center point in these spatial objectcentric 3D parameter maps, we predict vectors denoting the complete 3D information (i.e. encoding 3D shape, 6D pose and sizes codes). A 3D auto-encoder [31,32] is designed to learn canonical shape codes from a large database of shapes. A joint optimization for detection, reconstruction and 6D pose and sizes for each object's spatial center is then carried out using learnt shape priors. Hence, we perform complete 3D scene-reconstruction and predict 6D pose and sizes of novel object instances in a single-forward pass, foregoing the need for complex multi-stage pipelines [19,22,24].

Our proposed method leverages a simpler and computationally efficient pipeline for a complete object-centric 3D understanding of multiple objects from a single-view RGB-D observation. We make the following contributions:

• Present the first work to formulate object-centric holistic scene-understanding (i.e. 3D shape reconstruction and 6D pose and size estimation) for multiple objects from a single-view RGB-D in a single-shot manner. • Propose a fast (real-time) joint reconstruction and pose estimation system. Our network runs at 40 FPS on a NVIDIA Quadro RTX 5000 GPU. • Our method significantly outperforms all baselines for 6D pose and size estimation on NOCS benchmark, with over 12% absolute improvement in mAP for 6D pose.

II. RELATED WORK 3D shape prediction and completion: 3D reconstruction from a single-view observation has seen great progress with various input modalities studied. RGB-based shape reconstruction [31,33,34] has been studied to output either pointclouds, voxels or meshes [11,32,35]. Contrarily, learning-based 3D shape completion [36,37,38] studies the problem of completing partial pointclouds obtained from masked depth maps. However, all these works focus on reconstructing a single object. In contrast, our work focuses on multi-object reconstruction from a single RGB-D. Recently, multi-object reconstruction from RGB-D has been studied [19,39,40]. However, these approaches employ complex multi-stage pipelines employing 2D detections and then predicting canonical shapes. Our approach is a simple, bounding-box proposal-free method which jointly optimizes for detection, shape reconstruction and 6D pose and size.

Instance-Level and Category-Level 6D Pose and Size Estimation: Works on Instance-level pose estimation use classical techniques such as template matching [41,42,43], direct pose estimation [13,15,18] or point correspondences [14,16]. Contrarily, our work closely follows the paradigm of category-level pose and size estimation where CAD models are not available during inference. Previous work has employed complex multi-stage pipelines [21,22,44] for category-level pose estimation. Our work optimizes for shape, pose, and sizes jointly, while leveraging the shape priors obtained by training a large dataset of CAD models. CenterSnap is a simpler, more effective, and faster solution. Per-pixel point-based representation has been effective for anchor-free object detection and segmentation. These approaches [28,30,45] represent instances as their centers in a spatial 2D grid. This representation has been further studied for key-point detection [27], segmentation [46,47] and bodymesh recovery [48,49]. Our approach falls in a similar paradigm and further adds a novelty to reconstruct objectcentric holistic 3D information in an anchor-free manner. Different from [39,48], our approach 1) considers pretrained shape priors on a large collection of CAD models 2) jointly optimizes categorical shape 6D pose and size, instead of 3D-bounding boxes and 3) considers more complicated scenarios (such as occlusions, a large variety of objects and sim2real transfer with limited real-world supervision).

## III. CENTERSNAP: SINGLE-SHOT OBJECT-CENTRIC SCENE UNDERSTANDING OF MULTIPLE-OBJECTS

Given an RGB-D image as input, our goal is to simultaneously detect, reconstruct and localize all unknown object instances in the 3D space. In essence, we regard shape reconstruction and pose and size estimation as a point-based representation problem where each object's complete 3D information is represented by its center point in the 2D spatial image. Formally, given an RGB-D single-view observation (I ∈ R ho×wo×3 , D ∈ R ho×wo ) of width w o and height h o , our aim is to reconstruct the complete pointclouds (P ∈ R K×N ×3 ) coupled with 6D pose and scales ( P ∈ SE(3), ŝ ∈ R 3 ) of all object instances in the 3D scene, where K is the number of arbitrary objects in the scene and N denotes the number of points in the pointcloud. The pose ( P ∈ SE(3)) of each object is denoted by a 3D rotation R ∈ SO(3) and a translation t ∈ R 3 . The 6D pose P, 3D size (spatial extent obtained from canonical pointclouds P ) and 1D scales ŝ completely defines the unknown object instances in 3D space with respect to the camera coordinate frame. To achieve the above goal, we employ an end-to-end trainable method as illustrated in Figure 2. First, objects instances are detected as heatmaps in a per-pixel manner (Section III-A) using a CenterSnap detection backbone based on feature pyramid networks [3,50]. Second, a joint shape, pose, and size code denoted by object-centric 3D parameter maps is predicted for detected object centers using specialized heads (Section III-C). Our pre-training of shape codes is described in Section III-B. Lastly, 2D heatmaps and our novel object-centric 3D parameter maps are jointly optimized to predict shapes, pose and sizes in a single-forward pass (Section III-D).

# A. Object instances as center points

We represent each object instance by its 2D location in the spatial RGB image following [28,30]. Given a RGB-D observation (I ∈ R ho×wo×3 , D ∈ R ho×wo ), we generate a low-resolution spatial feature representations f r ∈ R ho/4×wo/4×Cs and f d ∈ R ho/4×wo/4×Cs by using Resnet [51] stems, where C s = 32. We concatenate computed features f r and f d along the channel dimension before feeding it to Resnet18-FPN backbone [52] to compute a pyramid of features (f rd ) with scales ranging from 1/8 to 1/2 resolution, where each pyramid level has the same channel dimension (i.e. 64). We use these combined features with a specialized heatmap head to predict object-based heatmaps Ŷ ∈ [0, 1] ho R × wo R ×1 where R = 8 denotes the heat-map down-sampling factor. Our specialized heatmap head design merges the semantic information from all FPN levels into one output ( Ŷ ). We use three upsampling stages followed by element-wise sum and sof tmax to achieve this. This design allows our network to 1) capture multiscale information and 2) encode features at higher resolution 

## Point Encoder

Fig. 2: CenterSnap Method: Given a single-view RGB-D observation, our proposed approach jointly optimizes for shape, pose, and sizes of each object in a single-shot manner. Our method comprises a joint FPN backbone for feature extraction (Section III-A), a pointcloud auto-encoder to extract shape codes from a large collection of CAD models (Section III-B), CenterSnap model which constitutes multiple specialized heads for heatmap and object-centric 3D parameter map prediction (Section III-C) and joint optimization for shape, pose, and sizes for each object's spatial center (Section III-D).

for effective reasoning at the per-pixel level. We train the network to predict ground-truth heatmaps (Y ) by minimizing

of each center in the ground truth heat-maps (Y ) is relative to the scale-based standard deviation σ of each object, following [3,28,30,53].

# B. Shape, Pose, and Size Codes

To jointly optimize the object-based heatmaps, 3D shapes and 6D pose and sizes, the complete object-based 3D information (i.e. Pointclouds P , 6D pose P and scale ŝ) are represented as as object-centric 3D parameter maps (O 3d ∈ R ho×wo×141 ). O 3d constitutes two parts, shape latentcode and 6D Pose and scales. The pointcloud representation for each object is stored in the object-centric 3D parameter maps as a latent-shape code (z i ∈ R 128 ). The ground-truth Pose ( P) represented by a 3 × 3 rotation R ∈ SO(3) and translation t ∈ R 3 coupled with 1D scale ŝ are vectorized to store in the O 3d as 13-D vectors. To learn a shapecode (z i ) for each object, we design an auto-encoder trained on all 3D shapes from a set of CAD models. Our autoencoder is representation-invariant and can work with any shape representation. Specifically, we design an encoderdecoder network (Figure 3), where we utilize a Point-Net encoder (g φ ) similar to [54]. The decoder network (d θ ), which comprises three fully-connected layers, takes the encoded low-dimensional feature vector i.e. the latent shape-code (z i ) and reconstructs the input pointcloud Pi = d θ (g φ (P i )).

To train the auto-encoder, we sample 2048 points from the ShapeNet [55] CAD model repository and use them as ground-truth shapes. Furthermore, we unit-canonicalize the input pointclouds by applying a scaling transform to each shape such that the shape is centered at origin and unit normalized. We optimize the encoder and decoder networks jointly using the reconstruction-error, denoted by Chamferdistance, as shown below:

Sample decoder outputs and t-SNE embeddings [56] for the latent shape-code (z i ) are shown in Figure 3 and our complete 3D reconstructions on novel real-world object instances are visualizes in Figure 4 as pointclouds, meshes and textures. Our shape-code space provides a compact way to encode 3D shape information from a large number of CAD models. As shown by the t-SNE embeddings (Figure 3), our shape-code space finds a distinctive 3D space for semantically similar objects and provides an effective way to scale shape prediction to a large number (i.e. 50+) of categories.

# C. CenterSnap Model

Given object center heatmaps (Section III-A), the goal of the CenterSnap model is to infer object-centric 3D parameter maps which define each object instance completely in the 3D-space. The CenterSnap model comprises a taskspecific head similar to the heatmap head (Section III-A) with the input being pyramid of features (f rd ). During training, the task-specific head outputs a 3D parameter map Ô3d ∈ R ho R × wo R ×141 where each pixel in the downsampled map ( ho R × wo R ) contains the complete object-centric 3D information (i.e. shape-code z i , 6D pose P and scale ŝ) as 141-D vectors, where R = 8. Note that, during training, we obtain the ground-truth shape-codes from the pre-trained point-encoder ẑi = g φ (P i ). For Pose ( P), our choice of rotation representation R ∈ SO(3) is determined by stability during training [57]. Furthermore, we project the predicted 3 × 3 rotation R into SO(3), as follows:

To handle ambiguities caused by rotational symmetries, we also employ a rotation mapping function defined by [58]. The mapping function, used only for symmetric objects (bottle, bowl, and can), maps ambiguous ground-truth rotations to a single canonical rotation by normalizing the pose rotation.

During training, we jointly optimize the predicted objectcentric 3D parameter map ( Ô3d ) using a masked Huber loss (Eq. 1), where the Huber loss is enforced only where the Gaussian heatmaps (Y ) have score greater than 0.3 to prevent ambiguity in areas where no objects exist. Similar 

Auxiliary Depth-Loss: We additionally integrate an auxiliary depth reconstruction loss L D for effective sim2real transfer, where L D (D, D) minimizes the Huber loss (Eq. 1) between target depth (D) and the predicted depth ( D) from the output of task-specific head, similar to the one used in Section III-A. The depth auxiliary loss (further investigated empirically using ablation study in Section IV) forces the network to learn geometric features by reconstructing artifactfree depth. Since real depth sensors contain artifacts, we enforce this loss by pre-processing the input synthetic depth images to contain noise and random eclipse dropouts [3].

# D. Joint Shape, Pose, and Size Optimization

We jointly optimize for detection, reconstruction and localization. Specifically, we minimize a combination of heatmap instance detection, object-centric 3D map prediction and auxiliary depth losses as

where λ is a weighting co-efficient with values determined empirically as 100, 1.0 and 1.0 respectively.

Inference: During inference, we perform peak detection as in [28] on the heatmap output ( Ŷ ) to get detected centerpoints for each object, c i in R 2 = (x i , y i ) (as shown in Figure 2 middle). These centerpoints are local maximum in heatmap output ( Ŷ ). We perform non-maximum suppression on the detected heatmap maximas using a 3×3 max-pooling, following [28]. Lastly, we directly sample the object-centric 3D parameter map of each object from Ô3d at the predicted center location (c i ) via Ô3d (x i , y i ). We perform inference on the extracted latent-codes using point-decoder to reconstruct pointclouds ( Pi = d θ (z p i )). Finally, we extract 3 × 3 rotation Rp i , 3D translation vector tp i and 1D scales ŝp i from Ô3d to get transformed points in the 3D space  

## IV. EXPERIMENTS & RESULTS

In this section, we aim to answer the following questions: 1) How well does CenterSnap reconstruct multiple objects from a single-view RGB-D observation? 2) Does CenterSnap perform fast pose-estimation in real-time for real-world applications? 3) How well does CenterSnap perform in terms of 6D pose and size estimation?

Datasets: We utilize the NOCS [22] dataset to evaluate both shape reconstruction and categorical 6D pose and size estimation. We use the CAMERA dataset for training which contains 300K synthetic images, where 25K are held out for evaluation. Our training set comprises 1085 object instances from 6 different categories -bottle, bowl, camera, can, laptop and mug whereas the evaluation set contains 184 different instances. The REAL dataset contains 4300 images from 7 different scenes for training, and 2750 real-world images from 6 scenes for evaluation. Further, we evaluate multiobject reconstruction and completion using Multi-Object ShapeNet Dataset (MOS). We generate this dataset using the SimNet [3] pipeline. Our datasets contains 640px × 480px renderings of multiple (3-10) ShapeNet objects [55] in a table-top scene. Following [3], we randomize over lighting and textures using OpenGL shaders with PyRender [61]. Following [38], we utilize 30974 models from 8 different categories for training (i.e. MOS-train): airplane, cabinet, car, chair, lamp, sofa, table. We use the held out set (MOStest) of 150 models for testing from a novel set of categories bed, bench, bookshelf and bus. Evaluation Metrics: Following [22], we independently evaluate the performance of 3D object detection and 6D pose estimation using the following key metrics: 1) Averageprecision for various IOU-overlap thresholds (IOU25 and IOU50). 2) Average precision of object instances for which the error is less than n • for rotation and m cm for translation (5°5 cm, 5°10 cm and 10°10 cm). For shape reconstruction we use Chamfer distance (CD) following [38]. Implementation Details: CenterSnap is trained on the CAMERA training dataset with fine-tuning on the REAL training set. We use a batch-size of 32 and trained the network for 40 epochs with early-stopping based on the performance of the model on the held out validation set. We found data-augmentation (i.e. color-jitter) on the real-  training set to be helpful for stability and training performance. The auto-encoder network is comprised of a Point-Net encoder [54] and three-layered fully-connected decoder each with output dimension of 512, 1024 and 1024 × 3. The auto-encoder is frozen after initially training on CAMERA CAD models for 50 epochs. We use Pytorch [62] for all our models and training pipeline implementation. For shapecompletion experiments, we train only on MOS-train with testing on MOS-test. NOCS Baselines: We compare seven model variants to show effectiveness of our method: (1) NOCS [22]: Extends Mask-RCNN architecture to predict NOCS map and uses similarity transform with depth to predict pose and size.

Our results are compared against the best pose-estimation configuration in NOCS (i.e. 32-bin classification) (2) Shape Prior [21]: Infers 2D bounding-box for each object and predicts a shape-deformation. (3) CASS [44]: Employs a 2stage approach to first detect 2D bounding-boxes and second regress the pose and size. (4) Metric-Scale [60]: Extends NOCS to predict object center and metric shape separately (5) CenterSnap: Our single-shot approach with direct pose and shape regression. ( 6) CenterSnap-R: Our model with a standard point-to-plane iterative pose refinement [63,64] between the projected canonical pointclouds in the 3D space and the depth-map. Note that we do not include comparisons  to 6D pose tracking baselines such as [65,66] which are not detection-based (i.e. do not report mAP metrics) and require pose initialization.

Comparison with NOCS baselines: The results of our proposed CenterSnap method are reported in Table I and Figure 6. Our proposed approach consistently outperforms all the baseline methods on both 3D object detection and 6D pose estimation. Among our variants, CenterSnap-R achieves the best performance. Our method (i.e. CenterSnap) is able to outperform strong baselines (#1 -#5 in Table I) even without iterative refinement. Specifically, CenterSnap-R method shows superior performance on the REAL test-set by achieving a mAP of 80.2% for 3D IOU at 0.5, 31.6% for 6D pose at 5°10 cm and 70.9% for 6D pose at 10°10 cm, hence demonstrating an absolute improvement of 2.7%, 10.8% and 12.6% over the best-performing baseline on the Real dataset. Our method also achieves superior test-time performance on CAMERA evaluation never seen during training. We achieve a mAP of 92.5% for 3D IOU at 0.5, 71.7% for 6D pose at 5°10 cm and 87.9% for 6D pose at 10°10 cm, demonstrating an absolute improvement of 1.8%, 12.1% and 6.6% over the best-performing baseline.

NOCS Reconstruction: To quantitatively analyze the reconstruction accuracy, we measure the Chamfer distance (CD) of our reconstructed pointclouds with ground-truth CAD model in NOCS. Our results are reported in Table II. Our results show consistently lower CD metrics for all class categories which shows superior reconstruction performance on novel object instances. We report a lower mean Chamfer distance of 0.14 on CAMERA25 and 0.15 on REAL275 compared to 0.20 and 0.32 reported by the competitive baseline [21].

Comparison with Shape Completion Baselines: We further test our network's ability to reconstruct complete 3D shapes by comparing against depth-based shape-completion baselines i.e. PCN [38] and Folding-Net [67]. The results of our CenterSnap method are reported in Figure 5. Our consistently lower Chamfer distance (CD) compared to strong shape-completion baselines show our network's ability to reconstruct complete 3D shapes from partial 3D information such as depth-maps. We report a lower mean CD of 0.089 on test-instances from categories not included during training vs 0.0129 for PCN and 0.0124 for Folding-Net respectively.

Inference time: Given RGB-D images of size 640 × 480, our method performs fast (real-time) joint reconstruction and pose and size estimation. We achieve an interactive rate of around 40 FPS for CenterSnap on a desktop with an Intel Xeon W-10855M@2.80GHz CPU and NVIDIA Quadro RTX 5000 GPU, which is fast enough for realtime applications. Specifically, our networks takes 22 ms and reconstruction takes around 3 ms. In comparison, on the same machine, competitive multi-stage baselines [21,22] achieve an interactive rate of 4 FPS for pose estimation.

Ablation Study: An empirical study to validate the significance of different design choices and modalities in our proposed CenterSnap model was carried out. Our results are summarized in Table III. We investigate the performance im-   III) which indicates that sequential learning in this case leads to more robust sim2real transfer.

Qualitative Results: We qualitatively analyze the performance of CenterSnap on NOCS Real-275 test-set never seen during training. As shown in Figure 7, our method performs accurate 6D pose estimation and joint shape reconstruction on 5 different real-world scenes containing novel object instances. Our method also reconstructs complete 3D shapes (visualized with two different camera viewpoints) with accurate aspect ratios and fine-grained geometric details such as mug-handle and can-head.

V. CONCLUSION Despite recent progress, existing categorical 6D pose and size estimation approaches suffer from high-computational cost and low performance. In this work, we propose an anchor-free and single-shot approach for holistic objectcentric 3D scene-understanding from a single-view RGB-D. Our approach runs in real-time (40 FPS) and performs accurate categorical pose and size estimation, achieving significant improvements against strong baselines on the NOCS REAL275 benchmark on novel object instances.

