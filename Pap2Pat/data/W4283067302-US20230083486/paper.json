{
    "id": "https://semopenalex.org/work/W4283067302",
    "authors": [
        "R\u00e9mi Munos",
        "Michal Valko",
        "Bernardo \u00c1vila Pires",
        "Daniele Calandriello",
        "Corentin Tallec",
        "Yunhao Tang",
        "Mohammad Gheshlaghi Azar",
        "Florent Altch\u00e9",
        "Bilal Piot",
        "Jean-Bastien Grill",
        "Shantanu Thakoor",
        "Miruna P\u00eeslar",
        "Alaa Saade",
        "Zhaohan Daniel Guo"
    ],
    "title": "BYOL-Explore: Exploration by Bootstrapped Prediction",
    "date": "2022-06-16",
    "abstract": "We present BYOL-Explore, a conceptually simple yet general approach for curiosity-driven exploration in visually-complex environments. BYOL-Explore learns a world representation, the world dynamics, and an exploration policy all-together by optimizing a single prediction loss in the latent space with no additional auxiliary objective. We show that BYOL-Explore is effective in DM-HARD-8, a challenging partially-observable continuous-action hard-exploration benchmark with visually-rich 3-D environments. On this benchmark, we solve the majority of the tasks purely through augmenting the extrinsic reward with BYOL-Explore s intrinsic reward, whereas prior work could only get off the ground with human demonstrations. As further evidence of the generality of BYOL-Explore, we show that it achieves superhuman performance on the ten hardest exploration games in Atari while having a much simpler design than other competitive agents.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Exploration is essential to reinforcement learning (RL) [65], especially when extrinsic rewards are sparse or hard to reach. In rich environments, the variety of meaningful directions of exploration makes it impractical to visit everything. Thus, the question becomes: how can an agent determine which parts of the environment are interesting to explore? One promising paradigm to address this challenge is curiosity-driven exploration. It consists of (i) learning a predictive model of some information about the world, called a world model, and (ii) using discrepancies between predictions of the world model and real experience to build intrinsic rewards [58,64,59,34,50,51,2]. An RL agent optimizing these intrinsic rewards drives itself towards states where the world model is incorrect or imperfect, generating new trajectories on which the world model can be improved. In other words, the properties of the world model influence the quality of the exploration policy, which in turn gathers new data to shape the world model itself. Thus, it can be important not to treat learning the world model and learning the exploratory policy as two separate problems, but instead altogether as a single joint problem to solve.",
                "In this paper, we present BYOL-Explore, a curiosity-driven exploration algorithm whose appeal resides in its conceptual simplicity, generality, and high performance. BYOL-Explore learns a world model with a self-supervised prediction loss, and uses the same loss to train a curiosity-driven policy, thus using a single learning objective to solve both the problem of building the world model's representation and the curiosity-driven policy. Our approach builds upon Bootstrap Your Own Latent (BYOL), a latent-predictive self-supervised method which predicts an older copy of its own latent representation. This bootstrapping mechanism has already been successfully applied in computer vision [21,55], graph representation learning [69], and representation learning in RL [24,61]. However, the latter works focus primarily on using the world-model for representation learning in RL whereas BYOL-Explore takes this one step further, and not only learns a versatile world model but also uses the world model's loss to drive exploration.",
                "We evaluate BYOL-Explore on DM-HARD-8 [22], a suite of 8 complex first-person-view 3-D tasks with sparse rewards. These tasks demand efficient exploration since in order to reach the final goal and obtain the reward they require completing a sequence of precise, orderly interactions with the physical objects in the environment, unlikely to happen under a vanilla random exploration strategy (see Fig. 2 andvideos). To show the generality of our method we also evaluate BYOL-Explore on the ten hardest exploration Atari games [6]. In all these domains, BYOL-Explore outperforms other prominent curiosity-driven exploration methods, such as Random Network Distillation (RND) [9] and Intrinsic Curiosity Module (ICM) [50]. In DM-HARD-8, BYOL-Explore achieves human-level performance in the majority of the tasks using only the extrinsic reward augmented with BYOL-Explore's intrinsic reward, whereas previously significant progress required human demonstrations [22]. Remarkably, BYOL-Explore achieves this performance using only a single world model and a single policy network concurrently trained across all tasks. Finally, as further evidence of its generality, BYOL-Explore achieves superhuman performance in the ten hardest exploration Atari games [6] while having a simpler design than other competitive agents, such as Agent57 [4,5] and Go-Explore [15,16]. 2"
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "There is a large body of research in building world models either for planning [64,62,27,26,60], representation learning [61,24,41,20] or curiosity-driven exploration [58,66,59,34,50,51,2,62]. Most works consider world models that predict the entire observations [57,47,17,20], which necessitates a loss in pixel space when observations are visually complex images. Some works have considered predicting latent representations, whether they are random projections [8,9], or learned representations from a separate model, such as an inverse dynamics model [50] or an autoencoder [25,8]. Finally, some RL works [60] have focused on predicting lower-dimensional quantities such as the extrinsic reward, the action-selection policy, and the value function to build a world model.",
                "Our BYOL-Explore's world model operates in latent space and uses the same loss both for representation and intrinsic reward, simplifying and unifying representation learning and exploration. BYOL-Explore's world model is derived from recent self-supervised representation learning methods [21,55,54,69] and is similar to the ones in self-supervised RL [61,24]. These previous works focused on the benefit of shaping representations for policy learning and have not looked into exploration. We build on this previous work to show that we can take the impact of a good representation technique further and use it to drive exploration.",
                "While our approach belongs to the curiosity-driven exploration paradigm [49,42,48,58,6,66,59,34,50,51,2,62], other exploration paradigms have also been proposed. The maximum entropy paradigms try to steer the agent to a desired distribution of states (or state-action pairs) that maximizes the entropy of visited states [29,67,68,23]. The goal-conditioned paradigm has the agent set its own goal drive exploration [56,1,18,73,46,13,80,28,16,53,78,52]. The reward-free exploration paradigm consists of training an agent to explore the environment such that it would be able to produce a near-optimal policy for any possible reward function [37,39,44,76,72,10,77,79]."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [
                "Our agent has three components: a self-supervised latent-predictive world-model called BYOL-Explore, a generic reward normalization and prioritization scheme, and an off-the-shelf RL agent that can optionally share its own representation with BYOL-Explore's world model."
            ],
            "subsections": [
                {
                    "title": "Background and Notation",
                    "paragraphs": [
                        "We consider a discrete-time interaction process [43,35,36,14] between an agent and its environment where, at each time step t \u2208 N, the agent receives an observation o t \u2208 O and generates an action a t \u2208 A. We consider an environment with stochastic dynamics p : H \u00d7 A \u2192 \u2206 O3 that maps a history of past observations-actions and a current action to a probability distribution over future observations. More precisely, the space of past observations-actions is H = t\u2208N H t where H 0 = O and \u2200t \u2208 N * , H t+1 = H t \u00d7 A \u00d7 O. We consider policies \u03c0 : H \u2192 \u2206 A that maps a history of past observations-actions to a probability distribution over actions. Finally, an extrinsic reward function r e : H \u00d7 A \u2192 R maps a history of past observations-actions to a real number."
                    ],
                    "subsections": []
                },
                {
                    "title": "Latent-Predictive World Model",
                    "paragraphs": [
                        "BYOL-Explore world model is a multi-step predictive world model operating at the latent level. It is inspired by the self-supervised learning method BYOL in computer vision and adapted to interactive environments (see Section 3.1). Similar to BYOL, BYOL-Explore model trains an online network using targets generated by an exponential moving average (EMA) target network. However, BYOL obtains its targets by applying different augmentations to the same observation as the online representation, whereas BYOL-Explore model gets its targets from future observations processed by an EMA of the online network, with no hand-crafted augmentation. Also BYOL-Explore model, uses a recurrent neural network (RNN) [33,12] to build the agent state, i.e., the state of RNN, from the history of observations, whereas the original BYOL only uses a feed-forward network for encoding the observations. In the remainder of this section, we will explain: (i) how the online network builds future predictions, (ii) how targets for our predictions are obtained through a target network, (iii) the loss used to train the online network, and (iv) how we compute the uncertainties of the world model. (i) Future Predictions. The online network is composed of an encoder f \u03b8 that transforms an observation o t into an observation-representation f \u03b8 (o t ) \u2208 R N , where N \u2208 N * is the embedding size. The observation-representation f \u03b8 (o t ) is then fed alongside the previous action a t-1 to a RNN cell h c \u03b8 that is referred as the close-loop RNN cell. It computes a representation b t \u2208 R M of the history h t \u2208 H t seen so far as b (ii) Targets and Target Network. The target network is an observation encoder f \u03c6 whose parameters are an EMA of the online network's parameters \u03b8. It outputs targets f \u03c6 (o t+k ) \u2208 R N that are used to train the online network. After each training step, the target network's weights are updated via an EMA update \u03c6 \u2190 \u03b1\u03c6 + (1 -\u03b1)\u03b8 where \u03b1 is the target network EMA parameter. A sketch of the neural architecture is provided in Fig. 1, with more details in App. A.",
                        "(iii) Online Network Loss Function. Suppose our RL agent collected a batch of trajectories",
                        ", where T \u2208 N * is the trajectory length and B \u2208 N * is the batch size. Then, the loss L BYOL-Explore (\u03b8) to minimize is defined as the average cosine distance between the open-loop future predictions g \u03b8 (b j t,k ) and their respective targets f \u03c6 (o j t+k ) at time t + k:",
                        "where K(t) = min(K, T -1 -t) is the valid open-loop horizon for a trajectory of length T and sg is the stop-gradient operator.",
                        "(iv) World Model Uncertainties The uncertainty associated to the transition (o j t , a j t , o j t+1 ) is the sum of the corresponding prediction losses:",
                        "This accumulates all the losses corresponding to the world-model uncertainties relative to the observation o j t+1 . Thus, a timestep receives intrinsic reward based on how difficult its observation was to predict from past partial histories.",
                        "Intuition on why BYOL-Explore learns a meaningful representation. The intuition behind BYOL-Explore is similar in spirit to the one behind BYOL. In early training, the target network is initialized randomly, and so BYOL-Explore's online network and the closed-loop RNN are trained to predict random features of the future. This encourages the online observation representation to capture information that is useful to predict the future. This information is then distilled into the target observation encoder network through the EMA slow copy mechanism. In turn, these features become targets for the online network and predicting them can further improve the quality of the online representation. For further theoretical and empirical insights on why the bootstrap latent methods learn non-trivial representations see, e.g., [70,74]."
                    ],
                    "subsections": []
                },
                {
                    "title": "Reward Normalization and Prioritization Scheme",
                    "paragraphs": [
                        "Reward Normalization. We use the world model uncertainties j t as an intrinsic reward. To counter the non-stationarity of the uncertainties during training, we adopt the same reward normalization scheme as RND [9] and divide the raw rewards (( j t ) T -2 t=0 ) B-1 j=0 by an EMA estimate of their standard deviation \u03c3 r . The normalized rewards are j t /\u03c3 r . Details are provided in App. A.3.",
                        "Reward Prioritization. In addition to normalizing the rewards, we can optionally prioritize them by optimizing only the rewards with highest uncertainties and nullifying rewards with the lowest uncertainties. Because of the transient nature of the intrinsic rewards, this allows the agent to focus first on parts of the environment where the model is not accurate. Later on, if the previously nullified rewards remain, they will naturally become the ones with highest uncertainties and be optimized. This mechanism allows the agent to optimize only the source of high uncertainties and not optimize all sources of uncertainties at once. To do so, let us denote by \u00b5 /\u03c3r the adjusted EMA mean relative to the successive batch of normalized rewards (( j t /\u03c3 r ) T -2 t=0 ) B-1 j=0 . We use \u00b5 /\u03c3r as a clipping threshold separating high and low-uncertainty rewards. Then, the clipped and normalized reward that plays the role of intrinsic reward is: r j i,t = max( j t /\u03c3 r -\u00b5 /\u03c3r , 0)\u2022"
                    ],
                    "subsections": []
                },
                {
                    "title": "Generic RL Algorithm and Representation Sharing",
                    "paragraphs": [
                        "BYOL-Explore can be used in conjunction with any RL algorithm for training the policy. In addition to providing an intrinsic reward, BYOL-Explore can further be used to shape the representation learnt by the RL agent by directly sharing some components of the BYOL-Explore world model with the RL model. For instance, consider a recurrent agent composed of an encoder f \u03c8 , an RNN cell h c \u03c8 , a policy head \u03c0 \u03c8 and a value head v \u03c8 that are shaped by an RL loss. Then, we can share the weights \u03b8 of the BYOL-Explore world model and the weights \u03c8 of the RL model at the level of the encoder and the RNN cell: f \u03c8 = f \u03b8 and h c \u03b8 = h c \u03c8 and let the joint representation be trained via both the RL loss and BYOL-Explore. In our experiments, we will show results for both the shared and unshared settings. Architectural details are provided in Appendix A."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "We evaluate the algorithms on benchmark task-suites known to contain hard exploration challenges. These benchmarks have different properties in terms of the complexity of the observations, partial observability, and procedural generation, allowing us to test the generality of our approach. Our agents are implemented in the JAX ecosystem of libraries [3].",
                "Atari Learning Environment [7]. This is a widely used RL benchmark, comprising of approximately 50 Atari games. These are 2-D, fully-observable, (fairly) deterministic environments for most of the games but have a very long optimization horizon (episodes last for an average of 10000 steps) and complex observations (preprocessed greyscale images which are 84 \u00d7 84 byte arrays). We select the 10 hardest exploration games [6] to conduct our experiments: Alien, Freeway, Gravitar, Hero, Montezuma's Revenge, Pitfall, Private Eye, Qbert, Solaris and Venture.",
                "Hard-Eight Suite [22]. This benchmark comprises of 8 hard exploration tasks, originally built to emphasize the difficulties encountered by an RL agent when learning from sparse rewards in a procedurally-generated 3-D world with partial observability, continuous control, and highly variable initial conditions. Each task requires the agent to interact with specific objects in its environment in order to reach a large apple that provides reward (see Fig. 2). Being procedurally-generated, properties such as object shapes, colors, and positions are different every episode. The agent sees only the first-person view from its position; but for greater clarity we provide videos showing top-down view and a third-person view as well to ground the difficulty of these tasks. Note that the current best RL agents that solve these tasks require a small (but non-zero) amount of human expert demonstrations. Without demonstrations or reward shaping, state-of-the-art deep RL algorithms, such as R2D2 [38], do not get positive reward signal on any of the tasks. In our case, we train a single RL agent and a single world model to tackle the 8 tasks all-together, making for a challenging multi-task setting. Finally, we run experiments on two different evaluation regimes. The first regime uses a mixed reward function r t = r e,t + \u03bbr i,t which is a linear combination of the normalized extrinsic rewards r e,t and intrinsic rewards computed by the agent r i,t with mixing parameter \u03bb. This may be the most important regime for a practitioner as we can see if our intrinsic rewards help improve performance, with respect to the extrinsic rewards, compared to the pure RL agent. The second regime is fully self-supervised where only the intrinsic reward r i,t is optimized. This regime gives us a sense of how pure exploration methods perform in complex environments.",
                "Choice of RL algorithm. We use VMPO [63] as our RL algorithm. VMPO is an efficient on-policy optimization method that has achieved strong results across both discrete and continuous control tasks, and is thus applicable to all of the domains we consider. Further details regarding the RL algorithm setup and hyperparameters are provided in Appendix C.",
                "Performance Metrics. We evaluate performance in terms of the agent score at learner step t, Agent score (t), as measured by undiscounted episode return. We define the highest agent score through training as Agent score = max t Agent score (t), as done in [19,4]. We define, for each game, the Human Normalized Score (HNS) at learner step t:",
                "Humanscore-Randomscore as well as the HNS over the whole training: HNS = max t HNS(t). A HNS higher than 1 means superhuman performance on a specific task. We similarly define the CHNS Score as HNS clipped between 0 and 1. "
            ],
            "subsections": [
                {
                    "title": "Atari Results",
                    "paragraphs": [
                        "In these experiments, we set the target EMA rate \u03b1 = 0.99 and open-loop horizon K = 8. We use \u03bb = 0.1 to combine the intrinsic and extrinsic rewards. We follow the classical 30 random no-ops evaluation regime [45,71], and average performance over 10 episodes and over 3 seeds. Fig. 3 (left) shows that BYOL-Explore is almost superhuman on the 10-hardest exploration games and outperforms the different baselines of RND, ICM, and pure RL. Fig. 3 (right) compares BYOL-Explore against its ablations to gain finer insights into our method. The No clipping ablation performs comparably, showing that the prioritization of intrinsic rewards is not necessary on Atari tasks. Similarly, the Horizon=1 ablation performs slightly better, indicating that simply predicting one-step latents is sufficient to explore efficiently on the fully-observable Atari tasks. The Fixed Targets ablation performs much worse, showing that our approach of predicting learned targets (rather than fixed random projections) is vital for good performance. It is also worth noting that all the ablations except Fixed Targets outperform all of our baselines, demonstrating the robustness of our approach.",
                        "Finally, because the Horizon=1 ablation was close to superhuman on Atari, we run the same configuration but double the length of the sequences on which we train from 64 to 128 (also doubling memory requirements while learning). For fair comparison, we train for only half the learner steps to keep total computation performed roughly equivalent. With this small adjustment, this agent (BYOL-Explore (big)) becomes superhuman on all of the 10-hardest exploration games. Purely intrinsic exploration. To test how BYOL-Explore behaves when only given intrinsic rewards without any extrinsic signal, we test on the well-known Montezuma's Revenge game by setting \u03bb = 0. We measure exploratory behavior in terms of the number of different rooms of the dungeon the agent is able to explore over its lifetime. Note that accessing later rooms requires navigating complex dynamics such as collecting keys to open doors, avoiding enemies, and carefully traversing rooms filled with traps such as timed lasers. Figure 4 shows how much room coverage is achieved during training when no extrinsic reward is used, showing that BYOL-Explore explores further than the best result reported by RND [9]. Importantly, we use the episodic setting for intrinsic rewards whereas the published RND results considers the non-episodic setting for intrinsic rewards -facilitating exploration as the agent is less risk-averse. Therefore, our setting could be considered even more challenging. Our agent explores more than 20 rooms on average versus 17 with best published RND results. As expected in the episodic setting, our RND re-implementation visits even fewer rooms. However, we can reproduce the published RND results in the episodic setting when using recurrent policies.",
                        "Further results. More fine-grained results are reported in App D.1. We report, in Fig. 10 and in Fig. 11, the agent scores learning curves for each game. Tab. 1 and Tab. 2 have agent score at the end of training. Finally, Tab. 3 and Tab. 4 show the mean CHNS and different statistics (mean and percentiles) of the HNS across the selected games.",
                        "An interesting finding from examining the HNS is that clipping and longer-horizon predictions are critical for very high scores on some games such as Montezuma's Revenge or Hero. BYOL-Explore has a median HNS of 331.98 compared to the No-clipping ablation and the Horizon=1 which have a median HNS of only 181.39 and 199.80 respectively. Therefore, while clipping is not necessary to get to human-level performance, it is still crucial to achieve top performance.",
                        "We also provide further results regarding the pure exploration setting on all 10 games in App. D.2."
                    ],
                    "subsections": []
                },
                {
                    "title": "DM-HARD-8 Results",
                    "paragraphs": [
                        "In these experiments, we set the target EMA rate \u03b1 = 0.99 and open-loop horizon K = 10. We use \u03bb = 0.01 to combine the intrinsic and extrinsic rewards.",
                        "In contrast to prior work [22], we perform experiments in the more challenging multi-task regime, training a single agent to solve all eight tasks. At the beginning of each episode, a task is drawn uniformly at random from the suite.",
                        "In Fig. 5 (left) we report the mean CHNS(t) across the tasks, averaged over 3 seeds. We see that BYOL-Explore outperforms the baselines of RND, ICM, and pure RL by a large margin. Fig. 5 (right) compares the performance of BYOL-Explore to its various ablations. Note that the No-clipping ablation performs similarly to BYOL-Explore in terms of CHNS. However, unlike the fully-observable Atari tasks, the Horizon=1 ablation learns considerably slower and achieves lower final performance (see also our extended ablations on the horizon length in Fig. 14 in App. D.3). We note once again that the BYOL-Explore bootstrapping mechanism for learning representations is essential, as confirmed by the poor (but non-zero) performance of the Fixed-targets ablation. Due to computational limitations, we did not run the No Sharing ablation, as using separate networks requires twice the memory. We now analyze our method more closely by examining per-task performance. The full learning curves for each task can be found in Fig. 6 for BYOL-Explore and the main baselines and in Appendix D.3 (see Fig. 13) for the various ablations. First, we take note that other curiosity-driven methods (ICM and RND) cannot get any positive score on the majority of the DM-HARD-8 tasks, even with additional hyperparameter tuning and reward prioritizing (see Fig. 16 and Fig. 17  In contrast, we see that BYOL-Explore achieves strong performance on five out of the eight hard exploration tasks. Importantly, BYOL-Explore achieves this without human demonstrations, which was not the case in prior work [22]. BYOL-Explore even surpasses humans on 4 tasks, namely Navigate cubes, Throw-across, Baseball, and Wall Sensors (see Tab. 9 in App. D.3 for details). Most impressively, BYOL-Explore can solve Throw-across, which is a challenging task even for a skilful human player and was not solvable in prior work without collecting additional successful human demonstrations [22].",
                        "Interestingly, note that on the Navigate Cubes task, both RND and the Fixed-targets ablation achieve maximum performance alongside BYOL-Explore. We argue that this is because the prediction of random projections (either at the same step as done by RND or multi-step as done by BYOL-Explore) leads to the policy learned performing spatial, navigational exploration -this is the kind of behavior required to explore well on the Navigate Cubes task. In contrast, the other tasks require exploratory behavior involving interaction with objects and the use of tools, where both RND and the Fixed-targets ablation fail.",
                        "Finally, we observe that two games, namely Remember Sensor and Push Blocks, are particularly challenging, where all of our considered methods perform poorly. We hypothesize that this is due to the larger variety of procedurally generated objects spawned in these levels, and the need to remember previous cues in the environment leading to a hard credit assignment problem. Purely intrinsic exploration. Each of the DM-HARD-8 tasks has complex dynamics and object interactions, making it difficult to assess qualitatively the behavior of purely intrinsically motivated exploration. Nevertheless, for completeness, we provide results of BYOL-Explore trained only with intrinsic rewards in App. D.3, showing that it does achieve some positive signal on the Drawbridge and Wall Sensor tasks (see Fig. 18)."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We showed that BYOL-Explore is a simple curiosity-driven exploration method that achieves excellent performance on hard exploration tasks with fairly deterministic dynamics. BYOL-Explore is a multi-step prediction error method at the latent level that relies on recent advances in self-supervised learning to train its representation as well as its world-model without any additional loss. In Atari, BYOL-Explore achieves superhuman performance on the 10-hardest exploration games while being of much simpler design than other superhuman agents. Moreover, BYOL-Explore substantially outperforms previous exploration methods on DM-HARD-8 navigation and manipulation tasks in a 3-D, multi-task, partially-observable and procedurally-generated environment. This shows the generality of our algorithm to handle either 2-D or 3-D, single or multi-task, fully or partially-observable environments.",
                "In the future, we would like to improve performance in DM-HARD-8 and to demonstrate the generality of our method by extending it to other domains. In DM-HARD-8, we believe we can improve performance by scaling up the world model and finding better ways to trade off exploration and exploitation. Beyond DM-HARD-8, there are opportunities to tackle further challenges, most notably highly-stochastic and procedurally-generated environment dynamics such as NetHack [40]."
            ],
            "subsections": []
        },
        {
            "title": "A.2 Detailed BYOL-Explore architecture for DM-HARD-8",
            "paragraphs": [
                "In DM-HARD-8, the size of the observation-representation N = 256 and the size of the historyrepresentation M = 512.",
                "\u2022 Encoder: f \u03b8 : O \u2192 R N : The encoder operates over RGB image observations at each timestep, and is implemented as a Deep ResNet [30] stack. The image is passed through a stack of 3 units, each comprised of a 3 \u00d7 3 convolutional layer with stride 1, a 3 \u00d7 3 maxpool layer with stride 2, and 2 residual blocks. We use ReLU activations everywhere. The output of this stack is flattened and passed through a linear layer of width 1024, followed by LayerNorm and ReLU layers, and finally a linear layer of width 256. \u2022 Close-loop RNN cell:",
                "The RNN cell operates over the output of the encoder f \u03b8 in addition to some other observation signals per timestep, which we describe first. The observation contains (i) the force the agent's hand is currently applying as a fraction of total grip strength, a single scalar between 0 and 1; (ii) a boolean indicating whether the hand is currently holding an object; (iii) the distance of the agent hand from the main body; (iv) the last action taken by the agent; each of these are embedded using a linear projection to 20 dimensions followed by a ReLU. (v) the previous reward obtained by the agent, passed by a signed hyperbolic transformation; (vi) a text instruction specific to the task currently being solved, with each word embedded into 20 dimensions and processed sequentially by an LSTM to an embedding of size 64. All of these quantities, along with the output of f \u03b8 , are concatenated and passed through a linear layer to an embedding of size 512. An LSTM with embedding size 512 processes each of these observation representations sequentially to form the recurrent history-representation.",
                "\u2022 Open-loop RNN cell: h o \u03b8 : R M \u00d7A \u2192 R M : This is implemented as an LSTM. We discretize the action into 76 bins and provide them as one-hot representations of the action for the partial history unroll.",
                "\u2022 Policy head \u03c0 \u03c8 : R N \u2192 R |A| , value head v \u03c8 : R N \u2192 R, and BYOL-Explore predictor g \u03b8 : R M \u2192 R N : These are each implemented as an MLP with hidden sizes of (512,), (512,), and (128, 256, 512,) respectively. The policy network uses different linear projections of the shared hidden layer to compute components of the policy over different parts of the action space. The action space has a mix of both discrete (modeled using a softmax layer of logits computed as linear projection of the hidden layer) as well as continuous (modeled as Gaussian distributions over each dimension with the mean and variance modeled using a linear projection of the hidden layer) actions, described in detail in prior works [22]."
            ],
            "subsections": []
        },
        {
            "title": "A.3 Detailes of Reward Normalization Mechanism",
            "paragraphs": [
                "We use a similar reward normalization scheme as in RND [9] and normalize the raw rewards (( j t ) T -2 t=0 ) B-1 j=0 by an EMA estimate of their standard deviation.",
                "More precisely, we first set the EMA mean to r = 0, the EMA mean of squares to r 2 = 0 and the counter to c = 1. Then, for the c-th batch of raw rewards (( j t ) T -2 t=0 ) B-1 j=0 , we compute the batch mean r c and the batch mean of squares r 2 c :",
                "We then update r, r 2 and c:",
                "where \u03b1 r = 0.99. We compute the adjusted EMA mean \u00b5 r , the adjusted EMA mean of squares \u00b5 r 2 :",
                "Finally the EMA estimation of the standard deviation is \u03c3 r = max(\u00b5 r 2 -\u00b5 2 r , 0) + \u03b5, where \u03b5 = 10 -8 is a small numerical regularization. The normalized rewards are j t /\u03c3 r ."
            ],
            "subsections": []
        },
        {
            "title": "B Baselines",
            "paragraphs": [
                "Random Network Distillation (RND) [9] is a simple exploration method that consists in training an encoder such that its outputs fit the outputs of another fixed and randomly initialized encoder and using the training loss as an intrinsic reward to be optimized by an RL algorithm. More precisely, let N \u2208 N * be the embedding size and let us note f \u03b8 : O \u2192 R N the encoder, also called predictor network, with trainable weights \u03b8 and f \u03c6 : O \u2192 R N the fixed and randomly initialized encoder, also called target network, with fixed weights \u03c6. In addition, let us suppose that we have a batch of",
                "collected by our RL agent, then the loss L RND (\u03b8) to minimize w.r.t.",
                "the online network parameters is defined as:",
                "and the unnormalized reward associated to the transition (o j t , a j t , o j t+1 ) is defined as j t = L RND (\u03b8, j, t+ 1) where 0 \u2264 t \u2264 T -2. To obtained the final intrinsic rewards, we just normalize them to be as close as possible to the original RND implementation:r j i,t = i t \u03c3r . Intrinsic Curiosity Module (ICM) [50] is a one-step prediction error method at the latent level. It consists in training an encoder f \u03b8 : O \u2192 R N that outputs a representation that is robust to uncontrollable aspects of the environment and then use this representation as inputs of a one-step prediction error model g \u03c6 : R N \u00d7 A \u2192 R N which error is used as an intrinsic reward to be optimized by an RL algorithm. To build a representation robust to uncontrollable dynamics, the idea used in ICM is to train an inverse dynamics model p \u03b8 : R N \u00d7 R N \u2192 A that predicts the distribution of actions that led to the transition between two consecutive representations f \u03b8 (o t ), f \u03b8 (o t+1 ). More precisely, let us suppose that we have a batch of trajectories",
                "collected by our RL agent, then the loss L INV (\u03b8) to minimize in order to train our encoder and inverse dynamcis model is:",
                "which is a simple cross-entropy loss. Simultaneously, ICM also trains the one step prediction error model by minimizing the following one-step prediction loss:",
                "and the unnormalized reward associated to the transition (o j t , a j t , o j t+1 ) is defined as j t = L ICM (\u03b8, j, t) where 0 \u2264 t \u2264 T -2. To obtained the final intrinsic rewards, we just normalize them :r j i,t = j t \u03c3r . "
            ],
            "subsections": []
        },
        {
            "title": "C Hyperparameter Settings",
            "paragraphs": [
                "Atari Settings We perform PopArt-style [32] reward normalization with a step size of 0.01, and after normalizing rescale the rewards by 1 -\u03b3. We also similarly use PopArt normalization on the output of the value network.",
                "We use a discount factor of \u03b3 = 0.999. To train the value function, we use VTrace without offpolicy corrections to define TD targets for MSE loss with a loss weight of 0.5. We add an entropy loss with a loss weight of 0.001. The VMPO parameters \u03b7 init and \u03b1 init are initialized to 0.5. \u03b5 \u03b7 and \u03b5 \u03b1 are set to 0.01 and 0.005 respectively. We scale the BYOL loss by a factor of 5.0 when combining losses.",
                "The VMPO top-k parameter is set to 0.5. We use the Adam optimizer with learning rate 10 -4 and b 1 = 0.9. The target network for VMPO is updated every 10 learner steps.",
                "We use a batch size of 32 and a sequence length of 128; and a distributed learning setup using 4 TPUv2 for learning and 400 CPU actors for generating data via another inference server using 4 TPUv2 to evaluate the policy, similar to Agent57 [4]."
            ],
            "subsections": []
        },
        {
            "title": "DM-HARD-8 Settings",
            "paragraphs": [
                "We perform separately PopArt for both the intrinsic and extrinsic rewards, on the multi-task suite of environments. We use a step size of 10 -5 for extrinsic popart and 0.01 for intrinsic popart.",
                "We use a discount factor of \u03b3 = 0.99. To train the value function, we use VTrace without offpolicy corrections to define TD targets for MSE loss with a loss weight of 0.5. We do not add an entropy loss term. We scale the BYOL loss by a factor of 1.0 when combining losses. The VMPO top-k parameter is set to 0.5. The VMPO temperature parameters \u03b7 init and \u03b5 \u03b7 are set to 1.0 and 0.1 respectively. For the discrete components of the action, the VMPO KL constraint parameters \u03b1 init_categorical and \u03b5 \u03b1_categorical are set to 5.0 and 0.0016 respectively. For the continuous Gaussian actions, we found it important to apply the VMPO contraints separately to the mean and covariance of the distributions, per action dimension. Thus for the mean, \u03b1 init_mean and \u03b5 \u03b1_mean are 1.0 and 0.001 respectively; for the covariance, \u03b1 init_covariance and \u03b5 \u03b1_covariance are 1.0 and 0.0001 respectively.",
                "We use the Adam optimizer with learning rate 10 -4 and b 1 = 0.0. The target network for VMPO is updated every 10 learner steps.",
                "We use a batch size of 48 and a sequence length of 80; and use a distributed learning setup consisting of 8 TPUv3 divided into 6 for learning and 2 for acting similar to the Sebulba [31] framework."
            ],
            "subsections": []
        },
        {
            "title": "D.1 Atari: Detailed results when mixing intrinsic and extrinsic rewards",
            "paragraphs": [
                "For more detailed analysis of our agent performance, we provide detailed results broken down per-task and by showing statistics of the human normalized score across games other than simply the capped mean.",
                "Table 1 and Figure 10 compare agent performance for BYOL-Explore and our various baselines considered, while Table 1 and Figure 11 compare BYOL-Explore to its various ablations.",
                "Tables 3 and4 show various finer-grained statistics of human normalized score averaged across all games, for BYOL-Explore compared to our baselines and its ablations, respectively.   "
            ],
            "subsections": []
        },
        {
            "title": "D.2 Atari: Detailed results for the pure exploration regime",
            "paragraphs": [
                "To further evaluate BYOL-Explore as an approach for curiousity-driven exploration, we examine the performance of an agent trained only using intrinsic rewards, without ever using the extrinsic   reward signal. In the task Montezuma's Revenge, Figure 4 measures the quality of exploraton in terms of the number of different rooms of the dungeon the agent is able to explore without using extrinsic rewards. However, computing this requires access to privileged information and detailed understanding of the environment design, and this kind of analysis is not easily adapted to a wider range of diverse games. Thus, as a proxy for engaging in interesting behavior in the environment, we measure exploratory behavior in terms of the extrinsic reward the agent obtains. As obtaining large amounts of extrinsic reward requires visiting many different regions of the state space and efficient exploration is highly correlated with this behavior, this is a reasonable measure for evaluating the quality of our agents. However, we stress again that though we measure performance in terms of the extrinsic reward, the agent is never given access to these reward signals while training.",
                "Table 5 shows a summary of performance in each of the 10 hard-exploration games we consider, while Figure 12 shows performance through training.",
                "Notice that BYOL-Explore is the only method to get positive rewards in Pitfall, showing that it can learn to effectively navigate the complex environment even without complex mechanisms such as Episodic Memory Modules used by prior works [4].   "
            ],
            "subsections": []
        },
        {
            "title": "D.3 DM-HARD-8 Experiments",
            "paragraphs": [
                "We similarly examine various statistics of agent performance on the DM-HARD-8 suite in terms of human-normalized score for BYOL-Explore versus our baselines (in Figure 6) and its ablations (in Figure 7).",
                "Next, we examine per-task performance across each task within the DM-HARD-8 suite for BYOL-Explore compared with our baselines (in Table 8 and Figure 6), and compared with our main ablations (in Table 9).",
                "We also conduct further finer-grained ablations of BYOL-Explore parameters K and \u03b1. Recall that the main experiments used K = 10 and \u03b1 = 0.99. Figure 14 shows that increasing the open-loop horizon from K = 1 to 16 smoothly improves performance, showing the benefits of multi-step prediction of the future and the robustness of BYOL-Explore to this parameter. Figure 15 shows the effect of varying the EMA target parameter \u03b1. We see that values BYOL-Explore is fairly robust to this hyperparameter, with the highest performance obtained by our base setting of \u03b1 = 0.99.",
                "Next, we examine different improvements to our main baselines of RND and ICM. Note that for ease of comparison, the results reported for the baselines are using the same architectures and VMPO  Similarly to Atari, in Figure 18, we also compare our mixed-intrinsic-extrinsic reward optimizing agent against agents optimizing purely intrinsic or extrinsic rewards. In the DM-HARD-8 domains, due to the much sparser reward than Atari, pure exploration in most tasks does not perform very well. However, remarkably it is still capable of achieving some positive signal on a handful of tasks, particularly Wall Sensor. Thus, purely maximizing intrinsic reward on DM-HARD-8 still results in meaningful exploration that leads the agent to display interesting behavior.",
                "Finally, we note that we consider the considerably harder multi-task setting compared to the singletask setting chosen in prior work [22]. To ensure that the small amount of similarity between tasks does not induce any useful positive transfer and thus make the tasks easier when trained on multiple levels, for the sake of completeness we also report in Figure 19 the performance on single-task setting.",
                "As expected, performance is actually slightly higher in the single-task setting, thus showing that our strong multi-task results are not the result of any simplifying changes. Note also that the multi-task  setting not only has to learn using a single set of parameters, it also receives 1  8 of the training data per task in the DM-HARD-8 suite since we compare based on total experience across all tasks being equal.",
                "Note that due to computational constraints we train the single-task version for 320000 learner steps, but this is sufficient to demonstrate that single-task is not a harder setting than multi-task.   "
            ],
            "subsections": []
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "paragraphs": [
                "We would like to thank Abbas Abdolmaleki, Arunkumar Byravan, Adri\u00e0 Puidomenech Badia, Tim Harley, Steven Kapturowski, Thomas Keck, Jean-Baptiste Lespiau, Kat McKinney, Kyriacos Nikiforou, Georg Ostrovski, Razvan Pascanu, Doina Precup, Satinder Singh, Hubert Soyer, Pablo Sprechmann, and Karl Tuyls for their support and advice in developing and publishing this work."
            ],
            "subsections": []
        }
    ]
}