{
    "id": "https://semopenalex.org/work/W2964437700",
    "authors": [
        "Jinho Lee",
        "Jianzhong Zhang",
        "Jeffrey G. Andrews",
        "Joonyoung Cho",
        "Vikram Chandrasekhar",
        "Yuqiang Heng"
    ],
    "title": "Experience-Centric Mobile Video Scheduling Through Machine Learning",
    "date": "2019-01-01",
    "abstract": "Providing a high quality video streaming experience in a mobile data network via the ubiquitous HTTP Adaptive Streaming (HAS) protocol is challenging. This is largely because HAS traffic arrives as regular Internet Protocol (IP) packets, indistinguishable from those of other data services. This paper presents real-time network-based Machine Learning (ML) classifiers incurring low overhead and capable of (a) detecting the service type of different flows including HAS, and (b)detecting the player status for users with HAS flows. We utilize random forests, an ensemble classifier, relying only upon standard unencrypted packet headers. By applying the ML classifier outputs to derive scheduling metrics, we show how existing LTE base-station schedulers can improve video Quality-of-Experience (QoE) while incurring minimal overhead. For a simulated LTE cellular network, we present quantitative performance results that include misclassification errors. Our classification and scheduling framework is shown to provide an improved video QoE with tolerable impact on other non-video best effort services. These design insights can be applied to optimize video delivery in current and future wireless networks.",
    "sections": [
        {
            "title": "I. INTRODUCTION",
            "paragraphs": [
                "Mobile data traffic continues to grow at a roughly 50% annual rate, mostly due to demand for video streaming services. By 2022, streaming video and immersive multimedia traffic are projected to comprise 82% of all mobile data traffic at 225 EB per month [1]. This growth imposes significant challenges for mobile network operators (MNOs) and content providers, given their constrained bandwidth and infrastructure. Some operators offer unlimited streaming video plans, but in reality they include de facto limits on the streaming video bit rates [2] and differentially charge for video flows depending on the content provider.",
                "In our view, rather than capping video bit rates and intentionally limiting the video quality, Radio Access Networks (RANs) should implement a more holistic, user-experience based and end-to-end approach consisting of (a) deploying network upgrades, e.g. 5G, (b) bringing video content closer to the end user, for example, via edge-caching, (c) detecting",
                "The associate editor coordinating the review of this article and approving it for publication was Wenjie Feng.",
                "HAS flows and their underlying application characteristics and (d) optimizing the scheduling of HAS flows based on their end-user experience. The focus of this paper is on the latter two topics.",
                "Streaming video delivery is dominated by over-the-top (OTT) services such as Amazon Video, Netflix and YouTube, which provide live video or video-on-demand on top of network infrastructure that is owned and maintained by service providers [3]. Typically, OTT providers employ HAS, which uses HTTP over Transmission Control Protocol (TCP) for content delivery over the Internet. Due to TCP's native rate control mechanisms however, streaming video traffic can experience increased delay and jitter. Additionally, since HAS traffic originate over the internet rather than the MNO managed network, they are typically scheduled as best effort traffic (Quality of Service (QoS) class identifiers 8 and 9 [4]). In congested scenarios, HAS users experience prolonged playback delay, frequent resolution changes and prolonged or frequent rebuffering [5]. We designate ''unhappy'' HAS users as users that are either awaiting playback to commence, or are experiencing degraded streaming video quality.",
                "The premise of this work is that the key to improving streaming video experience is detecting the presence of unhappy users and their playback state, and prioritizing their scheduling. For example, if a HAS user in Initial Buffering state is scheduled more often, it commences video playback sooner, and transitions to Steady-State where it requests new video chunks less often. This frees radio resources for serving other services (e.g. file download), and ensures tolerable service degradation.",
                "We apply Random Forest Classifiers (RFCs) for detecting unhappy HAS users and their playback status. Unlike prior works that require inter-packet arrival time information and the monitoring of packets in both uplink and downlink directions, our RFC solely relies on standard IP/TCP headers of downlink user packets. Our classifiers show high precision and recall rates, use simpler features compared to prior works, require low computational and memory overhead, and rely only on (unencrypted) packet headers of downlink packets. For these reasons, our RFC can be easily deployed on top of existing base station (BS) implementations. We implemented and tested several other classifiers, such as a multi-layer perceptron (MLP), support-vector machine (SVM) and logistic regression, but none were as effective or straightforward as the RFC."
            ],
            "subsections": [
                {
                    "title": "A. PRIOR WORK",
                    "paragraphs": [
                        "Conventional traffic classification solutions typically rely on rule-based schemes (e.g. match HTTP headers against a pattern) or Deep Packet Inspection (DPI). Rule-based schemes are vulnerable to free-riding and spoofing (see [2] for a comprehensive study). DPI involves inspecting the user packet data payload to see if the flow carries a well-known signature or other protocol semantics [6]. Most OTT traffic employ end-to-end crpytographic protocols which encrypt the application payload data. In such cases, DPI methods fail. In contrast, ML classifiers passively infer traffic profile characteristics by examining the unencrypted packet headers.",
                        "The works in [7], [8] provide a comprehensive analysis on real-time ML based network traffic classification. Their schemes however rely on non standard features (e.g. based on individual packet time stamps) or require monitoring flows from client to server and vice versa. In a practical implementation, bidirectional packet monitoring introduces a slightly unpredictable feature extraction latency since the processing of uplink (UL) and downlink (DL) packets from the same flow is affected by a number of factors including the available UL/DL channel bandwidths, radio channel conditions and the resource allocation policy.",
                        "More recent work [9] proposes ML classifiers for service classification and video player status detection. The scheme relies on multiple sliding windows for extracting temporal information within each HAS flow and has high detection latency due to the length of sliding windows. Similarly, the buffer level estimation algorithms proposed in [10] also has high latency (order of tens of seconds), rendering them unsuitable in practical BS scheduler implementations. In [11], a traffic classifier is proposed based on the temporal arrival pattern of Downlink Control Information messages in an LTE BS. This entails modifying the physical layer in existing implementations. ML algorithms for video stalling and resolution detection are proposed in [12]. However they rely on bidirectional flow features. Furthermore, stalling detection only allows for reactive methods while our objective is to take proactive actions at the scheduler for improving video Quality of Experience (QoE).",
                        "In addition to traffic classification, prior research works on improving video QoE can be categorized into client-centric and network-centric schemes. The former involves Adaptive Bit Rate (ABR) algorithms that either learn [13] or apply control rules [14]- [16] for making bit rate decisions. The latter assume network-level traffic awareness regarding users with HAS flows and their streaming video quality, queue lengths etc. In [17], a weighted proportional fair (PF) metric is presented where the weights are provided by an analytics server. However, the server requires access to realtime user plane and control plane data and BS logs. Such a requirement may not scale well in a practical implementation. The works in [18]- [20] derive scheduling metric based on video player state, however, they assume ideal knowledge of the HAS playback buffer size and video chunk encoding rates. Authors of [21] study video QoE under different wireless conditions. However they do not consider the competition between video and non-video users. Network slicing forms an essential aspect of current and next-generation communication networks [22] [23]. Through software defined networking and network function virtualization technologies, network slicing can be applied to provide a customized resource allocation based on end-user application requirements. The ML classifiers presented in this work can form the constituent components of network slices for providing application-specific QoE for mobile video users.",
                        "We briefly outline existing works applying ML and deep learning in communication systems. The work in [24] applies deep neural networks (DNN) for channel estimation and detection. An autoencoder-based physical layer encoder is discussed in [25]. A deep reinforcement learning (DRL) based scheduling algorithm for Internet of Things (IoT) traffic is proposed in [26]. In [27], DRL is applied for allocating resources among existing network slices. The work in [28] applies convolutional neural networks (CNN) to routing in wireless mesh networks (WMN). A survey of deep learning applications in communication can be found in [29]."
                    ],
                    "subsections": []
                },
                {
                    "title": "B. CONTRIBUTIONS",
                    "paragraphs": [
                        "The takeaway from this work is that the key to ensuring satisfactory streaming video QoE is fast and accurate detection of HAS users experiencing poor video resolution quality or awaiting playback, and prioritizing their scheduling. Our contributions are listed below. "
                    ],
                    "subsections": [
                        {
                            "title": "1) NETWORK TRAFFIC CLASSIFICATION",
                            "paragraphs": [
                                "We present real-time network traffic classifiers that can be deployed in a cellular network. The models employ Random Forests, a type of ensemble classifiers. Unlike [7], [9], [12] which require packet inter-arrival time (IAT) information and bi-directional packet monitoring, our features only rely on standard IP/TCP headers of downlink packets. A first RFC detects the service type for a newly detected flow. Two RFCs then respectively determine the player status and the video resolution. As performance metrics, we use the obtained recall and precision scores. The recall score refers to the fraction of video flows that are correctly detected. The precision score refers to the fraction of detected video flows that were correctly classified. Performance results show our RFCs deliver high precision and recall scores."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "2) PRIORITIZED SCHEDULING",
                            "paragraphs": [
                                "We present a simulated LTE base station scheduler that applies the RFC outputs for prioritizing scheduling of HAS users based on their detected player state. For realistic modeling and to evaluate the service level impact on other traffic classes, we consider two traffic classes namely HAS video and File Downloads (FTP traffic). Our results show the modified scheduler provides improved QoE, including reduced playback delay and video Mean Opinion Scores (MOS), relative to a conventional PF scheduler. The QoE gains are mainly influenced by the classifier's recall rate of detecting HAS flows, while its precision has marginal impact. Simulation results show that under our considered scenarios, the total transported traffic volume for both traffic classes stays about the same suggesting that the prioritized scheduling of HAS users has tolerable service-level impact on File Download users."
                            ],
                            "subsections": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "II. SYSTEM MODEL",
            "paragraphs": [
                "The system model consists of one or more BSs each serving multiple users sharing common radio frequency spectrum. Each user's traffic originates from the internet and is composed of one or more than one traffic flows. The terminology ''flow'' defines a bi-directional session between two hosts, uniquely identified by the IP five-tuple {protocol (UDP or TCP), client-IP-address, server-IP-address, client-Port, server-Port} [8]. Each flow maps to a certain traffic session e.g. Web, Email, Chat, FTP, Progressive Video, etc. Streaming video users' traffic is hosted at a HAS server which could be hosted at a content distribution network or a HTTP proxy server.",
                "Figure 1 shows the processing work flow. A first RFC examines DL packet headers once a new flow is detected, and assigns its service category to one among a predetermined number of classes. For streaming video flows, an RFC estimates their client playback buffer status (whether in Buffering, Steady-State) and their video resolution quality (whether in {144p, 240p, 360p, 480p, 720p, 1080p}). The scheduler module applies a user-specific prioritization weight to HAS users on top of their PF scheduling metric. This user-specific prioritization is useful for the following reasons:",
                "1) Reducing the play out delay during Buffering state.",
                "2) Increasing video resolution quality when the current quality lies below a threshold (e.g. 360p)."
            ],
            "subsections": [
                {
                    "title": "A. PRIMER ON HAS",
                    "paragraphs": [
                        "A HAS server stores its video content in multiple encoded bit rates, corresponding to different representation levels, and broken into small chunks. Each video chunk corresponds to a few seconds (typically between 1 second -15 seconds [3]) of video playback data. A DASH compliant server extracts each chunk at run-time depending on the client's requested encoding bit rate. The server transfers its video file to the HAS client sequentially on a chunk-by-chunk basis."
                    ],
                    "subsections": [
                        {
                            "title": "1) CHUNK TRANSFER MECHANISM",
                            "paragraphs": [
                                "In a typical HAS session, the client player initiates an HTTP request containing details of its desired video clip for playback. The server transfers to the HAS client, a Media Presentation Description containing metadata of audio and video representations for the stored video including available chunk encoding rates. The client player communicates its desired encoding bit rate to the server, through a HTTP GET request, while requesting a subsequent chunk. The player continuously buffers chunks in its playback buffer; upon buffering sufficient data, it concurrently plays back its video."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "2) ADAPTIVE BIT RATE (ABR) ALGORITHMS",
                            "paragraphs": [
                                "The client player employs ABR algorithms to dynamically adapt their encoding bit rate per video chunk. This allows the streaming video encoding rate to adapt to the underlying network conditions which are affected by various factors such as congestion (at the transport, routing and medium access layers), channel fading and interference (physical layer)."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "3) HAS CLIENT PLAYER STATUS",
                            "paragraphs": [
                                "The client player requests new chunks differently depending on the status of its playback buffer. In Buffering state, the player requests a new chunk as soon as the previous chunk was downloaded. This enables the player to maximize its chunk request rate for building its playback buffer to a sufficient amount. Subsequently, the player transitions to Steady-State. In Steady-State, the player aims to maintain its playback buffer constant. Denoting one chunk duration as \u03c4 , the player requests a new chunk either \u03c4 seconds after its previous chunk request (if it takes less than \u03c4 to download the previously requested chunk) or as soon as the previously requested chunk is received (otherwise) [30]."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "B. ALGORITHM FLOW",
                    "paragraphs": [
                        "The algorithm flow of our experience-centric scheduling solution is shown in Fig. 2. Upon detecting the arrival of a new traffic flow destined for a particular user, the ML traffic service classifier extracts relevant features from the packets and determines whether the flow is generated from HAS traffic or not. The ML HAS state classifiers then extract features from packet headers belonging to detected HAS traffic flows. They subsequently determine the playback status and the video resolution quality at the client player belonging to the HAS user. HAS users that are detected lying in Buffering state, or whose resolution quality falls below a predetermined threshold are classified to be in Unhappy state. The scheduler prioritizes radio resource scheduling of Unhappy users. Other users are determined to be in Happy state. The scheduler applies no such prioritization for Happy users."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "III. ML TRAFFIC CLASSIFICATION",
            "paragraphs": [
                "The first step to providing an experience-aware RAN is detecting the type of traffic services, and their underlying characteristics at the end-user. We formulate this as a supervised-learning problem for (a) classifying the service category per traffic flow, and (b) detecting the player status and resolution quality for video flows. We designed the classifiers specifically for real-time user plane processing at practical BSs. First, to keep low overhead, we trained the classifiers to operate only using downlink packets. Second, the classifiers only access the packet headers and not their (encrypted) payloads. Last, to ensure the scheduler can adapt its allocation in real-time, the classifiers are designed to output the player state and video resolution with low latency. A compelling aspect of our classifiers are their simplicity and low latency. The traffic service classifier operates using just 3 features, derived from TCP/IP packet headers of a newly detected flow, and outputs its service type, using the first 5 downlink packets for that flow. Since UL packets are not processed (e.g. as in [6], [9], [12]), factors such as congestion and the radio channel conditions on the UL do not impact the feature extraction latency. The HAS state classifier operates only on the number of downlink packets and their cumulative payload sizes, both of which can be derived easily, during small time windows. The proposed classifiers do not need packet IAT information, which requires significant computation and memory overhead."
            ],
            "subsections": [
                {
                    "title": "A. CLASSIFIER DESIGN",
                    "paragraphs": [
                        "We use Random Forest Classifiers (RFCs) for both classification problems. The RFC is a simple ensemble classifier that trains multiple decision trees on various bootstrap samples of the training dataset. We choose RFC because of their proven capability in multi-class classification problems. Authors of [7] and [8] show that C4.5 trees can achieve high traffic classification accuracy while being one of the fastest algorithms. We expect RFC to achieve similar results. We have also tested other classification models including logistic regression, SVM and MLP. Logistic regression and SVM have significant lower precision and recall compared to RFC. While our tuned MLP models can achieve similar performance compared to the RFCs, the additional computation complexity and potential need for special hardware such as graphics processing units (GPUs) make them costprohibitive in BS implementations. We implemented our models in Python using the scikit-learn library. The classifiers provide over 95% accuracy for service classification and player status detection, and over 90% accuracy for video resolution detection."
                    ],
                    "subsections": [
                        {
                            "title": "1) TRAFFIC SERVICE CLASSIFIER",
                            "paragraphs": [
                                "The service category of packets arriving at BS queues provides the scheduler with information about the type of service consumed by users. The scheduler can intelligently prioritize certain packets to be delivered to better cater to the QoS requirements of users. We designed an RFC to determine the service category of every new traffic flow. The RFC relies on features extracted from the first 5 downlink packet headers of user plane packets belonging to a newly detected traffic flow. This guarantees low latency for service classification. We trained our RFCs using the dataset provided in [31], originally containing 249 features. We iteratively eliminated features with the lowest feature importance scores in the RFCs. We then compared our features with the 12 features used in [6] and selected a feature subset that requires monitoring of only downlink packet TCP/IP headers.",
                                "The feature set presented to the RFC comprises of (a) Server and Client port numbers (via the TCP Source Port and Destination Port fields), (b) Number of packets with PUSH bit set (contained within the TCP Flags field) and (c) the median IP packet size (derived from the IP Total Length field)."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "2) HAS STATE CLASSIFIER",
                            "paragraphs": [
                                "Streaming video player state and resolution detection provides the scheduler with necessary information to minimize video playout delay and to increase resolution for video users. We have designed two RFCs, that process packet headers of video flows, for detecting their client player status and the video resolution. The first RFC determines the player status within {Buffering, Steady-State}. The second RFC is invoked whenever the detected player status is Steady-State. This classifier assigns one video resolution quality within {144p, 240p, 360p, 480p, 720p, 1080p}.",
                                "Similar to [9] and [12], the feature set is derived from the temporal statistics of video flows. The RFCs are parameterized by two numbers, the sampling interval duration T w (units: seconds) and number of samples n. Each feature sample x is a vector of length n whose entries are obtained by observing user plane packets during the preceding n \u2022 T w seconds. At time t = m \u2022 T w , entry 1 \u2264 j \u2264 n corresponds to the observed total number of packets and/or the total packet size during the time interval [(mn + j -1) \u2022 T w , (mn + j) \u2022 T w ), j > 0. This minimizes feature complexity as the features are derived by observing packet headers and is independent on the transport layer protocol (UDP/TCP). Whenever the number of observations k is fewer than n, the ''missing'' entries in x are simply filled by the average of the k observations. We use a sliding window approach: data collected in the previous n -1 time intervals are stored after the initial n -1 time intervals. The data collected in the most recent T w seconds is combined with that in the previous n -1 time intervals to construct a feature sample. This ensures that the detection latency is bounded by the sampling interval duration T w . Larger n means a larger dimension of the feature set, hence it often increases the model complexity. On the other hand, larger T w increases the classifier's latency. Note that smaller n does not necessarily reduce classification complexity (may produce deeper trees).",
                                "Compared to [9] and [12], our features are significantly simpler due to the following reasons: (a)the features do not rely on packet IAT information and (b) the features only rely on the number of DL packets and/or the total DL packet sizes. Furthermore, the two RFCs use the same features with different sampling frequencies for reduced complexity during feature extraction. The video resolution classifier extracts features with a longer sampling interval duration that is an integer multiple of T w , hence if it is invoked, the samples collected for the player status classifier can simply be aggregated to obtain the required features of the resolution classifier."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "B. EVALUATION",
                    "paragraphs": [
                        "We evaluate the performance of the proposed ML traffic classifiers both on public datasets and our own experimental testbed. We designed the ML models and the feature sets on two large published datasets. We also implemented the traffic classifiers on a testbed simulating an LTE user plane packet processor to validate the performance of the proposed classifiers on realistic traffic data. We use 10 discriminators in the RFCs and use Gini impurity as the split metric."
                    ],
                    "subsections": [
                        {
                            "title": "1) PUBLIC DATA SETS a: TRAFFIC SERVICE CLASSIFICATION",
                            "paragraphs": [
                                "We use the University of Cambridge dataset published in [6] and [31] for traffic service classification. The network data is collected at a research site across different time periods. It contains a total of 11 hours worth of TCP network traffic including 107 M packets from 812k flows [6]. The labels are created in a supervised manner using DPI. We consider 7 traffic service classes and perform stratified sampling on the dataset to ensure balanced training data for each class. For initial model selection, we evaluate the performance of a model on the original 12 features presented in [6]. For RFC, we use 10 discriminators and Gini impurity as the split metric. For the MLP, we use 3 hidden layers with sigmoid activation. We train the MLP using ADAM [32] on cross-entropy loss for 200 epochs. We use an initial learning rate of 0.001 and reduce the learning rate once validation accuracy plateaus. Without model tuning, the MLP achieves a 5-fold cross-validation accuracy of 86.3% while RFC achieves 97.2%. Note that we use a larger set of features for initial model selection and deliberately did not fine-tune the models. Overall, the RFC is the superior classifier since it achieves higher accuracy and is also easier to train.",
                                "Table 1 presents the RFC performance. The first performance metric is the recall score which refers to the perclass probability that flows belonging to a given service class are correctly categorized. The second metric is the precision score which refers to the per-class probability that the service class output by the RFC is correct. We use 90% of the data for training and validation and the rest for testing. The recall and precision scores are calculated on the testing data. The classifier achieves good precision and recall of over 0.9 for all service classes. The overall mean classification accuracy across 5-fold cross-validation is approximately 98%."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "b: HAS STATE CLASSIFICATION",
                            "paragraphs": [
                                "We use the dataset published in [33] for HAS state classification, which is also used in [9]. This dataset contains packets captured during YouTube video streaming on smart phones. It includes labeled data from a total of 93 hours of streaming multiple videos at different resolutions under various network conditions. The video state labels are created manually and the resolution labels are extracted from the YouTube application when streaming.",
                                "We train an RFC to classify the play state within {Buffering, Steady-State} and another RFC to classify the video resolution within {144p, 240p, 360p, 480p, 720p, 1080p}. We perform stratified sampling to ensure the amount of data for each class is balanced. We train the RFCs with 10 trees and Gini impurity as the split metric. We also present the results from MLPs for play state and resolution classification for initial model selection. We use 2 hidden layers with sigmoid activation and train using ADAM [32] on cross-entropy loss for 200 epochs. We use an initial learning rate of 0.001 and reduce the learning rate once validation accuracy saturates. We did not fine-tune the MLP for initial model selection purposes.",
                                "Tables 2 and4 show the average 5 fold cross-validation accuracies of 0FC S for different values of n and T w . The minimum latency for player status classification with a 95% target accuracy is T w = 0.1 s and the minimum latency for video resolution classification with a 90% target accuracy is T w = 0.5 s.",
                                "Tables 3 and5 show the average 5 fold cross-validation accuracies of the MLPs for different values of n and T w . The MLP achieves slightly worse accuracy in player status classification compared to the RFC. For video resolution classification, the RFC out-performs the MLP by a significant margin. While it is possible to fine-tune the MLP model to improve its performance, we believe it would ultimately be futile since the RFC can already achieve good performance and MLPs are considerably more difficult to train. We use python control scripts to automatically generate internet traffic corresponding to the aforementioned traffic profiles on a laptop connected to the internet via Wi-Fi. We use PyShark [34]  Table 6 presents the classification accuracies obtained with the RFCs. Each RFC has 10 trees. The service classifier achieves a 5-fold cross-validation accuracy of 94%. Considering three resolution classes {High-Def, Med-Def, Low-Def }, the RFC for resolution detection has a 5-fold cross-validation accuracy of 87%. With two resolution classes, the accuracy improves to 95%."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "C. COMPUTATION AND MEMORY REQUIREMENTS",
                    "paragraphs": [
                        "We also evaluate the computational and memory requirements for the classifiers. Assuming n data samples, v features and m trees in the RFC, the training complexity is O(mvn log n) [35]. This is because the depth of a balanced tree with n leaves grows at rate O(log n). All n data samples need to be considered at each depth level of the tree in the worst-case scenario and all v features need to be considered at each node. Hence the complexity of building 1 unpruned tree is O(vn log n) and that of building m trees is O(mvn log n). Assuming all trees have depth k, the inference complexity of RFC is O(mnk) since we need to traverse the depth k of m trees for all n data samples. In comparison, assuming quadratic matrix-vector multiplication complexity and bias computation dominated by weights computation, an l-layer MLP with w nodes in each layer trained for n epoch epochs has a training complexity of O(n epoch nlw 2 ) and an inference complexity of O(nlw 2 ). This is because for all n data samples, each forward pass involves l matrix-vector multiplications to calculate the values at each node of the l layers which has O(nlw 2 ) complexity. For backward pass, starting from a previous layer, updating the current layer involves a matrixvector multiplication for the bias and an additional vector outer product for the weights, which has O(nlw 2 ) complexity. During training, a forward and a backward pass are performed n epoch times. During inference, only 1 forward pass is performed. In terms of computation and memory resources, assuming 500 newly arriving flows per second and 1000 ongoing video sessions, the proposed traffic profiling solution requires 12.7 Million Instructions Per Second (MIPS) and 2.26 MB of storage, as shown in Table 7. This equates to approximately 0.05% CPU utilization with an Intel i7 6700K CPU (25,476 MIPS).",
                        "For the traffic service classification, our proposed RFC consists of 10 trees with a maximum total depth of 246 nodes. In a practical implementation, a BS packet processor copies the TCP and IP packet headers of the first 5 packets belonging to a newly detected flow. It writes the contents into a reserved memory area and activates the RFC. The worst case CPU overhead is 367 instructions for feature extraction and classification of each flow. The memory overhead is 212 bytes for extracting features from each flow and 21,456 bytes for storing the 5364 node coefficients in the RFC. Assuming 500 new traffic flows arrive sequentially each second, the total CPU overhead is 0.1835 MIPS and the total memory overhead is 21,668 bytes, as shown in Table 7.",
                        "For player status detection, we assume sampling interval T w = 0.1 second and number of samples n = 5 to achieve a target accuracy of 95%. The memory overhead is 20 bytes for feature extraction in each video session and 120,624 bytes for storing the 30,156 nodes in the RFC. Assuming 1000 concurrent video sessions, the total CPU and memory overheads are 5.71 MIPS and 0.14 MB, as shown in Table 7. For video resolution detection, we assume sampling interval T w = 1 second and number of samples n = 15 to achieve a target accuracy of 90%. We also assume the 0.1 second samples from player status detection are available so that we only need to maintain and aggregate features used during player status detection. With a total maximum depth of 541 across 10 trees in the RFC, the CPU overhead for each video session is 3320 instructions per second for feature extraction and 3530 instructions per second for classification. The memory overhead is 660 bytes for feature extraction in each video session and 1.43 MB for storing the 358,206 nodes in the RFC. Assuming 1000 concurrent video sessions, the total CPU overhead is 6.85 MIPS and memory overhead is 2.09 MB, as shown in Table 7."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "IV. EXPERIENCE-CENTRIC TRAFFIC SCHEDULING EXPERIENCE-CENTRIC TRAFFIC SCHEDULING",
            "paragraphs": [
                "This section presents a simple approach for adapting the radio resource allocation at a LTE BS scheduler, to improve streaming video QoE, using the ML classifiers described in Section III-A. Figure 1 shows the overall framework. User plane traffic for each user arrives from the best effort internet, therefore, it is enqueued at a single bearer queue per user. The BS packet processor uses the RFC outputs to provide the scheduler with the estimated service category per traffic flow, and the detected player status and video resolution quality at HAS users.",
                "We assume wide-band scheduling of one user over a single time resource (one LTE subframe or 1 ms). We also assume that the BS derives a single scheduler metric per user, regardless of whether the user has one or more than one traffic flows. When scheduling at subframe n for user j, the scheduler applies traffic-dependent weights v j [n] and w j [n] on top of the (scalar) PF scheduling metric. These weights are nominally set to 1 in case the service type for that user's flow does not correspond to streaming video traffic. For a user with HAS traffic flow, the weight v j [n] is set based on its player state, while w j [n] is set based on the video resolution quality for its traffic player at time n.",
                "The player status and video resolution quality level output from the ML classifier are applied at the scheduler to categorize HAS users into two states. A HAS user j is detected to be in Unhappy state -and categorized as an unhappy userwhenever one of the following conditions is detected namely 1) its video playback is yet to begin (equivalently, the ML classifier detects its player status as lying in Buffering state) or 2) its player status is detected as Steady-State and its video resolution quality lies below a threshold. If neither of these conditions are met, or if user j is not engaging in a HAS session, then j is declared to be in Happy state, and categorized as a happy user.",
                "Our proposed scheduler prioritizes HAS users -according to their user experience -by assigning user-specific, timevariant weights, set either to 1 (for happy users) or greater than 1 (for unhappy users). During scheduling for unhappy user j at subframe n, the scheduler weights their scheduling metric with v j [n] > 1 (if Condition 1 is satisfied) and w j [n] > 1 (if Condition 2 is satisfied). Denoting d j [n] as the instantaneous rate and R j [n] as the throughput for user j in subframe n, the scheduler chooses a user j according to",
                "Following scheduling a user j according to equation 1 at n, the (exponentially averaged) throughput R j [n] is updated as follows:",
                "parameters \u03b1 in equation 1 and \u03b2 in equation 2 are PF scheduler specific non-negative parameters. The \u03b1 parameter controls fairness: smaller value favors users with higher rates while setting \u03b1 \u2192 \u221e assigns equal priorities to all users. The \u03b2 parameter controls the update rate for calculating per-user throughput. In this work, \u03b1 = 1 and \u03b2 = 0.98.",
                "We present two experience-centric scheduling schemes referred to as Strict Priority (SP) scheme and Weighted Proportional Fair (WPF) scheme respectively. These schemes adapt the weights weights v j and w j as a function of the happiness level, determined by the ML player status and resolution quality detection, at video user j. As comparison, we compare these two schemes against a Baseline scheduling scheme.",
                "SP: An SP scheduler gives highest priority for scheduling unhappy video users in each subframe. It does so by applying prioritization weights of w j = \u221e or v j = \u221e on top of their PF metric, whenever user j is detected as an unhappy HAS user. If multiple unhappy HAS users are simultaneously detected in given subframe, a single unhappy HAS user is prioritized uniformly randomly.",
                "WPF: A WPF scheduler applies fixed prioritization weights of v j = 4 or w j = 4 on top of their PF metric if user j is detected as an unhappy HAS user. Our analysis did not attempt optimizing the values of these weights. For both the SP and WPF scheduling approaches, whenever user j is a happy user, their prioritization weights v j and w j are identically set to one.",
                "Baseline: The baseline scheduler implements a PF scheduler typically used in current LTE BS scheduler implementations. Such a scheduler prioritizes all users equally (i.e. \u2200j, w j = 1, v j = 1) whether or not they are engaged in a HAS session. This approach is used as reference for comparing against the experience-centric scheduling approaches."
            ],
            "subsections": []
        },
        {
            "title": "V. RADIO SYSTEM SIMULATION",
            "paragraphs": [
                "This section presents radio system simulation results of the experience-aware schedulers. We consider the Baseline, WPF and SP schedulers. We have three objectives. The first is to characterize the video QoE gains by incorporating their user experience during cellular scheduling. Second, we evaluate the performance trade-off when HAS users are prioritized at expense of other traffic (e.g. file download). Last, we evaluate the system level impacts arising from misclassifying HAS users as Download users and vice versa.",
                "Section V-A describes the considered performance metrics. Sections V-B and V-C respectively describe the physical layer and user traffic modeling aspects of the simulator. Section V-D provides implementation details of the HAS server and client, including the ABR algorithm. Sections V-E describe the classification modeling, and V-F presents radio system simulation results."
            ],
            "subsections": [
                {
                    "title": "A. METRICS OF INTEREST",
                    "paragraphs": [
                        "In our simulations, we model users with two types of traffic namely HAS and File Downloads. For both sets of users, we evaluate the traffic volume metric which corresponds to the total transported data within the simulation time. The volume metric is a key performance indicator at MNOs as it indicates the service-level performance of their network.",
                        "The performance metrics for HAS users include their initial playback delays and video Mean Opinion Score (MOS) [5], a number between 1 (poor video quality) and 5 (excellent video quality). The MOS subjectively assesses video QoE, derived from the initial playback delay, the video stalling duration and the video stalling frequency. Our simulator implemented the International Telecommunication Union's ITU P-1203 specification [36] for estimating video MOS from the video quality and delay statistics of a video session. For File Download users, the metric of interest is the User Perceived Throughput (UPT). The UPT is obtained by normalizing the total served data for a user by the total time taken for delivering it (including Hybrid-ARQ re transmissions), counting only the times when that user is scheduled."
                    ],
                    "subsections": []
                },
                {
                    "title": "B. PHYSICAL LAYER MODELING",
                    "paragraphs": [
                        "We evaluate the proposed schedulers in a typical LTE scenario over 10 MHz bandwidth and Time Division Duplex operation. A Rel-13 LTE compliant MATLAB based dynamic system-level simulator was used. The simulation parameters are shown in Table 8. Results were averaged over 16 different drops, each drop run for 40 second time duration. To reduce simulation complexity, traffic and scheduling was restricted to a single randomly chosen cell while the rest generating random background interference as if they carry full buffer traffic. The simulation parameters are shown in Table 8."
                    ],
                    "subsections": []
                },
                {
                    "title": "C. USER ACTIVITY MODEL",
                    "paragraphs": [
                        "The 20 users were grouped at random into 5 HAS users and 15 File Download users. Each HAS user alternates between watching a HAS video clip and staying idle. Through controlled experiments, the proportion of HAS users and their idle-to-watching duration was chosen to ensure around 70% average resource utilization. The video clips were chosen randomly between two animated sequences Big Buck Bunny (14,315 video frames) and Elephants Dream (15,691 video frames). The viewing time per clip is uniform random within the time range [4,6] seconds. The idle time is uniform random within the time range [1,3] seconds. After the idle time elapses, the user watches the next video clip for a random time interval, and so on.",
                        "For HAS traffic modeling, we adopted a trace-driven methodology [37], [38]. The traces are stored in H.264 Scalable Video Coding (SVC) single layer format [38], available from the trace library at Arizona State University [39]. The video frame rate equals 24 frames/s and the Group of picture (GoP) size was 16 frames. Each trace is encoded at 9 different resolution levels with resolution level 1 corresponding to the  highest video encoding rate. For both animation sequences, the highest resolution quality level (= 1) corresponds to an average encoding bit rate (per video chunk) of about 1.8 Mbits/s and a peak encoding rate of about 14 Mbits/s).",
                        "The lowest resolution quality level (= 9) corresponds to an average encoding bit rate of about 0.02 Mbits/s and a peak encoding bitrate of about 0.38 Mbits/s.",
                        "The GoP structure was G16B15 indicating that every group of 16 video frames consists of one intra-coded (or I) frame and 15 bi-directionally predictive coded (B) frames. In the simulations, for modeling video transport over IP, we modeled 20 byte TCP and IP headers. Video frame (s) are encapsulated within a packet with a maximum transfer unit size of 1500 bytes.",
                        "File download activity is modeled according to the 3GPP's FTP Traffic Model 2 [40] with a 0.5 MB file size. After a file is downloaded, the user is assumed to ''read'' the file over a time modeled as an exponential random variable with mean 5s. After the reading time elapses, the user downloads the next 0.5 MB file and so on. If the file download takes more than 8s, the file is dropped and any previously downloaded file contents are discarded for throughput computation; subsequently the next file download begins."
                    ],
                    "subsections": []
                },
                {
                    "title": "D. HAS SERVER AND CLIENT PLAYER MODEL",
                    "paragraphs": [
                        "We implemented a simple model for a HAS server and a client player at each HAS user. When a new chunk request is received, the HAS server loads one chunk worth of video data from stored video traces. The chunk duration was chosen to correspond to an integer multiple of the GoP size. In this work, we considered 2/3 second chunk sizes (or 16 video frames).",
                        "The simulator models the client player state machine corresponding to the Buffering and Steady-State states. The player enters Buffering state whenever its playback buffer is empty. Video data is held until the user receives an integer number of GoPs; subsequently the video data is enqueued at the playback buffer. The player resides in the Buffering state until the playback buffer has 4 seconds worth of data. Subsequently, the player beings playing the video and transitions to Steady-State. During playback, the simulator deprecates the player's buffer by one LTE subframe (1 ms) worth of video data. Each time a new GoP arrives, the buffer size is incremented by one GoP duration in time (or 2/3 second). During any time the player buffer is empty, the player transitions back to the Buffering state.",
                        "The simulator accurately models chunk requests as a function of the player state. During each chunk request, the player indicates to the server, its preferred chunk encoding bit rate among the 9 available encoding rates as described in Section V-C. Given the measured IP throughput over the last received chunk equals R ip and the chunk encoding bit rate at index j equals R j , the client's ABR algorithm selects an index j = arg min j {1 \u2264 j \u2264 9 : R j \u2264 R ip }.",
                        "For prioritized scheduling of HAS uses, the simulator models ideal detection of the HAS player status and video resolution quality, via the HAS state classifier, at the BS. The experience-centric schedulers assign HAS users as belonging to unhappy state if their player either is in the Buffering state, or lying in Steady-State, yet their video resolution quality index is at or below 5 (average encoding bit rate of 0.2 Mbits/s)."
                    ],
                    "subsections": []
                },
                {
                    "title": "E. FLOW CLASSIFICATION MODEL",
                    "paragraphs": [
                        "Our evaluations considered both ideal and non-ideal service classification scenarios. In the former, the RFC accurately detects the set of HAS users and File Download users. Thus, the scheduler applies the correct prioritization weights for users belonging to either category.",
                        "For the latter, we considered two extreme scenarios for evaluating the recall versus precision trade-off for video flow  detection, and its ensuing impact on system performance. In Misclassification Scenario 1, one HAS user, chosen uniformly randomly among the pool of 5 HAS users, is modeled as being misclassified as a File Download user. This models a service classifier with a video recall score -or the fraction of video flows that are correctly classified -equaling 80%. The affected user receives no scheduling prioritization regardless of their client player status and video resolution quality. In Misclassification Scenario 2, three randomly chosen File Download users are modeled as being misclassified as HAS users. Consequently such users receive identical scheduling prioritization as an unhappy HAS user. This models a service classifier with a video precision score -or the fraction of detected video flows that are correctly classified -equaling 80%. The above scenarios represent a conservative model of service classification performance; in reality, as shown in Table 6, the empirically observed recall and precision rates obtained by the RFCs were over 85% in most scenarios."
                    ],
                    "subsections": []
                },
                {
                    "title": "F. SIMULATION RESULTS",
                    "paragraphs": [
                        "We first consider the case of ideal flow classification. Figs. 3(a), 3(b) and 3(c) respectively plot the cumulative distribution functions (CDFs) of the initial playback delay (s), video MOS and average chunk representation quality levels across different scheduling schemes. The playback delays for both the median (50 percentile) and worst (95 percentile) users are improved by over 30% for the WPF scheme and by over 50% with the SP scheme. The MOS scores are improved by 0.44 (12.1%) for worst 5-percentile users and by 0.15 (3.5%) for median users with SP scheduling. The CDFs for the chunk representation quality level for the experiencecentric scheduling schemes lie to the left of the baseline scheduling scheme, indicating that HAS users view their video with higher average video quality. Prior works [41] indicate that our MOS score gains are significant in terms of end-user perceived video quality.",
                        "We proceed to consider impact under service misclassification scenarios. Table 9 presents the MOS values covering Misclassification Scenarios 1 and 2. In Scenario 2 (where three randomly chosen Download users are misclassified as HAS users), the experience-centric schedulers deliver sizable video QoE gains as the Ideal classification scenario. This suggests that the QoE gains from experience-centric scheduling are not sensitive to the ML classifier precision scores (i.e. fraction of video flows that are correctly detected). On the other hand, considering Scenario 1 (where a single randomly chosen HAS user is misclassified as a Download user), the video QoE gains are significantly smaller compared to the ideal classification scenario. The takeaway is that that ML service classifier design should maximize the video recall score (or probability of detecting video flows) to realize the QoE gains from experience-centric scheduling.",
                        "We now analyze UPTs at Download users and HAS users as shown in Table 10. We observe that the 5 percentile UPT at HAS users exceeds 3.5 Mbps under both ideal and misclassification scenarios. Given the average video chunk encoding rates at the highest representation level equals 1.8 Mbps, this suggests that even the worst-off HAS users are able to download their video chunks with high quality.",
                        "As expected, we observe that the experience-centric scheduling schemes trade-off UPTs at File Download users for improved UPTs at HAS users. The 5%-ile UPT degradation varies between 20% (Ideal classification), 10% (Misclassification Scenario 2) and 30% (Misclassification Scenario 1). As an additional performance metric,  the traffic volume metric is of interest at MNOs as it indicates the service-level performance of their network.",
                        "In Figs. 4(a) and 4(b), assuming ideal performance, we provide the total transported traffic volume of HAS and File Download traffic. The experienceoptimized scheduling schemes deliver almost identical traffic volume for both HAS and File Download traffic as the baseline scheduling scheme. We now provide clarifying explanations as to why the File Download traffic volume stays relatively unchanged across the baseline and experience-centric scheduling schemes. The first reason is that by prioritizing scheduling of HAS users in Buffering state, the experience-centric scheduling schemes helps their client player for such users to quickly fill their playback buffers. This reduces the playout delay and enables the player to transition to Steady-State where it requests newer chunks relatively less often. The reduced frequency of chunk requests increases scheduling opportunities for File Download users and offsets their UPT degradation arising from serving HAS users with higher priority. The second reason is that the idle time (resp. reading time) across video chunk requests (resp. file requests) reduces the service-level impact caused to File Download users from prioritized scheduling of HAS users. HAS users spend between 1-3 seconds of idle time between downloading consecutive video clips. File Download users spend an exponentially distributed reading time with 5 second average duration once their file is downloaded.",
                        "For manageable simulation complexity, our simulations have considered a limited set of HAS users (5 HAS users among a total of 20 downlink active users). In scenarios with a larger number of HAS users, a practical QoE-centric scheduler should also consider the ensuing fairness impact on Download users while operating on a constrained resource budget. Our work has not considered this important aspect. A simple heuristic to ensure fairness is to prioritize HAS users as a function of their current resolution level, as done in this work. An acceptable trade-off could, for example, capture the notion that for some users the marginal improvement in perceived video quality decreases at higher bitrates [13], [42]. Therefore, it may be preferable to prioritize HAS users with low video resolution quality (e.g. 240p), even if it results in HD video users (720p and higher) receiving a lower video resolution quality (e.g. 480p)."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "VI. DISCUSSION AND FUTURE EXTENSIONS",
            "paragraphs": [
                "The key advantage of the proposed framework is that it is lightweight and modular in nature. The traffic classifier only needs a few downlink packet headers for deriving its features, implying its detection latency is very small, on the order of a few packets. The ML classifiers employ simple features that are easily extracted from standard downlink user packet headers. Since there is no need for bidirectional packet monitoring, the ML classifiers can be easily added on top of existing BS scheduler implementations. Last, the modular nature of an alternative classior scheduler, e.g. token-bucket scheduler can be used without affecting the other components. The main weakness of the current framework is that it only detects the service type per flow, and can only differentiate between HAS users and non-HAS users. Our state classifiers currently provide end-user experience information, in terms of video playback buffer state and video resolution quality, which is only relevant to HAS traffic.",
                "Future work could extend this framework to passive detection of other service categories such as augmented and virtual reality and deriving service-specific QoE information for cellular scheduling. While the feature overhead and associated memory and CPU complexity incurred by such extensions remains unclear, the modular nature of our framework makes this feasible. Secondly, ML algorithms require high quality data to achieve good performance. To the best of our knowledge, none of the available public datasets for network traffic classification cover newer application use-cases such as virtual reality and social media. A more modern dataset, including data from these emergent use-cases, would be useful for applying our framework over a wider set of traffic categories. Last, the motivation for this work stemmed from the fact that existing standards lack easy access to application layer QoE metrics during scheduling. Future standards could provide mechanisms and interfaces for the physical and medium access layers to incorporate higherlayer state information to facilitate BS schedulers to proactively improve end-user QoE."
            ],
            "subsections": []
        },
        {
            "title": "VII. CONCLUSIONS",
            "paragraphs": [
                "This work has presented an end-to-end experience-centric scheduling framework for improving streaming video quality over cellular radio networks. By inspecting IP/TCP headers of downlink user plane packets, the network can reliably detect the presence of HAS flows and infer their application characteristics. By applying such application layer HAS state information during downlink scheduling of video users, our results show improved QoE at streaming video users. This motivates an end-to-end user application-centric approach towards designing next generation mobile wireless networks."
            ],
            "subsections": []
        }
    ]
}