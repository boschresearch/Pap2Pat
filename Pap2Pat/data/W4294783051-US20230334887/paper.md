# Introduction

During a visit to a medical care provider, a physician typically records important information about patient presentation, diagnosis, and treatment via free-form text. These notes contain rich clinical data not found in structured electronic medical records. For example, information about patient experience and even some diseases may not be documented with standardized codes, but evidence of symptoms or diagnoses may be recorded in free-form text. Clinical notes can also prove useful for extracting relevant medical conditions for cohort building, such as for identifying patients for clinical trials or developing disease progression models using features from the text. Because these text sequences can be very long, digging through patient records to find relevant information is a tedious, manual process, and automation with standard machine learning (ML) approaches is often insufficient. This moti-vates the development of an accurate, automated, and interpretable method for extracting medical conditions from long medical documents.

Convolutional neural network (CNN) models have been the architecture of choice for long medical text (Mullenbach et al., 2018;Gehrmann et al., 2018;Li and Yu, 2019;Reys et al., 2020;Hu et al., 2021), largely due to the computational complexity of the self-attention mechanism in Transformers like BERT (Devlin et al., 2019). However, recent advancements in sparse attention or long language models (LMs) (Zaheer et al., 2021b;Choromanski et al., 2021;Beltagy et al., 2020;Kitaev et al., 2020) suggest it is now possible to represent long medical documents without convolutions that fail to capture interactions between distant tokens in a text sequence, or the truncation and segmentation with pooling methods that ML practitioners apply to standard Transformers in practice (Huang et al., 2020).

While there are approaches for interpreting the predictions of traditional ML models and neural networks (Sundararajan et al., 2017;Lundberg and Lee, 2017), understanding the blocks of text driving the predictions of long LMs is not straightforward. A common approach to interpret the predictions of Transformers is to examine the attention weights of tokens. Although subword tokenization has been shown to be performant in downstream classification tasks, the attention weights of individual subword tokens are not always informative or interpretable, and attention weight interpretation has been criticized (Jain and Wallace, 2019). The limitations of word and subword level explanations are especially prevalent in a healthcare context where word pieces are often divorced of clinical meaning or capture only part of a phrase representing a clinical concept. For example, consider the phrase, "patient developed atrial fibrillation," consisting of many tokens. Understanding the impact of blocks of text (represented as sequences of subword tokens) on model predictions only becomes more challenging for models with sparse attention, as not all subwords attend to each other in the self-attention layers.

In this work, we introduce a novel method, the Masked Sampling Procedure (MSP), to identify important blocks of text used by long LMs or any text classifier to predict document labels. Our method is unique and valuable in that we simultaneously mask multi-token blocks to answer the counterfactual: "what if this block of text had been absent?" Unlike previous work, runtime does not depend on document length, and we provide a rigorous method to compute p-values for each text block. Our method extends to any number of multi-token text blocks to measure interactions, and we report the benefits of specific masking probabilities.

We validated that MSP returns clinically informative explanations of medical condition predictions from a very long LM with a blinded experiment involving two physicians. In Section 5.1 we share insights from our clinician collaborators regarding the explanations surfaced by MSP and describe the superior performance and runtime efficiency of MSP compared to the state-of-theart, showing that our method is up to 100× faster and ≈ 1.7× better at identifying important text blocks from a very long LM applied to long medical documents. Finally, in Section 5.2, we describe the benefit of using sparse attention LMs in the context of predicting medical conditions from long medical documents (up to 32,768 tokens), extending the length of the typical LM for clinical documents from 512 to 32,768 tokens with an over 5% absolute improvement in microaverage-precision over a popular and effective CNN architecture for predicting medical conditions from text on four different size train sets.

# Related Work

Many methods have been proposed to explain the predictions of text classifiers, such as those that examine the individual attention weights of tokens in LMs ( Škrlj et al., 2021), gradient-based methods that attempt to reveal the saliency of individual tokens (Yin and Neubig, 2022), and approaches that perturb the input text to measure importance (Kokalj et al., 2021). The most similar approach to our procedure is likely the Sampling and Occlusion (SOC) algorithm (Jin et al., 2020). Jin et al. (2020) apply SOC to BERT and show that SOC outperforms a variety of competitive baselines including GradSHAP (Lundberg and Lee, 2017), a popular approach combining ideas from Integrated Gradients (Sundararajan et al., 2017) with perturbation-based feature importance, on three benchmark datasets. SOC masks one word or text block at a time to compute the impact on label predictions and eliminates the dependence on surrounding context for a given block by sampling neighboring words from a trained LM. However, if the trained LM performs well, the sampled neighboring words will be similar to the original context. This sampling procedure is computationally expensive which we discuss in Section 5.1.3.

Further related work includes traditional text representation approaches like TF-IDF (Sparck Jones, 1988) and word2vec (Mikolov et al., 2013), predicting medical conditions using CNNs with attention (Mullenbach et al., 2018;Hu et al., 2021;Lovelace et al., 2020), LMs that improve on traditional text representations (Vaswani et al., 2017;Devlin et al., 2019), sparse attention LMs (Kitaev et al., 2020;Beltagy et al., 2020;Choromanski et al., 2021;Zaheer et al., 2021b), and domain-specific pretraining of LMs for medical text (Alsentzer et al., 2019;Lee et al., 2020;Liu et al., 2021). See Supplemental Section A for more details.

To our knowledge, the only research applying long LMs to the clinical domain is Li et al. (2022). The authors fine-tuned Longformer and Big Bird (Zaheer et al., 2021b) for clinical question answering (Pampari et al., 2018) and named-entity recognition. We benchmark the first clinically pretrained long LM for multi-label classification of conditions from clinical notes, extending the typical sequence length of long LMs by 8× (from 4,096 to 32,768 tokens) to address the problem of extracting information from long, individual, patient medical histories.

# Cohort

We use two document types in our experiments: medical charts, which are long-form clinical notes concatenated from many visits, and discharge summaries. In both cases, we consider a single document to be the entire sequence of tokens.

The Optum Chart dataset consists of 6,526,116 full-length medical charts for Medicare patients from 2017-2018 and was obtained by Covered Entity customers of Optum Labs to provide quality improvement services. We used 5,481,937 unlabeled charts for pretraining text representations. We used 640,000 labeled charts for training, 64,000 for validation, and 187,953 for testing. Labels were generated by human medical coders in a process wherein three coders had to agree on each medical condition label before assigning it. Data split sizes were determined to ensure a fair comparison to existing models and generating these final splits involved downsampling to measure the effect of training set size and reduce evaluation time on the validation set during training. Specifically, we downsampled at random from 730,925 train samples and 125,301 validation samples while maintaining an initial split of 187,953 testing samples. These original split sizes were the result of a multi-label, stratified shuffle split using iterative stratified sampling (Sechidis et al., 2011) with target proportions of 70%, 12%, and 18% respectively. This splitting procedure was used to ensure a roughly uniform distribution of labels across train, validation, and test splits. The remaining, unlabeled charts not included in this splitting procedure were used for pretraining. While we broke the train set into four datasets to measure the effect of train set size, the same validation and test sets were used in all experiments. The median length in subword tokens of documents in the Optum Chart dataset is 4,043 tokens with interquartile range [1,830 -9,142]. Descriptive statistics can be found in Supplemental Table 6 and condition prevalence in Supplemental Table 14. MIMIC-III (Johnson et al., 2016) contains de-identified clinical records for intensive care unit (ICU) patients treated at Beth Israel Deaconess Medical Center. Included is a set of discharge summary notes and International Classification of Disease ICD-9 diagnoses associated with each ICU stay. We use the subset of discharge summaries from Mullenbach et al. (2018) consisting of 11,371 notes from 2001-2012 and the top 50 most common ICDs appearing in each summary. The median length in subword tokens of documents in this dataset is 1,430 tokens with interquartile range [1,029 -1,929]. Descriptive statistics can be found in Supplemental Table 6. We use the same 8,067 sample train, 1,574 validation, and 1,730 test sets as in Mullenbach et al. (2018).

# Methods

## Masked Sampling Procedure

To reveal which text blocks have the largest effect on the predictions of long LMs or any text classifier, we propose MSP (Algo-rithm 1). To explain predictions from a text sequence, MSP randomly masks all text blocks of size B subwords with probability P , feeds the new sequence to the classifier, then measures the difference in label probability between the masked and unmasked versions of the sequence (see Figure 1). Over many iterations N , large differences in predicted probabilities originating from masking a given text block suggest the block contributed important evidence to the label prediction. MSP outputs the top K most important blocks for each label along with a measure of statistical significance computed by comparing to randomly sampled text blocks using a bootstrap procedure, with the null hypothesis, that, text blocks with high importance, as determined by MSP, are no more important to a label prediction than randomly sampled blocks (see Algorithm 2).

Algorithm 1: Masked Sampling Procedure (MSP) and used as input to a classifier. For each masked row we get an importance score, defined as the difference between the baseline prediction, when no text blocks are masked, and the prediction when blocks are masked. For a specific text block of interest (e.g., block 3 in blue), we then calculate the mean difference in importance scores when the block is/is not masked to measure the contribution of that text block to the prediction.

In a blind experiment, two clinicians validated the ability of MSP to explain predictions from a very long Big Bird model (Section 4.3) on randomly sampled discharge summaries from MIMIC compared to SOC (Jin et al., 2020) and a random algorithm. We selected the very long Big Bird model for its ability to represent long medical documents and because attention weight analysis of this model is not straightforward due to the sparse self-attention mechanism. Each clinician received the same 400 text block-diagnosis pairs from each algorithm and independently annotated the text blocks as either uninformative or informative for making the ICD diagnosis. Each of the 1,200 total text blocks supplied to each clinician were among the top five most important for the corresponding label according to MSP, SOC, or by random selection (see Supplemental Methods B.8 for more details on how text blocks were selected). We compared the number of informative text blocks from each method along with differences in runtime.

For MSP, we set P = 0.1 according to an experiment with a single clinical reviewer comparing values of P shown in Supplemental Table 16. For a fair comparison to SOC, we fixed B = 10 and set the expected number of times a given phrase is masked to 100. We used the sampling radius of 10 tokens recommended by Jin et al. (2020) and set the number of sampling rounds to 100.

## Baseline Text Representations

We compared the performance of several text representations and classifiers for the task of predicting medical conditions from clinical text to the Big Bird LM for which we generated explanations with MSP. These methods operated at either the word or subword level following text preprocessing (see Supplemental Methods B.3). More details on baseline text representations can be found in Supplemental Methods B.5.1

## Very Long Big Bird

Big Bird's sparse attention mechanism approximates full self-attention with a combination of global tokens, sliding window attention, and random approximations of fully connected graphs representing full selfattention. These mechanisms take the memory consumption from O(L 2 ) to O(kL),

where k is the size of the sliding attention window. To pretrain a Big Bird LM on clinical text, we first trained a Byte Pair Encoding subword tokenizer (Sennrich et al., 2016) to tokenize the text. This same tokenization approach was used by Zaheer et al. (2021b). After cleaning, we truncated all text to 32,768 subwords following tokenization, and pretrained with masked language modeling (MLM) as in Zaheer et al. (2021b). We selected 32,768 subword tokens to increase the maximum sequence length represented by Big Bird by another 8×, given that the original Big Bird model is 8× the maximum sequence length of BERT (512 to 4,096). Furthermore, over 95% of the medical charts in the Optum Chart dataset are less than 32,768 tokens in length.

## Text Classifiers

We are interested in identifying conditions from medical documents relevant to diagnosing or treating patients and focused our experiments on two datasets with 85 and 50 medical condition labels respectively. To predict these conditions, we used ElasticNet, a Feed Forward Neural Network (FFNN), BERT variants with text segmentation and pooling (on MIMIC only), CAML, and Big Bird, all trained as multi-label classifiers.

Here we describe CAML and Big Bird, which were the most competitive. Details on all models can be found in Supplemental Methods B.5.2.

CAML uses a CNN layer to extract features from the word2vec embedding matrix and an attention mechanism to localize signal for a particular prediction task. We implemented CAML as described in Mullenbach et al. (2018) using a CNN layer with filter size between 32 and 512, kernel size between 3 and 10, and dropout on the embedding layer between 0 and 0.5. The output is a vector of probabilities, one for each label, to which we applied the sigmoid function and trained the model to minimize binary crossentropy loss.

The Optum Chart sequences are 8× larger than the typical "long" sequence (Tay et al., 2020) at a maximum length of 32,768 tokens. We pretrained Big Bird from random initialization on medical documents, added a classification head with a single feed-forward layer of size 1,536 (2x the hidden size), an output layer with one neuron per label, and trained using binary cross-entropy loss.

# Results

## Clinical Validation of MSP

We examine the clinical utility of MSP in a blind experiment with two clinicians, first discussing examples of informative text blocks, then comparing the number of informative text blocks surfaced by MSP to the SOC algorithm in the blind experiment. Finally, we discuss runtime.

### Informative Text Blocks

Table 1 depicts example text blocks and their importance computed via MSP that were deemed informative during an initial clinical review. This review confirmed three general features that drive text block "informativeness."

The most obvious were exact matches with diagnosis text. For example the text block "pneumonia patient being discharged o n maximal copd regimen including," was highly informative in implying a diagnosis Other common elements in highly informative blocks were drugs that are always, or almost always used for a particular diagnosis. MSP identified the block "consulted amiodarone was held rhythm slowly began to recover she," associated with the diagnosis "atrial fibrillation." Amiodarone is an antidysrhythmic drug mostly used for atrial fibrillation.

MSP also identified obscure but clinically relevant blocks, such as "al likely improve as pna improves s p cabg complicated," including "s", "p", and "cabg." Grouped together, these suggest the patient is "status-post" coronary artery bypass grafting, meaning they have had the procedure. In order to graft coronary arteries, the patient must be placed on an aortocoronary bypass machine to allow the procedure to be completed. This block was associated with the diagnosis of "aortocoronary bypass status." Another seemingly obscure but clinically informative block, "albuterol and ipra prn his acidosis slowly improved as did" appeared for the diagnosis of "acute respiratory failure." Even though none of the words comprising the diagnosis exist in the block, clinician review confirmed that the block is associated with acute respiratory failure, despite the lack of matches for words in the diagnosis label. Albuterol, a fully and correctly spelled-out drug associated with respiratory distress, is related to bronchial obstruction, seen in chronic obstructive pulmonary disease (COPD). Ipra, an abbreviation for ipratropium bromide, is used in COPD. COPD is a common cause of acute respiratory failure. Acidosis, identified through arterial blood testing, is a sign of hypoventilation, which causes elevation in blood carbon dioxide levels and resultant accumulation of H 2 CO 3 (an acid). This is seen in people with COPD exacerbation who experience respiratory failure. Two clinicians received 400 text blockdiagnosis pairs from each of MSP, SOC, and the random algorithm and independently annotated the text as either uninformative or informative for making the ICD diagnosis. Table 2 depicts the number of informative text blocks surfaced by each explainability algorithm. Figure 2 depicts the precision of each algorithm according to both reviewers. MSP is superior to SOC in terms of the total number of clinically informative text blocks surfaced and precision, especially when limiting the number of blocks surfaced to a small number. Supplemental Figure 8 depicts performance of these algorithms from an information retrieval perspective.

### Runtime Comparison

On the MIMIC discharge summaries of modest length (IQR 1,029-1,929), MSP was up to 100× faster than SOC (Table 3). For J sampling iterations per block, masking probability P , and document length L, using MSP, the number of evaluations of the text by the classifier is O(J/P ) for computing the importance of individual sentences and O(J/P 2 ) for pairs. Using SOC, the number of evaluations is O(JL) and O(JL 2 ) respectively. Thus, the run time of our approach does not grow with the document length as the number of model evaluations does not depend on L. Since SOC has a quadratic dependency on L, it is very expensive for computing the importance of individual sentences in documents of even modest length and infeasible for computing the importance of sentence pairs (see example in Table 4). In the medical and other domains, we expect distant pieces of information to interact, and use pairs analysis with MSP to demonstrate how Big Bird integrates distant contextual information in Supplemental Results C.4.3.

## Medical Condition Prediction

We assessed model performance when predicting medical conditions in long medical charts from the Optum Chart dataset. Since the prevalence of each label is often very low (median: 0.6%), we used precision and recall as our metrics of interest (specifically, area-under the precision-recall  6.05) hours MSP (P = 0.1) 0.89 (0.05) hours MSP (P = 0.5) 0.18 (0.01) hours (AUPR) curve, or average-precision (AP)) (Saito and Rehmsmeier, 2015). In Table 5 we show performance in terms of AP microand macro-averaged across labels. For most labels, Big Bird outperformed CAML, (see Supplemental Figure 4a), and across training datasets of four sizes performed over 5% better than CAML in micro-average-precision (see Supplemental Results C.3 and Supple- Algorithm 1000 Tokens 10,000 Tokens SOC 100,000,000 10,000,000,000 MSP (P = 0.1) 10,000 10,000 MSP (P = 0.5) 400 400

mental Tables 8,9, 10, 11). Supplemental Figure 4c shows the log 2 -scaled ratio of the Big Bird AUPR to the CAML AUPR as a function of label prevalence. Big Bird generally outperforms CAML on labels with prevalence > 5%, but many of the most significant improvements are found in rare labels (prevalence ≤ 5%). AUPRs for each label are included in Supplemental Table 14. Next, we assessed performance for predicting any of the 50 most common ICD-9s assigned to a MIMIC discharge summary. As baselines, we trained multiple TF-IDFbased models, CAML, and several BERT  12). This supports previous work demonstrating that in-domain pretraining from scratch is superior to cross-domain fine-tuning for tasks in the biomedical domain (Lee et al., 2020). Of all the baselines we compared with various flavors of Big Bird, CAML performed best on MIMIC. Supplemental Figure 4b and d shows the performance of Big Bird and CAML for each label. BERT variants using pooled segment representations performed worse than CAML and Big Bird with clinical pretraining (Supplemental Table 13). This suggests learning a representation of an entire input sequence with sparse self-attention outperforms aggregation over segments.

# Discussion

The purpose of this research is to extract meaningful insights from long medical documents in an auditable and transparent way. We discussed and demonstrated the performance benefits of sparse attention LMs for extracting medical conditions from very long text and proposed MSP to address the major challenge of interpreting long LM predictions. MSP can explain medical condition predictions from discharge summaries using the very long Big Bird LM ≈ 1.7× better than a state-of-the-art explainability algorithm and up to 100× faster. It's also tractable for generating important text block pairs.

We note that among other limitations of our research, medical charts contain more than just free-form text.

Some semistructured information from tables and bulleted lists is lost when representing an entire chart as a single text sequence. Discharge summaries, as in the MIMIC dataset, are just one type of medical note, specific to an inpatient setting. The Optum Chart dataset is more comprehensive in that it consists of full length medical charts, but these charts are only for Medicare patients. Future work should examine other types of clinical notes as well as other populations.

Additional opportunities for further research include modifying the pretraining procedure for long LMs applied to medical text to take advantage of additional information available in electronic health records, evaluating a highlighting system that surfaces information relevant to diagnosing and treating patients, and assessing long LMs for clinical information extraction for bias using methods like MSP.

We view improving the underlying representations of medical text and understanding the predictive elements as key steps toward ensuring that ML can be safely deployed in the medical domain but acknowledge there is more work to do to scale ML across the healthcare system in a just and transparent way.

# Appendix A. Further Discussion of Related Work

We consider TF-IDF (Sparck Jones, 1988) and word2vec (Mikolov et al., 2013)  LMs have yet to be widely adopted for the task of predicting medical conditions from text, though architectures such as the Transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) have revolutionized the way ML practitioners classify documents, outperforming CNN-based approaches on generic, English classification tasks (Sun et al., 2020). We observe two main challenges with applying Transformer architectures like BERT to medical documents: first, medical documents differ from generic, English text, and second, medical documents containing entire patient medical histories can be very long, far exceeding the maximum sequence length used by BERT.

The Clinical BERT (Alsentzer et al., 2019), BioBERT (Lee et al., 2020), and Sap-BERT (Liu et al., 2021) architectures were all designed to tackle the important differ-ences between generic, English natural language and text encountered in the clinical and biomedical domains. Clinical BERT further pretrains the standard BERT model on medical notes from MIMIC-III, though MIMIC-III is limited in the total number of documents. The BioBERT architecture solves this problem by training on more samples and demonstrates that in-domain pretraining from scratch, outperforms continued pretraining on data from the BLURB biomedical benchmark. That said, the domain of biomedical papers used to pretrained BioBERT differs from the domain of individual, patient medical histories and the task of predicting specific conditions from text. One solution is to incorporate more information about synonym and subtype relationships found in medical language. The SapBERT authors demonstrate the value of this approach through a continued pretraining strategy whereby the BERT model learns to align entities in the Unified Medical Language System. All of these approaches demonstrate benefits over base BERT, but none are well-suited to long documents due to BERT's quadratic time and memory complexity.

To address the problem of long documents, sparse self-attention architectures have been proposed that approximate BERT's selfattention including Reformer (Kitaev et al., 2020), Longformer (Beltagy et al., 2020), Performer (Choromanski et al., 2021), and Big Bird (Zaheer et al., 2021b). Reformer replaces global self-attention by localitysensitive hashing (LSH) self-attention based on similarity of the query vectors in the softmax(QK T )V self-attention equation. This takes the attention time complexity from O(L 2 ) to O(L log(L)) where L is the length of the input sequence. Longformer also applies self-attention over windows of nearby tokens and introduces a subset of global tokens that attend to every other to-ken. The Big Bird LM combines the ideas of local and global attention with a random attention mechanism using a random graph approximation of fully connected graphs representing full self-attention. Intuitively, Big Bird attempts to create a path between any two tokens in the input sequence with the limitation that many layers might be required to connect any two tokens. Because Big Bird combines multiple self-attention approximation strategies, provides an intuitive implementation of sparse self-attention, and achieves the best average performance across six long document tasks reported on the Long-Range Arena (LRA) benchmark (Tay et al., 2020), we focus on Big Bird in our research. Luna (Ma et al., 2021) and S4 (Gu et al., 2022) report higher performance numbers on LRA in their papers, though, at the time of writing, these numbers have not yet been added to the LRA website. 2

# B.1. Cohort Descriptive Statistics

The preprocessing steps used by Mullenbach et al. (2018) for MIMIC discharge summaries are provided here. 3

# B.2. Exploratory Data Analysis

Descriptive statistics for the medical documents used in this research can be found in Table 6. 

# B.3. Dataset Preprocessing

To clean the text of each document prior to tokenization, we strip these characters from the start and end of all words: .!"#$&'()*+,/:;?@[\]^_'{|}~.

We then remove these characters entirely: !"#$&'()*+,;?@[\]^_'{|}~". These characters are not relevant in a healthcare context in the same way as retained characters such as, for example, "/", which is important for blood pressure readings. We then eliminate words longer than 39 characters, characters occurring three times in a row within a word, dates, phone numbers, URLs, states, names, cities, and emails. We leave numbers as they are (no bucketing). Numbers appearing in a healthcare context are relatively small, and it has been shown that Transformer LMs can effectively represent the relative size of numbers in the range of -500 to 500 (Wallace et al., 2019).

# B.4. Pretraining and Fine-Tuning

Approach for Clinical Text

# B.5. Model Development and Training

We implemented the deep learning models described using PyTorch (Paszke et al., 2019) (version 1.9.1). For all models except the Big Bird LM, we selected the best model hyperparameters using 20 iterations of Bayesian grid search with version 2.10.0 of the Optuna library (Akiba et al., 2019) and a batch size of 112. Final hyperparameters were selected according to the best micro-averageprecision over 2-fold cross-validation. Because some conditions are rare, we applied iterative stratified splitting (Sechidis et al., 2011) to create folds during hyperparameter optimization and when creating train, validation, and test sets to ensure a roughly uniform distribution of labels across data splits.

Using the best hyperparameter settings, the final models were trained on the entire train- 

# B.5.1. Language Models

With TF-IDF (Sparck Jones, 1988), we represented each document as a bag of phrase frequencies using word-level 3 and 4 grams occurring with 0.001 to 0.5 frequency across all documents.

With word2vec (Mikolov et al., 2013) we represented text sequences as embedding vectors concatenated in order of occurrence into a matrix X = [x 1 ; x 2 , . . . , x N ] where x n ∈ R de is the n th embedding vector and N is the sequence length. To train word2vec on our data, we used a word-level vocabulary consisting of the 50,000 most common words identified across our pretraining dataset consisting of 5,481,937 medical charts represented as text sequences and truncated to the first 32,768 words. We trained word2vec embeddings of dimension 128 using the continuous bag of words training objective, whereby the neural network used to create the embeddings takes as input a one-hot representation of the five word-level tokens to the left and right of a given token, and uses this informa-tion to predict the given token. This process was repeated for all words in all input sequences, and we trained the model for five epochs. All other parameters were set to the defaults in version 3.8.3 of the gensim (Rehurek and Sojka, 2011) library for Python. We set the embedding layer of the CNN models to trainable, enabling updates to the word representations during model training. for the supervised classification task.

The parameters of the very long Big Bird LM are shown in Table 7. The main aspect of the architecture we modified is the maximum sequence length, which poses challenges for training, especially data loading. Consider that each token in each input sequence is represented by a vector and the multiple attention heads of the LM run the block sparse self-attention operation in parallel, processing the same sequence with multiple heads at once. Even without full self-attention, the training process is memory intensive.

We selected hyperparameters similar to the base Big Bird model in Zaheer et al. (2021b) and report them in Table 7. We pretrained the model for the MLM objective and measured validation loss after every 200 steps. We could fit only one sample at a time on GPUs with 32GB RAM, and so used a batch size of 32 (there were 32 GPUs total in the cluster) with four gradient accumulation steps for an effective batch size of 128. Training for 50 steps took approximately 20 minutes, and so we limited training to three full epochs, after which, decreases in validation loss of 0.01 took over 20,000 steps. We expect further pretraining of the LM to lead to better downstream performance and leave this as future work.

When pretraining the Big Bird model with a 4,096 token maximum sequence length on the Optum Chart dataset, we trained for the same number of steps, using the same hardware configuration and model hyperparameters as described above except for the maximum sequence length. We used the same pretraining dataset but split documents into chunks of 4,096 tokens, such that some medical charts were represented as multiple samples. Samples from medical charts for the same patient were always in the same data split. For example, if multiple samples from a given patient were in the validation set, all samples for that patient were in the validation set, and so forth for all data splits to avoid leakage.

# B.5.2. Text Classifiers

Using the TF-IDF representation of a text document, we trained two different classification models for baseline comparison: logistic regression (LR) with ElasticNet (Zou and Hastie, 2005) regularization, and a multilayer FFNN.

For the ElasticNet model, we performed one round of feature selection using the Lasso penalty selected from the range 1 to 1,000,000 over 20 Optuna trials. We then applied the ElasticNet penalty with L1:L2 ratio selected from the range 0 to 1, and regularization strength selected from the range 1 to 1,000,000. For the FFNN, we selected from between 1 and 4 hidden layers of size between 4 and 192 and dropout rates between hidden layers between 0.05 and 0.5.

In addition to the ElasticNet and FFNN baselines, we fine-tuned several short LMs pretrained on public text data such as Wikipedia on the MIMIC dataset to predict the 50 conditions of interest. For details on the models and pretraining datasets, please see the roberta-base, Bio-ClinicalBERT, and bigbird-roberta-base (Public Big Bird) model cards from Hugging Face. Because the BERT models (RoBERTa and Bio-ClinicalBERT) can only represent sequences of 512 tokens at maximum, we tested truncating text to the first 512 tokens and pooling predictions over many overlapping representations of a full length sequence by taking either the mean or max prediction from each sequence chunk during fine-tuning. These pooling strategies are based on comments from the BERT author, Jacob Devlin, here4 . The code used to fine-tune these models accordingly was modified from this implementation5 . Bio-ClinicalBERT is pretrained on MIMIC notes, resulting in some leakage when predicting on the MIMIC 50 test set in our experiments. We take the fact that the clinically pretrained CNN and Big Bird models outperform the fine-tuned Bio-ClinicalBERT models even with leakage as evidence that pooling over predictions on windowed chunks of the input text is inferior to text-based CNNs and long document LMs implementing sparse self-attention which can represent full length sequences. All BERT models were fine-tuned using Hugging Face Transformers 4.10.3 and PyTorch 1.9.1 with a maximum learning rate of 0.00001, a linear warmup for 1000 steps, and the AdamW optimizer with 0.01 weight decay. All BERT models used early stopping on validation loss, training until 20 epochs of no improvement. Effective batch size (with gradient accumulation) for all BERT training runs was 128. For pooling, windows of 510 tokens were created with 128 token overlaps. The custom aggregation function for Bio-ClinicalBERT was implemented as described in equation 4

# B.6. Data Loading

All data preprocessing happened on a single CPU machine with 672 GB RAM and 96 cores. We pre-tokenized the data in batches so that when data was loaded on the fly, the model did not need to wait for any transformations to be applied.

To train the ElasticNet (Zou and Hastie, 2005) LR, FFNN, and CNN models in our experiments, we applied a straightforward data loading procedure. This consisted of splitting batches of 112 training samples onto four GPUs one at a time by first loading multiple batches from disk, splitting one batch onto the four GPUs, moving to the next batch, and loading the next set of multiple batches once all batches in memory were exhausted.

To pretrain the Big Bird long document LM on 5,481,937 sequences of 32,768 subwords, we created input tensors of tokenized and masked text prior to training. Even before moving tensors to the GPU, we were limited by the size of tensor we could fit into RAM. As such, training tensors were chunked and loaded into memory one at a time. We broke the charts into chunks of 6,400 samples each and one chunk with less than 6,400 samples. We considered a full epoch to be one pass through all chunks. We considered one training round to be a pass through 40 chunks followed by evaluation on 64,000 samples to compute validation loss, using this for early stopping.

To accommodate a validation set of arbitrary size, validation set sequences were saved as individual samples and loaded on different GPUs during validation epochs according to a mapping used by the distributed sampler. The mapping pointed a randomly sampled index to a file for that sample, similar to how image classification models often load and train on individual images. positive label (ICD-9 code) from each. We also randomly sampled text blocks from the discharge summaries, the same number of which were identified via MSP. This resulted in 1,850 line items having valid ICD-9 descriptions. To limit clinical review time to about six hours, estimating 200 lines items per hour, we randomly sampled 400 ICD-9 label-document combinations for each of the three algorithms from the 1,850 line items. This resulted in a total of 1,200 line items which we provided to each clinical reviewer.

# C.1.1. Optum Chart Dataset

In Supplemental Figure 5 we show microand macro-averaged model performance using area under the ROC (AUROC) curve, AUPR, and the F1 metric. Of the models evaluated, we found that models utilizing the TF-IDF representations had the poorest performance across all metrics. We expected this to be the case, as the TF-IDF text representation fails to account for word order. The CAML model with pretrained word2vec embeddings outperformed the TF-IDF models, but was worse on average than the very long Big Bird model. The Big Bird model outperforms the CAML model by a statistically significant margin in every classification metric. Because all layers of the Big Bird LM are pretrained on the text of medical charts, we expect the model carries more information than the individual word vectors used in the CAML model. Additionally, while the attention mechanism in the CAML model enables tokens near each other in an input sequence to attend to each other, tokens in the Big Bird model can attend to each other across distant pieces of the input sequence via global tokens and random connections. We explore this further in Supplemental Results C.4.3.

When comparing the performance of each model for a particular medical code, it is important to include the label prevalence for context. Whereas the AUROC for a random model is 0.5 regardless of the label prevalence, for AUPR the random model performance is equal to the label prevalence.

In Supplemental Figure 6a we show the AUPR for the CAML and Big Bird models for the top 10 medical conditions where the difference between the Big Bird AUPR and the CAML AUPR was the greatest. As a baseline for each condition we also show the prevalence. The improvements of the very long Big Bird model over the CAML model appear largest when the prevalence is low.

# C.1.2. MIMIC Dataset

Figure 6b shows the AUPR for the CAML and Big Bird models for the top 10 ICD codes where the difference between the Big Bird AUPR and the CAML AUPR was the greatest.

# C.2. Performance Tables

The main text of the paper refers to both average and per label performance metrics for both datasets. Figure 5 compares the test set performance of all models trained using the largest training set from the Optum Chart dataset. Average performance over all medical conditions for the various training sets from the Optum Chart dataset can be found in Table 8, Table 9, Table 10, andTable 11. Average performance over all medical conditions for the MIMIC dataset can be found in Table 12. Per label metrics for both datasets can be found in Table 14 and      (Top) During pretraining, vector-based representations of tokens are learned via masked language modeling (MLM) where 15% of tokens are masked from the input sequence and the remaining tokens are used to predict missing tokens, thus, incorporating word context into the learned representations. (Bottom) During fine-tuning, the whole system is trained end-to-end with new labels, jointly updating token representations while learning to predict the provided labels. We focus on the task of medical condition prediction where the labels are ICD codes representing diagnoses.    

# C.4. Masked Sampling Procedure Additional Details

Table 16 depicts the proportion of informative text blocks identified using MSP at var-ious values of P and randomly selected text blocks where informative text blocks were annotated by a single clinical reviewer. In these experiments, we set the number of iterations, N , such that the expected number of times a given text block is masked when computing importance is equal to 1000. For example, when the masking probability, P is 0.1, we set N = 10, 000. For each sampled discharge summary, we selected the K = 5 most important text blocks for each positive label (ICD-9 code). For each masking probability tested, and for the random set of blocks, the clinician received 117 samples. We chose 117 by randomly sampling at least five discharge summaries for each masking probability, P , and taking the minimum number of ICD-9 label and discharge summary combinations associated with each P , such that the number of samples provided to the clinician was equal for each masking probability. Of the 117 text blocks deemed important by the random masking procedure for the best value of P , 35 (29.9%) were considered relevant to the diagnosis according to clinical review.

All values of P except for P = 0.9 are significant with α = 0.05 and remain significant after Bonferroni correction. These results suggest that MSP indeed identifies text blocks relevant to the predicted medical condition labels. Based on the proportion of informative blocks for each P , we hypothesize that lower values of P better isolate the effects of individual blocks than higher values of P in which most blocks are masked. The proportion of clinically informative text blocks for all masking probabilities is shown in Table 16.  LM inference time grows with the length of the document. As such, the runtime for both MSP and SOC grows with document length, however, SOC requires additional sampling iterations to compute the importance of each new phrase in a document as document length grows. Table 17 depicts the change in runtime averaged over 20 trials for various fixed document lengths for the MSP and SOC algorithms. Note that even at a modest document length of 1000 tokens, identifying the important text blocks in a single document with SOC takes over an hour.

# C.4.3. Integrating Distant Contextual Information

We repeated MSP for pairs of text blocks by identifying which pairs of text blocks have the largest impact on the probability of each label. For this analysis, we focused on the case where P = 0.1 and N = 10, 000, such that the expected number of times a given pair of blocks is masked in the same iteration is 100. For these pairs, we computed the distance between the start of each text block in the pair to understand whether the long document LM is incorporating information from distant parts of each document in its predictions. In general, this procedure can be used to identify combinations of many text blocks, and is flexible to different definitions of a block. We run experiments with B = 10, identifying important blocks of 10 subword tokens, but blocks could be defined by splitting on punctuation or even entire paragraphs.

We are interested in block pairs for which the importance score of the pair is greater than the sum of the importance scores of each block in the pair and consider these cases interactions. Such interactions would indicate the model recognizes the joint influence on label predictions of text snippets in pairs beyond the individual contributions of blocks in a pair. Figure 9 shows the in- teractions for all block pairs with positive importance scores for 15 randomly sampled discharge summaries from the MIMIC test set along with the relative distance between blocks in each pair and the relative strength of the interactions. Figure 9 illustrates that there are many block pairs for which the combined importance score of pairs is relatively higher than each block in the pair while the distance between blocks is great, often 100s of tokens (median distance of 490 tokens). In the Big Bird model, local attention is applied over windows of 64 tokens (from Supplemental Table 7), suggesting the model, through the use of global and random attention across 12 layers, is integrating information from distant locations within the discharge summaries to predict the ICD-9 labels assigned to each summary. In this way we demonstrate that not only can MSP be used to identify clinically informative text blocks used by the long LM to make predictions, it can also uncover pieces of information which, though distant within the document, in combination, influence the predicted probabilities of ICD labels. If no blocks surfaced were important for a given document-label pair, the value for that example is 0. This metric is valuable in that it privileges algorithms that assign a high rank to informative text blocks.

Table 17: Below are mean runtimes over 20 experiments for each text block importance algorithm on documents of various fixed lengths. Standard deviation is reported in parentheses. We compare our masked sampling procedure (MSP) at two masking probabilities P to the Sampling and Occlusion (SOC) algorithm (Jin et al., 2020). Note the rapid increase in SOC runtimes, even at these modest document lengths, making SOC intractable for very long documents.

Algorithm 50 Token Doc 100 Token Doc 500 Token Doc 1,000 Token Doc SOC 0.31 (0.05) mins 0.47 (0.04) mins 2.17 (0.03) mins 65.49 (0.59) mins MSP (P = 0.1) 0.27 (0.03) mins 0.26 (0.03) mins 0.33 (0.02) mins 6.31 (0.08) mins MSP (P = 0.5) 0.11 (0.02) mins 0.12 (0.02) mins 0.12 (0.02) mins 1.41 (0.04) mins

