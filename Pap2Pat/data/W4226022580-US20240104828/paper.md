# Introduction

We set out to develop a method that generates photorealistic humans under novel views and unseen poses from monocular RGB videos. To represent static scenes, neural radiance fields (NeRF) [29] learn an implicit representation using neural networks, which has enabled photo-realistic rendering of shape and appearance from images. With dense multi-view observations as input, NeRF encodes density and color as a function of 3D coordinates and viewing directions by the MLPs along with a differentiable renderer to synthesize novel views. While it shows unprecedented Figure 1. Novel View Synthesis on Unseen Poses. Given a monocular video, we predict novel views with body poses unseen from training with fine-level details (wrinkles) that works such as NeuralBody [34] or HumanNeRF [46] struggle to obtain. visual quality on static scenes, applying it to high quality free-viewpoint rendering of humans in dynamic videos remains a challenging task. Aiming to generalize NeRF to dynamic videos, D-NeRF [36] encodes a time step t to differentiate motions across frames and converts scenes from the observation space to a shared canonical space to model the neural radiance field. As such, they can handle dynamic scenes to some extent but the poses remain uncontrollable by users. Furthermore, some approaches [25,35] introduce human pose as an additional input to serve as a geometric guidance for different frames. However, they either cannot generalize to novel poses or need more than one input view.

To overcome these limitations, we propose a novel approach by learning implicit radiance fields based on pose and appearance representations for high fidelity novel view and pose synthesis. We leverage the human pose extracted from the parametric body model as a geometric prior to model motion information across frames. Shared latent codes anchored to the human poses are optimized, so that they integrate information across frames. However, a model that only formulates latent codes in a shared space will not generalize well to unseen poses without test-time optimiza-tion of the latent codes. To address this, we propose to model the appearance information by utilizing single-view depth information obtained by a depth estimation network. Our model learns the appearance code anchored to incomplete point clouds in the 3D space. Point clouds are obtained by using single-view depth information to lift the RGB image to the 3D space, which provides partial information of the visible parts of the human body. The learned implicit representation enables reasoning of the unknown regions and complements the missing details on the human body.

To further leverage the temporal information from multiple frames, we introduce a temporal transformer that aggregates the trackable information. We utilize the parametric body model to track points from the query frame to the key frames. Following that, based on the learned implicit representation, we extract the pose code across frames and feed it into the temporal transformer for feature aggregation. Our method is extensively evaluated against state-ofthe-art techniques on several sequences of humans in motion and exhibits significantly higher rendering quality under new views and unseen poses. In addition, we reconstruct fine-level details such as cloth wrinkles, hand details at a resolution and fidelity that several prior top-performing methods such as NeuralBody [35] or HumanNeRF [46] fail to recover (Figure 1). The contributions of this work are:

• A new novel view synthesis framework that shows significant improvement on unseen poses compared to existing video methods, with high-fidelity reconstruction of fine-level facial, cloth and body details. • We combine pose and appearance representations by modeling shared information across frames and specific information at each individual frame. These two representations help generalize better to novel poses compared to only utilizing the pose representation. • A temporal transformer is introduced to combine information across frames, which helps to recover nonvisible details in the query frame (at unseen views).

# Related Work

3D Neural Representations. Early 3D shape representation works can be classified into three categories: pointbased methods [1,37], voxel-based methods [5, 47] and mesh-based methods [2,13,50]. Implicit representations are then used to represent shapes by reconstructing a continuous surface geometry, which utilizes the spatial coordinates as the input and outputs the signed distances or occupancy values. With advances in differential rendering methods, geometry and appearance can be learned from multiview observations. Related works can be categorized into static [26,29,40,41,52,51] and dynamic scenes [4, 8,9,10,15,17,20,22,31,32,33,34,35,36,39,44]. Static Scene Representations. SRN [41] represents scenes as continuous functions that maps 3D coordinates to a fea-ture representation of local scene properties and formulates the image as a differentiable ray-marching algorithm. NSVF [26] utilizes a sparse voxel octree to represent a set of voxel-bounded implicit fields. A differentiable raymarching operation is adopted to render views from a set of posed RGB images. NeRF [29] optimizes a neural radiance field for a scene, which maps 3D coordinates and viewing directions to density and color using a neural network. While NeRF can render photo-realistic images given dense images as input, it is limited mostly to static scenes. Dynamic Scene Representations. Dynamic NeRFs [36,35] extend NeRF to dynamic scenes by introducing a latent deformation field or human poses. NeuralBody [35] proposes a set of latent codes shared across all frames anchored to a human body model in order to replay character motions from arbitrary view points under training poses. HumanNeRF [46] or A-NeRF [42] learn the motion information by combining the skeletal and the non-rigid transformations. For these methods, the synthesis fails under novel poses. Human pose based representation can model the body shape at any time step but will fail to capture detailed appearance. To overcome this problem, we propose to construct the appearance-based representation by utilizing the 2D features anchored to the point clouds as an input. Dynamic Scene Fusion. To model the temporal cues across frames, previous works [7,20,22,48,53] combine motion information and introduce animatable avatar approaches [11,12,16,21,27,28,42,43,45,46,49,55,56]. Some of these approaches rely on keypoints [28], correspondences [21] or vertex normal alignment [49] to generate details. Li et al. [22] learn dense scene flow fields that learn 3D offset vectors from a point in time t to the same point in time t-1 and time t+1. The offsets are implicitly supervised with 2D optical flow. Kwon et al. [20] employ a temporal transformer to integrate skeletal features across different frames. The vertices of the human body are first reprojected to the 2D plane and then image features are sampled to obtain the skeletal features. Although both [20] and our method use a temporal transformer, the way we use the transformer is substantially different. Kwon et al. [20] use the transformer to combine pixel-aligned skeletal features obtained by projecting the vertices to the 2D image plane and then sampled from images using bilinear interpolation. They require multiple views as input as inaccurate features are extracted when projecting the 3D vertices into single view. Instead of combining skeletal features, we propose to use a transformer to combine the pose codes for any 3D point and its tracked points. Our method optimizes the 3D feature volumes and does not require multi-view inputs.

# Methodology

Given a monocular video of a human in motion, we synthesize free-viewpoint videos of the person under novel Figure 2. Overview. Given a query point as input, our method learns pose and appearance codes which can simultaneously integrate shared information across frames and model the appearance information at each frame. The pose and appearance codes are anchored to the human pose and point clouds, respectively. The points clouds are obtained by lifting 2D information to the 3D using the predicted depth. In addition, we use the body motion to track 3D points from the query frame to the key frames and extract the pose code from the key frames. Given the pose codes from the query point and tracked points as input, a temporal transformer aggregates the codes. We use spatial location and viewing direction as extra inputs and train the model to predict the density and color for each 3D point.

views and new poses. During training multi-view videos are utilized to train our pipeline. We denote the set of input video frames as {I t |t = 1, ..., N f }, where t represents the frame index and N f is the number of frames. To avoid the influence of background changes due to the camera movement, we remove the background color with the mask using [23] and only focus on the human in the foreground. The overview of our approach is illustrated in Figure 2.

## Neural Radiance Fields

NeRF [29] represents a static scene as a radiance field and renders color using volume rendering [18]. It utilizes the 3D location x = (x, y, z) and 2D viewing direction d as input and outputs color c and volume density σ with a network for any 3D point:

γ x and γ d are the positional encoding functions for viewing direction and spatial location, respectively. To render the pixel color, NeRF uses the volume rendering integral equation by accumulating volume densities and colors for all sampled points along the ray. Let r be the camera ray emitted from the center of projection to a pixel on the image. The pixel color bounded by h n and h f is given by:

where T (h) = exp(-h hn σ(r(s))ds) denotes the accumulated transmittance along the ray from h n to h. NeRF is scene-specific with known camera parameters, and renders photo-realistic scenes with. To extend NeRF to model dynamic humans, we propose to learn the implicit representation to represent the shape and appearance information of the human. Specifically, we introduce a pose-conditioned representation shared by all frames and an appearanceconditioned representation specific to each frame.

## Pose-conditioned Representation

Following [34,35], we assume the 3D human model is given for each frame (i.e. we use the pre-computed available SMPL body fits or do the body fitting at each frame as a pre-processing step). We first extract the vertices from the posed 3D mesh and aim to learn a set of pose codes Z = {z 1 , z 2 , . . . , z Nm } anchored to the vertices of the human body model. Here N m denotes the number of codes whereas the dimension of each pose code is set to 16 similar to [35]. The implicit representation is then learned by forwarding the pose code into a neural network, which represents the geometry and shape of a human performer. The pose space is shared across all frames, which can be treated as a common canonical space and enables the representation of a dynamic human based on the NeRF. Finally a neural network learns the density and color for any 3D point and volume rendering is used to render per-pixel RGB values.

The pose codes anchored to the body model are relatively sparse in the 3D space and as such, directly calculating the pose codes using trilinear interpolation would lead to less effective features for most points. During our experimen-tal investigation we identified that a SparseConvNet is the right design choice as it propagates the codes defined on the mesh surface to the nearby 3D space. The SparseConvNet encodes the pose codes with N m vertices which correspond to N m pose codes are optimized during training. To acquire the pose code for each point sampled along the camera ray, we use trilinear interpolation to query the code at continuous 3D locations. Here the pose code for the i-th point x i t at frame t is represented by φ(x i t , Z) and is then fed to a neural network to predict the density and color. The pose codes learned in the shared space of all frames model the human shape well in both known and unseen poses. However, the synthesized views still lack details under novel poses without optimizing the pose codes. Hence to model per-frame details, we propose an appearance-conditioned implicit representation using the monocular image and its predicted depth as the reference inputs.

## Appearance-conditioned Representation

An image along with its predicted depth can serve as the appearance human body prior under a single view. To learn detailed information at each individual frame, we learn the appearance code anchored to the point clouds. The point clouds are obtained by lifting the RGB to the 3D space using the depth image which is generated by finetuning a state-ofthe-art depth estimation method [54] on our dataset. In that way the point clouds model the partially visible body of the human performer and capture details such cloth wrinkles. Given a 2D pixel p i t and its corresponding depth value d i t , the point cloud generation process is formulated as

where p i t is the generated 3D point for frame t, and F (•, •) is the function generating a 3D point given a 2D pixel and a camera pose {K t , [R|t] t }. Different from the pose-conditioned latent codes that are shared across all frames, the proposed appearance-conditioned codes are anchored to the point clouds, which are obtained from the pixel-aligned features extracted from the image encoder E. To take advantage of the rich semantic and detailed cues from images, we use ResNet34 [14] to encode the image feature map E(I t ) for the input image I t . Specifically, we first extract features from the ResNet34, that are passed to three Conv2D layers that reduce the dimension followed by a SparseConvNet to encode the features anchored to the sparse point clouds. To obtain the appearance code for each point sampled along the camera ray, we use trilinear interpolation to query the code at the continuous 3D locations. ψ(x i t , E) is adopted to represent the appearance code for point x i t . The appearance code together with the pose code are forwarded into a neural network to predict the density and color. The appearance code learned on each single frame models the details on the human body and recovers some missing pixels in the 3D space.

## Temporal Fusion Module

Frames from different timesteps provide complementary information to the query frame, and will be referred as key frames. A temporal transformer then effectively integrates the features extracted from the query and key frames. To obtain the corresponding pixels in the key frame, we use the parametric body model of each frame to track the points. Point Tracking. First, N a points on each face of the mesh are randomly sampled, which results in N s × N a points on the whole body surface where N s represents the number of faces. We calculate the distance between a 3D point sampled on the camera ray and all points on the surface at the query frame I t . We keep each sample x i t close to the surface for rendering the color if min v∈Vt ||x i t -v|| 2 < γ and obtain the nearest point xi t on the surface at frame I t , where V t is the set of sampled points. Furthermore, we track the points at different frames that match xi t by the body motion, and assign the feature of the tracked points to x i t . Key Frame Selection. We automatically select three key frames from training frames. We first rotate the human pose along the Y-axis by 90 • , 180 • and 270 • and calculate the distances between all training poses and the rotated poses for the query frame S t by ||S t -S j || 2 . We keep the frames with the K-NN distances, where j is the index of the training frames, S are the coordinates of the vertices extracted from the body mesh and K is set to 1 for each rotated frame. Temporal Fusion. After obtaining the pose codes from N frames (K f key frames and one query frame), a transformer based structure [6] is introduced that takes the N features as input and employs a multi-head attention mechanism along with an MLP for feature aggregation. The fusion module is described in Figure 2. Query pose code φ(x i t , Z) is combined with the key frame pose codes by using the attention weight. Here we use f q (•), f k (•) and f v (•) generated by fully-connected layers to represent the query, key and value. The query and the key are used to calculate the attention map using the multiplication operation, which represents the correlation between the query pose code and the key pose code. The attention map retrieves all key pose codes and combines with the value by an addition operation. Formally, the attention weight for point x i t in frame t and tracked point x i k in frame k is calculated by:

where √ d is a scaling factor based on the network depth, and Ω(•) denotes the softmax operation. The aggregated feature for input φ(x i t , Z) is formulated as:

where K denotes the index set of the combined frames. In this work, multi-head self-attention is adopted by running multiple self-attention operations, in parallel. The results from different heads are integrated to obtain the final output. After the self-attention mechanism, each input feature contains its original information and also takes into account the information from all other frames. As such, the information from key frames and the query frame are combined together. Average pooling is then employed to integrate all features, which serves as the output of the temporal fusion module. In our implementation, we do not adopt any positional encoding on the input feature sequence.

## Density and Color Regression

Figure 2 shows the prediction of density and color that are represented by a neural network. For each frame, the network takes the pose code, appearance code, spatial location and viewing direction as the input and outputs the density and color for each point in the 3D space. Similar to [29,38], we apply positional encoding to both the viewing direction d and the spatial location x by mapping the inputs to a higher dimensional space. For frame t, the volume density and color at point x i t is predicted as a function of the latent codes, which is defined as:

where M (•) represents a neural network. γ d (d i t ) and γ x (x i t ) are the positional encoding functions for viewing direction and spatial location, respectively.

## Objective Functions

The objective function of our approach is defined as L = L c1 + L c2 , where L c1 is the reconstruction loss for the rendered pixels and L c2 is the image loss for the image decoder network D. The image decoder comprises multiple Conv2D layers behind the ResNet34, and aims to reconstruct the input image. The reconstruction loss forces the encoder to be optimized and generates better pixel-aligned features. We render the color of each ray using both the coarse and fine set of samples, and minimize the mean squared error between the rendered pixel color Cc (r) and ground-truth color C(r) for training:

where R is the set of rays. Cc and Cf denote the predictions of the coarse and fine networks. Finally, Ĩ(p) and I(p) are the reconstructed and ground truth colors for pixel p in the set of pixels I and are used to compute the image loss:

# Implementation Details

Network Details. For the encoder E, we extract a feature pyramid [24] from each image similar to [52]. A ResNet34 backbone pretrained on ImageNet is utilized for our experiments. The output feature of the decoder has 1/4 the spatial resolution compared to the input image. Multi-scale features are extracted prior to the fourth pooling layer. We extract pixel-aligned features using bilinear interpolation, and then concatenate them to form a latent vector of size 256. To construct the image decoder D, we simply connect several Conv2D/Upsampling layers to reconstruct the input image.

For the depth prediction network, we utilize [54] which takes a single frame as input and outputs a depth map. This network is fine-tuned using our training images and ground truth depths. For inference, the depth map is predicted and is then processed by lifting the pixels to the 3D space to remove the points outside the dilated posed mesh. For the videos without ground truth depths for training, we use the depth map predicted by the NeuralBody [35].

For the transformer network, we utilize three heads for the self-attention module, which has a similar structure as [6]. Following NeRF [29], we perform hierarchical volume sampling and simultaneously optimize a coarse and fine network with identical network architecture. At the coarse scale, we sample a set of M c points using stratified sampling. With the prediction of the coarse network, we then sample another set of points along each camera ray, where samples are more likely to be located at relevant regions for rendering. We sample additional M f locations and use all M c + M f locations to render the fine results, where M c and M f are set to 64. Training Details. We train all layers using Adam [19] with base learning rates for the encoder-decoder network and other layers set to 10 -3 and 5 × 10 -4 , which decay exponentially during the optimization. Additional network architecture as well as implementation details are provided in the supplementary material. It takes about 48 hours using 4 GeForce RTX 3090 GPUs to train our method for 200 frames and 30 views each. The inference time on a single image is ∼50s. Note that fast training is not a primary goal of our work and several recent techniques [3,30] for accelerating our training regime could be used in the future.

# Experiments

Datasets. To train our method, we rely on the proposed dataset (four sequences of real humans in motion that captured with a 3dMD full-body scanner and a single sequence of a synthetic human in motion) and the public ZJU-MoCap dataset [35]. The 3dMD body scanner comprises 18 calibrated RGB cameras that capture a human in motion performing various actions and facial expressions and output a reconstructed 3D geometry and texture per frame. These scans are noisy but capture facial expressions and fine-level details like cloth wrinkles. The synthetic scan is a high-res animated 3D human model with synthetic clothes (T-shirt and pants) that were simulated. Unlike the 3dMD scans, the 3D geometry is clean but lacks facial expressions. We render RGB and Depth for all sequences from 30 views that cover the whole hemisphere (similarly to the way NeRF data are generated) at 6 fps using Blender Cycles. Each video has more than 200 frames of 1024×1024 resolution. For the proposed dataset, we select the first half of the frames for training and the rest for inference. For the ZJU-MoCap dataset, we use the same training frames as [35]. Both training and test frames contain large variations in terms of the motion and facial expressions. At training and testing, a single image at each frame is used as the input. All the input images at different frames share the same static camera pose. In addition, 29 (proposed dataset) or 14 (ZJU-MoCap) more views with different camera poses are used to train the network. The output is a rendered view given any camera pose (not including the camera pose of the input image). Baselines for Quantitative Comparisons.

• NeuralBody [35] models dynamic scenes using latent codes anchored to the human pose as an extra input besides the coordinate and viewing direction. • NHP [20] extends NeuralBody to a generalizable model by aggregating temporal and multi-view pixelaligned features. We remove the multi-view branch with only a monocular video as the input. • Ani-NeRF [34] combines NeRF and 3D human skeletons by learning the blend weight field to recover animatable human models. • HumanNeRF [46] learns a volumetric representation of the person in a canonical T-pose and a motion field that maps the estimated canonical representation to every frame of the video. Evaluation Metrics. Following existing approaches [29,35], we evaluate the performance on the proposed dataset using two metrics, including the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM).

## Experimental Results

We conduct a wide range of quantitative and qualitative comparisons to demonstrate the key contributions of our work along with ablation studies against simplified variants where proposed modules are removed. We evaluate our approach against the aforementioned works on the task of view synthesis for new unseen poses. All methods except HumanNeRF (monocular video for training) use the same camera views and human poses during the training and test stages for fair comparison. Novel View and Novel Pose. We quantitatively evaluate our approach on novel views of all sequences and report our findings at the bottom part of Table 1 andTable 2 1 and Table 2. We conduct this experiment to showcase that our approach generalizes well to unseen poses as the difference between training and testing views is fairly small. On most video sequences, the proposed method performs better compared to all other baselines. ZJU-MoCap Results. In Table 2 we provide quantitative comparisons on the publicly available ZJU-MoCap dataset and show that clearly outperforms all prior work on both training and novel views under unseen poses. Our method benefits from the intra-frame point clouds representation and inter-frame temporal information. Qualitative Results. We qualitatively compare our approach other top-performing methods under novel poses in Fig. 4. With the human pose as the geometric guidance, NeuralBody predicts the body shape well but fails to render fine-level details on the human body. NeuralBody does  Limitations. The temporal transformer recovers more nonvisible pixels in the body. Without encoding facial expressions, our method can handle humans without substantial expression variations. However when the the query frame has a facial expression different from the key frames our method predicts blurred facial expressions as combining the key frames with the query frame makes the network unable to differentiate the specific facial characteristics in the query frame. Future work will encode facial expressions for each frame as a separate code and thus being able to render such diverse expressions under new views.

## Ablation Studies

Effect of the Appearance-based Representation. Using the appearance code brings performance improvements (Table 3 and Fig. 3) on the fine structures (cloth wrinkles, facial  3 and Fig. 3, temporal fusion module can help the model generate better rendering performance. We observe that the details like wrinkles on the pants are finer, the hands are cleaner and the face is significantly more crisp.

Effect of the Number of Key Frames. To evaluate the impact of the number of key frames, we report the performance in Table 3. We observe that the performance increases with more key frames and saturates with 5 frames. Effect of the Depth Estimation. Our proposed approach relies on depth estimation that allows us to lift the RGB input to 3D and obtain the pointcloud that is then fed to our 3D backbone architecture. To identify the impact of the depth estimation module we conducted an ablation study where ground-truth depth is utilized for Sequence 4 of the proposed dataset. We observed that PSNR and SSIM are 24.96 and 0.77 (compared to 24.84 and 0.76) respectively when using ground-truth depth which are just slightly higher than using our depth estimation model. This indicates that even using the inaccurate depth information, our method can generalize well to the unseen poses. For videos without ground truth depth (i.e. ZJU-MoCap data) we use the depth predicted by NeuralBody and despite relying on their estimation, we clearly outperform them (shown in Tab. 2) which showcases the importance of using depth information for the rendering of articulated avatars. 

# Conclusion

In this paper, we built upon recent advances of neural radiance fields pertaining to digital humans and addressed key challenges that existing human body based methods suffer from, preventing them to generalize well to unseen poses. Towards that direction, we proposed to integrate a pose code and an appearance code to synthesize humans in novel views and different poses with high fidelity. The pose code that is anchored to the human pose models the human shape, whereas the appearance code anchored to the point clouds infers the fine-level details and recovers the missing parts. The point clouds are generated by lifting the 2D information to the 3D space using an estimated depth map. To leverage temporal information, we proposed to use the body motion to track points from the query frame to a few automatically-selected key frames and adopted a temporal transformer to aggregate information across multiple frames. The transformer-based fusion module recovers the non-visible part in the query frame. Our approach achieves significantly better results against several prior methods under novel views and unseen poses with quality that has not been observed in prior work. We provided a plethora of experimental comparisons, qualitative results and ablation studies to back-up our claims and we showcase that finelevel information such as fingers, logos, cloth wrinkles and face details are faithfully rendered with high fidelity.

