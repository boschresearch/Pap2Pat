# DESCRIPTION

## TECHNICAL FIELD

- introduce masked autoencoders for computer vision

## BACKGROUND

- motivate deep learning and self-supervised pre-training

## SUMMARY OF PARTICULAR EMBODIMENTS

- pre-train machine-learning model using masked autoencoder approach
- apply pre-trained model to downstream computer vision tasks
- describe embodiments of pre-training and applying models

## DESCRIPTION OF EXAMPLE EMBODIMENTS

- introduce masked autoencoder approach
- describe asymmetric encoder-decoder design
- explain masking criteria and techniques
- illustrate example input and output images
- describe encoder architecture and operation
- explain multi-head attention component
- describe feedforward network and layer normalization
- illustrate decoder architecture and operation
- explain masked multi-head attention component
- describe output and reconstruction process
- discuss model update and evaluation
- illustrate example output images and ground-truth images
- discuss applying pre-trained encoder to other models
- introduce refinement techniques for pre-trained encoder
- describe fine-tuning technique
- describe linear probing technique
- compare accuracy of fine-tuning and linear probing techniques
- describe partial fine-tuning technique
- illustrate example method for pre-training machine-learning model
- describe steps for pre-training model using MAE architecture
- discuss applying pre-trained encoder to second machine-learning model
- describe computer system architecture
- detail processor components
- explain memory and storage components
- outline input/output interface components
- describe communication interface components
- detail bus components
- define computer-readable non-transitory storage medium

