# INTRODUCTION

Research suggests that the most enjoyable in-person interactions involve hanging out with close ties while engaging in leisure activities such as eating, playing games, making music, and sharing hobbies [40]. However, digital devices often disrupt these activities. A common view holds that digital technology interferes with colocated moments in ways that reduce their richness or rob them of meaning-as Sherry Turkle put it, "we'd rather text than talk" [73]. In this work, we explore how we might design digital technology to support, and even enhance in-person interactions by piggybacking on leisure activities that people already engage in.

Researchers have advocated for "piggybacking" when building social computing systems by creating prototypes on top of existing social platforms rather than building something entirely new [26]. We adopt a new approach to piggybacking. We explore "piggybacking" that focuses on the conceptualization of the interaction design rather than its implementation. This involves identifying the core elements and mechanics of familiar activities before augmenting those aspects with novel digital tools. In this work, we explore what it might mean for technology to piggyback on everyday leisure activities that people engage in when in their personal, private spaces with other people and/or with their pets.

We are guided by the following questions:

• How might we foster meaningful, co-located interactions with technology? • What is possible, desired, or valued in technologies designed to piggyback on activities that people already do together? • What are the pitfalls of using technology to piggyback on existing experiences?

In this work, we aim to see if we can use technology to enhance people's interactions when they are physically together with humans or pets they are close to. Augmented reality (AR) is particularly well-suited to this goal: it enables designers to easily extend people's realities in a way that considers their physical worlds [19]. AR can take into account its users, their digital environments, and their physical environments to create experiences that are embodied and contextually relevant. While AR has commercially available specialized platforms (e.g., Magic Leap [44], HoloLens [32]), it can also be experienced through smartphones. Smartphones are particularly true to the notion of piggybacking because they are already part of many people's co-located leisure time [40]. Smartphone AR also allows for spontaneity: if a situation arises that lends itself to AR piggybacking, rather than interrupting the flow of the social interaction to set up a technology that might otherwise be stowed in a drawer or powered off, people can simply reach for the devices already in their pockets. In this vision of mobile AR, reaching for a cell phone while hanging out with a friend might signal, "I have something to add to this moment, and we can experience it together" just as readily as, "Time for me to detach from this moment for a scroll break." Bringing mobile AR to settings involving spending time together can allow researchers to transform people's "digital bubbles" [58,73] 1 into "augmented bubbles" that they can share with others.

This paper introduces three mobile AR apps that we created as design probes. The apps piggyback on three leisure activities: lull moments with peers (Mindful Garden), playing with cats (Compurrsers), and caring for dogs (Petpet). We created these apps for in-person technology use with social partners (people and pets) with whom people already spend leisure time. Two of our apps are intended for a person to play with a pet; here, we subscribe to the notion that pets are co-located social partners with agency, and experiment with giving them control and agency over technology in their space [30,31]. Our exploration of piggybacking allows us to frame the problem of fostering co-located interactions as a form of interdependence [7], a collective effort between all parties involved-people, pets, the environment itself, and the mobile AR app-to enhance the social bonding occasion. Interdependence and collaboration occur when each party contributes something toward an outcome, and those contributions depend on each other. Our piggybacking apps structure the social bonding occasions on which they piggyback such that each agent involved (multiple human players, pet players, and the app itself) does something different to enhance the occasion.

By way of a remote study with 41 participants and 19 pets, the apps we developed helped us uncover insights about the design space of co-located AR designed for piggybacking. The study highlighted the importance of personalized AR experiences and revealed the merits of several design aspects including "AR coupling," i.e., having semantically meaningful relationships between augmented and physical elements, and "AR mementos, " i.e., having persistent digital artifacts that people take from the experience. We contribute design guidelines that AR creators and researchers can apply when creating novel AR experiences. Finally, we discuss future research directions for co-located AR.

# RELATED WORK 2.1 From "alone together" to "together together" with technology

In her book Alone Together: Why We Expect More from Technology and Less from Each Other [72], Sherry Turkle famously described how technology, broadly speaking, draws us apart in the physical realm as it draws us closer together in the digital one. Researchers have described the negative social impacts of "phubbing": when someone uses their phone to do a solo activity (scrolling, texting, etc.) in the presence of others [17,18,23,57]. This has been an issue of debate within the human-computer interaction (HCI) community and beyond it, with studies finding mixed results about the relationship between time spent online and psychological wellbeing (e.g., [13,37,71]), designers innovating ways to foster a sense of relatedness over distance [27], and various authors making psychologically and socioeconomically motivated cases against the demonization of smartphones [4,21].

Less attention has been given to how people use smartphones when physically together. Past research shows that when co-located, people commonly use one or multiple smartphones and other screens in a "together together" way and not just in an "alone together" way: they show each other photos and tell stories about the people and events they depict [41,42,75], they brainstorm together [43], and they co-watch YouTube videos [67]. In some cultures, it is the historical default for multiple members of a family or social group to share a single mobile phone [12,61].

Scholars within the HCI community have convened workshops [2,25,45,62] calling for research that focuses explicitly on the role and potential of technology in co-located interactions. In parallel, research has advanced technology that supports co-located social interaction by adding tangible gestural interfaces to existing colocated games to enhance play [24], improving gestural interaction during co-located interactions with augmented reality through the use of specific visual cues [14], facilitating icebreaker activities [34], and fostering rapport among people who are already physically together in settings ranging from partner intimacy [76] to gatherings of large crowds [74]. A recent (2019) literature review [50] advocated a shift in mentality from designing to enable co-located interaction to designing to enhance it: rather than existing in tandem with or simply functionally supporting conversation, collaboration, or other forms of face-to-face interaction, technologies designed for co-location can play "an active role in deliberately attempting to improve its quality, value or extent. "

Taken together, these perspectives shed light on a rather optimistic view of the role of tech in our relationships: that introducing technologies to the interactions of close ties can enrich what they are already gaining from being physically together. This reveals a need and an opportunity to design expressly for co-location. Work by Dagan et al. [19] reveals several design factors that are important for facilitating co-located AR experiences, but does not explore principles for piggybacking on people's existing co-located leisure activities and does not advance guidelines for piggybacking itself. In the present work, we explore this new use case for AR to inform the design of technologies that can improve the co-located experiences that people already have.

## Piggybacking domains

Per each of our apps' focus, we draw from the literature on animalcomputer interaction (ACI) and human-pet-computer interaction (HPCI), virtual pets, and mindfulness in HCI.

### Cat play and HCI.

Work in ACI and HPCI has explored colocated technology-mediated play between humans and cats [70], [78]. Purrfect Crime [70] and Felino [78] drew on existing humancat play to create new playful interactions between cats and their humans. Purrfect Crime is designed as a virtual bird-catching competition between a cat and their human using a projector, a Kinect sensor, and a Wii remote. In Felino, a human controls the size and movement of sea creatures as they swim by on a tablet for the cat to "catch". Our AR app Compurrsers piggybacks on cats' chaotic behaviors during play. It builds on previous work in that rather than constraining play to a radius, the cat leads the way.

### Dog play, virtual pets, and HCI.

Past research has also investigated technology-mediated human-dog interaction [79], [28]. Canine Amusement and Training or CAT [79] took a serious games approach to teaching humans how to train their dogs using a projector, TV, Wii remote, and sensors. Other work [28] proposed sensors to support accessible toys to foster play important to guide dog teams' wellbeing. EmotoPet [80] was an environment-sensing AR pet that responded to human speech and gestures. Inspired by these works, our AR app Petpet piggybacks on key day-to-day humancanine interactions: giving treats, petting, and calling dogs by their names.

### Mindfulness and HCI.

For the purposes of this paper, we discuss "mindfulness" as a state a person can enter to be more present at a specific moment [11]. Research in HCI explored technologymediated mindfulness for individuals and co-located groups. JeL [65] used a projector, two VR headsets, and breathing sensors to facilitate mindfulness practices based on breathing exercises. Inner Garden [59] used a projector, physical sandbox, physiological sensors, and an optional VR headset to facilitate mindfulness inspired by zen gardening. Building on these, our AR app Mindful Garden piggybacks on lull moments with peers by adding a simple and laid-back mindfulness practice to such moments.

## Shared digital possessions

Designing "piggybacking" interactions for close ties poses the challenge of durability, or replay value. While replayability is not always necessary for a co-located play experience (escape-room-in-a-box games like Flashback [9] have become popular in recent years despite some of them being playable only once), we believe that short-form playful experiences should be able to be returned to without being entirely redundant. We drew on preexisting notions of persistence in embodied playful experiences [6,10], co-creation and interpersonal relationships [60], and digital possessions [49,51] to incorporate "takeaways": each experience delivers as its outcome something that depends on both players' participation and combines aspects of the physical and digital worlds. These "takeaways" allow our apps to repeatedly deliver novelty despite their simplicity: they can be returned to again and again and produce different results every time.

# AUGMENTED REALITY APPS

We designed our three AR experiences through a process of ideation and iteration. First, our team identified aspects of multi-person AR experiences that were under-explored in HCI publications and apps on the market. Inspired by these under-explored design concepts, we brainstormed and created storyboards (see Figure 2) for 45 ideas for co-located AR apps. As we prioritized the concepts, we abandoned some of our ideas, merged others, and iterated on them as they converged. It was through this process-and especially motivated by Abowd's identity (who), activity (what), location (where), and time (when) elements of context [1]-that we conceived of the goal of piggybacking leisure time. Through this process, we also noted the potential of designing AR experiences that are brief in duration and lightweight in requirements, but that persist over time.

Table 1 summarizes how each of our apps was designed to support a type of co-located leisure (i.e., spending meaningful time together, or bonding); the specific activity that was piggybacked; and the role of the players and the app in enhancing that co-located leisure activity through piggybacking AR.

In addition to exploring the notions of piggybacking and persistence, we also aimed to make meaningful use of co-location and AR. To achieve the former, each app featured an interaction that relied on the physical presence of two players in the same space and would not be the same if done remotely or by just one player. To make meaningful use of AR, the apps incorporated semantically relevant information from the physical world into the virtual aspects of the experience (we call this coupling, and explore the impact of it in detail in the Findings and Discussion sections). Our apps were designed as Snapchat "Lenses" and built using Lens Studio [63], an authoring tool for creating Lenses (augmented reality filters) for Snapchat users using visual and audio assets, machine learning models, and scripting.  

## Mindful Garden

Mindful Garden is intended for moments of lull or quiet experienced while spending time with someone else. It piggybacks on these moments by adding to them a short, guided meditation practice consisting of a simple breathing exercise. We chose this activity because mindfulness meditation is a calm interaction with precedent in HCI design (see, e.g., [16,20,53]). Through its prompts, the experience also aims to draw attention to an underappreciated aspect of co-location: noticing one another's physical presence. On-screen instructions prompt participants to observe each other's breathing, listen to one another's voices, and appreciate that another person is physically "there" with them. To underscore the shared aspect, the pair collaboratively "grows" an AR garden that they can both see.

The app instructs each player to focus on their breathing, and prompts them to notice their pulse by placing their fingertip over the rear camera lens. This concept is inspired by smartphone apps that use changes in color to "track heart rate" [52] 2 Covering the lens while growing the garden also prevents players from seeing the full visual of their digital garden atop its physical stage until the mindfulness activity is complete, making for a moment of surprise and delight when the garden reveals itself. The interaction requires physical co-presence in that players are asked to read instructions aloud while their partner closes their eyes and relaxes 3 . The experience is coupled as players are asked to sit on the ground while a virtual garden sprouts around them (as if sitting among flowers in a real garden), blossoming in tandem with the players' sense of inner calm.

Mindful Garden begins by asking the user who opens it ("Player 1") to choose a friend to play with. Player 1 can then choose someone ("Player 2") from their Snapchat friends list Once Player 2 accepts, and the players are connected, the app asks them to look around; when they do, it triggers AR grass to grow on the physical ground. The activity comprises three phases: Player 2 Breathing, Player 1 Breathing, and Breathing Together. First, the app prompts Player 1 to read instructions aloud to Player 2, who is prompted to listen: "We will use this Lens to share a moment of mindfulness with each other and grow a shared garden in the process. We will each take a turn guiding one another through a breathing exercise . . . The breathing exercise goes: Breathe in for 4 counts; Hold for 7 counts; Breathe out for 8 counts. " [77]. Player 1 guides Player 2 through the 4-7-8 breathing exercise which repeats four times. Then, they switch, and Player 1 is prompted to close their eyes while Player 2 guides them through the breathing exercise four times. Familiar with the breathing exercise after doing it individually, the two players then do four rounds of 4-7-8 breathing together. At the end of the breathing together phase, the app tells players to look around at the garden they've grown. By panning their rear camera around the space, they find their garden now contains three different flower types: One representing Player

## Compurrsers

We designed Compurrsers to piggyback on playtime with cats, using augmentation to celebrate the eccentricities of their physical behaviors. Cats have a reputation-particularly online [3,55]-of behaving erratically and acting against their owner's wishes. Compurrsers celebrates, rather than downplays, that well-earned "personal brand" with augmentation that responds to the movement that occurs in human-cat interaction. As a cat runs, jumps, and rolls around, its movements trigger music that plays through the app.

If the cat goes outside the frame during play, the text "looking for cat" and a scanning animation appear on the screen, and the game resumes once a cat is located.

The interaction relies on the physical co-presence of both "players" in that the human chooses to play Compurrsers and points the camera, and the cat moves to trigger the music (via the app's cat detection model). The tune's features are determined by the cat's position in the coordinate space on the screen. The audio "instrument" used is a cat's pitched meow. Compurrsers is primarily an aural experience, deviating from the typical AR mold of visual in, visual out. Leveraging audio allows a human and pet to experience something interesting together, rather than constraining the human's attention to the phone and not the pet.

## Petpet

Petpet piggybacks on three everyday exchanges between humans and dogs: feeding, petting, and calling the dog by their name. It uses a digital pet or "petpet" to accentuate physical displays of affection and attention with a real-life dog. Petpet supports ARmediated human-canine interaction by facilitating persistent play by co-raising a petpet. Both human and dog need to be physically co-present for the app to work. When the user first opens it, they are prompted to find their dog. When the dog is recognized by their phone's rear camera, an egg appears on the dog's head. After a few seconds, the egg hatches and becomes the first form of the petpet-a small, blue, 3D blob with eyes. Over the course of the interaction between the person (or people) and the dog (or dogs), the app uses human hand tracking, dog face tracking, and a dog emotion detection model to determine the next activity. Each interaction triggers a response from the petpet that is intended to mimic what the dog might be doing or feeling. For example, when the dog is fed a treat, the petpet is fed a juicy digital steak, and while the dog enjoys their treat, the petpet emits gold heart emojis while smiling, wiggling, and trilling in delight. If the dog goes outside the frame during play, the text "looking for dog" and a scanning animation appear on the screen, and the game resumes once a dog is located.

Part of the inspiration for this app was our research team's own memories of Tamagotchi [5], the Japanese digital pet toy popularized in the late 1990s. Like how Tamagotchi evolved over many play sessions, the petpet evolved as the app detects humans repeatedly responding to its prompts and interacting with their dogs. The petpet changes physical form after the camera recognizes more human-canine interaction: the small blue baby blob becomes larger and grows a spiky appendage but retains the same eyes. In this way,  the petpet's development is driven by the dog's emotions and their interactions with their human. At the end of each play session, the app saves the petpet's state so it can be loaded for future nurturing.

The human can also take Snaps with their dog and co-raised petpet to share with friends.

Figure 5: Left: Players using the Petpet app with their dog. Right: Screenshots from the Petpet app during a play session. After the dog is detected, the app says the petpet is "syncing with your dog's vibe". In the feed interaction, the user extends their hand, and an AR chicken leg is fed to the petpet in time with the dog getting a treat. Then, the petpet responds by emitting gold hearts (far right).

## Technical Implementation

We implemented our applications using the Snap AR platform and the desktop authoring environment Lens Studio. This allowed us to deploy the applications on Android and iOS as Snapchat lenses. For Mindful Garden, we used Snap AR's "Connected Lenses" functionality [64] to start a shared session for the two players. We used Snap AR's built-in object tracking framework [48] to recognize and track pets [56] 4 . The pet tracker came with world 3D coordinates for the pet's face. For Compurrsers, we projected these coordinates to the floor plane, and calculated their proximity to each of the "music note lines" on the floor (see Figure 4). Below a certain threshold distance to the closest line, a note is triggered, or timed out in sync with the backing track's tempo. To test if a note can be played, we prevent repeat notes from playing (so if the cat lingers in the same position, a note will not play continuously). In our tests and during the sessions with participants, the cat detection technology was reliable.

For the dog emotion detection functionality, we trained a bespoke convolutional neural network that uses an input image of a dog to estimate the probability of certain dog emotions being present. The possible emotions are "playful", "alert/aroused", "appeasing," and "neutral" (i.e., not exhibiting one of the first three). We arrived at this set of dog emotions after consulting animal behavior experts from the Yale Canine Cognition Center [22]. Dogs exhibiting one of these emotions usually display strong visual cues through body language that can be captured and detected in images. To create a ground truth dataset, we hired 30 pet photographers on Upwork.com to take photos of dogs exhibiting each emotion. To better match the real-world application scenario of our model, the photographers were instructed to take pictures using mobile phones. We compiled a dataset of 7,380 images approximately evenly split among the four categories. A team of three expert labelers in our organization created bounding boxes and labels on the images. We trained the labelers with a written labeling guide showing positive and negative examples for each class, and examples of images to exclude from the final labeled set of images. This helped them identify the dog poses that matched the different emotional states. We disposed of the photo if the label did not match the photo category. We used separable convolution with batch normalization as the building blocks of our model (see [33]), reserving 20% of the images as a validation set. Averaging over five different random assignments of labeled images to the training and validation sets, we achieved precision, recall, and F1 scores of 0.91, 0.69, and 0.79, respectively. The distribution of label counts for each class within the dataset was reasonably balanced, with the label counts per class not differing more than 10% from each other. Therefore, we did not implement measures to rebalance the data or add specific weights for the labels of each class. Although we did not do a proper evaluation of the quality of our model, in our tests and during the sessions with participants, the dog detection technology was reliable. Unfortunately, the photo dataset is proprietary and, at present, cannot be made available to the public.

## Participants

We recruited participants for our remote study through social media and word of mouth. For Mindful Garden, we asked participants to sign up in pairs. For Compurrsers and Petpet respectively, we recruited participants who could bring at least one cat or one dog, and, optionally, another person. We aimed to run between 8 and 12 sessions for each app. Challenges with recruitment impacted the number of sessions we ultimately ran: 11 for Mindful Garden (a total of 22 people), 8 for Compurrsers (10 people and 11 cats), and 7 for Petpet (9 people and 8 dogs). This meant that across our 26 study sessions for the three apps, we had 41 human participants and 19 pet participants.

The age range of human participants was 19 to 51 (mean: 27, median: 25) 5 . 25 identified as female and 12 as male. Most were undergraduate students, graduate students, or professional engineers. Other professions included designer, registered nurse, nanny, and paralegal. For the pairs of participants (15 in total), 7 were spouses or significant others, 7 were friends and/or roommates, and 1 was a pair of siblings. 12 pairs lived together, and 3 did not. Cats ranged in age from 1 to 11 years (mean: 4) and dogs from 2 to 11 years (mean: 6). The animals were various breeds, and were described by their owners as being varying levels of active, lazy, or both.

# STUDY PROCEDURE

We facilitated participants' use of one of three apps over a Google Meet video call. First, the one or two researchers present 6 introduced themselves and the study. Participants gave informed consent to participate and be recorded, then scanned a code on their own devices to allow them temporary access to the app via Snapchat. They then tested the app for an unspecified amount of time 7 and told the researchers when they were done. To prioritize cat and dog participants' comfort and agency [31], we conducted tests remotely while pets were in familiar environments with their human(s). We also ended testing when the human perceived that their pet did not want to engage anymore and/or their pet was no longer interested (e.g. walking away and not returning if called). In cases where this did not happen, we ended testing when the pet participants had played all available interactions of the app or when the human participant(s) felt satisfied with their exploration of it. We then conducted a semi-structured interview. Finally, we asked participants to complete a short questionnaire collecting demographic information and additional thoughts. Most study sessions took about 60 minutes (a few were shorter). Although these relatively brief exposures did not allow us to eliminate novelty effects, we prioritized using our time to run more sessions, with more apps, in more contexts, which helped us understand how piggybacking AR might work in many different types of settings and among many different types of players. We ran 2-3 pilot sessions for each app within our organization.

In interviews, we asked questions including how participants felt about their co-participant and/or pet during the experience, the value or challenges they found in the physicality of the experience, and what metaphor they would use to describe the role of the physical device in-app. The post-study questionnaire asked questions about interpersonal closeness, with whom else participants could imagine using the app and under what circumstances, and demographic information. The interview guides and questionnaires for each app are included as supplementary material.

# FINDINGS

We analyzed the interview data using a hybrid approach informed by thematic analysis [35] and affinity diagramming [8]. We discussed the interviews as they were taking place to begin to interpret them and to determine a natural stopping point for the study given the recruitment challenges mentioned earlier. We then annotated and collectively interpreted the interview transcripts, amassing several hours' worth of discussion to formulate and debate our understanding of salient themes. We referred to questionnaire data to supplement the interview analysis and confirm our interpretations of what participants said.

Direct quotes from participants included in this section come from both open-ended questionnaire responses and interviews. We refer to quotes with a letter indicating which app participants used in the study session (M-Mindful Garden, C-Compurrsers, P-Petpet) and a number representing the session ID. Where it is important to include dialogue to provide context for a quote or explain a finding, we use "A" and "B" to distinguish between the participants.

Through iterative affinity diagramming and discussion, we identified seven insights about piggybacking co-located leisure activities with AR. We describe each insight in a subsection below. The findings are summarized in Table 2 for easy reference.

## AR inspires newfound and renewed motivations to play

First, we found that piggybacking AR provided motivation, awareness, and reminders to engage with each other and with the activities of mindfulness and pet playtime. Participants mentioned that by facilitating interactions that supported familiar activities in novel ways, the apps attracted and held their attention, giving them renewed drive to participate in the activity. In Mindful Garden, being prompted to meditate together with someone was perceived as especially meaningful. One participant said, "It's motivating to have someone at your side when you're trying to do better for yourself" (M13). Another talked about how co-location could improve focus: "I think it's good that we're doing it together because I feel like sometimes if you're doing it alone, then maybe you can't really concentrate on that" (M12). Finally, positive feedback from the virtual flowers could encourage replays: "Normally, when you do breathing exercises, you don't see something happen and you just feel good, but now you see a flower bloom, that motivates you to do it every day" (M13).

In Compurrsers, the promise of new and different tunes motivated owners to keep playing with the app, which also meant attending more to their cat. One participant described trying to make sense of and control the tune that was generated: "The music that was in the background, I liked that a lot, it was upbeat. It did motivate me to try and play a little bit" (C2) (this participant used "play" as in "play an instrument" rather than as in "play the game"). AR could also enliven play sessions at the point when the owner has become bored or tired but the cat has not. One person explained: "I will get bored and then he will be like, 'Oh, yes, keep playing, keep playing, " but if I can use the [app] as a way to entertain myself as well while playing with him, maybe he will encourage me to play with him longer" (C8). In Petpet, people mentioned that the need to maintain the petpet's health and help it evolve-something only possible with the active participation of both human and physical dog-could keep them coming back to the app, and, more importantly, back to their dog. As one participant put it, "This is a great tool for getting people to be aware like, 'Oh shit, I have a pet, I should play with my pet, ' and then put the phone down and play with your pet" (P12). The pet apps also encouraged novel kinds of group HPCI. Several Petpet sessions had multiple human participants to begin with; in these sessions, individuals reacted not only to the petpet, but also to each other's reactions to the petpet, which often resulted in laughter and banter. In a handful of cases, multiple members of a household (e.g., partners, roommates) signed up for Compurrsers and Petpet sessions. In these cases, people sometimes passed cell phones back and forth during the session and joined ongoing interactions (e.g., treat-giving) partway through, making the experience more social and more lively for all involved.

## Participants had platform-driven expectations about what mobile AR experiences entail

We noticed that participants brought different "baggage" that reflected preconceived notions about what phones are meant to be used for. Many expected the apps to be gamified or competitive, and some were thrown off by the lack of emphasis on "snappable" moments. This was most prevalent in Mindful Garden-possibly because it involved two players. Several participants (M7, M10, M12, M14) mused about adding a competitive element to the garden. In one case, this led to a debate about whether this was or was not in the spirit of the app: "I guess I was trying to go and see whether we could see whose flowers were more at the end of the day. " (M10A); "Then it becomes competitive. This is a mindful game, though. It's not about the number of flowers. It's just about growing flowers" (M10B). The use of Snapchat as a platform also affected expectations about what the apps were "for". One participant who was familiar with Snapchat Lenses asked about Mindful Garden, "But why is it a Lens? It didn't want anything from me? I would never be like, let's open Snapchat and open a Lens to help us meditate" (M8). These assumptions and preferences about the "game" being "winnable" raise the question of how piggybacking AR can be designed to get away from the notion of "mobile app means gaming means trying to win. "

## Augmentation illuminates information that is otherwise hidden

Participants commented that the apps allowed them to connect in new ways, using information about one another that would be either unavailable or unattended to without the augmentations. In Mindful Garden, participants appreciated that they could interact with a familiar person, in a known space, in a new way. The app brought a novel sensory experience to a familiar activity: the sight, sound, and touch sensations of sitting near someone and sharing a mood and mindset were embellished with a new visual that embodied their sense of focus, calm, and co-presence. In M8, a participant said: "[It was] like we were giving each other a little gift of mindfulness. " In Compurrsers, participants liked that the app gave the cat the reins: "Pets only have so many ways to communicate with you, so if the app can be used as a way for my cat to communicate with me or present some new human-cat interaction, I'm definitely down for the cat to take the lead" (C9). Cats sometimes responded negatively when they heard the musical meows-in C10, a cat was terrified of the sounds and ran away from her owner each time she tried to restart Compurrsers. 8 In Petpet, participants commented that the petpet gave them new reasons to give their dog attention and affection. They found joy in the prompts to both focus on their phone and check on their dog: "Trying to get your dog's vibe was just really cool and it really sucked me in because I was like, 'What is my dog's vibe?' [chuckles] Feed time sounds about right" (P6). When the dog emotion detector consistently read "appeasing", the human felt that the "vibe sync" was accurate for the dog's mood, and enjoyed tuning into and acting on the desire for treats via the petpet.

Participants appreciated that both pet apps added new channels for engagement and communication without asking too much of the players. They enjoyed that cats and dogs could exist in their natural habitats-playing in their usual ways and not themselves needing to acknowledge the technology-while people could see, hear, or feel something new, brought about by the technology. For example, C6 said, "We never get to play with our cats and our phones . . . it's really fun to mix the technology with the pets . . . since they're not looking at the phone it was a lot more intuitive for them. " (C6).

## Holding a cell phone impacts the user experience of piggybacking

Our apps faced the dual challenge of justifying the presence of mobile devices in existing interactions and then deploying them in a way that did not detract from the intended experience. We found mixed responses as to whether the physical setup and use of the phone was a help or a hindrance in terms of appreciating the piggybacked occasion.

In Mindful Garden, participants were prompted by the on-screen instructions to focus on their breathing, listen to one another's voices, and meditate together. However, some participants ended up more focused on the physicality of interacting with their phones. Some were distracted by the physical discomfort or awkwardness of holding it. Others did not want to touch their camera lens ("I don't like leaving fingerprints" (M5)), or found that their choice of phone case made touching it difficult. The requirement of two phones also had pros and cons. In M10, participants compared their gardens when they looked at each other's screens, and one person said that their partner's was "so much prettier". They discussed how the temptation to compare the two screens made it harder to buy into the promise of a moment of mindfulness that was distinctly theirs. In M6, however, a participant stated that they liked being able to compare gardens, even if they ended up different. They appreciated the novelty: "I never really look at someone else's phone as much".

In Compurrsers and Petpet, participants reflected on the differences between "natural" (quote marks are our own) human-petphone configurations that they typically find themselves in (e.g., photographing a cat while it is asleep, taking selfies with a dog) and the "unnatural" human-pet-phone configurations required by the apps. C10 hypothesized that her cat did not like when her cell phone physically came between them: "she's seeing this big black box, and it's making scary noises . . . it became like the thing she wanted to avoid." P9 said they'd like to be able to take a selfie with the dog and the petpet together, and P9 felt that because the app used the rear-facing camera-instead of the front-facing camera, which they use to take selfies with their dog, which requires getting physically close-the dog wasn't involved enough. In these cases, drawing attention to a dog's physicality was not satisfying enough; people are used to having their dogs physically near when interacting with them and wanted that to be the case for this app too.

Across all three apps, participants had more difficulty paying attention to the physical presence of their partner when a design aspect or technical glitch reminded them that their phones were also, in some sense, a physical, co-located party. Successful connections were made when participants saw partners through the phone screen rather than seeing them on the phone screen.

## Participants wanted AR to not just augment moments but make them persist

In all three apps, participants talked about continuity across multiple plays. They also desired ways to save and download mementossomething by which to remember a piggybacked moment in time. Mindful Garden's participants suggested that being able to return to the same garden would make the experience substantially more meaningful. For example, one participant (M8) said: "I would like it if it wasn't just a one-time thing because . . . I feel like everyone could get into it. Like hey, the more people participate, the better, bigger your garden grows, and then over time, people can visit it." Participants were especially focused on the notion of downloading, saving, and sharing something after trying Compurrsers. One participant talked about how the music from the app could be an addition to the videos they already share with their friends of their cat doing different things: "[I'd] probably send it directly to people if I wanted to crack them up or share, 'This is what [my cat] was up to today"' (C10). Another reflected on how the combination of an interaction designed specifically for a person and a pet, a persistent artifact, and the fact that these were designed for a cell phone-something they already have and use-made the app feel like something special: "Now it feels like I could actually include my pets in like doing tech stuff, things that have to do with technology and I got to save the videos . . . If it's free and then you get to save it onto your phone, it's just like you're doing something like virtual reality or AR and you don't have to go searching for something like this" (C8). In Mindful Garden and Petpet, participants talked about wanting to take screenshots and videos of flowers growing and dogs and petpets receiving treats and cuddles to look back at later or share with friends.

## Tightly-coupled digital and physical signals bring about discovery and enjoyment

Our apps explored a few different approaches to tightly-coupled semantic relationships between physical and virtual aspects of AR experiences. We found that people appreciated when they could easily connect and interact with connections that the apps made between the physical and digital worlds. For one participant (C2), figuring out the connection between the cat's movements and the resulting sounds was part of what made Compurrsers feel engaging and fun: "I'm like, 'I want to do something different now. I want to experiment. Oh, this noise happened. Why did it happen?"' One participant, P11, was entertained by the way the petpet's state mirrored their dog's: "I think the reactions were pretty good, because, I feel like sometimes when [my dog] wanted another treat and I didn't give it to him right away, it had those exclamation marks saying like, 'Feed me, feed me. ' [laughs] I think they really made sense. It mirrored what [my dog] wanted or liked, or did not like. " Another, P9, enjoyed "troubleshooting" the input-output relationships in Petpet together with their dog: "I don't usually try to sit down and figure something out while when my dog is trying to play with me, that was nice" (P10).

On the other hand, when participants did not understand the relationship between physical triggers and augmentations, they did not grasp or appreciate the piggybacking as much (or at all). This was most obvious with Mindful Garden, where participants searched for a relationship that they could not find between what they were doing and the growth of the flowers. One participant of M7 explained that even though "it was nice to see a tangible outcome", the garden was lacking "an intuitive connection with how much the flowers would blossom. " Participants also commented on forms of temporal and spatial coupling: they wanted the garden to grow gradually rather than suddenly, and they wanted to link the garden to a physical place via GPS. In Compurrsers, some participants said that their inability to meaningfully connect the sounds to cat-generated inputs was a negative aspect of the experience. Some Petpet participants could not readily identify the reasons for the petpet's responses to the dog's movements. This made the experience feel more like a puzzle for a human to solve alone than a piggybacking embellishment on leisure time for humans and dogs. For example, P8 "felt like the petpet mimicked my dog's emotions and stuff when I scanned it, but then after that, it felt like just another pet that I was taking care of. I feel like when I was interacting with the petpet, I didn't really interact with my dog as much. "

## Participants wanted personalization linked to identity and preferences

Participants thought that piggybacking AR should allow people to choose settings and avatars that represent their personalities, relationships, and preferences. This could both enrich individual app plays and make multiple plays feel like a continuous, personal experience.

In Mindful Garden, participants discussed how they might be drawn to the garden by the promise of special flowers for special points in time, like an "anniversary flower" or "flowers for each year" (M6). Some wanted to tie the garden to a physical location in space using GPS or object landmarks (as mentioned in Section 5.6) so that it could only be returned to by the two people who created it Participants wanted personalization linked to identity and preferences when they were together in that space. Participants also discussed how being able to customize the colors and types of flowers in the garden would make it feel personal and more like the product of a shared experience between them. M10 said that they would have liked to see "a statue or something, a bird that we both see-then we could know for a fact . . . that it's the same garden". In Compurrsers, many comments about personalization and customization centered on the possibility of the music reflecting the human's musical creativity or preferences. C5 said, "if there were a different instrument like tones or more like precise movements you can make, then feel like I could actually really enjoy something like this." C9 said: "As someone who just revels in chaos, there's not enough technologies out there that just produce chaos so I'm always excited to see that." C9 also suggested that the music could reflect the cat's mood or personality: "Since my cats are being so lazy right now, it'd be nice to have a low-key waltz [chuckles] or meditative music to match their lack of activity." In Petpet, participants commented that the more the petpet's behaviors reflected characteristics of the dog (its appearance, age, personality, emotions, or needs at a particular time), the more significant the relationship between the dog and the petpet would be. For example, one participant said, "Not all dogs look the same. If it shows up looking like a golden retriever and I'm playing with my completely black, total mutt of a dog, it's not going to look the same, " (P7), and another said, "[If] every time he even had like a tiny piece of kibble he would lick his lips afterward, it'd be cute" (P10).

In sum, people want a signal that the app is doing something for the specific people and pets who experience it over repeated interactions. In broad terms, HCI touts the value of having easilydiscoverable options beyond the default as a way of designing for individual users' abilities, preferences, and patterns of use. In AR applications, customization can serve this purpose, but it can also do more: it can embody the notion of "This reflects something that I am doing, here, with you, now" in the design, which is central to the experience that "piggybacking" seeks to provide.

Table 3: Summary of our design implications for supporting piggybacked co-located leisure via AR, along with the finding(s) from which each is derived. The findings are summarized in Table 2 and the design implications are elaborated in the Discussion.

# Design implication

Finding(s)

Calibrate expectations and mood by setting the tone within the app or experience.

# F1, F2

Find meaningful and delightful ways to piggyback physical inputs by semantically connecting them to digital outputs.

# F1, F6

Emphasize physical co-presence and human and pet embodiment.

# F3, F4

Afford personalization, persistence, and memento-making.

# F1, F5, F7

Avoid blocking natural human-pet spatial configurations and interactions.

# F2, F4

Be wary of device and design elements that may cause physical discomfort.

# DISCUSSION

Our study revealed several insights about how AR technology can enhance rather than detract from everyday leisure time that people share. In this section, we interpret those insights to pose design implications for creating AR experiences that piggyback on close ties' existing interactions. We structure these around our three research questions. The design implications and the findings from which they are derived are summarized in Table 3.

6.1 How might we foster meaningful, co-located interactions with technology?

6.1.1 Calibrate expectations and mood by setting the tone within the app or experience. In our study, introductions were useful for "getting into the zone", or the mood on which the apps aimed to piggyback. In Mindful Garden, the first player to read aloud needed time to process the instructions and get used to verbally leading their partner, and both players needed time to get into the headspace of quietness and mindfulness. In Compurrsers and Petpet, the lack of an introduction sometimes broke flow: participants had to figure out what the app expected of them and their pets, and then had to get situated to deliver that, which sometimes meant rousing their cats from sleep or getting treats and toys. People sometimes had miscalibrated expectations about what the app would ask (Section 5.2), and enjoyed the experiences more once they had adjusted their expectations to fit the app's intended "vibe". Therefore, we recommend that designers of piggybacked experiences incorporate a "burn-in" phase: an interaction that helps the users understand the app's intent and sets a tone for the experience to come.

6.1.2 Find meaningful and delightful ways to piggyback physical inputs by semantically connecting them to digital outputs. Co-located leisure activities are multi-modal experiences. They leverage a variety of our emotions and senses including vision, hearing, touch, and smell. In this paper, we explored multiple ways of mapping physical inputs (changes in the observable physical world) to AR outputs. Petpet, for example, leveraged pets' emotions as an input, amplifying this unique aspect of users' physical experience and visualizing that aspect via a digital pet. We found that participants strongly resonated with how extensively the apps integrated their physical context into the augmented experience (Section 5.6). Participants were excited about Compurrsers' auditory focus, which allowed their cats to experience it as well (they heard and reacted to the sounds) (Section 5.3) and challenged their existing assumptions about what AR apps are. We recommend that piggybacking AR apps make deliberate use of coupling in mapping physical-world inputs to augmented-world outputs in multiple modalities. Previous work in AR has leveraged semantic associations between virtual and physical elements in a user's work environment [15]. Work in VR interaction design from the disability community such as Canetroller [81] and Acoustic Minimaps [46] has made strides in augmenting multiple modalities. Future piggybacking AR apps should continue to go beyond the visual, and could even draw on unnoticed or "invisible" inputs such as emotions or biosignals [39] that are already present in users' leisure activities.

6.2 What is possible, desired, or valued in technologies designed to piggyback on activities that people already do together?

6.2.1 An emphasis on physical co-presence and human and pet embodiment. Where our apps succeeded in drawing out physicality, participants expressed joy and understanding; where they did not, participants expressed confusion and felt that phones were barriers to rather than facilitators of in-person interactions. For example, participants enjoyed when they attended to one another's voices and breathing patterns when growing their gardens (Mindful Garden). Future versions of Mindful Garden could potentially detect lulls or slow moments between friends-opportunities to appreciate each other's presence, without the distractions of other topics or activities-and insert its augmentations into that moment, turning the mundane and ignored into something that can be noticed and valued. Participants also appreciated the emphasis on physical human-pet interaction, expressing delight when they were encouraged to touch their dogs to help the petpet evolve (Petpet) (Section 5.3). We also identified obstacles to drawing out physicality: preconceived notions about what to expect when opening a Snapchat Lens (Section 5.2) and instances of weak coupling between digital outputs and physical inputs (Section 5.6) made it harder for people to notice or appreciate how the apps highlighted physical co-presence. We therefore posit that a mobile AR app's success or failure in piggybacking physicality hinges on whether the phone directs attention toward physical co-location or away from it (Section 5.4). This tracks with related work in digital game design: in Frolic, a mobile game that encouraged young girls to engage in physical play, the phone functioned as a facilitator so that it would not be a distraction [36]. Future work should examine how design elements of AR apps and AR-enabling devices can highlight or undermine physicality. Here, there is an opportunity to draw on research that examines how VR experiences can foster a sense of embodiment in technology-mediated environments [38].

6.2.2 Affordances for personalization, persistence, and mementomaking. Participants felt that AR had to piggyback on the interaction between them, then and there-and not just anyone, anywhereto avoid being noise and instead be special. They also wanted more ways to control the outputs of the apps both during play (Section 5.7) and after (Section 5.5). Our findings about preservation through selfexpression and preservation through mementos are interrelated: by giving users something personal to save, apps can create personal and even sentimental souvenirs rather than generic participation trophies. We therefore recommend that piggybacking AR apps give participants personalization options and mementos that last beyond the end of the immediate experience. This aligns with past work about how physical artifacts generated during computer-mediated play are valued after play [66] and how digital artifacts from gameplay experiences can have lasting value [69]. In future iterations, Mindful Garden could let participants choose their own flowers and activate only in specific locations, Compurrsers could allow people to pick instruments and background music that reflect the cat's mood, and Petpet could offer petpet avatars that resemble the live dogs. Personalization is especially important for piggybacking; aspects of how people engage in leisure activities without AR can (and, in fact, need to) make their way into AR to make the piggybacking feel genuine. For example, if a person usually meditates near a cactus plant, then their garden could have cacti instead of tulips. If a person usually plays the ukulele and films their cat's responses, their Compurrsition could sound like a ukulele.

# 6.3

What are the pitfalls of using technology to piggyback on existing experiences?

6.3.1 Blocking natural human-pet spatial configurations and interactions. In Compurrsers, participants valued how easily they could integrate the phone into the piggybacked interaction. They went from simply watching their cats (and occasionally petting or throwing something to them) to co-creating music while watching their cats-and the phone enabled this without getting in the way too much. In contrast, in Petpet, it was more difficult to manage holding phones, physically interacting with dogs, and monitoring the dogs' responses all at once (Section 5.4). The design implication we take from this is: technologies that piggyback on playtime with pets should be used in ways that do not interfere with humans' ability to look at and touch their pets. Designers of piggybacked experiences should start with scenarios that already involve a phone and a pet (such as taking selfies, filming a cat zooming around, or trying to create viral videos) and build off of those to create novel interactions that work with pets' existing behaviors. Forcing a device into an interaction in which the device is usually not involved at all will likely detract from piggybacking. Prior HPCI innovations helped humans and animals find joy when they could interact with the technology together (vs. just the human using the device) [70,78].

Our design recommendation adds to this the idea that HPCI can work well when it allows pets to do what they already do while also giving humans something new to appreciate without monopolizing the humans' attention. We suggest that future work continue to explore how mobile AR technology might unobtrusively facilitate cross-species play at a low cost and with minimal setup.

6.3.2 Causing physical discomfort. Participants were uncomfortable using Compurrsers and Petpet for more than several seconds at a time due to these apps' requirement that phones be continuously held up (Section 5.4). Different people who use piggybacking AR apps will have different body types and tolerances for the physical movements that apps involve. We draw from this the following design recommendation: keep piggybacking mobile AR apps lightweight and short to make them more accessible and comfortable for more people. Physical discomfort also arose when players were asked to do unnatural things with their phones, such as when they placed their fingers on the camera lens and dirtied the Lens when playing Mindful Garden. Whenever a piggybacking augmentation requires hardware components to serve unusual purposes or be put in unusual positions-e.g., players placing their phones face-down on the floor or touching their camera lenses-it is especially important for designers to account for physical comfort. One way to do this is to make these "unnatural" interactions part of the app's narrative. As a retrospective example, Mindful Garden may have been more effective if the experience concluded with screens that instructed participants to wipe down the camera lens and stand up and stretch. Likewise, Compurrsers could have limited the length of each Compurrsition to a certain number of seconds. In general, AR designers should choose design metaphors that allow the AR device to set the stage and then get out of the way, making itself truly "visible" (noticeable) again only to provide structure and guidance when needed. Work in AR/VR has considered how to incorporate natural, expressive gestures for AR/VR interactions [54] and examined the opportunistic design of tangible AR interfaces [29]. Some AR technologies that use headsets, such as ARcall [68] and Friendscope [47], have leveraged this approach to provide novel experiences without asking users to perform unusual physical actions. We emphasize that designers of mobile AR apps need to account for this too, even though the platform they are working with appears commonplace and convenient to hold and use.

## Limitations

Our Lenses had some technical drawbacks: the flowers in Mindful Garden did not always perfectly align with the floor, and the dog emotion detector only had four classes. We do not believe these detracted from participants' experiences aside from being minor inconveniences. In our study, each of 41 participants briefly played with one of our apps. As discussed in Section 4, this approach allowed us to reach more participants than we would have been able to with longer or repeated sessions, and we believe it resulted in novel and valuable insights for piggybacking AR. We acknowledge, however, that giving participants only a single encounter with the apps does not give us a sense of how people would engage with them over time. Future work could expand on this research with longitudinal studies focused on understanding the influence and implications of piggybacking AR over time. While we were able to solicit feedback on our apps from people of many different ages and backgrounds in a variety of physical settings, our sample size was still relatively small, and our participants self-selecting; our insights may not reflect more diverse perspectives. Our interpretations of what participants said about our apps and what this means for piggybacking AR are also limited by our own perspectives. Finally, we only probed three co-located interaction scenarios, all of which were social in nature. There are other categories of meaningful colocated interaction-e.g., professional, educational, or health-related settings-that we did not address. Future work could investigate piggybacking AR in these other contexts.

# CONCLUSION

In this research, we explored the potential for augmented reality technologies to contribute to co-located leisure time by "piggybacking" on naturally occurring scenarios among people and pets. We defined piggybacking as a way for social AR to embellish existing interactions by adding something to what people and pets are already doing rather than asking them to do something entirely new. We developed three AR applications that "piggyback" on activities that people already do with their close ties. The first, Mindful Garden, piggybacks on sharing quiet moments with peers (and, to an extent, practicing mindfulness). The second, Compurrsers, piggybacks on playing with cats, and highlights their "brand" of independence and chaos. The third, Petpet, piggybacks on playtime with dogs, and emphasizes attentive human-dog interaction. We shared the apps with 41 people and 19 pets with the goal of understanding what it takes for an AR technology to genuinely piggyback an existing co-located situation (rather than introducing a new activity). Through observations and interviews, we identified several new insights relating to how technology can piggyback on commonplace interactions in engaging and meaningful ways. Based on these insights, we pose six design implications for creating piggybacking AR technologies. Our work contributes novel findings about co-located multi-user AR, three new apps designed to support meaningful AR-mediated interactions among people and pets, and several design guidelines for piggybacking AR.

# ACKNOWLEDGMENTS

We thank all of our participants for their time and feedback. We are grateful to Jack Tsai and our colleagues for their contributions to this project. Katie, Indy, Grampa, and Desmond contributed invaluably to the inspiration and development of the pet apps. Funding was provided by Snap Inc. The first author is also supported by a NASA grant (80NSSC19K1133).

