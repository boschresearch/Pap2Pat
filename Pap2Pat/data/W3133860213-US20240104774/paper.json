{
    "id": "https://semopenalex.org/work/W3133860213",
    "authors": [
        "Slobodan Ili\u0107",
        "Sergey Zakharov",
        "Ivan Pavlov",
        "Ivan Shugurov"
    ],
    "title": "Multi-View Object Pose Refinement With Differentiable Renderer",
    "date": "2021-04-01",
    "abstract": "This letter introduces a novel multi-view 6 DoF object pose refinement approach focusing on improving methods trained on synthetic data. It is based on the DPOD detector, which produces dense 2D-3D correspondences between the model vertices and the image pixels in each frame. We have opted for the use of multiple frames with known relative camera transformations, as it allows introduction of geometrical constraints via an interpretable ICP-like loss function. The loss function is implemented with a differentiable renderer and is optimized iteratively. We also demonstrate that a full detection and refinement pipeline, which is trained solely on synthetic data, can be used for auto-labeling real data. We perform quantitative evaluation on LineMOD, Occlusion, Homebrewed and YCB-V datasets and report excellent performance in comparison to the state-of-the-art methods trained on the synthetic and real data. We demonstrate empirically that our approach requires only a few frames and is robust to close camera locations and noise in extrinsic camera calibration, making its practical usage easier and more ubiquitous.",
    "sections": [
        {
            "title": "I. INTRODUCTION",
            "paragraphs": [
                "O BJECT detection and 6D pose estimation in RGB images are among the most fundamental problems in computer vision, with applications encompassing autonomous driving, augmented reality and robotics. A large body of work has already been presented, but recent advances in deep learning have opened up new horizons for RGB-based algorithms, which have now started to dominate the field. However, precise 6 DoF pose estimation, required, in autonomous robotic grasping systems, for example, still remains a challenging problem. This is mainly due to the perspective ambiguity, lightning changes, clutter and occlusions. Current industrial implementations [1] are not based on deep learning and rely on depth data for increased pose accuracy. Moreover, these approaches use 3D models directly, while deep learning methods still struggle with the domain gap when trained on synthetic data rendered from 3D CAD models. Recently, CosyPose [2], a deep learning method that operates on RGB images, assumed the lead over traditional methods in the BOP challenge [3], especially when multiple images are used for pose refinement.",
                "The task of pose refinement has also been addressed by deep learning approaches. Methods like [4]- [6] trained CNN networks on pairs of images, the aim being to learn to predict the pose offset between the predicted object pose and the observed detected object. There, 3D models were rendered in the predicted pose and real images were given as patches with the detected object. The main disadvantage of these refiners is that the are dependent on the correct pose error priors used during training coupled with the necessity to re-train them for each new object in order to obtain high-quality results.",
                "We address the aforementioned problems in this paper by introducing a multi-view refinement procedure. We first train the DPOD detector [4] on synthetic data to overcome the dataset bias and the lack of real training data. We then exploit geometric multi-view constraints during refinement to cope with perspective ambiguities and occlusions in monocular RGB images that cause their sub-par performance. Compared to existing deep learning refiners [4]- [6], the proposed approach has the following advantages: 1) it does not require training of the refiner itself; 2) it is not object-specific; 3) it can be applied to arbitrary many images without the need for any modification; 4) it has explicit geometric constraints and, thus, an explicit objective function which is optimized during the refinement.",
                "The proposed pose refinement procedure is based on differentiable renderering. It uses multiple views to add relative camera poses as constraints to the pose optimization procedure. It is done by comparing predicted and rendered dense correspondences in each frame and then transmitting the error back though the differentiable renderer to update the pose. The proposed loss function allows varying numbers of frames to be used without any changes to the optimization procedure. The loss formulation also does not impose any restrictions on where in the 3D world the cameras are placed and whether or not views from different cameras overlap as long as the object is visible in the images. We assume the availability of relative camera poses. In practice, they can be easily obtained by a number of various methods, such as placing the object on the markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard. In the scenario of robotic grasping, a camera can be mounted on the robotic arm to enable observation of the object from several viewpoints.",
                "We evaluate this approach on LineMOD [7], Occlusion [8], Homebrewed [9] and YCB-V [10] datasets and report performance that is superior to any related method trained on synthetic data and similar to or better than methods which use real training data and post-refinement. Our experiments Fig. 1: Multi-view inference and pose optimization. 1) Inputs to the algorithm are an unordered set of images and corresponding relative camera transformations. 2) YOLO is applied to each image separately to detect the object of interest in each of them. 3) Dense correspondences are predicted with the DPOD network. 4) Rough object pose in the reference frame is estimated using PnP and RANSAC using the predicted 2D-3D dense correspondences. 5) The final pose is iteratively refined using the multi-view optimization based on differentiable rendering.",
                "show that our approach can robustly perform multi-view pose refinement even when relative poses are imprecise. Our results also demonstrate that the proposed refinement remains effective even in degenerate cases, when cameras are in close proximity to each other.",
                "Further, we show that our framework can be used for the task of auto-labeling real data as in [11], thus removing the need for manual pose labeling. The networks are first trained on synthetic data and then used to label the real images. This procedure enables a considerable reduction in the time and effort needed to annotate the data. Our multi-view refinement pipeline enables us to automatically produce high quality pose annotations for these real images and re-train the detector on them."
            ],
            "subsections": [
                {
                    "title": "II. RELATED WORK",
                    "paragraphs": [
                        "This section first discusses the state of the art of monocular pose estimation, before going on to review popular pose refinement techniques and discuss methods that use multiple images for object detection and pose estimation.",
                        "6 DoF pose estimation SSD-6D [12] was one of the first deep learning methods for 6 DoF pose estimation from RGB images. SSD-6D extends the off-the-shelf SSD object detector and was fully trained on synthetic data. Since the method is based on learning discrete poses, it requires refinement to obtain reasonable results. Subsequent approaches predicted a fixed number of 2D keypoints, whose locations on the 3D model are known. Using those 2D-3D correspondences, the full pose is estimated using a PnP algorithm. BB8 [13] proposed predicting 2D projections of the 3D bounding box corners. In contrast, PVNet [14] used per-pixel Hough voting to allow all object's pixels to vote for the few keypoints lying on the object. HybridPose [15] extended correspondence prediction by predicting edge vectors and symmetry correspondences, which were then used jointly in an augmented PnP algorithm. The concept of 2D-3D correspondence estimation was further extended in the methods that rely on dense correspondences: DPOD [4], Pix2Pose [16], CDPN [17] and EPOS [18]. They predicted a 3D correspondence for each foreground object pixel, thus increasing the number of correspondences. AAE [19] used an off-the-shelf 2D object detector trained on real data and an autoencoder, trained on sythetic data, followed by feature matching to predict discrete object poses. PoseCNN [10] relied on Hough voting to locate the 2D projection of object's center and distance from the object to estimate the translation. Rotation was directly regressed as a quaternion.",
                        "Of the aforementioned methods, only SSD6D, DPOD, AAE and EPOS reported results on synthetic train data in the original papers, though for some of them the results on synthetic data were later submitted to the BOP challenge [3].",
                        "6D pose refinement. Object pose refinement has been studied thoroughly in the past. Iterative Closest Point [20] (ICP) is one of the most classical and used approaches. ICP refines a given pose iteratively by establishing one-to-one correspondences between the point cloud and object's vertices based on their spatial proximity and then minimizing distances between them. There are various variants of ICP and ICP-like algorithms, but their common fundamental limitation is that they require reliable registered depth information, which is not always readily available. It restricts their applicability in many scenarios, for example, on images from mobile phones Fig. 2: Example refinement results on the Homewbrewed dataset [9]. The top row shows initial per-frame poses produced by PnP before refinement, while the bottom row shows them after refinement. The outline of the object is visualized in green for the ground truth pose, and in blue for the estimated pose. This illustrates that the proposed refiner is capable of selecting a reference frame with a good initial pose and refining it even in the presence of occlusions and imprecise correspondences or when some of the initial pose hypotheses are completely incorrect, as in (b). or from edge devices. Another line of research attempts to replace depth-based ICP with deep learning on point clouds.",
                        "The most relevant example is DenseFusion [21], which first applies a convolutional neural network to segment out the objects of interests. It then fuses RGB and depth features to predict the pose. A trainable iterative deep refiner is applied subsequently to directly regress the pose offset.",
                        "In RGB images, the object pose has been traditionally refined using edge alignment. The idea is to align the edges of the object rendered in the pose hypothesis with the edges observed in the image. This approach is very sensitive to the image quality, clutter, occlusions and to the way in which the edge correspondences are computed. Later, deep learning pose refinement approaches [4]- [6] operating on RGB images were introduced. The idea here is to use an external pose estimation algorithm to obtain the initial pose approximation. Then, the object is rendered in the predicted pose. A rendered image and a given input RGB image are then put into another convolutional neural network, which directly regresses a pose offset. In BB8 [13], on the other hand, the refiner was trained to update predicted locations of keypoints.",
                        "Disadvantages of the aforementioned deep learning-based refiners are their dependence on the correct pose error priors used during training and the need to retrain them for each new object in order to obtain high-quality results.",
                        "Object pose estimation and refinement from multiple views. This topic has been studied in the past in several works, most notably in [22]- [24]. The papers display two main trends. First, they all rely on independent pose hypothesis prediction for each monocular image. Secondly, relative camera transformations between frames are assumed to be known beforehand. Known camera transformations are used to fuse pose predictions from several views in the global coordinate system. Then, either the pose hypothesis that aligns the best with the other hypotheses is chosen or poses are refined to align better in the 3D space. Orthogonal to those methods, are the approaches proposed in [2], [25]. In [25], a set of uncalibrated RGB images is used to create a scaleambiguous 3D reconstruction. Then, objects are detected in the 3D reconstruction that correspond to joint detection in all the frames. Despite its good performance, the method had the downside that it comprised a number of complicated and time-consuming steps and the need to use a large number (72) of frames to obtain reliable reconstructions. CosyPose [2] also relies on independent pose hypotheses from each frame. However, in contrast to [22]- [24], they are then used in a RANSAC scheme to match pose hypotheses from several frames and produce a unified object-level scene reconstruction and approximate relative camera poses. They are jointly optimized by minimizing the multi-view reprojection error. Unfortunately, each of these methods is evaluated on different datasets, and different pose metrics are reported, which makes it impossible to compare them either with each other or with the recent pose estimation papers. We compare our refiner to CosyPose on Homebrewed [9] and YCB [10] datasets."
                    ],
                    "subsections": [
                        {
                            "title": "III. PROPOSED METHOD",
                            "paragraphs": [
                                "The complete inference pipeline is shown in Figure 1. The proposed method is divided into the following steps. Step 1: takes a set of images together with their known intrinsic parameters and relative transformations between cameras; Step 2: The objects of interest in each image are detected separately;",
                                "Step 3: The per-object per-pixel 2D-3D correspondences are predicted independently for each detected object; Step 4: The object 6 DoF pose is estimated with EPnP [26] and RANSAC using the predicted 2D-3D correspondences; Step 5: The initial rough pose is iteratively refined to find a pose which better aligns with predicted dense correspondences in all the frames. This is done by defining a loss function over the predicted correspondences and the ideal 2D-3D correspondences which correspond to the object in the given pose. These ideal correspondences are produced with a differentiable renderer so that the entire multi-view alignment procedure is differentiable. We will now describe each step in more detail. However, the main contribution of the paper comprises the multi-view refinement in Step 5. In all experiments, we used the Soft Rasterizer renderer [27]."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "A. Object Detection and Pose Estimation",
                    "paragraphs": [
                        "As the focus of the paper is on refinement and not on the whole pipeline, we have opted for one of the already available dense correspondence-based detectors. We slightly extended the DPOD [4] and trained it on synthetic training data (later referred to as PBR data) provided by the BOP challenge [3]. We separated the original DPOD architecture into two parts: 1) the YOLOv3 [28] detector trained to output tight object bounding boxes with corresponding semantic labels, and 2) the DPOD-like architecture, which predicts object masks and dense correspondences from these detections. The proposed two-stage approach simplifies and accelerates the training procedure of each component, slightly improves the quality of correspondences and leads to better performance on more challenging Homebrewed [9] and YCB-V [10] datasets. In contrast to DPOD [4], which relied on two-dimensional UV maps, we use the three-dimensional Normalized Object Coordinates Space (NOCS) [29]. This parameterization permits trivial conversion between the object coordinate system and the NOCS coordinate system. Additionally, we switched from the ResNet18 backbone to the ResNet34 backbone. The first block of layers is frozen to avoid overfitting when training on synthetic data. Data augmentation and transfer learning allowed the reliable training of the networks. YOLO was trained for 100 epochs, the augmented DPOD for 240. The last checkpoint was used in all experiments.",
                        "Let us formally define a model as a set of its vertices: M := {v \u2208 R 3 }. Operators that compute minimum and maximum coordinates along the vertex dimension i of all v \u2208 M are defined as:",
                        "Then, for any point p, the NOCS projection operator is defined w.r.t. the model as",
                        "and its inverse as \u03c0 -1 M . The 6 DoF pose is defined as the standard rigid body transformation, where T \u2208 SE(3)."
                    ],
                    "subsections": []
                },
                {
                    "title": "B. Pose refinement with differentiable renderer",
                    "paragraphs": [
                        "Given a ground truth pose T gt and a predicted pose T pr , the aim of pose refinement is to find a pose update T \u2206 that satisfies T \u2206 \u2022 T pr = T gt . It is, however, impossible to estimate a perfect T \u2206 , because the ground truth pose T gt is not available. For this reason, proxy loss functions have to be used, which results in a sub-optimal pose update T \u2206 . We propose refining T pr by optimizing the discrepancy between the predicted noisy NOCS maps from several frames and the perfect NOCS map rendered in the estimated pose with a differentiable renderer.",
                        "Assuming that N frames are used for multi-view refinement, we define the corresponding set of predicted segmentations S := { S1 , . . . , SN }, each of which is a binary segmentation mask S \u2208 {0, 1} W \u00d7H . A set of predicted NOCS correspondences is defined as C := { C1 , . . . , CN }, where each C \u2208 [0, 1] W \u00d7H\u00d73 . These are noisy estimates obtained with the modified DPOD. They are computed once and remain unchanged during refinement. The differentiable renderer is used for a differentiable definition of the following functions: binary foreground/background rendering S : M \u00d7 SE(3) \u2192 {0, 1} W \u00d7H and NOCS rendering C : M \u00d7 SE(3) \u2192 [0, 1] W \u00d7H\u00d73 . We will omit their dependence on the CAD model M to keep the notation concise. For each set of images used for multi-view refinement, one of the images is taken as the reference frame, and then for each f -th image, its relative pose \u039e ref \u2192f \u2208 SE(3) is recomputed w.r.t. to the reference frame. Initial pose hypotheses T := {T 1 , ..., T N } are estimated separately for each frame with PnP+RANSAC. The pose of the object in the coordinate system of the reference frame is later denoted by T pr .",
                        "We use pre-computed NOCS maps in each frame and NOCS maps of the model in the estimated pose in the reference frame transformed to the coordinate system of the f -th frame by transformation \u039e ref \u2192f \u2022T pr to define a per-pixel loss function over predicted and rendered correspondences. The per-pixel NOCS discrepancy relates directly to the 3D structure of the object and the error in 3D. The per-pixel loss for pixel p in the f -th frame is defined as follows:",
                        "(3) In the above equation, \u03c1 stands for the arbitrary distance function in 3D. Essentially, for each frame, the object pose T \u2206 \u2022 T pr is first transformed to the frame's coordinate system using \u039e ref \u2192f and then rendered. For each pixel, the predicted and rendered NOCS correspondences are projected into the 3D coordinate system of the model. Then, the discrepancy between them is penalized. The overall loss function is fully differentiable and dependent only on T \u2206 , since the rendering of NOCS maps is performed using the differentiable renderer.",
                        "On the level of the full set of images used for refinement, the objective of the refinement is defined as:",
                        "Here, Sf,p \u2022 S (\u039e ref \u2192f \u2022 T \u2206 \u2022 T pr ) p serves as an indicator function regarding whether or not the pixel is foreground in both the rendered and the predicted NOCS maps.",
                        "The minimization problem, however, cannot be solved optimally due to its non-convexity. Therefore, the loss function is minimized iteratively by gradient descent over the pose update T \u2206 . This can be done using any gradient-based method. We normally observe convergence within 50 optimization steps. A trivial degenerate solution to the optimization problem exists, which sets the loss to zero, namely non-overlapping rendered and predicted S segmentation maps. But in reality, this is not a problem, because PnP+RANSAC provides reliable initial pose There are numerous ways os implementing the distance function \u03c1 : R 3 \u00d7 R 3 \u2192 R + and parameterizing SO(3) rotations. We use the continuous rotation parameterization from [30], which enables faster and more stable convergence during the optimization procedure than quaternions and Euler angles. As the predicted correspondences and matched correspondences might contain a potentially large number of outliers, a robust \u03c1 function must be used to mitigate this. We experimented with several options and ended up with a particular case of the general robust function introduced in [31]. This function is defined as follows:",
                        "c is a hyper-parameter that stands for the scale of the loss function. In our experiments, we adjusted it dynamically according to the median absolute residuals: c := 2 \u2022 M EDIAN (|e|).",
                        "The choice of the reference frame can affect the effectiveness of pose refinement. The goal is to automatically choose a pose which has a high overlap with predicted segmentations when transformed and rendered in other views. Additionally, it should have the smallest possible loss L f . For each image batch, the reference frame is chosen as follows:",
                        ") where K stands for the number of frames with non-zero loss. Additionally, argmin ignores zero values, as they correspond to degenerate poses that are not re-projected correctly onto other frames.",
                        "The proposed refinement procedure has a certain similarity to the point-to-point ICP as well as to PnP. In terms of the ICP, point-to-point correspondences are established by rendering NOCS maps and using the predicted NOCS maps with the same spatial location in 2D, as opposed to the nearest neighbor search in the standard 3D ICP. The backpropagation through the renderer corresponds directly to the distance minimization step of the ICP. However, projective transformation is essential. This is because if only RGB information is used, and therefore predicted NOCS maps in all frames are already in the model's coordinate system and do not impose any additional spatial 3D constraints. On the other hand, rendering in order to establish correspondences and minimize the discrepancy directly relates to PnP. The proposed refiner can be seen as a multi-view PnP, where direct pixel-wise error, rather than the reprojection error, is minimized."
                    ],
                    "subsections": []
                },
                {
                    "title": "C. Autolabeling",
                    "paragraphs": [
                        "We explore the usefulness of the proposed refiner for the self-annotation of weakly labeled real data, similar to the work of [11]. We assume access to weakly labeled real images with no pose labels. Again, relative transformations between cameras are needed, which is a weaker assumption than having precises per-object pose annotations. For the sake of simplicity, we use the weak 2D labels to filter out correct detections, although this is not necessary, as the detections can be filtered out with the epipolar constraints.",
                        "The autolabeling pipeline operates as follows. First, the YOLO detector and the DPOD network are trained in the usual way on synthetically generated data. They are then applied to a partition of the real data and detections are filtered out. The poses are computed using PnP+RANSAC and fed into our multi-view refinement pipeline. Finally, we retrieve the newly estimated labels and use them to build a dataset with real images. The DPOD part is trained as usual, but the predicted NOCS maps are used instead of the ground truth NOCS. The procedure allows us to achieve the performance at the level of DPOD trained on fully annotated real data. No filtering of pose predictions is performed, as ground truth poses are not available in the given scenario. This results in a few images with bad pose annotations being used for training, but they have no significant negative effect on the final performance. The augmented DPOD trained in the standard way for 240 epochs, and the last checkpoint is then used for evaluation."
                    ],
                    "subsections": [
                        {
                            "title": "IV. EXPERIMENTS",
                            "paragraphs": [
                                "In this section, we evaluate our multi-view refinement pipeline on LineMOD [7], Occlusion [8], Homebrewed [9] and YCB-V [10] datasets to assess its properties. We then evaluate the proposed autolabeling pipeline. Lastly, we test the robustness of the proposed refiner to the imprecision of relative camera transformations and the choice of frames for refinement. We follow the standard evaluation procedure of [4], [5], [12] and report the pose accuracy only for objects correctly detected in 2D on Linemod and Occlusion. Pose quality is computed in accordance with the ADD metric [7].",
                                "On Homebrewed and YCB-V, on the other hand, we submit the predicted poses to the BOP challenge [3] for evaluation and report the Average Recall (AR) metric returned by it.",
                                "Single object pose estimation. Here, we compare the quality of poses predicted with our method to different approaches on the LineMOD [7] dataset. The results are summarized in Table I. The table compares our refinement method to various top-performing deep learning methods which reported an ADD score with different pose refinement approaches. It is not our aim to make direct comparisons with monocular methods, but rather to compare the quality of poses after various refinement methods.",
                                "Our augmented DPOD, labeled as OURS, achieves 53.55% of the average ADD pose accuracy without any refinement, slightly outperforming the original DPOD, which showed 50% pose accuracy. Next, we evaluate the performance of our differentiable rendering refiner in the monocular scenario (OURS DR1) and compare it to previous state-of-the-art methods based on deep learning (DL): [4]- [6], [13]. In this case, the pose accuracy increases from 53.55% to 67.69%, even though only one frame is used and no additional multiview constraints. The refiner outperforms all the competitors in that category apart from DeepIM, even though BB8 and DeepIM have the advantage of using real train data.",
                                "The addition of multiple frames introduces spatial geometric constraints that result in a significant performance boost w.r.t. the ADD score when two (OURS DR2) or four (OURS DR4) frames are used. Even though, the ADD score depends on the choice of the frames, i.e. the closest frames corresponding to weaker constraints, even the two-view refiner can outperform or perform similarly to other approaches using depth-based ICP ( [19] and [12]) and DenseFusion, which uses real train data, RGBD inference and iterative DL-based refinement. The table clearly shows that even a minimum multi-view setup can bring a significant performance boost without any need for precise calibrated depth information.",
                                "Robustness to occlusions. The robustness of our method to occlusions was evaluated on Occlusion [8], Homebrewed [9] and YCB-V [10] datasets. The results are presented in Table II, Table III and Table IV respectively.",
                                "Unfortunately, detectors trained on synthetic data are seldom evaluated on Occlusion (Table II). To the best of our knowledge, there is only an ADD score from SSD6D with the DL refinement [5]. Therefore, we instead compare to the approaches trained with real data. The advantage of these approaches is that they use real data from the target domain and also overfit to the particular occlusions present in the test images. This enables comparison with the best detectors and assessment of how closely the refiner is able to approach the performance of the algorithms trained on real data. As can be seen in Table II, our refiner is able to significantly improve the performance of the non-refined baseline. If 4 views are used for refinement, the performance of our approach is on-par with the state-of-the-art results, even though we do not use any real data annotations.",
                                "With the Homebrewed dataset [9] (Table III), all topperforming methods were trained on the synthetic PBR images.Our main aim here is to compare our refiner to Cosy-Pose [2], which also utilizes multi-view refinement. It is clear from the table that correspondence-based pose estimation methods (ours and CDPN) confidently outperform the direct pose prediction of CosyPose if no refinement is used. With the multi-view refinement, the proposed methods achieves top results even if only 2 closest views are used for refinement. Additionally, the proposed approach outperforms CDPN, CosyPose and Pix2Pose even if their predictions are refined with ICP.",
                                "Table IV shows a comparison of various top-performing methods on the YCB-V [10] dataset. The dataset comes with a real train set and a set of pre-rendered synthetic images (marked as 'real' and 'synt' in the table). On the other hand, synthetic PBR images are also available. When trained on PBR images, raw non-refined CosyPose poses outperform ours. With refinement, our methods outperforms CosyPose, CDPN and EPOS trained on the same PBR data. However, training on real data still has a huge advantage on this dataset.",
                                "Runtime. To allow faster refinement time, the camera intrinsics were re-computed such that the rendered image always has the dimensions 128\u00d7128. Moreover, each model was sub-sampled to contain only 1000 faces. After performing an ablation study on the Linemod dataset, we set the number of refinement iterations to 50. As a result, if 2 views are used Robustness to camera choices. For all datasets used for evaluation, we split each sequence into sets of 2 or 4 images. Images in the different sets do not overlap. For each set, we compute relative camera poses from individual ground truth camera poses. The pose is optimized jointly for images in the set. We experiment with three different view sampling strategies: closest views, random views and furthest views. Tables I, II, III, IVshow a quantitative comparison of these strategies. It is clear from the tables that even though close camera locations essentially constitute a bad setup for multiview pose refinement, as the pose error in one frame is not necessarily visible in other frames, the refiner still improves the poses. Random and furthest view selections tend to perform similarly on all the datasets.",
                                "Relative Pose Noise. The aim of this experiment is to demonstrate how noise in relative poses affects the overall performance of the multi-view optimization pipeline. To do this, we add noise separately to translation and rotation. For rotation transformation, we sample the perturbation angle for each axis from a normal distribution with zero mean and a standard deviation of 5, 7.5 or 10 degrees. For translation transformation, perturbations are sampled from the normal distribution with zero mean and a standard deviation computed on the basis of an object diameter: \u03c3 = 1 3 \u2022 (0.1 \u2022 diam obj ). Larger deviations render the problem innately ill-posed due to the definition of the ADD measure. If the camera is at a distance of more than 10% of the model's diameter, the pose will always be classified as incorrect according to the ADD metric. The results can be seen in Table V. Even with the noisy poses, the refiner still ensures a reasonable pose quality. This table also shows that having more views is beneficial in TABLE IV: Results on the YCB-V dataset reported according to the Average Recall (AR) metric of the BOP challenge [3] on the BOP challenge subset of test images. CosyPose [2] results labeled with * were obtained by re-running the official implementation of the paper."
                            ],
                            "subsections": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "Method",
            "paragraphs": [
                "Train data Refinement AR Time (s)",
                "CosyPose [2] synt + real ICP 0.861 2.736 CosyPose [2] synt + real 8 views 0.853 0.285 CosyPose [2] synt + real 4 views 0.84 0.318 CosyPose [2] synt + real no 0.821 0.241 Pix2Pose [16]  Autolabeling. In this experiment, we retrieve the estimated OURS+DR2 and OURS+DR4 labels and use them to replace the synthetic training set by the automatically generated real labels as discussed above. The extended training set is then used to fine-tune the synthetically trained baseline. The results can be seen in Table VI. It can be clearly seen that the finetuned network trained on autolabels significantly outperforms the synthetic DPOD and is very competitive when compared to the state-of-the-art methods trained on full ground truth labels. V. CONCLUSIONS",
                "In this paper, we propose a novel object pose refinement pipeline. We adopt the idea of using multiple frames to produce a single joint object pose estimate. The use of multiple frames enables effective use of geometric constraints. The proposed refiner is based on the external object detector, which outputs deep 2D-3D correspondences in the form of Normalized Object Coordinate space. Predicted dense correspondences from calibrated multiple frames can be brought together to obtain a better pose estimate using a differentiable renderer. The proposed approach imposes no constraints on the number of frames used and the exact positions of the cameras in the 3D world, and it remains robust even when the cameras are in close proximity to one another. We experimentally show that the refiner works excellently on Linemod, Occlusion, Homebrewed and YCB-V datasets. As an alternative use case, we demonstrate how the proposed approach can be applied to automatically annotate real train data, which has no object pose annotations. The approach even remains effective if the relative transformations in-between frames are imprecise. Moreover, even a multi-view setup with only two frames already produces excellent results. This demonstrates that the approach can be used successfully in practical applications."
            ],
            "subsections": []
        }
    ]
}