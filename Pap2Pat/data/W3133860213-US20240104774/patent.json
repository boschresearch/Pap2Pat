{
    "id": "US20240104774",
    "authors": [
        "Slobodan Ilic",
        "Ivan Shugurov",
        "Sergey Zakharov",
        "Ivan Pavlov"
    ],
    "title": "Multi-dimensional Object Pose Estimation and Refinement",
    "date": "2021-12-09 00:00:00",
    "abstract": "Various embodiments include a pose estimation method for refining an initial multi-dimensional pose of an object of interest to generate a refined multi-dimensional object pose Tpr(NL) with NL\u22651. The method may include: providing the initial object pose Tpr(0) and at least one 2D-3D-correspondence map \u03a8pri with i=1, . . . , I and I\u22651; and estimating the refined object pose Tpr(NL) using an iterative optimization procedure of a loss according to a given loss function LF(k) based on discrepancies between the one or more provided 2D-3D-correspondence maps \u03a8pri and one or more respective rendered 2D-3D-correspondence maps \u03a8rendk,i.",
    "sections": [
        {
            "title": "DESCRIPTION",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "TECHNICAL FIELD",
                    "paragraphs": [
                        "The present disclosure relates to the estimation and especially for the refinement of an estimated pose of an object of interest in a plurality of dimensions."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "define technical field"
                    ],
                    "num_characters": 158,
                    "outline_medium": [
                        "define technical field"
                    ],
                    "outline_short": [
                        "define technical field"
                    ]
                },
                {
                    "title": "BACKGROUND",
                    "paragraphs": [
                        "Object detection and multi-dimensional pose estimation of the detected object are regularly addressed in computer vision since they are applicable in a wide range of applications in different domains. Just for example, autonomous driving, augmented reality, and robotics are hardly possible without fast and precise object localization in 2D and 3D. A large body of work has been invested in the past, but recent advances in deep learning opened new horizons for RGB-based approaches (red-green-blue) which started to dominate the field. Often, RGB images and corresponding depth data are utilized to determine poses. Current state-of-the-art utilizes deep learning methods in connection with the RGB images and depth information. Thus, artificial neural networks are often applied to estimate a pose of an object in a scene based on images of that object from different perspectives and based on a comprehensive data base.",
                        "However, such approaches are often time consuming and, especially, a suitable data base with a sufficient amount of labeled training data in a comprehensive variety which allows the accurate detection of a wide range of objects is hardly available.",
                        "Thus, multi-dimensional pose estimation, in the best case covering six degrees of freedom (6DoF), from monocular RGB images still remains a challenging problem. Methods for coarse estimation of such poses are available, but the accuracy is often not sufficient for industrial applications."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "motivate object detection",
                        "limitations of current approaches"
                    ],
                    "num_characters": 1464,
                    "outline_medium": [
                        "motivate object detection"
                    ],
                    "outline_short": [
                        "motivate object detection"
                    ]
                },
                {
                    "title": "SUMMARY",
                    "paragraphs": [
                        "Therefore, the teachings of the present disclosure serve the need to determine an exact multi-dimensional pose of an object of interest. For example, some embodiments include a A computer implemented pose estimation method PEM for refining an initial multi-dimensional pose Tpr(0) of an object of interest OBJ to generate a refined multi-dimensional object pose Tpr(NL) with NL\u22651, including providing the initial object pose Tpr(0) and at least one 2D-3D-correspondence map Tpr with i=1, . . . , I and I\u22651, and estimating the refined object pose \u03a8pr(NL) by an iterative optimization procedure IOP of a loss according to a given loss function LF(k) and depending on discrepancies between the one or more provided 2D-3D-correspondence maps \u03a8pri and one or more respective rendered 2D-3D-correspondence maps \u03a8rendk,i.",
                        "In some embodiments, the loss function LF is defined as a per-pixel loss function over provided correspondence maps \u03a8pri and rendered correspondence maps \u03a8rendk,i, wherein the loss function LF(k) relates the per-pixel discrepancies of provided correspondence maps \u03a8pri and respective rendered correspondence maps \u03a8rendk,i to the 3D structure of the object and its pose Tpr(k), wherein the rendered correspondence maps \u03a8rendk,i depend on an assumed object pose Tpr(k) and the assumed object pose Tpr(k) is varied in the loops k of the iterative optimization procedure.",
                        "In some embodiments, the iterative optimization procedure IOP of step S2 comprises NL\u22651 iteration loops k with k=1, . . . , NL, wherein in each iteration loop k, an object pose Tpr(k) is assumed, a renderer dREND renders one respective 2D-3D-correspondence map \u03a8rendk,i for each provided 2D-3D-correspondence map \u03a8pri, utilizing as an input: a 3D model MODOBJ of the object of interest OBJ, the assumed object pose Tpr(k), and an imaging parameter PARA(i) which represents one or more parameters of capturing an image IMA(i) underlying the respective provided 2D-3D-correspondence map \u03a8pri.",
                        "In some embodiments, the assumed object pose Tpr(k) of loop k of the iterative optimization procedure IOP is selected such that Tpr(k) differs from the assumed object pose Tpr(k\u22121) of the preceding loop k\u22121, wherein the iterative optimization procedure applies a gradient-based method for the selection, wherein the loss function LF is minimized in terms of object pose updates \u0394T, such that Tpr(k)=\u0394T\u00b7Tpr(k\u22121).",
                        "In some embodiments, in each iteration loop k a segmentation mask SEGrend(k, i) is obtained by the renderer dREND for each one of the respective rendered 2D-3D-correspondence maps \u03a8rendk,i, which segmentation masks SEGrend(k, i) correspond to the object of interest OBJ in the assumed object pose Tpr(k), wherein each segmentation mask SEGrend(k, i) is obtained by rendering the 3D model MODOBJ using the assumed object pose Tpr(k) and imaging parameter PARA(i).",
                        "In some embodiments, the loss function LF(k) is defined as a per pixel loss function in a loop k of the iterative optimization procedure IOP, wherein",
                        "\\({{LF}(k)} = {\\frac{1}{I}{\\sum\\limits_{i = 1}^{I}{L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)}}}\\)\n\\(with\\)\n\\({L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)} = {\\frac{1}{N}{\\sum\\limits_{{({x,y})} \\in {{{SEG}_{pr}(i)}\\bigcap{{SEG}_{rend}({k,i})}}}{\\rho\\left( {{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{pr}^{i}\\left( {x,y} \\right)} \\right)},{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{rend}^{k,i}\\left( {x,y} \\right)} \\right)}} \\right)}}}\\)",
                        "and wherein: I expresses the number of provided 2D-3D-correspondence maps \u03a8pri, x, y are pixel coordinates in the correspondence maps \u03a8pri, \u03a8rendk,i, \u03c1 stands for a distance function in 3D, SEGpr(i)\u2229SEGrend(k, i) is the group of intersecting points of predicted and rendered correspondence maps \u03a8pri, \u03a8rendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i), N is the number of such intersecting points of predicted and rendered correspondence maps end \u03a8pri, \u03a8rendk,i, and  is an operator for transformation of the respective argument into a suitable coordinate system.",
                        "In some embodiments, the renderer dREND is a differentiable renderer.",
                        "In some embodiments, in a step 50 for the determination of the initial object pose Tpr(0) of the object of interest OBJ to be provided in the first step S1, a number I of images IMA(i) of the object of interest OBJ with i=1, . . . , I and I\u22652 as well as known imaging parameters PARA(i) are provided, wherein different images IMA(i) are characterized by different imaging parameters PARA(i), the provided images IMA(i) are processed in a determination step DCS to determine for each image IMA(i) a respective 2D-3D-correspondence map \u03a8pri as well as a respective segmentation mask SEGpr(i), and at least one of the 2D-3D-correspondence maps \u03a8pri is further processed in a coarse pose estimation step CPES to determine the initial object pose Tpr(0).",
                        "In some embodiments, one of the plurality J of the 2D-3D-correspondence maps \u03a8prj with j=1, . . . , J and I\u2265J\u22652 is processed in the coarse pose estimation step CPES to determine the initial object pose Tpr(0).",
                        "In some embodiments, each one j of a plurality J of the 2D-3D-correspondence maps \u03a8prj with j=1, . . . , J and I\u2265J\u22652 is processed in the coarse pose estimation step CPES to determine a respective preliminary object pose Tpr,j(0), wherein the initial object pose Tpr(0) represents an average of the preliminary object poses Tpr,j(0).",
                        "In some embodiments, a dense pose object detector DPOD which is embodied as a trained artificial neural network is applied in the preparation step PS to determine the 2D-3D-correspondence maps \u03a8pri and the segmentation masks SEGpr(i) from the respective images IMA (i).",
                        "In some embodiments, the coarse pose estimation step CPES applies a Perspective-n-Point approach (PnP), especially supplemented by a random sample consensus approach (RANSAC), to determine a respective object pose Tpr(0), Tpr,j(0) from the at least one 2D-3D-correspondence map \u03a8pri, \u03a8prj.",
                        "As another example, some embodiments include a pose estimation system (100) for refining an initial multi-dimensional pose Tpr(0) of an object of interest OBJ to generate a refined multi-dimensional object pose Tpr(NL) with NL\u22651, including a control system (120) configured for executing one or more of the pose estimation methods PEM described herein."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "introduce pose estimation method",
                        "provide initial object pose",
                        "estimate refined object pose",
                        "define loss function",
                        "describe iterative optimization procedure",
                        "render 2D-3D-correspondence maps",
                        "obtain segmentation masks",
                        "define per-pixel loss function",
                        "describe gradient-based method",
                        "determine initial object pose",
                        "apply dense pose object detector",
                        "apply Perspective-n-Point approach"
                    ],
                    "num_characters": 6451,
                    "outline_medium": [
                        "motivate pose estimation",
                        "introduce pose estimation method",
                        "describe iterative optimization procedure",
                        "define loss function",
                        "describe renderer",
                        "outline embodiments"
                    ],
                    "outline_short": [
                        "introduce pose estimation method",
                        "describe iterative optimization procedure",
                        "outline loss function"
                    ]
                },
                {
                    "title": "DETAILED DESCRIPTION",
                    "paragraphs": [
                        "Some embodiments of the teachings herein include a computer implemented pose estimation method PEM which refines an initial multi-dimensional pose Tpr(0) of an object of interest OBJ and generates a refined multi-dimensional object pose Tpr(NL) with NL\u22651. In some embodiments, the method includes providing the initial object pose Tpr(0) and at least one 2D-3D-correspondence map \u03a8pri with i=1, . . . , I and I\u22651. In some embodiments, the method includes estimating the refined object pose Tpr(NL), including an iterative optimization, i.e. minimization, procedure IOP comprising a number NL of loops k=1, . . . , NL of a loss is applied, the loss according to a given loss function LF(k) and depending on discrepancies between the one or more provided 2D-3D-correspondence maps \u03a8pri and one or more respective rendered 2D-3D-correspondence maps \u03a8rendk,i.",
                        "The 6DoF pose estimation first utilizes a predicted initial pose Tpr(0). The initial predicted pose Tpr(0) is then refined in NL\u22651 iterations and loops k, so that in the end a refined pose Tpr(k=NL) is available. The pose refinement is based on an optimization of the discrepancy between provided correspondence maps \u03a8pri and related rendered correspondence map \u03a8rendk,i for each i. The rendered correspondence maps \u03a8rendk,i and, correspondingly, the discrepancy directly depend on the assumed object pose Tpr(k) so that a variation of the object pose Tpr(k) leads to a variation of the discrepancy, such that a minimum discrepancy can be considered to be an indicator for correctness of the assumed object pose Tpr(NL).",
                        "The loss function LF is defined as a per-pixel loss function over provided correspondence maps \u03a8pri and rendered correspondence maps \u03a8rendk,i, wherein the loss function LF(k) relates the per-pixel discrepancies of provided correspondence maps \u03a8pri and respective rendered correspondence maps \u03a8rendk,i to the 3D structure of the object and its pose Tpr(k), wherein the rendered correspondence maps \u03a8rendk,i and, therewith, the loss function LF(k) depend on an assumed object pose Tpr(k) and the assumed object pose Tpr(k) is varied in the loops k of the iterative optimization procedure.",
                        "The iterative optimization procedure IOP of step S2 comprises NL\u22651 iteration loops k with k=1, . . . , NL. In each iteration loop k an object pose Tpr(k) is assumed and a renderer dREND renders one respective 2D-3D-correspondence map \u03a8rendk,i of the one or more model 2D-3D-correspondence maps Trend(k, i) for each provided 2D-3D-correspondence map \u03a8pri. For that purpose, the renderer dREND utilizes as an input a 3D model MODOBJ of the object of interest OBJ, the assumed object pose Tpr(k), and an imaging parameter PARA(i) which represents one or more parameters of capturing an image IMA(i) underlying the respective provided 2D-3D-correspondence map \u03a8pri.",
                        "Therein, the term \u201cfor\u201d within the expression \u201cfor each provided 2D-3D-correspondence map \u03a8pri\u201d essentially represents that the provided 2D-3D-correspondence map \u03a8pri and the respective rendered 2D-3D-correspondence map \u03a8rendk,i are assigned to each other. Moreover, it expresses that the rendering of the \u201crelated\u201d rendered 2D-3D-correspondence map \u03a8rendk,i and the earlier capturing of the image IMA(i) underlying, i.e. being selected and used for, the determination of the provided 2D-3D-correspondence map \u03a8pri utilize the same imaging parameters PARA(i).",
                        "In summary, given the 3D model, imaging parameters PARA(i) including, for example, the camera position POS(i) and corresponding intrinsic camera parameters CAM(i) applied for capturing the image IMA(i), as well as the assumed object pose Tpr(k), it is possible to compute which vertex of the 3D model MODOBJ would be projected on which pixel of a rendered 2D image IMA(i) and vice versa. This correspondence is expressed by a respective 2D-3D-correspondence map \u03a8rendk,i. This process is deterministic and errorless. In some embodiments, a differentiable renderer is applied to achieve this and the resulting rendered correspondence map \u03a8rendk,i corresponds to the 3D model in the given pose Tpr(k) from the perspective PER(i) of the respective camera position POS(i).",
                        "The iterative optimization procedure of step S2 ends at k=NL when the loss function LF converges or falls under a given threshold or similar. I.e. NL is not pre-defined but depends on the variation of Tpr(k) and the resulting outcome of the loss function LF. However, general criteria for ending an iterative optimization procedure of a loss function as such are well known from prior art and do not form an essential aspect of the invention.",
                        "The assumed object pose Tpr(k) of loop k of the iterative optimization procedure IOP is selected such that Tpr(k) differs from the assumed object pose Tyr (k\u22121) of the preceding loop k\u22121, wherein the iterative optimization procedure applies a gradient-based method for the selection, wherein the loss function LF is minimized in terms of object pose updates \u0394T, such that Tpr(k)=\u0394T\u00b7Tpr(k\u22121). I.e. the loss function LF is minimized iteratively by gradient descent over the object pose update \u0394T. This could be done with any gradient-based methods, such as [Kingma2014].",
                        "Convergence might be achieved within, for example, 50 optimization steps, i.e. NL=50.",
                        "Furthermore, in each iteration loop k a segmentation mask SEGrend(k, i) is obtained by the renderer dREND for each one of the respective rendered 2D-3D-correspondence maps \u03a8rendk,i, which segmentation masks SEGrend(k, i) correspond to the object of interest OBJ in the assumed object pose Tpr(k), wherein each segmentation mask SEGrend(k, i) is obtained by rendering the 3D model MODOBJ using the assumed object pose Tpr(k) and imaging parameter PARA(i). The segmentation masks are binary masks, having pixel values \u201c1\u201d or \u201c0\u201d.",
                        "The loss function LF(k) can be defined as a per pixel loss function in a loop k of the iterative optimization procedure IOP, wherein",
                        "\\({{LF}(k)} = {\\frac{1}{I}{\\sum\\limits_{i = 1}^{I}{L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)}}}\\)\n\\(with\\)\n\\({L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)} = {\\frac{1}{N}{\\sum\\limits_{{({x,y})} \\in {{{SEG}_{pr}(i)}\\bigcap{{SEG}_{rend}({k,i})}}}{\\rho\\left( {{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{pr}^{i}\\left( {x,y} \\right)} \\right)},{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{rend}^{k,i}\\left( {x,y} \\right)} \\right)}} \\right)}}}\\)",
                        "and wherein\n\n\n- - I expresses the number of provided 2D-3D-correspondence maps\n    \u03a8_(pr)^(i),\n  - x, y are pixel coordinates in the correspondence maps \u03a8_(pr)^(i),\n    \u03a8_(rend)^(k,i),\n  - \u03c1 stands for an arbitrary distance function in 3D,\n  - SEG_(Pr)(i)\u2229SEG_(rend)(k, i) is the group of intersecting points of\n    predicted and rendered correspondence maps \u03a8_(pr)^(i),\n    \u03a8_(rend)^(k,i), expressed by the corresponding segmentation masks\n    SEG_(pr)(i), SEG_(rend)(k, i),\n  - N is the number of such intersecting points of predicted and\n    rendered correspondence maps \u03a8_(pr)^(i), \u03a8_(rend)^(k,i), expressed\n    by the corresponding segmentation masks SEG_(pr)(i), SEG_(rend)(k,\n    i),\n  - is an operator for transformation of the respective argument into a\n    suitable coordinate system,\n    can be the inverse of a \u201cNOCS\u201d operator.",
                        "In some embodiments, the renderer dREND is a differentiable renderer. Therein, a differentiable renderer is a differentiable implementation of a standard renderer, e.g. known from computer graphics applications. For example, such differentiable renderer takes a textured object model, an object's pose, light courses, etc. and produces a corresponding image. In contrast to standard rendering, a differentiable renderer allows to define any function over the image and to compute its derivatives w.r.t. all the renderer inputs, e.g. a textured object model, an object's pose, light courses, etc. as mentioned above. In such a way, it is possible to directly update the object, its colors, its position, etc. in order to get the desired rendered image.",
                        "The initial object pose Tpr(0) to be provided in the first step S1 can be determined in a step S0, which is, consequently, executed before step S1. In the step S0, a number I of images IMA(i) of the object of interest OBJ with i=1, . . . , I and I\u22652 as well as known imaging parameters PARA(i) are provided, wherein different images IMA(i) are characterized by different imaging parameters PARA(i), e.g. camera positions POS(i) and intrinsic camera parameters CAM(i). I.e. different images IMA(i) represent different perspectives PER(i) onto the object of interest OBJ, i.e. different images IMA(i) have been captured from different camera positions POS(i) and possibly with different intrinsic camera parameters CAM(i). All those imaging parameters PARA(i) for all views PER(i) and, as the case may be, all cameras can be considered to be known from an earlier image capturing step, during which the individual images IMA(i) have been captured from different camera positions POS(i) either by different cameras being positioned at POS(i) or by one camera being moved to the different positions POS(i). While the intrinsic camera parameters CAM(i) might be the same for different images IMA(i), at least the positions POS(i) and perspectives PER(i), respectively, would be different for different images IMA(i). The provided images IMA(i) are then processed in a determination step DCS to determine for each image IMA(i) a respective 2D-3D-correspondence map \u03a8pri as well as a respective segmentation mask SEGpr(i). At least one of the 2D-3D-correspondence maps \u03a8pri is further processed in a coarse pose estimation step CPES to determine the initial object pose Tpr(0).",
                        "In some embodiments, indeed only one of the 2D-3D-correspondence maps \u03a8pri is further processed in the coarse pose estimation step CPES to determine the initial object pose Tpr(0). In another embodiment, each one j of the plurality J of the 2D-3D-correspondence maps \u03a8pri with j=1, . . . , J and I\u2265J\u22652 is processed in the coarse pose estimation step CPES to determine a respective preliminary object pose Tpr,j(0), wherein the initial object pose Tpr(0) represents an average of the preliminary object poses Tpr,j(0).",
                        "In the preparation step PS, a dense pose object detector DPOD which is embodied as a trained artificial neural network is applied to determine the 2D-3D-correspondence maps \u03a8pri and the segmentation masks SEGpr(i) from the respective images IMA(i). DPOD, as described in detail in [ZAKHAROV2019], regresses a multi-class object mask and segmentation mask SEGpr(i), respectively, as well as a dense 2D-3D correspondence map \u03a8pri between image pixels of an image IMA(i) and a corresponding 3D model MODOB of the object OBJ depicted in the image IMA(i). Thus, DPOD estimates both a segmentation mask SEGpr(i) and a dense multi-class 2D-3D correspondence map \u03a8pri between an input image IMA(i) and an available 3D model, e.g. MODOBJ, from the image IMA(i).",
                        "The coarse pose estimation step CPES applies a Perspective-n-Point approach (PnP), especially supplemented by a random sample consensus approach (RANSAC), to determine a respective initial or preliminary, as the case may be, object pose Tpr(0), Tpr,j(0) from the at least one 2D-3D-correspondence map \u03a8pri, \u03a8prj. Given the estimated ID mask, we can observe which objects were detected in the image and their 2D locations, whereas the correspondence map maps each 2D point to a coordinate on an actual 3D model. The 6D pose is then estimated using the Perspective-n-Point (PnP) pose estimation method, e.g. described in [ZHANG2000], that estimates the camera pose given correspondences and intrinsic parameters of the camera. Since a large set of correspondences is generated for each model, RANSAC is used in conjunction with PnP to make camera pose prediction more robust to possible outliers: PnP is prone to errors in case of outliers in the set of point correspondences. RANSAC can be used to make the final solution for the camera pose more robust to such outliers.",
                        "In some embodiments, a pose estimation system for refining an initial multi-dimensional pose Tpr(0) of an object of interest OBJ to generate a refined multi-dimensional object pose Tpr(NL) with NL\u22651 includes a control system configured for executing one or more of the pose estimation methods PEM described above.",
                        "As a summary, the objective achieved by the presented solution is to further reduce the discrepancy between performances of the detectors trained on synthetic and on real data by introducing a novel geometrical refining method, building up in the earlier determined initial coarse pose estimation Tpr(0). In regular operation, the proposed pose refining procedure utilizes the differentiable renderer in the inference phase. It uses multiple views PER(i), POS(i), and PARA(i), respectively, adding relative camera poses POS(i) as constraints to the pose optimization procedure. This is done by comparing the provided \u03a8pri and rendered dense correspondences \u03a8rendk,i for each image IMA(i) and then transmitting the error back though the differentiable renderer to update the pose Tpr(k). This assumes the availability of camera positions POS(i) or perspectives PER(i), wherein such poses or perspectives might be relative information, referring to one reference position, e.g. POS(0), or reference perspective, e.g. PER(0). In practice, POS(i) and PER(i), as the case may be, can be easily obtained by a number of various methods, such as placing object on a markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard or the camera. The markerboard will allow to compute camera poses POS(i), PER(i) in the markerboard coordinate system and consequently compute relative poses between the cameras. Moreover, in the scenario of robotic grasping, the robotic arm can be equipped with a camera to observe the object from several viewpoints POS(i). There, one can rely on precise relative poses between them provided by the robotic arm. However, the 6DoF pose of the object in the images stays unknown. Therefore, we aim at estimating the 6DoF object pose in one reference view with relative camera poses used as constraints.",
                        "In further summary, a multi-view refinement method is proposed that can be used to significantly improve detectors trained on synthetic data via multi-view pose refinement. In this way, the proposed approach completely avoids use of labeled real data for training.",
                        "The elements and features recited in the appended claims may be combined in different ways to produce new claims that likewise fall within the scope of the present disclosure.",
                        "The specification provides the following publications to achieve a detailed explanation of the teachings herein and their execution. Each publication is incorporated by reference:\n\n\n- \\[Barron2019\\] J. T. Barron, \u201cA general and adaptive robust loss\n  function,\u201d in CVPR, 2019.\n- \\[Kingma2014\\] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic\n  optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n- \\[Redmon2016\\] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\n  Farhadi. \u201cYou only look once: Unified, real-time object detection\u201d. In\n  CVPR, 2016.\n- \\[Redmon2017\\] Joseph Redmon and Ali Farhadi. Yolo9000: better,\n  faster, stronger. In CVPR, 2017.\n- \\[Redmon2018\\] Joseph Redmon and Ali Farhadi. \u201cYolov3: An incremental\n  improvement\u201d, arXiv preprint arXiv:1804.02767, 2018.\n- \\[Wang2019\\] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,\n  Shuran Song, and Leonidas J Guibas. \u201cNormalized object coordinate\n  space for category-level 6d object pose and size estimation\u201d. In CVPR,\n  2019.\n- \\[Zakharov2019\\] Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.\n  \u201cDpod: 6d pose object detector and refiner\u201d. In ICCV, 2019.\n- \\[Zhou2019\\] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li, \u201cOn the\n  continuity of rotation representations in neural networks,\u201d in CVPR,\n  2019.\n- \\[Zhang2000\\] Zhengyou Zhang. \u201cA flexible new technique for camera\n  calibration\u201d. IEEE Transactions on pattern analysis and machine\n  intelligence, 22, 2000.",
                        "FIG. 1 shows an exemplary real world scene with an object of interest OBJ. The object's OBJ real position in the scene is such that it can be described by a ground truth 6D object pose Tgt, including three translational degrees of freedom and coordinates, respectively, as well as three rotational degrees of freedom and coordinates, respectively. However, the real pose Tgt is unknown and has to be estimated by a pose estimation system 100 as described herein.",
                        "The pose estimation system 100 shown in FIG. 1 comprises a control system 120 for executing the pose estimation method PEM described below and camera system 110 with a plurality of cameras 110-i with i=1, . . . , I and I\u22652 which are located at different positions POS(i). In the setup shown in FIG. 1, which is only exemplary, I=3 cameras 110-i are shown. The cameras 110-i are positioned such that they capture images IMA(i) of the scene, therewith depicting the object of interest OBJ. Especially, the cameras 110-i are positioned such that they depict the object OBJ from different perspectives PER(i), e.g. under different viewing angles.",
                        "Instead of utilizing a plurality of cameras it would also be possible to use one single camera (not shown) which is movable so that it can be moved to the different positions POS(i) to depict the object from corresponding different perspectives PER(i).",
                        "Independent from whether a movable camera or a plurality of cameras is used to capture images IMA(i) from different positions POS(i), the method described in the following assumes that the camera positions POS(i) or perspectives PER(i) are known. Therein, the positions POS(i) can be relative positions, either expressed relative to each other or by selecting one of them, e.g. POS(1), as the reference position POSref and expressing the remaining positions relative to POSref. Thus, transformations between camera positions POS(i) are known as well.",
                        "In practice, the positions POS(i) can be obtained by a number of various methods, such as placing object on a markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard. A markerboard will allow to compute camera positions POS(i) in the markerboard coordinate system and consequently compute relative poses between the cameras 110-i. In the scenario of robotic grasping, the robotic arm can be equipped with a camera to observe the object from several viewpoints. Therein, one can rely on precise relative poses between them provided by the robotic arm. However, the 6DoF pose of the object in the images stays unknown. Therefore, an estimation of the 6DoF object pose in one reference view with relative camera poses used as constraints is applied.",
                        "Moreover, intrinsic camera parameters CAM(i) for capturing a respective image IMA(i) of the object of interest OBJ are known. Therein, \u201cintrinsic camera parameters\u201d is a well-defined term which refers to how the camera projects the 3D scene onto a 2D plane. The parameters include focal lengths, principal point and sometimes distortion coefficients.",
                        "As a summary, for each image IMA(i) imaging parameters PARA(i) applied for capturing a respective image IMA(i) are assumed to be known. The imaging parameters PARA(i) include the respective camera position POS(i) and corresponding intrinsic camera parameters CAM(i). For example, parameters PARA(i) can be provided to the control system 120 for further processing with the captured images IMA(i).",
                        "The method described in the following moreover uses the availability of a 3D model MODOBJ of the object of interest OBJ, e.g. a 3D CAD model. This can be stored in a corresponding memory of the control system 120 or it can be provided from elsewhere when required.",
                        "As shown in FIG. 2, the pose estimation method PEM is subdivided into two procedures, namely an initial pose estimation procedure PEP and a subsequent pose refinement procedure PRP. PEM receives as input at least the images IMA(i), parameters PARA(i), and the model MODOBJ and produces as an output the object pose Tpr(NL).",
                        "For an initial estimation of the coarse object pose Tpr(0) of the object of interest OBJ in the initial pose estimation procedure PEP, which shown in FIG. 3, at least one such image IMA(i), e.g. IMA(1), has to be captured in a capturing step CAP with corresponding imaging parameters PARA(1). However, since more than one image IMA(i) with different imaging parameters PARA(i) should be available in the pose refinement procedure PRP, several images IMA(i), still fulfilling i=1, . . . , I and I\u22652, are captured in the capturing step CAP. The captured images IMA(i) are processed in a step DCS of determining, for each image IMA(i), a respective segmentation mask SEGpr(i) as well as a respective 2D-3D-correspondence map \u03a8pri between the 2D image IMA(i) and the 3D model MODOBJ of the object of interest OBJ. Step DCS forms the first step of the pose estimation procedure PEP.",
                        "Therein, a segmentation mask SEGpr(i) of an image IMA(i) is a binary 2D matrix with pixel values \u201c1\u201d or \u201c0\u201d, marking the object of interest in the image IMA(i). I.e. only pixels of SEGpr(i) which correspond to pixels of IMA(i) which belong to the depicted representation of the object OBJ in IMA(i) receive a pixel value \u201c1\u201d in SEGpr(i).",
                        "2D-3D-correspondence maps are described in [Zakharov2019]. A 2D-3D-correspondence map \u03a8pri between the pixels of the image IMA(i) and the 3D model MODOBJ directly provides a relation between 2D IMA(i) image pixels and 3D model MODOBJ vertices. For example, a 2D-3D-correspondence map can have the form of a 2D frame, describing a bijective mapping between the vertices of the 3D model of the object OBJ and pixels on the image IMA(i). This provides easy-to-read 2D-3D-correspondences since given the pixel color one can instantaneously estimate its position on the model surface by selecting the vertex with the same color value.",
                        "The step DCS of determining, for each image IMA(i), the respective segmentation mask SEGpr(i) as well as the respective 2D-3D-correspondence map \u03a8pri can be executed by a DPOD approach as described in detail in [Zakharov2019]: DPOD is based on an artificial neural network ANN which processes an image IMA(i) as an input to produce a segmentation mask SEGpr(i) and a 2D-3D-correspondence map \u03a8pri. For training of ANN and DPOD, respectively, the network ANN is trained separately for each potential object of interest. To train the network ANN for an object, a textured model MOD of that object is required. The model MOD is rendered in random poses to produce respective images IMApose. For each of the rendered images IMApose a foreground/background segmentation mask SEGpose and per-pixel 2D-3D correspondence map \u03a8pose is generated. The availability of a 2D-3D correspondence map \u03a8pose means that for every foreground pixel in the rendered image IMApose it is known which point on the 3D model MOD it corresponds to. Then, the network ANN is trained to take a RGB image and output the segmentation mask SEG and the correspondence map \u03a8. This way the network ANN memorizes the mapping from object views to the correct 2D-3D correspondence maps \u03a8 and can extrapolate that to unseen views.",
                        "In some embodiments, the step DCS of determining for each image IMA(i) the respective segmentation mask SEGpr(i) and the respective 2D-3D-correspondence map \u03a8pri can apply a modified DPOD approach, being subdivided into two substeps DCS1, DCS2.",
                        "In the first substep DCS1, the provided images IMA(i) are processed to detect a respective object of interest OBJ in the respective image IMA(i) and to output a tight bounding box BB(i) around the detected object OBJ and a corresponding semantic label LAB(i), e.g. an object class, characterizing the detected object. The label LAB(i) is required in the approach described herein because DPOD is trained separately for each object. This means that one DPOD can only predict correspondences for one particular object. Therefore, the object class is needed to choose the right DPOD. DCS1 might apply an approach called \u201cYOLO\u201d as described in [Redmon2016], [Redmon2017], and especially [Redmon2018], i.e. an artificial neural network ANN\u2032 trained to detect an object in an image and to output a corresponding bounding box and label.",
                        "The second substep DCS2 of the step DCS of determining, for each image IMA(i), the respective segmentation mask SEGpr(i) as well as the respective 2D-3D-correspondence map \u03a8pri, a DPOD-like architecture DPOD\u2032 can be applied which predicts object masks SEGpr(i) and dense correspondences \u03a8pri from the detections provided by DCS1. I.e. DPOD\u2032 predicts for each pixel the corresponding point on the object's surfaces. Therefore, 2D-3D-correspondences between image pixels and points of the surface of the 3D model are produced.",
                        "The two-stage approach of DCS including DCS1 and DCS2 simplifies and accelerates the training procedure of each substep and improves the quality of correspondences \u03a8pri, but in essence does not affect the accuracy of the original one-step approach via DPOD.",
                        "In some embodiments, in contrast to the DPOD approach described in [Zakharov2019], which utilizes UV mapping, the further optional embodiment applies the 3D normalized object coordinates space (NOCS) as described in [Wang2019]. Each dimension of NOCS corresponds to a uniformly scaled dimension of the object to fit into a [0, 1] range. This parameterization allows for trivial conversion between the object coordinate system and the NOCS coordinate system, which is more suitable for regression with deep learning. A model M can be defined as a set of its vertices v with ={v\u2208}. Furthermore, operators can be defined which compute minimal and maximal values along a vertex dimension DIMi as minDIMi()=min and maxDIMi()=max. Then, for any point px, a NOCS projection operator (px) is defined with respect to the model M as",
                        "\\({\\pi_{\\mathcal{M}}({px})} = \\left\\{ {\\frac{{px}_{x} - {\\min_{x}(\\mathcal{M})}}{{\\max_{x}(\\mathcal{M})} - {\\min_{x}(\\mathcal{M})}},\\frac{{px}_{y} - {\\min_{y}(\\mathcal{M})}}{{\\max_{y}(\\mathcal{M})} - {\\min_{y}(\\mathcal{M})}},\\frac{{px}_{z} - {\\min_{z}(\\mathcal{M})}}{{\\max_{z}(\\mathcal{M})} - {\\min_{z}(\\mathcal{M})}}} \\right\\}\\)",
                        "with a corresponding inverse .",
                        "However, a subsequent coarse pose estimation step CPES of the initial pose estimation procedure PEP provides an initial estimation Tpr(0) of the object pose. The coarse pose estimation step CPES applies a Perspective-n-Point approach (PnP), e.g. described in [Zhang2000], preferably supplemented by a random sample consensus approach (RANSAC), to determine the initial object pose Tpr(0) based on an output of the preceding determination step DCS. RANSAC is used in conjunction with PnP to make the estimation of Tpr(0) more robust to possible outliers. PnP is prone to errors in case of outliers in the set of point correspondences. RANSAC can be used to make the final estimation more robust to such outliers.",
                        "In some embodiments of CPES, not all but only one reference map of the plurality I of 2D-3D-correspondence maps \u03a8pri provided by DCS is utilized by PnP and RANSAC to determine Tpr(0), e.g. with i=1.",
                        "In some embodiments, not only one but a plurality J with I\u2265J\u22652 of 2D-3D-correspondence maps \u03a8pri is selected to be utilized to determine Tpr(0). Each selected 2D-3D-correspondence map \u03a8pri is processed as described above with PnP and RANSAC to determine a respective preliminary object pose Tpr,j(0). The initial object pose Tpr(0) is then calculated to be an average of the preliminary object poses Tpr,j(0).",
                        "As an intermediate summary, the initial pose estimation procedure PEP which is completed at this point of the overall pose estimation method PEM comprises the determination step DCS of determining, for each image IMA(i) captured in the upstream image capturing step CAP, the respective segmentation mask SEGpr(i) as well as the respective 2D-3D-correspondence map \u03a8pri and the coarse pose estimation step CPES in which at least one of the 2D-3D-correspondence maps \u03a8pri is further processed to determine the initial object pose Tpr(0). Thus, at this point of the overall pose estimation method PEM a plurality of 2D-3D-correspondence maps \u03a8pri, a respective plurality of segmentation masks SEGpr (i), the initial object pose Tpr (0), the model MODOBJ, as well as the imaging parameters PARA(i) are available and are provided to the next step of the pose estimation method, i.e. to the pose refinement procedure PRP.",
                        "The pose refinement procedure PRP as shown in FIG. 4 and as described in detail below is based on a differentiable renderer dREND. It uses multiple views i adding camera positions POS(i) as constraints to an iterative pose Tpr(k) optimization procedure with a number NL of loops k=1, . . . , NL based on the optimization of a loss function LF. The procedure compares the 2D-3D-correspondence maps \u03a8pri provided by the pose estimation procedure PEP with rendered correspondence maps \u03a8rendk,i computed for each i by the differentiable renderer dREND and then transmits an error back though the differentiable renderer dREND to update the object pose from Tpr(k\u22121) to Tpr(k) in terms of object pose updates \u0394T, such that Tpr(k)=\u0394T\u00b7Tpr(k\u22121). The pose refinement procedure PRP first utilizes the initial object pose Tpr(0). The initial predicted pose Tpr(0) is then refined in NL\u22651 iterations and loops k, so that in the end a refined pose Tpr(k=NL) is available. The pose refinement is based on an optimization of the discrepancy between provided correspondence maps \u03a8pri and related rendered correspondence map \u03a8rendk,i for each i. The rendered correspondence maps \u03a8rendk,i and, correspondingly, the discrepancy directly depend on the assumed object pose Tpr(k) so that a variation of the object pose Tpr(k) leads to a variation of the discrepancy, such that a minimum discrepancy can be considered to be an indicator for correctness of the assumed object pose Tpr(NL).",
                        "Thus, the pose refinement procedure PRP estimates the refined object pose Tpr(NL) by an iterative optimization procedure IOP of a loss. The loss is according to the given loss function LF(k) and depends on discrepancies between the provided 2D-3D-correspondence maps \u03a8pri and respective rendered 2D-3D-correspondence maps \u03a8rendk,i. Thus, for every provided 2D-3D-correspondence map \u03a8pri a respective rendered 2D-3D-correspondence map \u03a8rendk,i is required, so that a comparison becomes possible. Such correspondence maps \u03a8pri, \u03a8rendk,i might be considered to be assigned to each other and have in common that they both relate to the same image IMA(i), position POS(i), perspective PER(i), and imaging parameters PARA(i), respectively, indicated by the common parameter \u201ci\u201d.",
                        "In some embodiments, the iterative optimization procedure IOP might end at k=NL when the corresponding loss function LF, which depends on Tpr(k) and \u0394T, respectively, converges or falls under a given threshold or similar. I.e. NL is not pre-defined but depends on the variation of Tpr(k) and the resulting outcome of the loss function LF. General criteria for ending an iterative optimization procedure of a loss function as such are known. However, the object pose Tpr(NL) achieved in loop k=L is finally assumed to be the aspired, refined object pose.",
                        "In more detail, a starting point of the pose refinement procedure PRP in each loop k of the iterative optimization procedure IOP would be the rendering of a rendered 2D-3D-correspondence map \u03a8rendk,i and of a segmentation map SEGrend (k, i) for each i. Such rendering is achieved by the differentiable renderer dREND mentioned above. Therein, the differentiable renderer dREND can be a differentiable implementation of a standard renderer, e.g. known from computer graphics applications. For example, such differentiable renderer takes a textured object model, an object's pose, light courses, etc. and produces a corresponding image. In contrast to standard rendering, a differentiable renderer allows to define any function over the image and to compute its derivatives w.r.t. all the renderer inputs, e.g. a textured object model, an object's pose, light courses, etc. as mentioned above. In such a way, it is possible to directly update the object, its colors, its position, etc. in order to get the desired rendered data set.",
                        "In each loop k, starting with k=1, the differentiable renderer dREND requires as an input an assumed object pose Tpr(k), the 3D model MODOBJ of the object of interest OBJ, and the imaging parameters PARA(i), especially the camera position POS(i) and the intrinsic parameters CAM(i). In that loop k, the differentiable renderer dREND produces as an output the rendered 2D-3D-correspondence map \u03a8rendk,i and the segmentation map SEGrend(k, i) for each respective i from the provided input. I.e. given the 3D model MODOBJ, camera position POS(i), corresponding intrinsic camera parameters CAM(i), and Tpr(k) it is possible to compute which vertex of the 3D model MODOBJ would be projected on which pixel of a rendered 2D image. This correspondence is expressed by a respective 2D-3D-correspondence map \u03a8rendk,i. Such process is deterministic and errorless and can be executed by the differentiable renderer dREND. The resulting correspondence map \u03a8rendk,i exactly corresponds to the model MODOBJ in the given pose Tpr(k) from the perspective PER(i) of the respective camera position POS(i) and can be compared to the respective provided 2D-3D-correspondence map \u03a8pri.",
                        "The assessment whether an object pose Tpr(k) assumed in loop k is sufficiently correct happens in a loss determination step LDS based on the determination of the per pixel (x, y) loss function LF(k), wherein LF(k) is defined as",
                        "\\({{LF}(k)} = {\\frac{1}{I}{\\sum\\limits_{i = 1}^{I}{L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)}}}\\)",
                        "with summands",
                        "\\({L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)} = {\\frac{1}{N}{\\sum\\limits_{{({x,y})} \\in {{{SEG}_{pr}(i)}\\bigcap{{SEG}_{rend}({k,i})}}}{\\rho\\left( {{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{pr}^{i}\\left( {x,y} \\right)} \\right)},{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{rend}^{k,i}\\left( {x,y} \\right)} \\right)}} \\right)}}}\\)",
                        "Therein, I expresses the number of provided 2D-3D-correspondence maps \u03a8pri, x, y are pixel coordinates in the correspondence maps \u03a8pri, \u03a8rendk,i, SEGpr(i)\u2229SEGrend(k, i) is the group of intersecting points of provided \u03a8pri and rendered correspondence maps \u03a8rendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i), N is the number of such intersecting points of provided \u03a8pri and rendered correspondence maps \u03a8rendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i), and \u03c1 stands for an arbitrary distance function in 3D. There are numerous possible ways to implement the distance function \u03c1:\u00d7\u2192R+. As the provided \u03a8pri and rendered correspondence maps \u03a8rendk,i might contain a potentially large number of outliers, a robust function must be used to mitigate the problem. For example, the general robust function introduced in [Barron2019] qualifies. Moreover, the continuous rotation parameterization from [Zhou2019] can be applied used. This parameterization enables faster and more stable convergence during the optimization procedure. (px) and its inverse  represent the NOCS transformation introduced above.",
                        "Actually, the loss function LF describes a pixel-wise comparison of \u03a8pri and \u03a8rendk,i and the pixel-wise difference is minimized across the loops k of the iterative optimization procedure IOP.",
                        "Therein, the object pose Tpr(k) assumed in loop k is selected such that Tpr(k) differs from the assumed object pose Tpr(k\u22121) of the preceding loop k\u22121, wherein the iterative optimization procedure IOP applies a gradient-based method for the selection, wherein the loss function LF is minimized in terms of object pose updates \u0394T, such that Tpr(k)=\u0394T\u00b7Tpr(k\u22121). I.e. the loss function LF(k) is minimized iteratively across loops k by gradient descent over the object pose update \u0394T. This could be done with any gradient-based methods, such as [Kingma2014]. Convergence might be achieved within, for example, 50 optimization steps, i.e. NL=50.",
                        "In general, the describes approach allows accurate 6DoF pose estimation, even in a monocular case in which only one image IMA(i) with I=1 and correspondingly only one rendered correspondence map \u03a8rendk,i is utilized. Any imprecision in pose estimation, which is not visible in the monocular case, will easily be seen when the object OBJ is observed from a different perspective PER(j).",
                        "For example, during the pose refinement procedure PRP only a single transformation is optimized, namely the reference pose Tpr. For each image in the set of calibrated cameras, object pose is transformed from the coordinate system of a reference image, e.g. IMA(1), to the coordinate system of each particular camera CAM(i) using the known relative camera positions POS(i). Given a vertex v\u2208 in the model coordinate system, it is transformed to the i-th camera CAM(i) coordinate system via vC=Pirel\u00b7Tpr\u00b7{tilde over (v)}.",
                        "Respectively, the transformation Pirel\u00b7Tpr is used by the renderer dREND to render SEGrend(i) and \u03a8rendk,i for the i-th image IMA(i). Loss in each frame is then used to compute gradients through the renderer dREND in order to compute a joint update Tpr(k)=\u0394T\u00b7Tpr(k\u22121).",
                        "The teachings of this disclosure may further overcome discrepancies between performance of detectors trained on synthetic and on the real data. Following the dense correspondence paradigm of DPOD, the DPOD detector is trained only on synthetically generated data. The introduced pose refinement procedure PRP is based on the differentiable renderer dREND in the inference phase. Herein, the refinement procedure is extended from a single view with I=1 to multiple views with I>1, adding relative camera poses POS(i) as constraints to the iterative optimization procedure IOP. In practice, relative camera poses POS(i) can be easily obtained by placing the object of interest on the markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard or the camera. The markerboard will allow to compute camera poses in the markerboard coordinate system and consequently compute relative poses between the cameras. In reality, this scenario is easy to imagine for robotic grasping where the robotic arm equipped with the camera can observe the object from several viewpoints. There, one can rely on precise relative poses provided by the robotic arm. The 6DoF pose of the object in the images stays unknown. Therefore, we aim at estimating the 6 DoF object pose in one reference view with relative camera poses used as constraints."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "introduce pose estimation method PEM",
                        "refine initial multi-dimensional pose Tpr(0) of object OBJ",
                        "generate refined multi-dimensional object pose Tpr(NL) with NL\u22651",
                        "provide initial object pose Tpr(0) and 2D-3D-correspondence map \u03a8pri",
                        "estimate refined object pose Tpr(NL) using iterative optimization procedure IOP",
                        "define loss function LF(k) depending on discrepancies between \u03a8pri and \u03a8rendk,i",
                        "render 2D-3D-correspondence map \u03a8rendk,i using renderer dREND",
                        "utilize 3D model MODOBJ, assumed object pose Tpr(k), and imaging parameter PARA(i)",
                        "obtain segmentation mask SEGrend(k, i) for each rendered 2D-3D-correspondence map \u03a8rendk,i",
                        "define loss function LF(k) as per-pixel loss function",
                        "compute loss function LF(k) using distance function \u03c1",
                        "determine initial object pose Tpr(0) in step S0",
                        "provide images IMA(i) of object OBJ with known imaging parameters PARA(i)",
                        "process images IMA(i) in determination step DCS to determine 2D-3D-correspondence map \u03a8pri and segmentation mask SEGpr",
                        "further process 2D-3D-correspondence map \u03a8pri in coarse pose estimation step CPES",
                        "determine initial object pose Tpr(0) using Perspective-n-Point approach (PnP) and RANSAC",
                        "apply differentiable renderer in inference phase",
                        "use multiple views PER(i), POS(i), and PARA(i) as constraints",
                        "compare provided \u03a8pri and rendered dense correspondences \u03a8rendk,i for each image IMA(i)",
                        "transmit error back through differentiable renderer to update pose Tpr(k)",
                        "assume availability of camera positions POS(i) or perspectives PER(i)",
                        "obtain camera positions POS(i) or perspectives PER(i) using various methods",
                        "use robotic arm to observe object from several viewpoints",
                        "rely on precise relative poses between cameras provided by robotic arm",
                        "estimate 6DoF object pose in one reference view with relative camera poses used as constraints",
                        "provide imaging parameters PARA(i) for capturing image IMA(i)",
                        "include camera position POS(i) and intrinsic camera parameters CAM(i) in PARA(i)",
                        "use 3D model MODOBJ of object OBJ",
                        "store 3D model MODOBJ in memory of control system 120",
                        "provide 3D model MODOBJ from elsewhere when required",
                        "apply dense pose object detector DPOD to determine 2D-3D-correspondence maps \u03a8pri and segmentation masks SEGpr(i)",
                        "regress multi-class object mask and segmentation mask SEGpr(i) using DPOD",
                        "estimate dense 2D-3D correspondence map \u03a8pri between image pixels and 3D model MODOBJ",
                        "apply Perspective-n-Point approach (PnP) and RANSAC to determine initial object pose Tpr(0)",
                        "use PnP to estimate camera pose given correspondences and intrinsic camera parameters",
                        "use RANSAC to make camera pose prediction more robust to outliers",
                        "include control system configured for executing pose estimation methods PEM",
                        "refine initial multi-dimensional pose Tpr(0) of object OBJ using pose estimation system",
                        "achieve objective of reducing discrepancy between performances of detectors trained on synthetic and real data",
                        "introduce pose estimation method PEM",
                        "subdivide PEM into initial pose estimation procedure PEP and pose refinement procedure PRP",
                        "describe initial pose estimation procedure PEP",
                        "capture images IMA(i) with parameters PARA(i)",
                        "process images IMA(i) to determine segmentation mask SEGpr(i) and 2D-3D-correspondence map \u03a8pri",
                        "apply DPOD approach to determine SEGpr(i) and \u03a8pri",
                        "train DPOD for each object of interest",
                        "describe modified DPOD approach with two substeps DCS1 and DCS2",
                        "apply YOLO in DCS1 to detect object and output bounding box and label",
                        "apply DPOD-like architecture in DCS2 to predict object masks and dense correspondences",
                        "describe NOCS parameterization",
                        "define NOCS projection operator",
                        "apply Perspective-n-Point approach and RANSAC to determine initial object pose Tpr(0)",
                        "utilize one or multiple 2D-3D-correspondence maps to determine Tpr(0)",
                        "summarize initial pose estimation procedure PEP",
                        "introduce pose refinement procedure PRP",
                        "describe differentiable renderer dREND",
                        "refine object pose Tpr(k) in NL iterations",
                        "optimize loss function LF based on discrepancies between provided and rendered correspondence maps",
                        "end iterative optimization procedure when loss function converges or falls under threshold",
                        "render correspondence map and segmentation map for each i",
                        "compute loss function LF(k) based on pixel-wise comparison of correspondence maps",
                        "minimize loss function LF(k) iteratively across loops k",
                        "apply gradient-based method to select object pose Tpr(k)",
                        "describe advantages of approach for 6DoF pose estimation",
                        "describe application of approach for robotic grasping",
                        "describe use of relative camera poses as constraints",
                        "compute camera poses in markerboard coordinate system",
                        "estimate 6DoF object pose in one reference view",
                        "describe use of differentiable renderer dREND in inference phase",
                        "extend refinement procedure from single view to multiple views",
                        "add relative camera poses as constraints to iterative optimization procedure",
                        "overcome discrepancies between performance of detectors trained on synthetic and real data",
                        "train DPOD detector on synthetically generated data",
                        "refine object pose using differentiable renderer dREND",
                        "describe advantages of approach for overcoming discrepancies",
                        "summarize pose refinement procedure PRP",
                        "conclude pose estimation method PEM"
                    ],
                    "num_characters": 39030,
                    "outline_medium": [
                        "introduce pose estimation method PEM",
                        "provide initial multi-dimensional pose Tpr(0) of object of interest OBJ",
                        "estimate refined object pose Tpr(NL) with NL\u22651",
                        "define loss function LF(k) for iterative optimization procedure IOP",
                        "describe iterative optimization procedure IOP",
                        "apply gradient-based method for object pose updates \u0394T",
                        "obtain segmentation mask SEGrend(k, i) for each rendered 2D-3D-correspondence map \u03a8rendk,i",
                        "define loss function LF(k) as per-pixel loss function",
                        "describe differentiable renderer dREND",
                        "determine initial object pose Tpr(0) in step S0",
                        "provide images IMA(i) of object of interest OBJ with known imaging parameters PARA(i)",
                        "process images IMA(i) in determination step DCS to determine 2D-3D-correspondence maps \u03a8pri and segmentation masks SEGpr",
                        "apply coarse pose estimation step CPES to determine initial object pose Tpr(0)",
                        "describe Perspective-n-Point approach (PnP) and random sample consensus approach (RANSAC)",
                        "introduce pose estimation system for refining initial multi-dimensional pose Tpr(0)",
                        "summarize objective of presented solution",
                        "describe multi-view refinement method",
                        "provide publications for detailed explanation of teachings",
                        "describe exemplary real-world scene with object of interest OBJ",
                        "introduce pose estimation method PEM",
                        "subdivide PEM into initial pose estimation procedure PEP and pose refinement procedure PRP",
                        "describe initial pose estimation procedure PEP",
                        "capture images IMA(i) with parameters PARA(i)",
                        "process images IMA(i) to determine segmentation masks SEGpr(i) and 2D-3D-correspondence maps \u03a8pri",
                        "apply DPOD approach to determine SEGpr(i) and \u03a8pri",
                        "describe modified DPOD approach with two substeps DCS1 and DCS2",
                        "apply NOCS parameterization",
                        "perform coarse pose estimation step CPES",
                        "apply Perspective-n-Point approach (PnP) and RANSAC",
                        "summarize initial pose estimation procedure PEP",
                        "introduce pose refinement procedure PRP",
                        "describe iterative optimization procedure IOP",
                        "render correspondence maps \u03a8rendk,i and segmentation maps SEGrend(k, i)",
                        "determine loss function LF(k)",
                        "update object pose Tpr(k) based on loss function LF(k)",
                        "describe loss determination step LDS",
                        "apply gradient-based method for object pose update \u0394T",
                        "conclude pose refinement procedure PRP"
                    ],
                    "outline_short": [
                        "introduce pose estimation method PEM",
                        "describe initial multi-dimensional pose Tpr(0) and 2D-3D-correspondence map \u03a8pri",
                        "explain iterative optimization procedure IOP with loss function LF",
                        "define loss function LF as per-pixel loss function",
                        "describe renderer dREND and segmentation mask SEGrend",
                        "explain determination of initial object pose Tpr(0) in step S0",
                        "describe coarse pose estimation step CPES using Perspective-n-Point approach",
                        "summarize pose estimation system for refining initial pose Tpr(0)",
                        "outline multi-view refinement method for improving detectors trained on synthetic data",
                        "introduce pose estimation method PEM",
                        "describe initial pose estimation procedure PEP",
                        "motivate DPOD approach for determining segmentation masks and 2D-3D-correspondence maps",
                        "describe coarse pose estimation step CPES using PnP and RANSAC",
                        "summarize initial pose estimation procedure PEP",
                        "introduce pose refinement procedure PRP using differentiable renderer dREND",
                        "describe iterative optimization procedure IOP for refining object pose",
                        "detail loss function LF for assessing object pose correctness",
                        "conclude pose refinement procedure PRP"
                    ]
                }
            ],
            "outline_long": [],
            "num_characters": 0,
            "outline_medium": [],
            "outline_short": []
        }
    ],
    "claims": [
        "1. A pose estimation method for refining an initial multi-dimensional pose of an object of interest to generate a refined multi-dimensional object pose Tpr(NL) with NL\u22651, the method comprising:\nproviding the initial object pose Tpr(0) and at least one 2D-3D-correspondence map \u03a8pri with i=1, . . . , I and I\u22651; and\nestimating the refined object pose Tpr (NL) using an iterative optimization procedure of a loss according to a given loss function LF(k) based on discrepancies between the one or more provided 2D-3D-correspondence maps \u03a8pri and one or more respective rendered 2D-3D-correspondence maps \u03a8rendk,i.",
        "2. A method according to claim 1, wherein:\nthe loss function LF is defined as a per-pixel loss function over provided correspondence maps \u03a8pri and rendered correspondence maps \u03a8rendk,i;\nthe loss function LF(k) relates the per-pixel discrepancies of provided correspondence maps \u03a8pri and respective rendered correspondence maps \u03a8rendk,i to the 3D structure of the object and its pose Tpr(k); and\nthe rendered correspondence maps \u03a8rendk,i depend on an assumed object pose Tpr(k) and the assumed object pose Tpr(k) is varied in the loops k of the iterative optimization procedure.",
        "3. A method according to claim 1, wherein:\nthe iterative optimization procedure comprises NL\u22651 iteration loops k with k=1, . . . , NL;\nin each iteration loop k\nan object pose Tpr(k) is assumed, and\na renderer renders one respective 2D-3D-correspondence map \u03a8rendk,i for each provided 2D-3D-correspondence map \u03a8pri, utilizing as an input:\na 3D model of the object of interest,\nthe assumed object pose Tpr(k), and a\nn imaging parameter PARA(i) which represents one or more parameters of capturing an image IMA(i) underlying the respective provided 2D-3D-correspondence map \u03a8pri.",
        "4. A method according to claim 3, wherein:\nthe assumed object pose Tpr(k) of loop k of the iterative optimization procedure is selected such that Tpr(k) differs from the assumed object pose Tpr(k\u22121) of the preceding loop k\u22121;\nthe iterative optimization procedure applies a gradient-based method for the selection; and\nthe loss function LF is minimized in terms of object pose updates \u0394T, such that Tpr(k)=\u0394T\u00b7Tpr(k\u22121).",
        "5. A method according to claim 3, wherein:\nin each iteration loop k a segmentation mask SEGrend(k, i) is obtained by the renderer for each one of the respective rendered 2D-3D-correspondence maps \u03a8rendk,i, which segmentation masks SEGrend(k, i) correspond to the object of interest OBJ in the assumed object pose Tpr(k); and\neach segmentation mask SEGrend(k, i) is obtained by rendering the 3D model using the assumed object pose Tpr(k) and imaging parameter PARA(i).",
        "6. A method according to claim 5, wherein:\nthe loss function LF(k) is defined as a per pixel loss function in a loop k of the iterative optimization procedure;\n\n\n\\({{LF}(k)} = {\\frac{1}{I}{\\sum\\limits_{i = 1}^{I}{L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)}}}\\)\n\\(with\\)\n\\({{L\\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\\left( {k,i} \\right)},\\Psi_{pr}^{i},\\Psi_{rend}^{k,i}} \\right)} = {\\frac{1}{N}{\\sum}_{{({x,y})} \\in {{{SEG}_{pr}(i)}\\bigcap{{SEG}_{rend}({k,i})}}}{\\rho\\left( {{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{pr}^{i}\\left( {x,y} \\right)} \\right)},{\\pi_{\\mathcal{M}}^{- 1}\\left( {\\Psi_{rend}^{k,i}\\left( {x,y} \\right)} \\right)}} \\right)}}};\\)\nand\nI expresses the number of provided 2D-3D-correspondence maps \u03a8pri,\nx, y are pixel coordinates in the correspondence maps \u03a8pri, \u03a8rendk,i,\np stands for a distance function in 3D,\nSEGpr(i)\u2229SEGrend(k, i) is the group of intersecting points of predicted and rendered correspondence maps \u03a8pri, \u03a8rendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i),\nN is the number of such intersecting points of predicted and rendered correspondence maps \u03a8pri, \u03a8rendk,i, and\n is an operator for transformation of the respective argument into a suitable coordinate system.",
        "7. A method according to claim 3, wherein the renderer comprises a differentiable renderer.",
        "8. A method according to claim 1, further comprising determining the initial object pose Tpr(0) of the object of interest by:\nproviding a number of images IMA(i) of the object of interest with i=1, . . . , I and I\u22652 as well as known imaging parameters PARA(i), wherein different images IMA(i) are characterized by different imaging parameters PARA(i),\nprocessing the provided images IMA (i) to determine for each image IMA(i) a respective 2D-3D-correspondence map \u03a8pri as well as a respective segmentation mask SEGpr (i); and\nfurther processing at least one of the 2D-3D-correspondence maps \u03a8pri in a coarse pose estimation step CPES to determine the initial object pose Tpr(0).",
        "9. A method according to claim 8, further comprising processing one of the plurality J of the 2D-3D-correspondence maps \u03a8pri with j=1, . . . , J and I\u2265J\u22652 to determine the initial object pose Tpr(0).",
        "10. A method according to claim 8, further comprising processing each one j of a plurality J of the 2D-3D-correspondence maps \u03a8prj with j=1, . . . , J and I\u2265J\u22652 to determine a respective preliminary object pose Tpr,j (0), wherein the initial object pose Tpr(0) represents an average of the preliminary object poses Tpr,j (0).",
        "11. A method according to claim 8, further comprising applying a dense pose object detector comprising a trained artificial neural network in the preparation step PS to determine the 2D-3D-correspondence maps \u03a8pri and the segmentation masks SEGpr(i) from the respective images IMA(i).",
        "12. A method according to claim 8, wherein coarse pose estimation includes applying a Perspective-n-Point approach supplemented by a random sample consensus approach to determine a respective object pose Tpr(0), Tpr,j(0) from the at least one 2D-3D-correspondence map \u03a8pri, \u03a8prj.",
        "13. A pose estimation system for refining an initial multi-dimensional pose Tpr(0) of an object of interest to generate a refined multi-dimensional object pose Tpr (NL) with NL\u22651, the system comprising a control system programmed to:\nprovide the initial object pose Tpr(0) and at least one 2D-3D-correspondence map \u03a8pri with i=1, . . . , I and I\u22651; and\nestimating the refined object pose Tpr(NL) using an iterative optimization procedure of a loss according to a given loss function LF(k) based on discrepancies between the one or more provided 2D-3D-correspondence maps \u03a8pri and one or more respective rendered 2D-3D-correspondence maps \u03a8rendk,i."
    ]
}