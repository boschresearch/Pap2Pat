# DESCRIPTION

## TECHNICAL FIELD

The present disclosure relates to the estimation and especially for the refinement of an estimated pose of an object of interest in a plurality of dimensions.

## BACKGROUND

Object detection and multi-dimensional pose estimation of the detected object are regularly addressed in computer vision since they are applicable in a wide range of applications in different domains. Just for example, autonomous driving, augmented reality, and robotics are hardly possible without fast and precise object localization in 2D and 3D. A large body of work has been invested in the past, but recent advances in deep learning opened new horizons for RGB-based approaches (red-green-blue) which started to dominate the field. Often, RGB images and corresponding depth data are utilized to determine poses. Current state-of-the-art utilizes deep learning methods in connection with the RGB images and depth information. Thus, artificial neural networks are often applied to estimate a pose of an object in a scene based on images of that object from different perspectives and based on a comprehensive data base.

However, such approaches are often time consuming and, especially, a suitable data base with a sufficient amount of labeled training data in a comprehensive variety which allows the accurate detection of a wide range of objects is hardly available.

Thus, multi-dimensional pose estimation, in the best case covering six degrees of freedom (6DoF), from monocular RGB images still remains a challenging problem. Methods for coarse estimation of such poses are available, but the accuracy is often not sufficient for industrial applications.

## SUMMARY

Therefore, the teachings of the present disclosure serve the need to determine an exact multi-dimensional pose of an object of interest. For example, some embodiments include a A computer implemented pose estimation method PEM for refining an initial multi-dimensional pose Tpr(0) of an object of interest OBJ to generate a refined multi-dimensional object pose Tpr(NL) with NL≥1, including providing the initial object pose Tpr(0) and at least one 2D-3D-correspondence map Tpr with i=1, . . . , I and I≥1, and estimating the refined object pose Ψpr(NL) by an iterative optimization procedure IOP of a loss according to a given loss function LF(k) and depending on discrepancies between the one or more provided 2D-3D-correspondence maps Ψpri and one or more respective rendered 2D-3D-correspondence maps Ψrendk,i.

In some embodiments, the loss function LF is defined as a per-pixel loss function over provided correspondence maps Ψpri and rendered correspondence maps Ψrendk,i, wherein the loss function LF(k) relates the per-pixel discrepancies of provided correspondence maps Ψpri and respective rendered correspondence maps Ψrendk,i to the 3D structure of the object and its pose Tpr(k), wherein the rendered correspondence maps Ψrendk,i depend on an assumed object pose Tpr(k) and the assumed object pose Tpr(k) is varied in the loops k of the iterative optimization procedure.

In some embodiments, the iterative optimization procedure IOP of step S2 comprises NL≥1 iteration loops k with k=1, . . . , NL, wherein in each iteration loop k, an object pose Tpr(k) is assumed, a renderer dREND renders one respective 2D-3D-correspondence map Ψrendk,i for each provided 2D-3D-correspondence map Ψpri, utilizing as an input: a 3D model MODOBJ of the object of interest OBJ, the assumed object pose Tpr(k), and an imaging parameter PARA(i) which represents one or more parameters of capturing an image IMA(i) underlying the respective provided 2D-3D-correspondence map Ψpri.

In some embodiments, the assumed object pose Tpr(k) of loop k of the iterative optimization procedure IOP is selected such that Tpr(k) differs from the assumed object pose Tpr(k−1) of the preceding loop k−1, wherein the iterative optimization procedure applies a gradient-based method for the selection, wherein the loss function LF is minimized in terms of object pose updates ΔT, such that Tpr(k)=ΔT·Tpr(k−1).

In some embodiments, in each iteration loop k a segmentation mask SEGrend(k, i) is obtained by the renderer dREND for each one of the respective rendered 2D-3D-correspondence maps Ψrendk,i, which segmentation masks SEGrend(k, i) correspond to the object of interest OBJ in the assumed object pose Tpr(k), wherein each segmentation mask SEGrend(k, i) is obtained by rendering the 3D model MODOBJ using the assumed object pose Tpr(k) and imaging parameter PARA(i).

In some embodiments, the loss function LF(k) is defined as a per pixel loss function in a loop k of the iterative optimization procedure IOP, wherein

\({{LF}(k)} = {\frac{1}{I}{\sum\limits_{i = 1}^{I}{L\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\left( {k,i} \right)},\Psi_{pr}^{i},\Psi_{rend}^{k,i}} \right)}}}\)
\(with\)
\({L\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\left( {k,i} \right)},\Psi_{pr}^{i},\Psi_{rend}^{k,i}} \right)} = {\frac{1}{N}{\sum\limits_{{({x,y})} \in {{{SEG}_{pr}(i)}\bigcap{{SEG}_{rend}({k,i})}}}{\rho\left( {{\pi_{\mathcal{M}}^{- 1}\left( {\Psi_{pr}^{i}\left( {x,y} \right)} \right)},{\pi_{\mathcal{M}}^{- 1}\left( {\Psi_{rend}^{k,i}\left( {x,y} \right)} \right)}} \right)}}}\)

and wherein: I expresses the number of provided 2D-3D-correspondence maps Ψpri, x, y are pixel coordinates in the correspondence maps Ψpri, Ψrendk,i, ρ stands for a distance function in 3D, SEGpr(i)∩SEGrend(k, i) is the group of intersecting points of predicted and rendered correspondence maps Ψpri, Ψrendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i), N is the number of such intersecting points of predicted and rendered correspondence maps end Ψpri, Ψrendk,i, and  is an operator for transformation of the respective argument into a suitable coordinate system.

In some embodiments, the renderer dREND is a differentiable renderer.

In some embodiments, in a step 50 for the determination of the initial object pose Tpr(0) of the object of interest OBJ to be provided in the first step S1, a number I of images IMA(i) of the object of interest OBJ with i=1, . . . , I and I≥2 as well as known imaging parameters PARA(i) are provided, wherein different images IMA(i) are characterized by different imaging parameters PARA(i), the provided images IMA(i) are processed in a determination step DCS to determine for each image IMA(i) a respective 2D-3D-correspondence map Ψpri as well as a respective segmentation mask SEGpr(i), and at least one of the 2D-3D-correspondence maps Ψpri is further processed in a coarse pose estimation step CPES to determine the initial object pose Tpr(0).

In some embodiments, one of the plurality J of the 2D-3D-correspondence maps Ψprj with j=1, . . . , J and I≥J≥2 is processed in the coarse pose estimation step CPES to determine the initial object pose Tpr(0).

In some embodiments, each one j of a plurality J of the 2D-3D-correspondence maps Ψprj with j=1, . . . , J and I≥J≥2 is processed in the coarse pose estimation step CPES to determine a respective preliminary object pose Tpr,j(0), wherein the initial object pose Tpr(0) represents an average of the preliminary object poses Tpr,j(0).

In some embodiments, a dense pose object detector DPOD which is embodied as a trained artificial neural network is applied in the preparation step PS to determine the 2D-3D-correspondence maps Ψpri and the segmentation masks SEGpr(i) from the respective images IMA (i).

In some embodiments, the coarse pose estimation step CPES applies a Perspective-n-Point approach (PnP), especially supplemented by a random sample consensus approach (RANSAC), to determine a respective object pose Tpr(0), Tpr,j(0) from the at least one 2D-3D-correspondence map Ψpri, Ψprj.

As another example, some embodiments include a pose estimation system (100) for refining an initial multi-dimensional pose Tpr(0) of an object of interest OBJ to generate a refined multi-dimensional object pose Tpr(NL) with NL≥1, including a control system (120) configured for executing one or more of the pose estimation methods PEM described herein.

## DETAILED DESCRIPTION

Some embodiments of the teachings herein include a computer implemented pose estimation method PEM which refines an initial multi-dimensional pose Tpr(0) of an object of interest OBJ and generates a refined multi-dimensional object pose Tpr(NL) with NL≥1. In some embodiments, the method includes providing the initial object pose Tpr(0) and at least one 2D-3D-correspondence map Ψpri with i=1, . . . , I and I≥1. In some embodiments, the method includes estimating the refined object pose Tpr(NL), including an iterative optimization, i.e. minimization, procedure IOP comprising a number NL of loops k=1, . . . , NL of a loss is applied, the loss according to a given loss function LF(k) and depending on discrepancies between the one or more provided 2D-3D-correspondence maps Ψpri and one or more respective rendered 2D-3D-correspondence maps Ψrendk,i.

The 6DoF pose estimation first utilizes a predicted initial pose Tpr(0). The initial predicted pose Tpr(0) is then refined in NL≥1 iterations and loops k, so that in the end a refined pose Tpr(k=NL) is available. The pose refinement is based on an optimization of the discrepancy between provided correspondence maps Ψpri and related rendered correspondence map Ψrendk,i for each i. The rendered correspondence maps Ψrendk,i and, correspondingly, the discrepancy directly depend on the assumed object pose Tpr(k) so that a variation of the object pose Tpr(k) leads to a variation of the discrepancy, such that a minimum discrepancy can be considered to be an indicator for correctness of the assumed object pose Tpr(NL).

The loss function LF is defined as a per-pixel loss function over provided correspondence maps Ψpri and rendered correspondence maps Ψrendk,i, wherein the loss function LF(k) relates the per-pixel discrepancies of provided correspondence maps Ψpri and respective rendered correspondence maps Ψrendk,i to the 3D structure of the object and its pose Tpr(k), wherein the rendered correspondence maps Ψrendk,i and, therewith, the loss function LF(k) depend on an assumed object pose Tpr(k) and the assumed object pose Tpr(k) is varied in the loops k of the iterative optimization procedure.

The iterative optimization procedure IOP of step S2 comprises NL≥1 iteration loops k with k=1, . . . , NL. In each iteration loop k an object pose Tpr(k) is assumed and a renderer dREND renders one respective 2D-3D-correspondence map Ψrendk,i of the one or more model 2D-3D-correspondence maps Trend(k, i) for each provided 2D-3D-correspondence map Ψpri. For that purpose, the renderer dREND utilizes as an input a 3D model MODOBJ of the object of interest OBJ, the assumed object pose Tpr(k), and an imaging parameter PARA(i) which represents one or more parameters of capturing an image IMA(i) underlying the respective provided 2D-3D-correspondence map Ψpri.

Therein, the term “for” within the expression “for each provided 2D-3D-correspondence map Ψpri” essentially represents that the provided 2D-3D-correspondence map Ψpri and the respective rendered 2D-3D-correspondence map Ψrendk,i are assigned to each other. Moreover, it expresses that the rendering of the “related” rendered 2D-3D-correspondence map Ψrendk,i and the earlier capturing of the image IMA(i) underlying, i.e. being selected and used for, the determination of the provided 2D-3D-correspondence map Ψpri utilize the same imaging parameters PARA(i).

In summary, given the 3D model, imaging parameters PARA(i) including, for example, the camera position POS(i) and corresponding intrinsic camera parameters CAM(i) applied for capturing the image IMA(i), as well as the assumed object pose Tpr(k), it is possible to compute which vertex of the 3D model MODOBJ would be projected on which pixel of a rendered 2D image IMA(i) and vice versa. This correspondence is expressed by a respective 2D-3D-correspondence map Ψrendk,i. This process is deterministic and errorless. In some embodiments, a differentiable renderer is applied to achieve this and the resulting rendered correspondence map Ψrendk,i corresponds to the 3D model in the given pose Tpr(k) from the perspective PER(i) of the respective camera position POS(i).

The iterative optimization procedure of step S2 ends at k=NL when the loss function LF converges or falls under a given threshold or similar. I.e. NL is not pre-defined but depends on the variation of Tpr(k) and the resulting outcome of the loss function LF. However, general criteria for ending an iterative optimization procedure of a loss function as such are well known from prior art and do not form an essential aspect of the invention.

The assumed object pose Tpr(k) of loop k of the iterative optimization procedure IOP is selected such that Tpr(k) differs from the assumed object pose Tyr (k−1) of the preceding loop k−1, wherein the iterative optimization procedure applies a gradient-based method for the selection, wherein the loss function LF is minimized in terms of object pose updates ΔT, such that Tpr(k)=ΔT·Tpr(k−1). I.e. the loss function LF is minimized iteratively by gradient descent over the object pose update ΔT. This could be done with any gradient-based methods, such as [Kingma2014].

Convergence might be achieved within, for example, 50 optimization steps, i.e. NL=50.

Furthermore, in each iteration loop k a segmentation mask SEGrend(k, i) is obtained by the renderer dREND for each one of the respective rendered 2D-3D-correspondence maps Ψrendk,i, which segmentation masks SEGrend(k, i) correspond to the object of interest OBJ in the assumed object pose Tpr(k), wherein each segmentation mask SEGrend(k, i) is obtained by rendering the 3D model MODOBJ using the assumed object pose Tpr(k) and imaging parameter PARA(i). The segmentation masks are binary masks, having pixel values “1” or “0”.

The loss function LF(k) can be defined as a per pixel loss function in a loop k of the iterative optimization procedure IOP, wherein

\({{LF}(k)} = {\frac{1}{I}{\sum\limits_{i = 1}^{I}{L\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\left( {k,i} \right)},\Psi_{pr}^{i},\Psi_{rend}^{k,i}} \right)}}}\)
\(with\)
\({L\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\left( {k,i} \right)},\Psi_{pr}^{i},\Psi_{rend}^{k,i}} \right)} = {\frac{1}{N}{\sum\limits_{{({x,y})} \in {{{SEG}_{pr}(i)}\bigcap{{SEG}_{rend}({k,i})}}}{\rho\left( {{\pi_{\mathcal{M}}^{- 1}\left( {\Psi_{pr}^{i}\left( {x,y} \right)} \right)},{\pi_{\mathcal{M}}^{- 1}\left( {\Psi_{rend}^{k,i}\left( {x,y} \right)} \right)}} \right)}}}\)

and wherein


- - I expresses the number of provided 2D-3D-correspondence maps
    Ψ_(pr)^(i),
  - x, y are pixel coordinates in the correspondence maps Ψ_(pr)^(i),
    Ψ_(rend)^(k,i),
  - ρ stands for an arbitrary distance function in 3D,
  - SEG_(Pr)(i)∩SEG_(rend)(k, i) is the group of intersecting points of
    predicted and rendered correspondence maps Ψ_(pr)^(i),
    Ψ_(rend)^(k,i), expressed by the corresponding segmentation masks
    SEG_(pr)(i), SEG_(rend)(k, i),
  - N is the number of such intersecting points of predicted and
    rendered correspondence maps Ψ_(pr)^(i), Ψ_(rend)^(k,i), expressed
    by the corresponding segmentation masks SEG_(pr)(i), SEG_(rend)(k,
    i),
  - is an operator for transformation of the respective argument into a
    suitable coordinate system,
    can be the inverse of a “NOCS” operator.

In some embodiments, the renderer dREND is a differentiable renderer. Therein, a differentiable renderer is a differentiable implementation of a standard renderer, e.g. known from computer graphics applications. For example, such differentiable renderer takes a textured object model, an object's pose, light courses, etc. and produces a corresponding image. In contrast to standard rendering, a differentiable renderer allows to define any function over the image and to compute its derivatives w.r.t. all the renderer inputs, e.g. a textured object model, an object's pose, light courses, etc. as mentioned above. In such a way, it is possible to directly update the object, its colors, its position, etc. in order to get the desired rendered image.

The initial object pose Tpr(0) to be provided in the first step S1 can be determined in a step S0, which is, consequently, executed before step S1. In the step S0, a number I of images IMA(i) of the object of interest OBJ with i=1, . . . , I and I≥2 as well as known imaging parameters PARA(i) are provided, wherein different images IMA(i) are characterized by different imaging parameters PARA(i), e.g. camera positions POS(i) and intrinsic camera parameters CAM(i). I.e. different images IMA(i) represent different perspectives PER(i) onto the object of interest OBJ, i.e. different images IMA(i) have been captured from different camera positions POS(i) and possibly with different intrinsic camera parameters CAM(i). All those imaging parameters PARA(i) for all views PER(i) and, as the case may be, all cameras can be considered to be known from an earlier image capturing step, during which the individual images IMA(i) have been captured from different camera positions POS(i) either by different cameras being positioned at POS(i) or by one camera being moved to the different positions POS(i). While the intrinsic camera parameters CAM(i) might be the same for different images IMA(i), at least the positions POS(i) and perspectives PER(i), respectively, would be different for different images IMA(i). The provided images IMA(i) are then processed in a determination step DCS to determine for each image IMA(i) a respective 2D-3D-correspondence map Ψpri as well as a respective segmentation mask SEGpr(i). At least one of the 2D-3D-correspondence maps Ψpri is further processed in a coarse pose estimation step CPES to determine the initial object pose Tpr(0).

In some embodiments, indeed only one of the 2D-3D-correspondence maps Ψpri is further processed in the coarse pose estimation step CPES to determine the initial object pose Tpr(0). In another embodiment, each one j of the plurality J of the 2D-3D-correspondence maps Ψpri with j=1, . . . , J and I≥J≥2 is processed in the coarse pose estimation step CPES to determine a respective preliminary object pose Tpr,j(0), wherein the initial object pose Tpr(0) represents an average of the preliminary object poses Tpr,j(0).

In the preparation step PS, a dense pose object detector DPOD which is embodied as a trained artificial neural network is applied to determine the 2D-3D-correspondence maps Ψpri and the segmentation masks SEGpr(i) from the respective images IMA(i). DPOD, as described in detail in [ZAKHAROV2019], regresses a multi-class object mask and segmentation mask SEGpr(i), respectively, as well as a dense 2D-3D correspondence map Ψpri between image pixels of an image IMA(i) and a corresponding 3D model MODOB of the object OBJ depicted in the image IMA(i). Thus, DPOD estimates both a segmentation mask SEGpr(i) and a dense multi-class 2D-3D correspondence map Ψpri between an input image IMA(i) and an available 3D model, e.g. MODOBJ, from the image IMA(i).

The coarse pose estimation step CPES applies a Perspective-n-Point approach (PnP), especially supplemented by a random sample consensus approach (RANSAC), to determine a respective initial or preliminary, as the case may be, object pose Tpr(0), Tpr,j(0) from the at least one 2D-3D-correspondence map Ψpri, Ψprj. Given the estimated ID mask, we can observe which objects were detected in the image and their 2D locations, whereas the correspondence map maps each 2D point to a coordinate on an actual 3D model. The 6D pose is then estimated using the Perspective-n-Point (PnP) pose estimation method, e.g. described in [ZHANG2000], that estimates the camera pose given correspondences and intrinsic parameters of the camera. Since a large set of correspondences is generated for each model, RANSAC is used in conjunction with PnP to make camera pose prediction more robust to possible outliers: PnP is prone to errors in case of outliers in the set of point correspondences. RANSAC can be used to make the final solution for the camera pose more robust to such outliers.

In some embodiments, a pose estimation system for refining an initial multi-dimensional pose Tpr(0) of an object of interest OBJ to generate a refined multi-dimensional object pose Tpr(NL) with NL≥1 includes a control system configured for executing one or more of the pose estimation methods PEM described above.

As a summary, the objective achieved by the presented solution is to further reduce the discrepancy between performances of the detectors trained on synthetic and on real data by introducing a novel geometrical refining method, building up in the earlier determined initial coarse pose estimation Tpr(0). In regular operation, the proposed pose refining procedure utilizes the differentiable renderer in the inference phase. It uses multiple views PER(i), POS(i), and PARA(i), respectively, adding relative camera poses POS(i) as constraints to the pose optimization procedure. This is done by comparing the provided Ψpri and rendered dense correspondences Ψrendk,i for each image IMA(i) and then transmitting the error back though the differentiable renderer to update the pose Tpr(k). This assumes the availability of camera positions POS(i) or perspectives PER(i), wherein such poses or perspectives might be relative information, referring to one reference position, e.g. POS(0), or reference perspective, e.g. PER(0). In practice, POS(i) and PER(i), as the case may be, can be easily obtained by a number of various methods, such as placing object on a markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard or the camera. The markerboard will allow to compute camera poses POS(i), PER(i) in the markerboard coordinate system and consequently compute relative poses between the cameras. Moreover, in the scenario of robotic grasping, the robotic arm can be equipped with a camera to observe the object from several viewpoints POS(i). There, one can rely on precise relative poses between them provided by the robotic arm. However, the 6DoF pose of the object in the images stays unknown. Therefore, we aim at estimating the 6DoF object pose in one reference view with relative camera poses used as constraints.

In further summary, a multi-view refinement method is proposed that can be used to significantly improve detectors trained on synthetic data via multi-view pose refinement. In this way, the proposed approach completely avoids use of labeled real data for training.

The elements and features recited in the appended claims may be combined in different ways to produce new claims that likewise fall within the scope of the present disclosure.

The specification provides the following publications to achieve a detailed explanation of the teachings herein and their execution. Each publication is incorporated by reference:


- \[Barron2019\] J. T. Barron, “A general and adaptive robust loss
  function,” in CVPR, 2019.
- \[Kingma2014\] D. P. Kingma and J. Ba, “Adam: A method for stochastic
  optimization,” arXiv preprint arXiv:1412.6980, 2014.
- \[Redmon2016\] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali
  Farhadi. “You only look once: Unified, real-time object detection”. In
  CVPR, 2016.
- \[Redmon2017\] Joseph Redmon and Ali Farhadi. Yolo9000: better,
  faster, stronger. In CVPR, 2017.
- \[Redmon2018\] Joseph Redmon and Ali Farhadi. “Yolov3: An incremental
  improvement”, arXiv preprint arXiv:1804.02767, 2018.
- \[Wang2019\] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin,
  Shuran Song, and Leonidas J Guibas. “Normalized object coordinate
  space for category-level 6d object pose and size estimation”. In CVPR,
  2019.
- \[Zakharov2019\] Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic.
  “Dpod: 6d pose object detector and refiner”. In ICCV, 2019.
- \[Zhou2019\] Y. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li, “On the
  continuity of rotation representations in neural networks,” in CVPR,
  2019.
- \[Zhang2000\] Zhengyou Zhang. “A flexible new technique for camera
  calibration”. IEEE Transactions on pattern analysis and machine
  intelligence, 22, 2000.

FIG. 1 shows an exemplary real world scene with an object of interest OBJ. The object's OBJ real position in the scene is such that it can be described by a ground truth 6D object pose Tgt, including three translational degrees of freedom and coordinates, respectively, as well as three rotational degrees of freedom and coordinates, respectively. However, the real pose Tgt is unknown and has to be estimated by a pose estimation system 100 as described herein.

The pose estimation system 100 shown in FIG. 1 comprises a control system 120 for executing the pose estimation method PEM described below and camera system 110 with a plurality of cameras 110-i with i=1, . . . , I and I≥2 which are located at different positions POS(i). In the setup shown in FIG. 1, which is only exemplary, I=3 cameras 110-i are shown. The cameras 110-i are positioned such that they capture images IMA(i) of the scene, therewith depicting the object of interest OBJ. Especially, the cameras 110-i are positioned such that they depict the object OBJ from different perspectives PER(i), e.g. under different viewing angles.

Instead of utilizing a plurality of cameras it would also be possible to use one single camera (not shown) which is movable so that it can be moved to the different positions POS(i) to depict the object from corresponding different perspectives PER(i).

Independent from whether a movable camera or a plurality of cameras is used to capture images IMA(i) from different positions POS(i), the method described in the following assumes that the camera positions POS(i) or perspectives PER(i) are known. Therein, the positions POS(i) can be relative positions, either expressed relative to each other or by selecting one of them, e.g. POS(1), as the reference position POSref and expressing the remaining positions relative to POSref. Thus, transformations between camera positions POS(i) are known as well.

In practice, the positions POS(i) can be obtained by a number of various methods, such as placing object on a markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard. A markerboard will allow to compute camera positions POS(i) in the markerboard coordinate system and consequently compute relative poses between the cameras 110-i. In the scenario of robotic grasping, the robotic arm can be equipped with a camera to observe the object from several viewpoints. Therein, one can rely on precise relative poses between them provided by the robotic arm. However, the 6DoF pose of the object in the images stays unknown. Therefore, an estimation of the 6DoF object pose in one reference view with relative camera poses used as constraints is applied.

Moreover, intrinsic camera parameters CAM(i) for capturing a respective image IMA(i) of the object of interest OBJ are known. Therein, “intrinsic camera parameters” is a well-defined term which refers to how the camera projects the 3D scene onto a 2D plane. The parameters include focal lengths, principal point and sometimes distortion coefficients.

As a summary, for each image IMA(i) imaging parameters PARA(i) applied for capturing a respective image IMA(i) are assumed to be known. The imaging parameters PARA(i) include the respective camera position POS(i) and corresponding intrinsic camera parameters CAM(i). For example, parameters PARA(i) can be provided to the control system 120 for further processing with the captured images IMA(i).

The method described in the following moreover uses the availability of a 3D model MODOBJ of the object of interest OBJ, e.g. a 3D CAD model. This can be stored in a corresponding memory of the control system 120 or it can be provided from elsewhere when required.

As shown in FIG. 2, the pose estimation method PEM is subdivided into two procedures, namely an initial pose estimation procedure PEP and a subsequent pose refinement procedure PRP. PEM receives as input at least the images IMA(i), parameters PARA(i), and the model MODOBJ and produces as an output the object pose Tpr(NL).

For an initial estimation of the coarse object pose Tpr(0) of the object of interest OBJ in the initial pose estimation procedure PEP, which shown in FIG. 3, at least one such image IMA(i), e.g. IMA(1), has to be captured in a capturing step CAP with corresponding imaging parameters PARA(1). However, since more than one image IMA(i) with different imaging parameters PARA(i) should be available in the pose refinement procedure PRP, several images IMA(i), still fulfilling i=1, . . . , I and I≥2, are captured in the capturing step CAP. The captured images IMA(i) are processed in a step DCS of determining, for each image IMA(i), a respective segmentation mask SEGpr(i) as well as a respective 2D-3D-correspondence map Ψpri between the 2D image IMA(i) and the 3D model MODOBJ of the object of interest OBJ. Step DCS forms the first step of the pose estimation procedure PEP.

Therein, a segmentation mask SEGpr(i) of an image IMA(i) is a binary 2D matrix with pixel values “1” or “0”, marking the object of interest in the image IMA(i). I.e. only pixels of SEGpr(i) which correspond to pixels of IMA(i) which belong to the depicted representation of the object OBJ in IMA(i) receive a pixel value “1” in SEGpr(i).

2D-3D-correspondence maps are described in [Zakharov2019]. A 2D-3D-correspondence map Ψpri between the pixels of the image IMA(i) and the 3D model MODOBJ directly provides a relation between 2D IMA(i) image pixels and 3D model MODOBJ vertices. For example, a 2D-3D-correspondence map can have the form of a 2D frame, describing a bijective mapping between the vertices of the 3D model of the object OBJ and pixels on the image IMA(i). This provides easy-to-read 2D-3D-correspondences since given the pixel color one can instantaneously estimate its position on the model surface by selecting the vertex with the same color value.

The step DCS of determining, for each image IMA(i), the respective segmentation mask SEGpr(i) as well as the respective 2D-3D-correspondence map Ψpri can be executed by a DPOD approach as described in detail in [Zakharov2019]: DPOD is based on an artificial neural network ANN which processes an image IMA(i) as an input to produce a segmentation mask SEGpr(i) and a 2D-3D-correspondence map Ψpri. For training of ANN and DPOD, respectively, the network ANN is trained separately for each potential object of interest. To train the network ANN for an object, a textured model MOD of that object is required. The model MOD is rendered in random poses to produce respective images IMApose. For each of the rendered images IMApose a foreground/background segmentation mask SEGpose and per-pixel 2D-3D correspondence map Ψpose is generated. The availability of a 2D-3D correspondence map Ψpose means that for every foreground pixel in the rendered image IMApose it is known which point on the 3D model MOD it corresponds to. Then, the network ANN is trained to take a RGB image and output the segmentation mask SEG and the correspondence map Ψ. This way the network ANN memorizes the mapping from object views to the correct 2D-3D correspondence maps Ψ and can extrapolate that to unseen views.

In some embodiments, the step DCS of determining for each image IMA(i) the respective segmentation mask SEGpr(i) and the respective 2D-3D-correspondence map Ψpri can apply a modified DPOD approach, being subdivided into two substeps DCS1, DCS2.

In the first substep DCS1, the provided images IMA(i) are processed to detect a respective object of interest OBJ in the respective image IMA(i) and to output a tight bounding box BB(i) around the detected object OBJ and a corresponding semantic label LAB(i), e.g. an object class, characterizing the detected object. The label LAB(i) is required in the approach described herein because DPOD is trained separately for each object. This means that one DPOD can only predict correspondences for one particular object. Therefore, the object class is needed to choose the right DPOD. DCS1 might apply an approach called “YOLO” as described in [Redmon2016], [Redmon2017], and especially [Redmon2018], i.e. an artificial neural network ANN′ trained to detect an object in an image and to output a corresponding bounding box and label.

The second substep DCS2 of the step DCS of determining, for each image IMA(i), the respective segmentation mask SEGpr(i) as well as the respective 2D-3D-correspondence map Ψpri, a DPOD-like architecture DPOD′ can be applied which predicts object masks SEGpr(i) and dense correspondences Ψpri from the detections provided by DCS1. I.e. DPOD′ predicts for each pixel the corresponding point on the object's surfaces. Therefore, 2D-3D-correspondences between image pixels and points of the surface of the 3D model are produced.

The two-stage approach of DCS including DCS1 and DCS2 simplifies and accelerates the training procedure of each substep and improves the quality of correspondences Ψpri, but in essence does not affect the accuracy of the original one-step approach via DPOD.

In some embodiments, in contrast to the DPOD approach described in [Zakharov2019], which utilizes UV mapping, the further optional embodiment applies the 3D normalized object coordinates space (NOCS) as described in [Wang2019]. Each dimension of NOCS corresponds to a uniformly scaled dimension of the object to fit into a [0, 1] range. This parameterization allows for trivial conversion between the object coordinate system and the NOCS coordinate system, which is more suitable for regression with deep learning. A model M can be defined as a set of its vertices v with ={v∈}. Furthermore, operators can be defined which compute minimal and maximal values along a vertex dimension DIMi as minDIMi()=min and maxDIMi()=max. Then, for any point px, a NOCS projection operator (px) is defined with respect to the model M as

\({\pi_{\mathcal{M}}({px})} = \left\{ {\frac{{px}_{x} - {\min_{x}(\mathcal{M})}}{{\max_{x}(\mathcal{M})} - {\min_{x}(\mathcal{M})}},\frac{{px}_{y} - {\min_{y}(\mathcal{M})}}{{\max_{y}(\mathcal{M})} - {\min_{y}(\mathcal{M})}},\frac{{px}_{z} - {\min_{z}(\mathcal{M})}}{{\max_{z}(\mathcal{M})} - {\min_{z}(\mathcal{M})}}} \right\}\)

with a corresponding inverse .

However, a subsequent coarse pose estimation step CPES of the initial pose estimation procedure PEP provides an initial estimation Tpr(0) of the object pose. The coarse pose estimation step CPES applies a Perspective-n-Point approach (PnP), e.g. described in [Zhang2000], preferably supplemented by a random sample consensus approach (RANSAC), to determine the initial object pose Tpr(0) based on an output of the preceding determination step DCS. RANSAC is used in conjunction with PnP to make the estimation of Tpr(0) more robust to possible outliers. PnP is prone to errors in case of outliers in the set of point correspondences. RANSAC can be used to make the final estimation more robust to such outliers.

In some embodiments of CPES, not all but only one reference map of the plurality I of 2D-3D-correspondence maps Ψpri provided by DCS is utilized by PnP and RANSAC to determine Tpr(0), e.g. with i=1.

In some embodiments, not only one but a plurality J with I≥J≥2 of 2D-3D-correspondence maps Ψpri is selected to be utilized to determine Tpr(0). Each selected 2D-3D-correspondence map Ψpri is processed as described above with PnP and RANSAC to determine a respective preliminary object pose Tpr,j(0). The initial object pose Tpr(0) is then calculated to be an average of the preliminary object poses Tpr,j(0).

As an intermediate summary, the initial pose estimation procedure PEP which is completed at this point of the overall pose estimation method PEM comprises the determination step DCS of determining, for each image IMA(i) captured in the upstream image capturing step CAP, the respective segmentation mask SEGpr(i) as well as the respective 2D-3D-correspondence map Ψpri and the coarse pose estimation step CPES in which at least one of the 2D-3D-correspondence maps Ψpri is further processed to determine the initial object pose Tpr(0). Thus, at this point of the overall pose estimation method PEM a plurality of 2D-3D-correspondence maps Ψpri, a respective plurality of segmentation masks SEGpr (i), the initial object pose Tpr (0), the model MODOBJ, as well as the imaging parameters PARA(i) are available and are provided to the next step of the pose estimation method, i.e. to the pose refinement procedure PRP.

The pose refinement procedure PRP as shown in FIG. 4 and as described in detail below is based on a differentiable renderer dREND. It uses multiple views i adding camera positions POS(i) as constraints to an iterative pose Tpr(k) optimization procedure with a number NL of loops k=1, . . . , NL based on the optimization of a loss function LF. The procedure compares the 2D-3D-correspondence maps Ψpri provided by the pose estimation procedure PEP with rendered correspondence maps Ψrendk,i computed for each i by the differentiable renderer dREND and then transmits an error back though the differentiable renderer dREND to update the object pose from Tpr(k−1) to Tpr(k) in terms of object pose updates ΔT, such that Tpr(k)=ΔT·Tpr(k−1). The pose refinement procedure PRP first utilizes the initial object pose Tpr(0). The initial predicted pose Tpr(0) is then refined in NL≥1 iterations and loops k, so that in the end a refined pose Tpr(k=NL) is available. The pose refinement is based on an optimization of the discrepancy between provided correspondence maps Ψpri and related rendered correspondence map Ψrendk,i for each i. The rendered correspondence maps Ψrendk,i and, correspondingly, the discrepancy directly depend on the assumed object pose Tpr(k) so that a variation of the object pose Tpr(k) leads to a variation of the discrepancy, such that a minimum discrepancy can be considered to be an indicator for correctness of the assumed object pose Tpr(NL).

Thus, the pose refinement procedure PRP estimates the refined object pose Tpr(NL) by an iterative optimization procedure IOP of a loss. The loss is according to the given loss function LF(k) and depends on discrepancies between the provided 2D-3D-correspondence maps Ψpri and respective rendered 2D-3D-correspondence maps Ψrendk,i. Thus, for every provided 2D-3D-correspondence map Ψpri a respective rendered 2D-3D-correspondence map Ψrendk,i is required, so that a comparison becomes possible. Such correspondence maps Ψpri, Ψrendk,i might be considered to be assigned to each other and have in common that they both relate to the same image IMA(i), position POS(i), perspective PER(i), and imaging parameters PARA(i), respectively, indicated by the common parameter “i”.

In some embodiments, the iterative optimization procedure IOP might end at k=NL when the corresponding loss function LF, which depends on Tpr(k) and ΔT, respectively, converges or falls under a given threshold or similar. I.e. NL is not pre-defined but depends on the variation of Tpr(k) and the resulting outcome of the loss function LF. General criteria for ending an iterative optimization procedure of a loss function as such are known. However, the object pose Tpr(NL) achieved in loop k=L is finally assumed to be the aspired, refined object pose.

In more detail, a starting point of the pose refinement procedure PRP in each loop k of the iterative optimization procedure IOP would be the rendering of a rendered 2D-3D-correspondence map Ψrendk,i and of a segmentation map SEGrend (k, i) for each i. Such rendering is achieved by the differentiable renderer dREND mentioned above. Therein, the differentiable renderer dREND can be a differentiable implementation of a standard renderer, e.g. known from computer graphics applications. For example, such differentiable renderer takes a textured object model, an object's pose, light courses, etc. and produces a corresponding image. In contrast to standard rendering, a differentiable renderer allows to define any function over the image and to compute its derivatives w.r.t. all the renderer inputs, e.g. a textured object model, an object's pose, light courses, etc. as mentioned above. In such a way, it is possible to directly update the object, its colors, its position, etc. in order to get the desired rendered data set.

In each loop k, starting with k=1, the differentiable renderer dREND requires as an input an assumed object pose Tpr(k), the 3D model MODOBJ of the object of interest OBJ, and the imaging parameters PARA(i), especially the camera position POS(i) and the intrinsic parameters CAM(i). In that loop k, the differentiable renderer dREND produces as an output the rendered 2D-3D-correspondence map Ψrendk,i and the segmentation map SEGrend(k, i) for each respective i from the provided input. I.e. given the 3D model MODOBJ, camera position POS(i), corresponding intrinsic camera parameters CAM(i), and Tpr(k) it is possible to compute which vertex of the 3D model MODOBJ would be projected on which pixel of a rendered 2D image. This correspondence is expressed by a respective 2D-3D-correspondence map Ψrendk,i. Such process is deterministic and errorless and can be executed by the differentiable renderer dREND. The resulting correspondence map Ψrendk,i exactly corresponds to the model MODOBJ in the given pose Tpr(k) from the perspective PER(i) of the respective camera position POS(i) and can be compared to the respective provided 2D-3D-correspondence map Ψpri.

The assessment whether an object pose Tpr(k) assumed in loop k is sufficiently correct happens in a loss determination step LDS based on the determination of the per pixel (x, y) loss function LF(k), wherein LF(k) is defined as

\({{LF}(k)} = {\frac{1}{I}{\sum\limits_{i = 1}^{I}{L\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\left( {k,i} \right)},\Psi_{pr}^{i},\Psi_{rend}^{k,i}} \right)}}}\)

with summands

\({L\left( {{T_{pr}(k)},{{SEG}_{pr}(i)},{{SEG}_{rend}\left( {k,i} \right)},\Psi_{pr}^{i},\Psi_{rend}^{k,i}} \right)} = {\frac{1}{N}{\sum\limits_{{({x,y})} \in {{{SEG}_{pr}(i)}\bigcap{{SEG}_{rend}({k,i})}}}{\rho\left( {{\pi_{\mathcal{M}}^{- 1}\left( {\Psi_{pr}^{i}\left( {x,y} \right)} \right)},{\pi_{\mathcal{M}}^{- 1}\left( {\Psi_{rend}^{k,i}\left( {x,y} \right)} \right)}} \right)}}}\)

Therein, I expresses the number of provided 2D-3D-correspondence maps Ψpri, x, y are pixel coordinates in the correspondence maps Ψpri, Ψrendk,i, SEGpr(i)∩SEGrend(k, i) is the group of intersecting points of provided Ψpri and rendered correspondence maps Ψrendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i), N is the number of such intersecting points of provided Ψpri and rendered correspondence maps Ψrendk,i, expressed by the corresponding segmentation masks SEGpr(i), SEGrend(k, i), and ρ stands for an arbitrary distance function in 3D. There are numerous possible ways to implement the distance function ρ:×→R+. As the provided Ψpri and rendered correspondence maps Ψrendk,i might contain a potentially large number of outliers, a robust function must be used to mitigate the problem. For example, the general robust function introduced in [Barron2019] qualifies. Moreover, the continuous rotation parameterization from [Zhou2019] can be applied used. This parameterization enables faster and more stable convergence during the optimization procedure. (px) and its inverse  represent the NOCS transformation introduced above.

Actually, the loss function LF describes a pixel-wise comparison of Ψpri and Ψrendk,i and the pixel-wise difference is minimized across the loops k of the iterative optimization procedure IOP.

Therein, the object pose Tpr(k) assumed in loop k is selected such that Tpr(k) differs from the assumed object pose Tpr(k−1) of the preceding loop k−1, wherein the iterative optimization procedure IOP applies a gradient-based method for the selection, wherein the loss function LF is minimized in terms of object pose updates ΔT, such that Tpr(k)=ΔT·Tpr(k−1). I.e. the loss function LF(k) is minimized iteratively across loops k by gradient descent over the object pose update ΔT. This could be done with any gradient-based methods, such as [Kingma2014]. Convergence might be achieved within, for example, 50 optimization steps, i.e. NL=50.

In general, the describes approach allows accurate 6DoF pose estimation, even in a monocular case in which only one image IMA(i) with I=1 and correspondingly only one rendered correspondence map Ψrendk,i is utilized. Any imprecision in pose estimation, which is not visible in the monocular case, will easily be seen when the object OBJ is observed from a different perspective PER(j).

For example, during the pose refinement procedure PRP only a single transformation is optimized, namely the reference pose Tpr. For each image in the set of calibrated cameras, object pose is transformed from the coordinate system of a reference image, e.g. IMA(1), to the coordinate system of each particular camera CAM(i) using the known relative camera positions POS(i). Given a vertex v∈ in the model coordinate system, it is transformed to the i-th camera CAM(i) coordinate system via vC=Pirel·Tpr·{tilde over (v)}.

Respectively, the transformation Pirel·Tpr is used by the renderer dREND to render SEGrend(i) and Ψrendk,i for the i-th image IMA(i). Loss in each frame is then used to compute gradients through the renderer dREND in order to compute a joint update Tpr(k)=ΔT·Tpr(k−1).

The teachings of this disclosure may further overcome discrepancies between performance of detectors trained on synthetic and on the real data. Following the dense correspondence paradigm of DPOD, the DPOD detector is trained only on synthetically generated data. The introduced pose refinement procedure PRP is based on the differentiable renderer dREND in the inference phase. Herein, the refinement procedure is extended from a single view with I=1 to multiple views with I>1, adding relative camera poses POS(i) as constraints to the iterative optimization procedure IOP. In practice, relative camera poses POS(i) can be easily obtained by placing the object of interest on the markerboard and either using an actual multi-camera system or using a single camera but moving the markerboard or the camera. The markerboard will allow to compute camera poses in the markerboard coordinate system and consequently compute relative poses between the cameras. In reality, this scenario is easy to imagine for robotic grasping where the robotic arm equipped with the camera can observe the object from several viewpoints. There, one can rely on precise relative poses provided by the robotic arm. The 6DoF pose of the object in the images stays unknown. Therefore, we aim at estimating the 6 DoF object pose in one reference view with relative camera poses used as constraints.

