{
    "id": "https://semopenalex.org/work/W3201891406",
    "authors": [
        "Daniel Vlasic",
        "Forrester Cole",
        "Avneesh Sud",
        "Zhoutong Zhang",
        "Kyle Genova"
    ],
    "title": "Differentiable Surface Rendering via Non-Differentiable Sampling",
    "date": "2021-10-01",
    "abstract": "We present a method for differentiable rendering of 3D surfaces that supports both explicit and implicit representations, provides derivatives at occlusion boundaries, and is fast and simple to implement. The method first samples the surface using non-differentiable rasterization, then applies differentiable, depth-aware point splatting to produce the final image. Our approach requires no differentiable meshing or rasterization steps, making it efficient for large 3D models and applicable to isosurfaces extracted from implicit surface definitions. We demonstrate the effectiveness of our method for implicit-, mesh-, and parametric-surface-based inverse rendering and neural-network training applications. In particular, we show for the first time efficient, differentiable rendering of an isosurface extracted from a neural radiance field (NeRF), and demonstrate surface-based, rather than volume-based, rendering of a NeRF.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Computing the derivatives of rendered surfaces with respect to the underlying scene parameters is of increasing interest in graphics, vision, and machine learning. Triangle meshes are the predominant shape representation in many industries, but mesh-based derivatives are undefined at occlusions or when changing topology. As a result, volumetric representations have risen in prominence for computer vision applications, notably Neural Radiance Fields or NeRF [27]. So far, these volumetric shape representations have been rendered using volume rendering. Volume rendering is naturally differentiable, but is expensive and unnecessary if the underlying shape can be represented well by a surface.",
                "This paper proposes a method to render both explicit (e.g., mesh) and implicit (e.g., isosurface) representations and produce accurate, smooth derivatives, including at occlusion boundaries. Our method uses a non-differentiable rasterization step to sample the surface and resolve occlusions, then splats the samples using a depth-aware, differentiable splatting operation. Because the sampling operation need not be differentiable, any conventional surface extraction and rasterization method (e.g., Marching Cubes [24]) Figure 1. Our method provides efficient, differentiable rendering for explicit and implicit surface representations. Examples include a textured triangle mesh (YCB toy airplane [4]), a cubic Bspline surface, and an isosurface of a density volume (Lego from NeRF [27]). The Lego is rendered by turning a pretrained NeRF into a surface light field. Since surface light fields only require one evaluation per pixel, we achieve a 128\u00d7 speed up for rendering compared with the original NeRF. may be used. The splats provide smooth derivatives of the image w.r.t. the surface at occlusion boundaries. Splatting is performed on a fixed-size pixel grid and is easily expressed using automatic-differentiation, avoiding the need for custom gradients. Since no custom gradients are needed, both forward-and reverse-mode differentiation are immediately supported. We term this method rasterize-then-splat (RtS).",
                "In between the rasterization and splatting steps, the surface samples may be shaded by any differentiable function evaluated on a rasterized image buffer -not the original surface -using deferred shading [9]. Since the complexity of the shading and splatting computation is bounded by the number of pixels, not the complexity of the surface, RtS is able to scale to highly detailed scenes.",
                "One example of a differentiable shading function is a NeRF network: given a position in space and a viewing direction, it outputs the corresponding radiance. While NeRF is trained using volume rendering, our method can convert a pretrained NeRF into a surface light field [28,39], removing the need for expensive raymarching. We represent the surface as an isosurface of the density field extracted from a pretrained NeRF, shade it with the NeRF color prediction branch, and jointly finetune the NeRF network and the density field. The resulting optimized surface and surface light field matches the original NeRF network in rendering quality (within 0.3 PSNR) but requires only a single network evaluation per pixel, producing a 128\u00d7 speedup (Fig. 1).",
                "We further demonstrate that RtS provides high-quality derivatives for inverse rendering of meshes and parametric surfaces, while remaining simple to implement. An implementation of RtS for mesh-based rendering is provided as part of TensorFlow Graphics1 ."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Early differentiable rendering explored the derivatives of specialized shape parameterizations (e.g., a morphable model [3] or a heightfield [36, 15,2]). Recent work has focused on general 3D triangle meshes and implicit representations such as signed-distance fields and volumes."
            ],
            "subsections": [
                {
                    "title": "Rendering Triangle Meshes",
                    "paragraphs": [
                        "When rendering triangle meshes, topology is assumed to be constant. The remaining major challenge for computing derivatives is handling of occlusion boundaries. Previous work falls into four main categories: Gradient Replacement. Methods such as [16,23,14] use heuristics to define smooth derivatives for the mesh rendering while leaving the forward rendering unchanged. OpenDR [23] and DiRT [14] use image-space filtering operations to find the derivatives, while Neural Mesh Renderer (NMR) [16] defines a special, per-triangle derivative function. A differentiable version of surface splatting [42] is proposed in [41] with a modified gradient function. These approaches do not easily support textures or complex shading models, and in some cases produce convergence problems due to the mismatch between the rendering and its gradient. Custom gradient functions are implemented only for the Jacobian-vector product necessary for gradient backpropagation, and new, additional functions are necessary to support forwards-mode or higher-order derivatives. Edge Sampling. Redner [19], nvdiffrast [18], DEODR [8], and others [13,10] explicitly sample the occluding edges of the shape to compute derivatives. They require shape processing to find and sample the edges, so the cost of computing derivatives grows with the number of edges in the mesh. Nvdiffrast mitigates the cost using tightly optimized CUDA kernels, however their code requires specific GPU hardware and is not easy to alter for new systems. RtS can be implemented without any shape processing or custom derivative code, and the cost of the differentiable sections is independent of the size of the mesh. Reparameterizing the Rendering Integral. When performing Monte-Carlo path tracing, occlusion discontinu-ities may be handled using reparameterizations of the rendering equations [25,1]. These methods are related to ours in that they choose surface samples without explicit sampling of occlusion boundaries. However, these methods apply only in the context of path tracing, while RtS supports simple shading and rasterization. Forward Rendering with Smooth Gradients. Similar to our approach, Soft Rasterizer and related methods [21,32,6], as well as the differentiable visibility method of [34] change the forward rendering process such that its gradient is smooth by construction. Unlike Soft Rasterizer, RtS does not require costly closest-point queries or mesh processing."
                    ],
                    "subsections": []
                },
                {
                    "title": "Surface Splatting",
                    "paragraphs": [
                        "Surface splatting [42] treats the surface as a point cloud and renders disk primitives at each point that overlap to create a continuous surface. Splatting has been adapted for differentiable rendering of 3D shapes [41] and forwardwarping of images [30]. Similar to splats, differentiable visibility using 3D volumetric primitives has also been explored [34]. Compared to these approaches, our method uses a true surface representation as the underlying geometry, rather than a point set, and resamples the splats at each frame, avoiding issues with under-or over-sampling of splats as optimization proceeds."
                    ],
                    "subsections": []
                },
                {
                    "title": "Rendering Implicit Surfaces",
                    "paragraphs": [
                        "Implicit surface representations such as signed-distance fields naturally handle topological changes, but rendering still requires explicit handling of occlusion boundaries. In recent work, an occlusion mask is sometimes assumed to be provided by the user [40,29], or computed by finding the nearest grazing point on a ray that hits the background [22]. Neither method handles self-occlusion, which is the only type of occlusion in walkthrough-style scenes (Fig. 10).",
                        "Volume rendering [11,27] provides smooth derivatives at occlusions, including self-occlusions, but requires expensive ray marching to find the surface. Marching Cubes (MC) isosurface extraction [24] may be used to convert the volume into a surface for optimization, but this process is not naturally differentiable [20]. Our method extracts and rasterizes the isosurface in a single non-differentiable step, then computes derivatives in image-space, avoiding the singularity in the MC derivative.",
                        "Most related to RtS is MeshSDF [33], which also uses non-differentiable sampling of the implicit surface, followed by differentiable occlusion testing using NMR [16]. However, MeshSDF defines a custom derivative using the normal of the SDF, a technique that holds for true SDFs but not for general isosurfaces. Further, MeshSDF demonstrates only a neural representation of the surface, whereas our method allows isosurfaces parameterized by a grid or any other differentiable function. "
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Result",
            "paragraphs": [
                "Figure 2. Rasterize-then-splat system. Scene parameters \u03b8 are first passed through a sampling function U, yielding per-layer screen-space parameter buffers U k . These buffers are non-differentiable and computed only in the forward pass (dotted arrows). Evaluation function G uses them to interpolate shape attributes, such as positions and normals, into G-buffers G k . Automatic differentiation may be applied to G and downstream functions (solid arrows). The evaluated attributes are combined to compute splat colors C k via deferred shading function C and the corresponding screen-space positions P k via projection function P. Function S then splats the shaded colors the corresponding pixel locations using a 3x3 kernel to produce the final result S."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [
                "The rasterize-then-splat method consists of three steps: rasterization of the surface (Sec. 3.1), shading the surface samples (Sec. 3.2), and multi-layer splatting (Sec. 3.3). All derivatives are produced using automatic differentiation, so the implementer only needs to write the forward rendering computation (Fig. 2)."
            ],
            "subsections": [
                {
                    "title": "Rasterization via Non-Differentiable Sampling",
                    "paragraphs": [
                        "Rasterization can be expressed as a function that takes scene parameters \u03b8 containing geometry attributes such as position, normal, or texture coordinates, as well as camera parameters, and produces screen-space geometry buffers (or G-buffers [35]) G k\u22081..K containing interpolated attributes at the K closest ray intersections to the camera. To make this process both differentiable and efficient, we divide rasterization into two stages: a sampling function U(\u03b8) \u2192 U k that produces non-differentiable surface parameters U k , and an evaluation function G(\u03b8, U k ) \u2192 G k that produces the G-buffers. The necessary parameters vary with the surface type (see below).",
                        "Evaluation of surface attributes given surface parameters is typically a straightforward interpolation operation, so G can be easily expressed in an automatic-differentiation framework. The difficult and computationally-intensive operation is the sampling function U that finds the intersections of the surface with the camera rays. However, since we are not interested in derivatives w.r.t. the sampling pattern itself, U may act as a non-differentiable \"oracle\" that finds the intersections but produces no derivatives for them.",
                        "Below we give concrete examples of U and G for triangle meshes, parametric surfaces, and implicit surfaces."
                    ],
                    "subsections": [
                        {
                            "title": "Triangle Meshes",
                            "paragraphs": [
                                "For triangle meshes, the parameters U k consist of per-pixel triangle indices T k and the (perspective-correct) barycentric coordinates B k of the pixel inside the corresponding triangle. The sampling function U can compute these values extremely efficiently with conventional Z-buffer graphics processing, using depth peeling [12] to retrieve multiple intersections per pixel. The evaluation function G simply looks up the three vertex attributes for each pixel using T, then interpolates them using B."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "Parametric Surfaces",
                            "paragraphs": [
                                "Bicubic regular B-spline surfaces [5] are a type of smooth parametric surface, a representation that so far has not supported differentiable rendering. Efficient rasterization of these surfaces is achieved by subdividing rectangular patches until the resulting facets are smaller than a pixel, complicating the propagation of derivatives. We avoid this difficulty with the non-differentiable sampling function U that returns per-pixel patch indices and patch parameters. The evaluation function G then interpolates the patch vertex attributes using the parameters and the B-spline basis matrix (Equation 1 in [5]). This approach can be extended to all parametric surfaces with a closed form evaluation expression, such as Catmull-Clark subdivision surfaces [38] and B\u00e9zier surfaces."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "Implicit Surfaces",
                            "paragraphs": [
                                "We treat implicit surfaces as isocontours of a function f \u03b8 : R 3 \u2192 R over 3D space. Unlike meshes or spline patches, implicit surfaces do not have a natural parameterization. We choose a parameterization based on the triangulation of the isosurface provided by the Marching Cubes [24] algorithm.",
                                "The parameters U k are 9-D vectors, consisting of 6 lattice indices v 1..6 defining the 3 edges that cross the isosurface, and 3 triangle barycentric coordinates \u03b2 1..3 .",
                                "The evaluation function G evaluates f at the v 1..6 (simply looking up their values if f is already defined on a grid), interpolates along the edges to find the coefficients \u03b1 1..3 that define the triangle vertices, then interpolates the vertices using \u03b2 1..3 to produce the surface point x. Critically, this scheme hides the complex topological rules of Marching Cubes in the non-differentiable function U, removing the need to store or differentiate through the full mesh topology [20].",
                                "Note that while the Marching Cubes algorithm is not itself differentiable due to the singularity when neighboring grid values are nearly identical [20,33], our procedure sidesteps this issue by evaluating the surface only where the derivative is well-defined. We remove samples from nearly identical grid cells in the sampling function U."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "Shading",
                    "paragraphs": [
                        "The G-buffers G k contain various surface attributes depending on the shading required. Any shading function C that can be expressed as a deferred shading operation [9] can be applied. For a texture-mapped mesh (Fig. 6), each pixel in G k contains a 3D position, a 3D surface normal and 2D texture coordinates. For parametric surface rendering (Fig. 8) and implicit surface rendering using a NeRF shader (Fig. 10), G k contains only 3D world-space positions. The output of the shading step is a set of RGBA buffers C k ."
                    ],
                    "subsections": []
                },
                {
                    "title": "Depth-Aware Splatting",
                    "paragraphs": [
                        "The shaded colors C k have derivatives w.r.t. the surface attributes, but because they were produced using point sampling, they do not have derivatives w.r.t. occlusion boundaries. To produce smooth derivatives at occlusions, the splatting function S converts each rasterized surface point into a splat, centered at the corresponding pixel in P k and colored by the corresponding shaded color in C k . In order to handle splat overlap at occlusion boundaries, we introduce a multi-layer accumulation strategy for the splats based on depth (Sec. 3.3.2) that provides superior accuracy for occlusions and disocclusions (Sec. 4.1).",
                        "Though a splat is always centered on a pixel, the position of the splat must be computed using the surface definition in order for derivatives to flow from the image back to the surface. The splat positions are defined by an additional Gbuffer P k , which contains the screen-space xyz positions of each surface sample. P k may be computed by rendering a G-buffer of object-space xyz positions (Sec. 3.1), then applying the camera view and projection transformation at each pixel."
                    ],
                    "subsections": [
                        {
                            "title": "Single-Layer Splatting",
                            "paragraphs": [
                                "The splat kernel is defined by a Gaussian with narrow variance. If p is the center position of a single splat, the weight of the splat at a nearby pixel q is:",
                                "where is an small adjustment factor, and W p is a normalization factor computed from the sum of all weights in a 3x3 neighborhood. By setting \u03c3 = 0.5, we have:",
                                "The final color s q at pixel q is then the weighted sum of the shaded colors c r of the neighboring pixels r \u2208 N q divided by the accumulated weights:",
                                "where the normalization factor has a floor of 1 to handle boundaries where the weights in N q may sum to < 1.0. Due to the adjustment factor = 0.05, a full 3x3 neighborhood of weights always sum to > 1.0 (see below).",
                                "Importance of Normalization. The need for the adjustment factor in Eq. 1 and the additional normalization in Eq. 3 is subtle; the splats are resampled at exactly pixel rate every frame, so normalization by the accumulated weights of neighboring splats as in [42] is not necessary for forward rendering. The derivatives of s q , however, do not account for resampling, and do need to be normalized by the accumulated weights in order to match the forward pass. Since we want to allow the accumulated weights to sum < 1 at boundaries with the background, we add to ensure the normalization always occurs for interior splats."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "Multi-Layer Splatting",
                            "paragraphs": [
                                "Single-layer splatting treats all splats as existing at the same depth and ignores occlusions, producing spurious derivatives for occluded objects (Fig. 3c). Instead, depending on a splat's relation to the visible surface at a target pixel, it should either occlude the pixel, be occluded itself, or be accumulated as in Eq. 3. Our solution is to render multiple layers of G-buffers, and maintain three accumulation buffers during the splatting process: S + for splats occluding the target pixel, S - for occluded splats, and S o for splats at the same depth as the target pixel. When applying a splat centered at p to a pixel q, weighted colors and weights are accumulated into exactly one of the three buffers (Fig. 4).",
                                "To determine whether the splat lies in front on, behind, or coincident with the pixel, we propose a simple heuristic that is more robust than picking a fixed depth threshold. We pair up the multi-layer surface intersections at p with the closest intersections at q in depth. The p layer paired with the front-most q layer is assigned to S o , layers in front of it (if any) to S + , and the rest to S -.",
                                "Once all splats are rendered, buffers are separately normalized following Eq. 3 and composited in S -, S o , S + order using over-compositing [31] to produce the final result S. This scheme correctly handles occlusions between the first and second layers of the surface (Fig. 3d). When the splat at p is being applied to the pixel q, it contributes to one of the three accumulation buffers: S o (green) when p is coincident with the visible surface at q (thick curve), S + (red) when p is in front, or S -(blue) when p is behind. Different cases arise depending on whether there is an occlusion boundary between p and q. We ignore rare cases of multiple coincident occlusion boundaries."
                            ],
                            "subsections": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "Results and Evaluation",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Analysis of Derivatives",
                    "paragraphs": [
                        "Fig. 3 visualizes the image derivatives for a green bunny superimposed on a black background and on a diffuse red square. The derivative shown is \u2202S/\u2202t, where t is the translation of either the bunny or the square. Finite differences (Fig. 3a) provide a \"ground-truth,\" since \u2206t can be chosen to produce \u2248 1 pixel motion in this case. Multi-layer splatting (Fig. 3b) produces derivatives that closely resemble the finite difference result. Single-layer splatting (Fig. 3c) provides derivatives at occlusion boundaries, but confuses selfocclusions: when the red square moves behind the bunny (middle row), single-layer splatting produces a spurious derivative around the bunny's outline.",
                        "Fig. 3(d-h) show baseline methods for comparison. Differentiable rasterization without splatting (d) provides derivatives in the interior of the shape, but not at the occlusion boundaries. PyTorch3D [32] (e) produces inaccurate derivatives for self-occlusions (bottom row). Redner [19] (f) better handles self-occlusions, but may miss edges due to sampling (middle row). nvdiffrast [18] (g,h) relies on pixel sampling to find edges and so misses sub-pixel edges as exist along the bunny outline. FSAA (h) improves the result but does not solve the problem completely. See supplemental material for the parameters used for these comparisons.",
                        "While the result S looks as if C k were simply blurred slightly, blurring S is not sufficient to produce non-zero derivatives w.r.t. the surface at occlusions. As shown in Fig. 3b, rasterization without splatting produces zero derivative at occlusion boundaries, so any blur following rasterization will also produce zero derivative.",
                        "Effect of blur. The blur applied by our method is slight (\u03c3 = 0.5px), though not invisible. To analyze whether this blur affects optimization when image sharpness is important, we repeat the texture optimization experiment from Fig. 6 of [18], which optimizes a textured sphere to match target images. With mipmapping on, a blur of \u03c3 = 0.5 reduces PSNR by 1.5% from 33.7 to 33.2. With mipmapping off, the blur increases PSNR from 25.4 to 28.3, likely due to the slight anti-aliasing effect of the blur."
                    ],
                    "subsections": []
                },
                {
                    "title": "Pose Estimation",
                    "paragraphs": [
                        "A common use-case for differentiable rendering is to estimate the pose of a known object given one or more images of that object. Compared with previous work, RtS is particularly suitable for this task because its runtime increases slowly with mesh complexity, and it supports more sophisticated optimizers than gradient descent. Performance Comparison. RtS is fast for large meshes (Table 1) as it uses a conventional forward rendering pass over the geometry, followed by image-space operations. Py-Torch3D (based on Soft Rasterizer) requires spatial binning to achieve acceptable performance, and does not scale as well to large meshes. Redner [19] similarly suffers due to the cost of sampling and processing the occluding edges. Nvdiffrast [18] achieves excellent performance at the cost of a complex, triangle-specific implementation. On a task of pose estimation from silhouettes (Fig. 5), RtS achieves a speedup up to 20\u00d7 over PyTorch3D and Redner for the Nefertiti [7] mesh (2m faces), and smaller but significant speedups for the Teapot (2.5K faces). Our method performs within 2\u00d7 of Nvdiffrast without any custom CUDA kernels.",
                        "In the specific case of pose fitting to silhouettes, the sampling function U can return world-space positions directly, instead of triangle ids T and barycentric coordinates B. Since the mesh itself is not changing, only the pose defined by the projection function P, the world-space positions do not need to be differentiable and the evaluation function G  1. Pose estimation performance. Milliseconds per iteration for alignment to silhouettes on V100. Methods compared are \"RtS\" (ours), \"RtS-pose\" (ours optimized for pose fitting), \"P3D\" (PyTorch3D [32]), \"Redner\" [19], and \"Nvdr\" (Nvdiffrast [18]). can skip the potentially costly step of looking up vertex attributes given T. This optimization (\"RtS-pose\") removes the dependence on mesh complexity entirely from the differentiable components of the system, yielding performance largely independent of mesh complexity and faster than Nvdiffrast on the Nefertiti model. Optimization with Levenberg-Marquardt. Since RtS requires no custom gradient code, both forwards-mode and backwards-mode automatic differentiation can be applied. Pose estimation problems have fewer input variables (pose) than output variables (pixels), making forwardmode an efficient choice for computing the full Jacobian matrices required for optimization algorithms such as Levenberg-Marquadt [26], which are prohibitively costly using backwards-mode differentiation. LM optimization provides robust convergence compared to Adam, though under our current implementation, the extra cost of LM means the two methods have similar total runtimes of \u2248 4 seconds to convergence (Fig. 6)."
                    ],
                    "subsections": []
                },
                {
                    "title": "Mesh Optimization",
                    "paragraphs": [
                        "Fig. 7 shows optimization of the vertex positions and colors of a hand template mesh [8]. The hand is first roughly aligned to the input images and given a uniform gray color (Fig. 7a), then optimized to match the input photographs from 3 camera views using Adam. The surface is regularized using an As-Rigid-As-Possible deformation energy [37]. Rather than set a solid background color, a solid rectangle is placed behind the hand to show the effect of multiple layers of geometry.",
                        "As shown in Fig. 7 bottom, multi-layer splatting is important for convergence when optimizing the vertex positions. Convergence is slower for single-layer splatting, and single-layer optimization becomes unstable at higher learning rates. Each vertex depends on fewer pixels compared to pose estimation (Fig. 6) and the shape is more flexible, so errors in the image derivatives are more significant."
                    ],
                    "subsections": []
                },
                {
                    "title": "Parametric Surface Optimization",
                    "paragraphs": [
                        "Fig. 8 shows a simple demonstration of silhouette optimization for a bicubic uniform B-spline surface [5]. The surface is defined by a swept curve, producing an approximate surface-of-revolution. The variables of optimization are the radii of the 8 control points of the profile curve. The surface is initialized to a cylinder, and optimized to match the silhouette of another spline surface similar to a chess piece. The optimization converges in 200 iterations.",
                        "A triangle-based renderer would require differentiation through a densely tessellated mesh, whereas our method only uses tessellation to rasterize the surface parameters in-Figure 8. Silhouette fitting of a B-spline surface. A cylindrical subdivision surface (left) is deformed into a curved shape (right) by fitting silhouettes. Top row shows the optimized control mesh and the corresponding subdivided surface. Bottom row shows the silhouette overlap (black: silhouette agrees with ground truth, red: surface needs to be removed, green: surface needs to be added).  "
                    ],
                    "subsections": []
                },
                {
                    "title": "Implicit Surface Optimization",
                    "paragraphs": [
                        "Fig. 9 shows fitting an image of a torus using an implicit surface and demonstrates that our method can handle topology changes. We show two possible parameterizations based on spheres: the first sweeps a sphere of radius r 1 along a circle of radius r 2 , and a second defines the surface as the union of 200 spheres with individual radii and 2-D positions. The loss is mean-absolute error between the rendering and the target. Both optimizations are run for 400 iterations using Adam [17]. Note that for the swept sphere initialized in the center of the torus, a rotated camera view (Fig. 9 top) is necessary to break symmetry that otherwise traps the optimization in a local minimum.",
                        "These results may be compared to MeshSDF (Fig. 3 in [33]), which also optimizes a sphere into a torus to demonstrate change in topology. In their case, however, the parameterization is a latent code of a deep neural network trained to produce spheres and tori. Unlike MeshSDF, our method does not rely on a deep neural network to compute gradients, so we are free to choose any implicit surface parameterization that can be evaluated on a grid."
                    ],
                    "subsections": []
                },
                {
                    "title": "Surface NeRF",
                    "paragraphs": [
                        "NeRF [27] solves for a function from 3-D position x and viewing direction d to RGB color and density \u03c3, such that when this function is rendered using volumetric raycasting, the result matches a set of posed input images. This method produces very high-quality view synthesis results, at the price of long rendering times. Using our approach, however, we can convert a pre-trained NeRF into a 3D mesh and a surface light field [39]. This representation requires only a single NeRF evaluation per pixel vs. the 128 required by volume rendering, reducing per-pixel cost from 226 MFLOPS to 1.7 MFLOPS. Surface Optimization. We first evaluate NeRF on a regular grid to construct the input density field, then extract an isosurface at a fixed threshold. This surface is an accurate but noisy model of the subject shape (Fig. 10c). Furthermore, since NeRF is trained under the assumption of volume rendering, it must be finetuned in order to produce good results when evaluated only at the surface. Given this initialization, we directly optimize the density grid while finetuning the NeRF network under the original NeRF L2 loss. We render the isosurface using RtS, where the NeRF network produces the shaded colors C k , effectively acting as a surface light field [28]. After optimization, the isosurface is refined (Fig. 10e), and the output RGB has similar quality to the original NeRF rendering (Fig. 10f). We use Adam [17] for finetuning the NeRF network and standard gradient descent for optimizing the density grid, as Adam is unstable for grid optimization due to the isosurface not constraining all grid values at each iteration. At each iteration, we take one gradient step on the NeRF network while holding the density grid fixed, followed by one gradient step on the density grid while holding the NeRF network fixed. We use an isosurface threshold of 50 (suggested by [27]) and optimize for 5000 iterations. Evaluation. Table 2 shows the results of our approach on the NeRF \"Lego\" and \"Fern\" datasets (the two for which pre-trained weights are available). We compare image quality for baseline NeRF and our Surface NeRF (SNeRF). We also evaluate SNeRF without surface optimization (\"fixed surface\") where the NeRF network is finetuned by only sampling on the fixed isosurface. On Lego, a scene that was synthesized from a surface, SNeRF achieves PSNR within 0.3 of NeRF and improves 2.2 PSNR over the baseline without surface optimization. On Fern, SNeRF loses 0.9 PSNR to full volume rendering, and improves only 0.1 PSNR over the fixed surface baseline. This result is likely due to the extremely complex isosurface of the Fern scene (Fig. 10)."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Discussion and Limitations",
            "paragraphs": [
                "Rasterize-then-splat is a general method that addresses two key issues of differentiable surface rendering: handling varying surface representations, and providing derivatives at occlusion boundaries. Our method applies to any surface that can be expressed as a non-differentiable sampling and a differentiable evaluation function. This flexibility opens the door for researchers to explore surface representations not previously supported by differentiable rendering, including spline surfaces and general isosurfaces. We have demonstrated that isosurface rendering can be used to reduce the runtime cost of NeRF rendering by more than 100\u00d7.",
                "Our method requires a closed-form evaluation function, which may not be available at all (e.g., some subdivision schemes) or only available via a Marching Cubes discretization. For surfaces that are defined as continuous functions of space, the discretization can affect surface quality. Since the evaluation happens only near the surface, however, quality may be improved by increasing resolution at a quadratic (not cubic) cost in evaluations.",
                "While we render multiple layers in order to resolve occlusions, the splatting step currently assumes a single surface at each pixel and does not handle semi-transparent objects. A direction for future work is to extend the method to handle semi-transparent layers, which could improve quality on scenes that include reflections or translucency."
            ],
            "subsections": []
        }
    ]
}