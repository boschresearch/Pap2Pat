# DESCRIPTION

## CROSS REFERENCE(S)

- claim priority

## TECHNICAL FIELD

- define technical field

## BACKGROUND

- motivate cross-lingual sentence alignment

## DETAILED DESCRIPTION

- define network and module terms
- define rich-source and low-resource language pairs
- motivate cross-lingual sentence alignment
- describe limitations of existing approaches
- introduce proposed framework for cross-lingual sentence alignment
- describe use of pre-trained multi-lingual language model
- explain BERT score computation
- describe normalization layer
- illustrate training framework for alignment model
- describe embedding model
- explain BERT score computation module
- describe normalization layer
- illustrate BERT score computation
- describe contrastive learning approach
- illustrate method of training aligner model
- describe receiving training dataset
- form positive and negative input pairs
- compute pairwise token-level similarity
- update pre-trained multi-lingual model

### Example Performance

- describe aligner model training
- introduce evaluation datasets
- motivate cross-lingual sentence retrieval tasks
- summarize baseline models
- show performance results
- analyze data efficiency
- illustrate language pair performance
- show data availability impact
- discuss model generalizability
- conclude with BUCC F1 scores

