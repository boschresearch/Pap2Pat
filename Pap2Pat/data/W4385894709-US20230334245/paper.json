{
    "id": "https://semopenalex.org/work/W4385894709",
    "authors": [
        "Wenhao Liu",
        "Y. Bai",
        "Prafulla Kumar Choubey",
        "Chien-Sheng Wu",
        "Nazneen Fatema Rajani"
    ],
    "title": "Conformal Predictor for Improving Zero-Shot Text Classification Efficiency",
    "date": "2022-01-01",
    "abstract": "Pre-trained language models (PLMs) have been shown effective for zero-shot (0shot) text classification. 0shot models based on natural language inference (NLI) and next sentence prediction (NSP) employ cross-encoder architecture and infer by making a forward pass through the model for each label-text pair separately. This increases the computational cost to make inferences linearly in the number of labels. In this work, we improve the efficiency of such cross-encoder-based 0shot models by restricting the number of likely labels using another fast base classifier-based conformal predictor (CP) calibrated on samples labeled by the 0shot model. Since a CP generates prediction sets with coverage guarantees, it reduces the number of target labels without excluding the most probable label based on the 0shot model. We experiment with three intent and two topic classification datasets. With a suitable CP for each dataset, we reduce the average inference time for NLI- and NSP-based models by 25.6% and 22.2% respectively, without dropping performance below the predefined error rate of 1%.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Zero-shot (0shot) text classification is an important NLP problem with many real-world applications. The earliest approaches for 0shot text classification use a similarity score between text and labels mapped to common embedding space (Chang et al., 2008;Gabrilovich and Markovitch, 2007;Chen et al., 2015;Li et al., 2016;Sappadla et al., 2016;Xia et al., 2018). These models calculate text and label embeddings independently and make only one forward pass over the text resulting in a minimal increase in the computation. Later approaches explicitly incorporate label information when processing the text, e.g., Yogatama et al. (2017) uses generative modeling and generates text given label embedding, and Rios and Kavuluru (2018) uses \u2020 work was done at Salesforce AI Research. label embedding based attention over text, both requiring multiple passes over the text and increasing the computational cost.",
                "Most recently, NLI- (Condoravdi et al., 2003;Williams et al., 2018;Yin et al., 2019) and NSP- (Ma et al., 2021) based 0shot text classification formulations have been proposed. NLI and NSP make inferences by defining a representative hypothesis sentence for each label and producing a score corresponding to every pair of input text and hypothesis. To compute the score, they employ a cross-encoder architecture that is full self-attention over the text and hypothesis sentences, which requires recomputing the encoding for text and each hypothesis separately. It increases the computational cost to make inferences linearly in the number of target labels.",
                "NLI and NSP use large transformer-based PLMs (Devlin et al., 2019;Liu et al., 2019b;Lewis et al., 2019) and outperform previous non-transformerbased models by a large margin. However, the size of PLMs and the number of target labels drastically reduce the prediction efficiency, increasing the computation and inference time, and may significantly increase the carbon footprint of making predictions (Strubell et al., 2019;Moosavi et al., 2020;Schwartz et al., 2020;Zhou et al., 2021).",
                "In this work, we focus on the correlation between the number of labels and prediction efficiency and propose to use a conformal predictor (CP) (Vovk et al., 2005;Shafer and Vovk, 2008) to filter out unlikely labels from the target. Conformal prediction provides a model-agnostic framework to generate a label set, instead of a single label prediction, within a pre-defined error rate. Consequently, we use a CP, with a small error rate we select, based on another fast base classifier to generate candidate target labels. Candidate labels are then used with the larger NLI/NSP-based 0shot models to make the final prediction.",
                "We experiment with three intent classification (SNIPS (Coucke et al., 2018), ATIS (Tur et al., 2010) and HWU64 (Liu et al., 2019a)) and two topic classification (AG's news and Yahoo! Answers (Zhang et al., 2015)) datasets using 0shot models based on a moderately sized bart-large (NLI) (Lewis et al., 2020) and a small bert-base (NSP) PLM. We use four different base classifiers, each with different computational complexity, and a small error rate of 1%. By using the best CP for each dataset, we reduce the average computational time by 25.6% (22.2%) and the average number of labels by 41.09% (43.38%) for NLI-(NSP-) based models."
            ],
            "subsections": []
        },
        {
            "title": "Methodology",
            "paragraphs": [
                "We improve the efficiency of NLI/NSP models by restricting the number of target labels with a Conformal Predictor (CP). Using a fast but weak base classifier-based CP, we produce the label set that removes some of the target classes for the 0shot model without reducing the coverage beyond a predefined error rate."
            ],
            "subsections": [
                {
                    "title": "Building a Conformal Predictor (CP) for Label Filtering",
                    "paragraphs": [
                        "Conformal prediction (Vovk et al., 1999(Vovk et al., , 2005;;Shafer and Vovk, 2008;Maltoudoglou et al., 2020;Angelopoulos and Bates, 2021;Giovannotti and Gammerman, 2021;Dey et al., 2021) generates label sets with coverage guarantees. For a given error rate \u03b1 and a base classifier f : x \u2192 R K (here K is the total number of class labels), a CP outputs a label set \u0393 \u03b1 that also contains true class label y with probability at least 1\u03b1.",
                        "To build a CP, we need calibration data {(x 1 , y 1 ), (x 2 , y 2 ), .., (x n , y n )} and a measure of non-conformity s(x i , y i ) that describes the disagreement between the actual label y i and the prediction f (x i ) from the base classifier. As an example, a non-conformity score can be defined as the negative output logit of the true class. Assuming the base classifier outputs logit scores, in this case s(x i , y i ) will bef (x i ) y i . Next, we define q to be the \u2308(n + 1)(1\u03b1)\u2309/n empirical quantile of scores s(x 1 , y 1 ), s(x 2 , y 2 ), .., s(x n , y n ) on the calibration set. Finally, for a new exchangeable test data point x test , we output the label set \u0393 \u03b1 = {y k : s(x test , y k ) < q}, i.e., the classes corresponding to which the non-conformity score is lower than the q. \u0393 \u03b1 is finally used with the 0shot model to predict the final class label. Next, we discuss the two components of a CP, namely the calibration dataset and the non-conformity score."
                    ],
                    "subsections": []
                },
                {
                    "title": "Calibration Dataset",
                    "paragraphs": [
                        "We require a calibration dataset that is exchangeable with the test data. However, in a typical 0shot setting, we do not expect the availability of a human-labeled dataset. Therefore, we use the 0shot classifier to label samples for calibration. Since our goal is to obtain a label set that contains the class label which is most probable according to the 0shot classifier, we do not explicitly require humanlabeled samples. Using model-predicted labels for calibration guarantees the required coverage."
                    ],
                    "subsections": []
                },
                {
                    "title": "Non-Conformity score based on a Base Classifier",
                    "paragraphs": [
                        "We want the base classifier to be computationally efficient when compared to the 0shot model. We experiment with four base classifiers with different complexity for building our CPs,",
                        "\u2022 Token Overlap (CP-Token): For each target class label (y k \u2208 {y 1 , .., y K }), we make a list of representative tokens (C k w ) that includes all tokens in the calibration data samples corresponding to that class. Then, we define the non-conformity score using the percentage of common tokens between C k w and the input text (x). Given #x defines the unique tokens in x, the token overlapbased non-conformity score is defined as:",
                        "\u2022 Cosine Similarity (CP-Glove): Token overlapbased non-conformity score suffers from sparsity unless we use a large representative words-set for each target class label. Therefore, we also experiment with the cosine distance between the bag-of-words (BoW) representation of a target label description (C k E ) and input text (x E ). We use static GloVe embeddings (Pennington et al., 2014) to obtain BoW representations for labels.",
                        "\u2022 Classifier (CP-CLS): Besides the broadly applicable token overlap and cosine similarity, we propose to use a task-specific base classifier to generate label sets of smaller sizes. We fine-tune a distilled bert-base model on the data labeled using the 0shot model and use the negative of class logits as the non-conformity scores.  "
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "We evaluate our CP-based framework on the test set of intent (SNIPS, ATIS and HWU64) and topic (AG's news and Yahoo! Answers) classification datasets. SNIPS, ATIS, HWU64, AG's news and Yahoo! Answers have 7, 17, 64, 4 and 10 target labels, respectively. We use \"facebook/bart-large-nli\" and \"bert-base-uncased\" models from the Hugging Face hub (Wolf et al., 2020) as our 0shot NLI and NSP baselines. Our experimental setup is described in appendix ( \u00a7A). In Fig. 1 and 2, we plot the empirical coverage and the average prediction set (APS) size of four base classifiers on ATIS, HWU64 and Yahoo! An-swers datasets. Coverage defines the proportion of samples for which the predicted set contains the 0shot model-predicted label. APS size equals the average number of labels in the set predicted by the CP base classifier.",
                "In Table 1, we compare the accuracy (A), average inference time (T) and the APS size (|L|) used with the 0shot model. Average inference time is reported relative to the full model. \"Full\" represents the 0shot model that uses the full label set. During inference, we create one batch for all text-label pairs in a sample. For instance, with 64 labels, a batch includes 64 text-label pairs where each pair consists of text and one label. This allows us to measure the reduction in inference time while fully utilizing the available compute resources."
            ],
            "subsections": [
                {
                    "title": "Results",
                    "paragraphs": [
                        "A CP achieves a valid coverage. We find that for smaller values of \u03b1, all four base classifiers achieve valid coverage (Fig. 1), i.e., empirical and nominal coverages are identical, implying that we can use a CP to filter unlikely target labels without dropping the performance below a low predefined error rate \u03b1. For larger \u03b1s (>\u223c0.5), empirical coverage drops to 0 on intent datasets for token overlap-based nonconformity score. The reduced coverage for token overlap at lower \u03b1 results from an empty label set, as evident from 0 APS size in Fig. 2.",
                        "A CP reduces the average number of labels for the 0shot model. We find that a stronger base classifier (CLS and Distil) provides lower APS size for the same empirical (or nominal) coverage (Fig. 2). On average, CP-CLS provides the lowest APS size, reducing the average number of labels for both 0shot models by roughly 41% (Table 1). This suggests that fine-tuning a base classifier should be preferred when unlabelled samples are easily available.",
                        "A simpler and efficient CP base classifier may reduce the inference time the most. We observe that CP-Token achieves the best inference time with the NLI model on ATIS, SNIPS and AG's news datasets, and with the NSP model on ATIS and AG's news datasets. On the other hand, it achieves the lowest APS size for both models only on the ATIS dataset. Minimal complexity for calculating token-overlap adds negligible overhead to the 0shot model, thus, achieving the best speed up despite higher APS size in several cases.",
                        "A CP base classifier needs to be computationally inexpensive. CP-Distil improves inference time for the NLI model on all datasets but fails to do so for the NSP model, despite reduced APS size. This ineffectiveness is explained by the comparable inference time for the base (distil-nli) classifier and the 0shot NSP model. When building a CP, it is imperative to select a base classifier that is computationally economical relative to the 0shot model.",
                        "A CP improves efficiency the most on the dataset with many labels. We observe the maximum speed up on HWU64 and ATIS datasets. This is unsurprising given the relatively higher number of possible target labels for both datasets, emphasizing the benefit of a CP for tasks with many target labels.",
                        "A CP performs comparably to the 0shot model. CP-based label filtering retains the performance of the corresponding models that use a full label set. Among the four base classifiers, CP-Token performs the worst (-0.46% absolute drop) and CP-Distil performs the best (+0.31% absolute gain) on the average accuracy. It is noteworthy that the accuracy increases in many cases, suggesting that pruning label space using a CP may remove noisy labels and boost the performance."
                    ],
                    "subsections": []
                },
                {
                    "title": "Applying a CP in Practical Applications",
                    "paragraphs": [
                        "Our experiments show that the inference speed-up from a CP depends on the sizes of the zero-shot model, the base classifier, and the label space. A strong base classifier (e.g., CP-CLS, CP-Distil) often gives better APS size leading to faster zero-shot inference. But it is also slow in generating the label set for the 0shot model. On the other hand, weaker base classifiers are fast but generate larger prediction sets resulting in slower 0shot inference. Given the trade-off, a stronger base classifier model such as BERT/ RoBERTa (or distilled model) makes a better choice when the label space is large (e.g., 64 for HWU64) and (or) the zero-shot model is large (e.g., bart-large-mnli). Otherwise, a faster base classifier (e.g., token overlap matching) would be ideal."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Discussion and Future Work",
            "paragraphs": [
                "In this work, we show that CP-based target label filtering improves the efficiency of NLI-and NSPbased zero-shot text classification models. Our CP framework is generalizable to many formula-tions of zero-or few-shot models. For instance, prompt-based models that auto-regressively generate verbalizers (Schick and Sch\u00fctze, 2021) are very slow, as the number of forward passes increases with the number of labels and verbalizer's tokens (Karimi Mahabadi et al., 2022). Our conformal prediction framework can be directly used there to reduce the number of labels and improve efficiency. Additionally, the proposed conformal prediction framework can be used to filter training examples when constructing prompts for in-context learning (Brown et al., 2020), where we prime PLM with a sequence of training examples. For a task with many target labels, a naively constructed prompt would require at least one training example for each label. However, with CP, we can limit the number of labels (and consequently the number of training examples) in the prompt, minimizing the computational cost.",
                "In the future, we will explore newer methods to build conformal predictors that can further reduce the average prediction set size and inference time, as well as boost the performance of a 0shot model."
            ],
            "subsections": []
        },
        {
            "title": "Limitations",
            "paragraphs": [
                "The datasets utilized in this research contain texts in English and thus mainly represent the culture of the English-speaking populace. Gender or age biases may also exist in the datasets, and pre-trained models may also exhibit these biases. In addition, though the likelihood is slim, additional biases may also be introduced by CP-based label filtering.",
                "We recommend that any subsequent usage of the proposed technique clearly states that formal was used for label filtering. In addition, while results suggest that our framework is generally effective for different zero-shot tasks, it should not be utilized on any new task without thorough evaluation, including evaluating for ethical or social risks."
            ],
            "subsections": []
        },
        {
            "title": "A Experimental Details",
            "paragraphs": [
                "We use the entire training set of intent datasets and 5000 samples from the validation set of topic datasets to calibrate CP-Token, CP-Glove and CP-Distil. For CP-CLS, we use the entire training set of intent datasets and 2500 samples from the validation set of topic datasets to train the base classifier, and the entire validation set of intent datasets and 2500 samples from the validation set of topic datasets for calibration.",
                "For the CP-Distil base classifier, we use \"crossencoder/nli-distilroberta-base\" model from the Hugging Face hub. We describe the procedure for building CP-CLS in \u00a7A.1. Note that we only require the text to be classified (without labels) for calibrating base classifiers and training CP-CLS base classifier. We use the 0shot model to label the corresponding training and calibration samples."
            ],
            "subsections": []
        },
        {
            "title": "A.1 CP-CLS: Training and Evaluation",
            "paragraphs": [
                "We fine-tune distilbert-base PLM for 15 epochs with the batch size of 16. We use AdamW optimizer (Loshchilov and Hutter, 2019) with default hyper-parameters, warm-up steps of 100 and weight decay of 0.01. We choose the checkpoint with the highest accuracy on the calibration dataset. The accuracy of the CP-CLS base classifier with respect to the labels generated by the 0shot model is reported in   In our experiments, we assumed the availability of text from the target task for calibration (and training a base classifier). While we can use a   0shot base classifier (e.g., CP-Token /Glove/ Distil), we still require a few samples for the CP calibration. Next, we analyze the number of samples required to build a CP and the transferability of a CP from another different dataset in Fig. 3,4, 5 and 6. These plots belong to the NLI-based 0shot model on the HWU64 dataset with the corresponding best-performing CP-Distil non-conformity score. 200 (up to 2500) in Fig. 3 and 4 represents the number of samples used for calibration. The full model is calibrated on the entire dataset. In Fig. 5 and6, SNIPS+ATIS is calibrated using the samples from SNIPS and ATIS datasets, and ALL is calibrated using the samples from SNIPS, ATIS, Yahoo! Answers and AG's news datasets. We describe our findings below. A small sized-calibration set with low chosen \u03b1 improves 0shot classification efficiency without dropping the performance. We find that the empirical coverage is worse than the nominal coverage for smaller-sized calibration data (200-1000) (Fig. 3). A model using calibration data with 1500 or more samples has identical empirical and nominal coverages. At the same time, the difference in empirical and nominal coverage is negligible for all calibration data sizes at a low error rate (\u03b1 = 0.01), as evident from the same starting point for all plots (at 1\u03b1 = 0.99). Consequently, we can use the conformal prediction framework even with a small calibration set provided we use a low value for \u03b1. Next, as shown in Fig. 4, the dependence between the APS size and the size of calibration data is not linear. CP-Distil models based on 200, 400 and full calibration samples obtain comparable APS sizes. Additionally, APS sizes for other models are significantly better than the full label set (maximum APS size of 37 for models calibrated on 1000 and 1500 samples vs full label set size of 64). We can use a calibration set from another dataset only if the \u03b1 is set to a low value We observe that none of the ATIS, SNIPS, ATIS+SNIPS or ALL calibration sets obtains the same empirical and nominal coverage. But, similar to our observations on the size of the calibration set, samples from another dataset can be used for calibration (Fig. 5), provided we use a low \u03b1. However, the resulting APS size is larger than that of the model that uses calibration data from the target task (Fig. 6)."
            ],
            "subsections": []
        },
        {
            "title": "Acknowledgement",
            "paragraphs": [
                "We would like to thank the anonymous reviewers for their feedback."
            ],
            "subsections": []
        }
    ]
}