# Introduction

Federated Learning (FL) (McMahan et al., 2016) is a technique for training a centralized model using the private data of decentralized clients without the need for data sharing. It is typically implemented through the Federated Averaging Algorithm (McMahan et al., 2017) (FEDAVG), which combines model updates from a subset of clients in each round to produce a single update on the shared server model. However, this approach requires all clients to use the same model architecture. As the client pool is often composed of diverse hardware with varying capacities, sharing an architecture limits the shared model size to the lowest-commondenominator architecture of the federated pool. This results in unused modeling capacity on many clients, which detriments model performance. To overcome this limitation, newer federated learning methods are being developed (Lin et al., 2020;Ozkara et al., 2021) that remove the constraint that all models in the pool must share a single architecture. While some methods alleviate this constraint by soliciting logits from the clients instead of model differences (Lin et al., 2020), we seek to address this issue by building on top of the popular FEDAVG strategy. A thorough discussion of related work is presented in Appendix A. In this work, we propose to partition the set of clients into two or more pools by model-size capacity, and run (bi-directional) knowledge distillation (Hinton et al., 2015), known as codistillation (Anil et al., 2018), between the server models produced by each FEDAVG pool, leveraging (un)labeled server data as the distillation dataset. By co-distilling the two (or more) models frequently over the course of FEDAVG rounds, we can share information between the pools without sharing model parameters. This partitioning allows us to make fuller use of the computational capacity of the federated pool, which can lead to increased performance and convergence in fewer federated rounds. Our methods are agnostic to the FL method applied within each pool and can work in a complementary fashion with any of the variants of FEDAVG which still train a single server model 1 . Our approach places no additional computational burden on the client, as all distillation computation occurs on the server. 1 See related work discussion in Appendix A. arXiv:2310.02549v1 [cs.LG] 4 Oct 2023

# Methods

We present two novel federated learning algorithms that avoid the requirement for a common server model by using knowledge distillation to share information between the server models of different FEDAVG pools without sharing model parameters. We present two variants of our method: PERIODICCODIST in which co-distillation is applied independently from FEDAVG updates at fixed intervals of federated rounds, and MERGEDCODIST, in which co-distillation updates are merged with FEDAVG updates at each round. Neither approach is restricted to a specific distillation technique, as any underlying knowledge distillation strategy can be employed (Hinton et al., 2015;Romero et al., 2014;Tian et al., 2019;Müller et al., 2020;Amid et al., 2022). In this work, we use KL-divergence between output probabilities for performing distillation. We explore using two federated pools, one for low-capacity and another for high-capacity models, but our approach can be extended to work for any number of pools. The standard FEDAVG method and our proposed co-distillation approaches are illustrated in We denote by P the pool of clients with heterogeneous memory constraints. For ease of presentation, we consider the case where P is partitioned into two pools: the highperformance clients P H having devices with higher capacity, and the remaining (low-performance) clients P L = P -P H . Based on the distribution of the devices across a typical federated client pool (Wang et al., 2021), it is reasonable to assume that |P H | ≪ |P L |. We consider training two model architectures, parameterized by θ L and θ H , using the available clients in P, with |θ L | < |θ H |. Therefore, we assume that θ H is only deployable on the high-performance clients in P H ⊂ P, while θ L can be trained on any client in P. In particular, we assume that θ L represents the lowestcommon-denominator model size given the hardware constraints on the entire pool P. Therefore, vanilla FEDAVG may be applied to train θ L via FEDAVG on the entire pool P. However, θ H is not trainable on the client devices in P L due to resource constraints. θ H offers an advantage over θ L by having a higher capacity, while θ L benefits from a significantly larger pool of clients for training. Thus, model capacity imposes a trade-off in terms of modeling power and data availability. We aim to tackle this issue by allowing the larger model to benefit from the low-performance clients' data while allowing the smaller model to utilize the improved modeling power offered by the larger model as a teacher.

## Periodic Codistillation (PERIODICCODIST)

In our first approach, we apply vanilla FEDAVG for training each model on its corresponding available client pool. Let T denote the total number of FEDAVG rounds for training θ L and θ H . In our PERIODICCODIST approach, we perform vanilla FEDAVG on θ L and θ H using the pool of clients P and P H ⊂ P, respectively. However, we pause the process every p rounds and perform codistillation for s steps. 

# H

) and continue training the models using FEDAVG at round t + 1. The PERIODICCODIST strategy is summarized in Algorithm 1 and illustrated in Figure 2(a) in Appendix B.

## Merged Gradient Codistillation (MERGEDCODIST)

We consider an alternative strategy where we apply codistillation in conjunction with FEDAVG at every round. To motivate our strategy, we note that FEDAVG aggregates model differences from the clients participating in the round and treats the combined model difference as a single gradient to apply the update. We use a similar approach to treat the model difference obtained from applying a round of distillation as a gradient. By merging the two gradients, we can apply a single update to the model parameters at each round.

Similar to PERIODICCODIST, we conduct FEDAVG independently for θ L and θ H using their respective pools of clients P and P H ⊂ P. We denote the gradients obtained for the models θ L and θ H by aggregating the client gradients at round t, as in vanilla FEDAVG, as g t L and g t H , respectively. Before applying the FEDAVG step at each round using the aggregated client gradients, we calculate an additional gradient for each model via codistillation. To apply codistillation, we first create a copy of the parameters of each model at the start of the round to act as a student. Let θ student L and θ student H denote the student models for θ L and θ H , respectively. Using ). We then calculate the model difference between the model parameters at round t and the student models at the end of the codistillation round, that is

We call the model difference induced by the distillation round the distillation gradient. The last step involves merging the two gradients for each model: the aggregated FE-DAVG gradient and the distillation gradient. We can achieve this by a weighted combination of the two gradients using an interpolating factor α ∈ [0, 1]. However, before merging, we ensure that the two gradients have the same scale by rescaling the distillation gradient. The final merged gradient for each model can be written as

where ∥•∥ 2 denotes L 2 -norm. We then use the merged gradient for each model to perform the update and move on to the next FEDAVG round. Note that in this setting, α = 1 recovers the vanilla FEDAVG approach. The MERGEDCODIST approach is summarized in Algorithm 2 and illustrated in Figure 2(b) in Appendix B.

# Experiments

In this section, we provide experimental results. The details of the experimental setups are shown in Appendix C.

## Balanced Model Performance

As we are interested in sharing information between the two server models, we first explore an experimental setup where the two models obtain similar performance and where knowledge distillation does not have to work against a large gap in performance between the teacher and the student. To accomplish this, we sample ∼700 clients from the Stack-Overflow dataset and ∼50 clients from the CIFAR-100 dataset as the high-capacity pools in their respective experiments. The data splits used are shown in Appendix C. Acquiring in-domain unlabeled corpora for the distillation dataset may be even infeasible, but for many tasks, large out-of-domain corpora are often available. To evaluate how sensitive the distillation methods may be to distillation data domain shift, we run experiments with both in-and out-of-domain data. For image classification and language modeling, we use CIFAR-10 and C4-MOD, respectively, as the out-of-domain datasets. Results are shown in the middle column of Table 1. When the small and large models have similar performance PERIODICCODIST and MERGED-CODIST can improve performance over baseline FEDAVG for both small and large models. The gains are greater when in-domain distillation data is available, but it is still possible to obtain gains using out-of-domain distillation data. The MERGEDCODIST technique yields gains in both tasks for both models.

## Imbalanced Model Performance

We now evaluate the extent to which the distillation methods are effective when the two models have imbalanced baseline performance. We achieve this setting by either increasing or decreasing the number of clients in the high-capacity pool.

Results are shown in Table 1. The most striking gains are seen when the small model is used as the teacher for the under-performing large model (Columns corresponding to |P H |=3100 or 4700). In this case, the large model is limited by the size of its participating clients pool P H ; this allows us to leverage the small model trained on the low-capacity pool to make better use of the larger model's unutilized capacity. By sharing information bi-directionally via the distillation methods, we are able to improve the small model as well, via both methods for CIFAR and via MERGEDCODIST for StackOverflow.

## Domain-shifted Client Pools

We next introduce a domain-shift between the two FEDAVG pools by splitting the StackOverflow examples such that the high-capacity pool P H only contains questions and the lowcapacity pool P L only contains answers. Thus the pools are entirely distinct, as opposed to previous experiments when the high-capacity pool covered a subset of the clients in the low-capacity pool. As before, we use cross-domain distillation data, which contains both questions (∼ 45%) and answers (∼ 55%) to explore to what extent knowledge codistillation can help to address each model's weaknesses in their respective out-of-domain categories. We use the same test set as before and additionally evaluate on partitions of the test set, which contain only questions and only answers. The sizes of the pool are reported in Table 5 in Appendix C. Results are reported in Table 2.

MERGEDCODIST achieves domain transfer between the two models resulting in a +5% accuracy boost for the small model on out-of-domain questions and +0.5% accuracy boost for the large model on out-of-domain answers. The large model also improves on in-domain questions. Overall performance improves for both models, and MERGED-CODIST achieves faster convergence on the general performance (see Figure 3 in Appendix D). We analyze the poor performance of PERIODICCODIST in the domain-shifted setting in Appendix D.

# Discussion

Our experiments on the federated CIFAR-100 and Stack-Overflow datasets show that the effectiveness of the PERI-ODICCODIST and MERGEDCODIST strategies for codistillation depends on the sizes of the model, the client pool, and the distillation dataset. Codistillation offers the most benefit when the larger model, trained on the smaller subset of high-capacity clients, cannot utilize its full capacity due to limited training data. This is evident in Table 1 where the gains are more substantial for both codistillation strategies when the size of the high-capacity clients' pool is smaller.

In most cases, both models can benefit by exchanging information throughout training; the larger model utilizes the information in the larger pool of clients through the smaller model, and the smaller model can benefit from the higher capacity of the larger model. Of the two codistillation strategies, PERIODICCODIST is easier to integrate into standard FEDAVG setups and does not require significant changes on the server side. Additionally, PERIODICCODIST offers more flexibility in terms of computation by adjusting the codistillation period based on the available resources. However, MERGEDCODIST is the more effective strategy and consistently improves performance in most scenarios. A downside of MERGEDCODIST is that it requires running codistillation at every federated round. To reduce this overhead of MERGEDCODIST, we can adjust the combination factor α (Equation 2) dynamically during training. By setting α = 1, we can recover FEDAVG and skip the codistillation cost for specific rounds. We plan to explore such strategies for more efficient utilization of computing resources in the future. In our experiments, we investigated codistillation in settings where the clients were partitioned according to capacity. Alternative criteria for partitioning clients could be based on user characteristics, an aspect we plan to explore in the future.

# Conclusion

We have introduced two new methods for federated learning built on top of FEDAVG; PERIODICCODIST and MERGED-CODIST, which address the unused modeling capacity on high-quality hardware necessitated by the shared-modelarchitecture constraint of FEDAVG. We have conducted experiments on image classification and language modeling tasks and have shown that these methods outperform FE-DAVG. Both methods allow us to train a computationally efficient model on all clients while still benefiting from the enhanced modeling power of a larger model trained on a subset of the entire client pool. These methods are applicable even when only out-of-domain data is available or when only a limited amount of in-domain data can be collected.

We have demonstrated that a high-capacity teacher can help improve a low-capacity student, even when the teacher has lower performance than the student. In addition to providing performance gains, both MERGEDCODIST and PERIODIC-CODIST are useful for sharing domain-specific knowledge between models. Our proposed methods provide a means to more fully utilize the modeling capacity available within a federated population. We will release code for both methods on publication of this work.

A. Related Work Mora et al. (2022) present an overview of Knowledge Distillation (KD) techniques for Federated Learning (FL). In one set of approaches, client models transfer knowledge to a centralized served model (Seo et al., 2022), In the second set, called co-distillation (Anil et al., 2018), an ensemble of clients learn without a central server model. Jeong et al. (2018) propose a distillation strategy that significantly reduces communication costs by exchanging soft targets instead of model parameters between the server and clients. In this approach, the clients transmit their per-label averaged logits computed on their private data, and the server averages these logits and broadcasts them in the next round. The clients then regularize their local loss using the server's logits. Variants of this method are presented by Oh et al. (2020); Ahn et al. (2019;2020). Itahara et al. (2020) demonstrate that this technique does not perform as well as FEDAVG on non-IID data in terms of communication cost and accuracy. They present a variation where clients compute logits on unlabeled data, which are then averaged to compute a global logit after applying a temperature parameter that is chosen to minimize the entropy of the softmax. Their strategy leads to a lower communication cost and better accuracy than FEDAVG. Afonin & Karimireddy (2021) present a theoretical framework for KD in FL, showing that the performance is limited by data heterogeneity. Ozkara et al. (2021) propose a KD approach for learning personalized FL models where the clients can differ in terms of architecture and weight precision. FedDF (Lin et al., 2020) allows for different model topologies between clients, rather than requiring all clients to use the same architecture. This is done by maintaining multiple server model prototypes, one for each unique client topology. In each round of federated learning, the client models are updated using their local data and an ensemble is computed by averaging logits from all client models sampled in that round, regardless of the architecture. This step enables sharing of information across model topologies. Distillation is then performed using the ensemble as the teacher and the server model prototype as the student. Each client model is sent the updated server model prototype that corresponds to its topology. Both FedDF and our method relax the assumption of identical model topology in FedAVG and enable sharing of information between models with different topologies. In our method, this is made possible by dividing the set of clients into pools based on capacity and using co-distillation to share information between the server pools. While FedDF diverges from FEDAVG in using client logits for constructing server models, we use FEDAVG as a building block. This makes our method easier to implement when compared to FedDF, which requires considerable changes to the FedAVG implementation. Unlike FedDF, our approach additionally enables learning an explicit shared representation within each client pool, whereas in FedDF, this is only possible in an indirect manner by initializing the server model prototype with the average of the client models with that topology. By learning explicit shared representations within each client pool, our method is robust to data-heterogeneous scenarios, as each model can become an expert in its pool's domain, and be a useful teacher to the out-of-domain model(s). See Appendix D.1 for experiments demonstrating strong performance when splitting the clients introduces domain-shift between client pools. Zhu et al. (2022) and Diao et al. (2020) present FL approaches which are robust to heterogeneity in client sizes. In both approaches, each client updates a fraction of the parameters in the server model depending on its capacity. In Zhu et al. (2022), the model is learnt in an incremental manner by adding one parameter column at a time. At download (upload) time, a client only downloads (uploads) the first k columns. In Diao et al. (2020)., the parameters of the server model in each layer are decomposed into nested subsets. A client only receives the subset of the model depending on its capacity. The approach of Zhu et al. (2022) is also resilient to network unreliability because if there is a disruption in the network after downloading the first k columns, the remaining columns can be copied from the last round. Both these approaches and ours allow the possibility for different clients to have different network sizes. The difference between their work and ours is that in their case, there is no explicit strategy for the small (or large) capacity models to share representation amongst themselves. In contrast, we employ distinct pools for the small/large capacity clients which enables learning of a better shared representation within each pool, and makes the server models more robust to data heterogeneity between pools. Like their approaches, our approach also benefits from communication efficiency in that the model sizes in the low-capacity pool are smaller than that in the high-capacity pool. In their approaches, the smaller subsets of global parameters benefit more from aggregation relative to larger subsets, which are thus relatively undertrained. Our approach does not share this limitation because it updates all parameters of the server model in each client pool. Zhu et al. (2022) have an additional assumption that the model can be learnt in a progressive manner by adding one column at a time, but we do not rely on such an assumption. Additionally, any method which trains a single server model can be used in any client pool of our methods as a drop-in replacement for FEDAVG. This allows our method to subsume any gains made by FL methods which produce single server models, while retaining the advantages of model heterogeneity and pool-specific shared representations. Many other methods make improvements to FEDAVG while still training a single server model: Split-mix FL (Hong et al., 2022) extends HeteroFL by splitting the server model into base models based on width, and allowing all base models to be trained on all clients. FLANC (Mei et al., 2022) uses low rank approximation to assemble local models on the fly from a shared basis; each local model modifies the shared basis and the local model transformation coefficients. FedFTG (Zhang et al., 2022) fine-tunes the server model to correct the model shift after aggregation so that the server model preserves the information in the local models to the maximum extent. Other methods which modify FEDAVG but do not address the constraint of model-heterogeneity include FedProx (Li et al., 2020), FedMiD (Yuan et al., 2021), andFedDual (Chen et al., 2022). Since the above approaches train a single server model and can be thought of as improvements over FedAVG, our methods yield complementary and orthogonal gains as our co-distillation framework can be applied in conjunction by using any of these methods as the federated algorithm applied to a single pool.

# B. Methods

Algorithm Initialize

In parallel, distill to θ student i for s steps using θ teacher {L,H}-{i} as the teacher, where i ∈ {L, H} 7:

Calculate distillation gradients

Merge the gradients

Apply FEDAVG on θ t i using ∆ t i as the gradient to find θ t+1 In each round, we merge the scaled distillation direction (by calculating the distillation direction after s = 15 steps) with the FEDAVG direction before applying the update to the model. To evaluate PERIODICCODIST and MERGEDCODIST, we run experiments on the tasks of image classification and language modeling, comparing against a baseline of FEDAVG. For image classification experiments, we use the federated version of the CIFAR-100 dataset 2 (Krizhevsky et al., 2009), supplemented by CIFAR-10 as out-of-domain distillation data. For language modeling, we use federated Stack Overflow (SO) 3 , which we supplement with out-of-domain distillation data drawn from the C4 (Raffel et al., 2020) dataset and modified to match the casing and tokenization of the SO data; we refer to this as C4-MOD. All dataset sizes are shown in Table 3. For both CIFAR-100 and SO datasets, we hold out 10% of the original train split as validation data.

# C. Experiment Setup

All experiments are run in the FedJax framework (Ro et al., 2021), and all hyper-parameters are tuned on the held-out sets through the Vizier hyper-parameter tuning service 4 (Golovin et al., 2017) with default settings, to a max of 1024 trials. 5 As all experiment runs have both 'small' (low-capacity) and 'large' (high-capacity) server models, we tune hyper-parameters to maximize the average accuracy of the two models on the held-out set. 

# C.1. Image Classification

For the CIFAR-10 and CIFAR-100 datasets, we pre-process all examples with the default settings in FedJAX; all images are centered and normalized, and training images are randomly cropped and horizontally flipped. When using the data for distillation, we apply additional random flipping, which may be vertical or horizontal, and mixup (Zhang et al., 2017) with

For all image classification experiments, we train Convolutional Neural Network (CNNs) (Goodfellow et al., 2016) with five layers; for the model trained on the high-capacity pool, hereon the 'large' model, we train with approx 410k parameters. For the model trained on the low-capacity pool, hereon the 'small' model, we train with approx 109k parameters. The details of the model architectures are given in Appendix E.

# C.2. Language Modeling

For the StackOverflow dataset, we tokenize with a lower-cased 4k word-piece model (Schuster & Nakajima, 2012) fit to the training split. To render the C4 data in the same format as the SO data, we pre-process the C4 dataset by lower-casing and word-tokenizing the first sentence from each C4 example paragraph. 6 We additionally correct the remaining differences to the SO tokenization by replacing a small hand-coded list of incorrectly tokenized contractions. 7 As the full C4 dataset is substantially larger than what we require for distillation data, we draw from it a subset of 4.5M examples. We train LSTMs (Hochreiter & Schmidhuber, 1997) with one layer for the 'small' model and two layers for the 'large' model. For the 'large' model, we train with approx 4.3M parameters and the 'small' with approx 2M parameters. The details of the model architectures are given in Appendix E.

For the language modeling runs, rather than using the entire default held-out and test sets with ∼16M examples, we draw 100k examples from each. We tune hyperparameters on the held-out set and report accuracy on the test set. To simulate a practical federated learning setting that is constrained in modeling capacity and data availability, in all experiments, we use only a subset of the federated training pool of the StackOverflow dataset, leaving the remainder of examples unused, or for use as in-domain distillation data.

# C.3. Federated Averaging Baseline

For all experiments, we use a set of shared FEDAVG settings. All experiments are run for 1500 federated rounds, sampling 20 clients per round. The clients are optimized with SGD with a constant learning rate. We set the client batch size to 20; all clients train locally for one epoch. We optimize the server models with FedAdam (Kingma & Ba, 2014;Reddi et al., 2021), using the default hyper-parameters,8 and with separate tuned learning rates for the large and small models. We use a linearly decaying learning rate (with a zero final value) over the course of the federated rounds.

# C.4. Distillation

For each teacher model, we tune a softmax temperature parameter T > 0 which may either sharpen (when T < 1) or smooth (when T > 1) the output probability distribution: p(y

, where y i is a label and z i is the corresponding logit. We additionally tune a student regularization term which modifies the target distribution by interpolating between the temperature-scaled teacher distribution and the soft-labels of the initial parameters of the student. This corresponds to using both θ teacher {L,H}-i and θ teacher i as teachers, when distilling to θ student i , i ∈ {L, H}. The former teacher is used to predict soft-labels for distillation, while the latter acts as a regularizer, preventing the student model from drifting too far from the parameters at the start of the distillation round.

We use the Kullback-Leibler divergence (Murphy, 2022) between the student probabilities and the target probabilities as the distillation loss. For both MERGEDCODIST and PERIODICCODIST, we optimize the distillation updates with Adam using the default hyper-parameters. For each student optimizer, we tune a learning rate that decays linearly over all rounds.

Though either method can be trivially extended to incorporate labeled distillation data by tuning an additional interpolating value between the teacher soft-labels and the true one-hot labels, in all experiments, we use only unlabeled distillation data. Our goal is to avoid any gains by exposing the models to additional labeled data and, thus, demonstrate the improvements solely due to codistillation. Nonetheless, unlabeled data is easier to acquire in practice than labeled data, especially in our domain of interest.

In all experiments with PERIODICCODIST, we set the codistillation period to p = 200 rounds and run codistillation for s = 200 steps each codistillation round. When running PERIODICCODIST, we reset the momentum variables of the FEDAVG optimizers after each codistillation round, as the models have departed from their initial parameters, rendering the momentum stale. When running MERGEDCODIST, we take s = 32 distillation steps to form each round's distillation gradient.  PERIODICCODIST performs poorly compared to FEDAVG in domain-shifted experiments. Figure 3 shows that it struggles to share information between models because the student model never sees information from both domains at once. In each codistillation round, it shows upward spikes on out-of-domain accuracy and downward spikes on in-domain accuracy, which are smoothed out by subsequent FEDAVG rounds resulting in no net gains over FEDAVG. MERGEDCODIST avoids this issue by showing the model information from both domains at all times, leading to overall improvement for both models, as well as faster convergence on the general performance. In the experiments in Section 3.1, we use as much in-domain data as is available. Realistically, acquiring even unlabeled in-domain data can be expensive, so available in-domain data may be limited in size. In Figure 4, we evaluate how the performance of PERIODICCODIST and MERGEDCODIST varies as a function of the size of the distillation dataset, using the CIFAR-100 equally-performant setup from Section 3.1 as our baseline. Though MERGEDCODIST and PERIODICCODIST work best with more distillation data, it is possible to achieve good results on CIFAR-100 with only 600 unlabeled examples. This suggests that these methods may be practically useful even in settings where getting access to copious distillation data is expensive or infeasible.

# E. Model Architectures

For Image Classification, we train 5-layer CNNs with 3 convolutional layers and 2 dense layers. The small model has filter sizes [16,32,32] and dense layer sizes [64,128], with a total of 109,348 parameters. The large model has filter sizes [32,64,64] and dense layer sizes [128,256] with a total of 410,084 parameters.

For Language Modeling, we train LSTMs with either one or two LSTM layers, following the modeling architecture described by (Reddi et al., 2021). For the small model, we use an embedding size of 96 and LSTM hidden size of 70, for a total of 1,984,348 parameters. For the large model, we use an embedding size of 192 with a hidden size of 140, for a total of 4,278,644 parameters.

All models are built in the haiku framework. 9

# F. Hyper-Parameter Tuning

For all runs, we tune hyper-parameters to maximize the accuracy on held-out data. We use the Vizier hyper-parameter tuning service (Golovin et al., 2017) with the default setting, with 64 concurrent trials to a max of 1024 trials. For FEDAVG runs, we tune a single client learning rate, and server learning rates for the small and large models. Learning rates are all tuned on the range (0.0001, 0.1), which is explored on a log scale. When running PERIODICCODIST we additionally tune learning rates for the large model and small model distillation optimizers. We also tune decay coefficients for those learning rates, tuned on (0, 1) on a log scale. We further tune temperature coefficients on (0.1, 10) and server regularization interpolating variables on (0.001, 0.5), both also on log scales. For MERGEDCODIST runs, we tune all of the above, and additionally tune the interpolating variable α on a linear scale on the range (0.05, 0.95).

# G. Contractions for C4-MOD tokenization

After sentence-tokenizing, word-tokenizing, and lower-casing the C4 data, we manually corrected the tokenization of punctuation with the following list of replacements: 

