{
    "id": "https://semopenalex.org/work/W4206952906",
    "authors": [
        "Adrian Barbu",
        "Hongyu Mou"
    ],
    "title": "The Compact Support Neural Network",
    "date": "2021-12-20",
    "abstract": "Neural networks are popular and useful in many fields, but they have the problem of giving high confidence responses for examples that are away from the training data. This makes the neural networks very confident in their prediction while making gross mistakes, thus limiting their reliability for safety-critical applications such as autonomous driving and space exploration, etc. This paper introduces a novel neuron generalization that has the standard dot-product-based neuron and the radial basis function (RBF) neuron as two extreme cases of a shape parameter. Using a rectified linear unit (ReLU) as the activation function results in a novel neuron that has compact support, which means its output is zero outside a bounded domain. To address the difficulties in training the proposed neural network, it introduces a novel training method that takes a pretrained standard neural network that is fine-tuned while gradually increasing the shape parameter to the desired value. The theoretical findings of the paper are bound on the gradient of the proposed neuron and proof that a neural network with such neurons has the universal approximation property. This means that the network can approximate any continuous and integrable function with an arbitrary degree of accuracy. The experimental findings on standard benchmark datasets show that the proposed approach has smaller test errors than the state-of-the-art competing methods and outperforms the competing methods in detecting out-of-distribution samples on two out of three datasets.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Neural networks have proven to be extremely useful in many applications, including object detection, speech and handwriting recognition, and medical imaging, etc. They have become the state-of-the-art in these applications, and in some cases they even surpass human performance. However, neural networks have been observed to have a major disadvantage: they do not know when they do not know, i.e., do not know when the input is out-of-distribution (OOD), i.e., far away from the data they have been trained on. Instead of saying \u201cI do not know\u201d, they give some output with high confidence\u00a0[1,2]. An explanation of why this is happening for the rectified linear unit (ReLU) based networks has been given in\u00a0[3]. This issue is very important for safety-critical applications such as space exploration, autonomous driving, and medical diagnosis, etc. In these cases it is important that the system knows when the input data are outside its nominal range to alert the human (e.g., driver for autonomous driving or radiologist for medical diagnostic) to take charge in such cases.",
                "A common way to address the problem of high confidence predictions for OOD examples is through ensembles\u00a0[4], where multiple neural networks are trained with different random initializations, and their outputs are averaged in some way. The reason why ensemble methods have low confidence on OOD samples is that each NN\u2019s high-confidence domain is random outside the training data, and the common high-confidence domain has therefore shrunk through averaging. This works well when the representation space (the space of the NN before the output layer) is high-dimensional but fails when this space is low-dimensional \u00a0[5].",
                "Another popular approach is adversarial training\u00a0[6], where the training is augmented with adversarial examples generated by maximizing the loss starting from perturbed examples. This method is modified in adversarial confidence enhanced training (ACET)\u00a0[3], where the adversarial samples are added through a hybrid loss function. However, the instance space is extremely vast when it is high dimensional, and a finite number of training examples can only cover an insignificant part, and no matter how many OOD examples are used, there will always be parts of the instance space that have not been explored. Other methods include the estimation of the uncertainty using dropout\u00a0[7], softmax calibration\u00a0[8], and the detection of OOD inputs\u00a0[9]. CutMix\u00a0[10] is a method to generate training samples with larger variability, which helps improve generalization and OOD detection. All these methods are complementary to the proposed approach and could be used together with the classifiers introduced in this paper to improve accuracy and OOD detection.",
                "In\u00a0[11], two auto-regressive models are trained, one for the foreground in-distribution data and one for the background, and the likelihood ratio is used to decide for each observation whether it is OOD or not. This is a generative model, while our model is discriminative.",
                "A number of works assume that the distance in the representation space is meaningful. A trust score was proposed in\u00a0[12] to measure the agreement between a given classifier and a modified version of a k-nearest neighbor (k-NN) classifier. While this approach does consider the distance of the test samples to the training set, it only does so to a certain extent since the k-NN does not have a concept of \u201ctoo far\u201d and is also computationally expensive. A simple method based on the Mahalanobis distance is presented in\u00a0[13]. It assumes that the observations are normally distributed in the representation space, with a shared covariance matrix for all classes. Our distribution assumption is much weaker, assuming that the observations are clustered into a number of clusters, not necessarily Gaussian. In our representation, each class is usually covered by one or more compact support neurons, and each neuron could be involved in multiple classes. Furthermore, ref.\u00a0[13] simply replaces the last layer of the NN with their Mahalanobis measure and makes no attempt to further train the new model, while the CSN layers can be trained together with the whole network.",
                "The generalized ODIN\u00a0[14] decomposes the output into a ratio of a class-specific function \\[h_{i}\\left( \\mathbf{x} \\right)\\]  and a common denominator \\[g\\left( \\mathbf{x} \\right)\\] , both defined over instances \\[\\mathbf{x}\\]  of the representation space. Good results are obtained using \\[h_{i}\\]  based on the Euclidean distance or cosine similarity. Again, this approach assumes that the observations are grouped in a single cluster for each class, which explains why it uses very deep models (with 34\u2013100 layers) that are more capable of obtaining representations that satisfy this assumption. Our method does not make the single cluster per class assumption and can use deep or shallow models.",
                "The deterministic uncertainty quantification (DUQ)\u00a0[5] method uses an RBF network and a special gradient penalty to decrease the prediction confidence away from the training examples. The authors also propose a centroid updating scheme to handle the difficulties in training an RBF network. They claim that regularization of the gradient is needed in deep networks to enforce a local Lipschitz condition on the prediction function that will limit how fast the output will change away from the training examples. While their smoothness and Lipschitz conditions might be necessary conditions, they are not sufficient conditions since a smoothly changing function could still have arbitrarily high confidence far away from the training examples. In contrast, our proposed compact support neural network (CSNN) is guaranteed to have zero outputs away from the training examples, which reflects in the lowest possible confidence. Furthermore, the maximum gradient of the CSN layer can be computed explicitly, and the Lipschitz condition can be directly enforced by decreasing the neuron support and weight decay. The authors of DUQ also encourage their gradient to be bounded away from zero everywhere, which they recognize is based on a speculative argument. In contrast, the CSNN gradient is zero away from the training examples, while still obtaining better OOD detection and smaller test errors than DUQ.",
                "The contributions of this paper are the following:It introduces a novel neuron formulation that generalizes the standard neuron and the radial basis function (RBF) neuron as two extreme cases of a shape parameter. Moreover, one can smoothly transition from a regular neuron to an RBF neuron by gradually changing this parameter. The RBF correspondent to a ReLU neuron is also introduced and observed to have compact support, i.e., its output is zero outside a bounded domain.It introduces a novel way to train a compact support neural network (CSNN) or an RBF network, starting from a pre-trained regular neural network. For that purpose, the construction mentioned above is used to smoothly bend the decision boundary of the standard neurons, obtaining the compact support or RBF neurons.It proves the universal approximation theorem for the proposed neural network, which guarantees that the network will approximate any function from \\[L^{p}\\left( \\mathbb{R}^{k} \\right)\\]  with an arbitrary degree of accuracy.It shows through experiments on standard datasets that the proposed network usually outperforms existing out-of-distribution detection methods from the literature, both in terms of smaller test errors on the in-distribution data and larger Areas under the ROC curve (AUROC) for detecting OOD samples."
            ],
            "subsections": []
        },
        {
            "title": "Materials And Methods",
            "paragraphs": [
                "This paper investigates the neuron design as the root cause of high confidence predictions on OOD data and proposes a different type of neuron to address its limitations. The standard neuron is \\[f(x) = \\sigma\\left( \\mathbf{w}^{T}\\mathbf{x} + b \\right)\\] , a projection (dot product) \\[\\left. \\mathbf{x}\\rightarrow\\mathbf{w}^{T}\\mathbf{x} + b \\right.\\]  onto a direction \\[\\mathbf{w}\\] , followed by a nonlinearity \\[\\sigma( \\cdot )\\] . In this design, the neuron has a large response for vectors \\[\\mathbf{x} \\in \\mathbb{R}^{p}\\]  that are in a half-space. This can be an advantage when training the neural network (NN) since it creates high connectivity in the weight space and makes the neurons sensitive to far-away signals. However, it can be a disadvantage when using the trained NN, since it leads to neurons unpredictably firing with high responses to far-away signals, which can result (with some probability) in high confidence responses of the whole network for examples that are far away from the training data.",
                "To address these problems, a type of radial basis function (RBF) neuron\u00a0[15], \\[f\\left( \\mathbf{x} \\right) = g\\left( \\parallel \\mathbf{x} - \\mu \\parallel^{2} \\right)\\] , is used and modified to have zero response at some distance R from \\[\\mu\\] . Therefore, the neuron has compact support, and the same applies to a layer formed entirely of such neurons. Using one such compact support layer before the output layer, one can guarantee that the space where the NN has a nonzero response is bounded, obtaining a more reliable neural network.",
                "The loss function of such a compact support NN has many flat areas, and it can be difficult to train directly by backpropagation. However, the paper introduces a different way to train it, by starting with a trained regular NN and gradually bending the neuron decision boundaries to make them have smaller and smaller support.",
                "The compact support neural network consists of a number of layers, where the layer before last contains only compact support neurons, which will be described next. The last layer is a regular linear layer without bias, so it can output an all-zero vector when appropriate."
            ],
            "subsections": [
                {
                    "title": "1. The Compact Support Neuron",
                    "paragraphs": [
                        "The radial basis function (RBF) neuron\u00a0[15], \\[f\\left( \\mathbf{x},\\mathbf{w} \\right) = g\\left( \\parallel \\mathbf{x} - \\mathbf{w} \\parallel^{2} \\right)\\] , for \\[\\mathbf{x},\\mathbf{w} \\in \\mathbb{R}^{d}\\] , has a \\[g(u) = \\exp( - \\beta u)\\]  activation function. However, in this paper \\[g(u) = \\max(R - u,0)\\]  will be used because it is related to the ReLU.",
                        "A flexible representation. Introducing an extra parameter \\[\\alpha = 1\\] , the RBF neuron can be written as:\\[f_{\\alpha}\\left( \\mathbf{x},\\mathbf{w} \\right) = {g\\left( \\alpha\\left( \\parallel \\mathbf{x} \\parallel \\right. \\right.}^{2} + {\\parallel \\mathbf{w} \\parallel}^{2}{)\\left. - 2\\mathbf{w}^{T}\\mathbf{x} \\right)}.\\]",
                        "Using the parameter \\[\\alpha\\] , one can smoothly interpolate between an RBF neuron when \\[\\alpha = 1\\]  and a standard projection neuron when \\[\\alpha = 0\\] . However, starting with an RBF neuron with \\[g(u) = \\exp( - \\beta u)\\] , the projection neuron is obtained for \\[\\alpha = 0\\]  as \\[f_{\\alpha}\\left( \\mathbf{x},\\mathbf{w} \\right) = \\exp\\left( 2\\mathbf{w}^{T}\\mathbf{x} \\right)\\]  but which has an undesirable exponential activation function.",
                        "The compact support neuron. In order to obtain a standard ReLU based neuron \\[f_{\\alpha}\\left( \\mathbf{x},\\mathbf{w} \\right) = \\rho\\left( \\mathbf{w}^{T}\\mathbf{x} \\right)\\]  with \\[\\rho(u) = \\max(u,0)\\]  for \\[\\alpha = 0\\] , the activation \\[g(u) = \\rho(R - u)\\]  will be used, and the above construction is modified to obtain the compact support neuron:\\[f_{\\alpha}\\left( \\mathbf{x},\\mathbf{w},b,R \\right) = {\\rho\\left\\lbrack \\alpha\\left( R \\right. \\right.} - {\\parallel \\mathbf{x} \\parallel}^{2} - {\\parallel \\mathbf{w} \\parallel}^{2} - b{)\\left. + 2\\mathbf{w}^{T}\\mathbf{x} + b \\right\\rbrack},\\]  where a bias term b is also introduced for the standard neuron. Usually \\[b = 0\\]  is used for simplicity.",
                        "The parameter R determines the radius of the support of the neuron when \\[\\alpha > 0\\] . In fact, one can easily check that the support of \\[f_{\\alpha}\\left( \\mathbf{x},\\mathbf{w},b,R \\right)\\]  from Equation (2) (i.e., the domain where it takes nonzero values) is a sphere of radius \\[R_{\\alpha}^{2} = R + b(1/\\alpha - 1) + {\\parallel \\mathbf{w} \\parallel}^{2}\\left( 1/\\alpha^{2} - 1 \\right)\\]  centered at \\[\\mathbf{w}_{\\alpha} = \\mathbf{w}/\\alpha\\] . Therefore, the neuron from (2) has compact support for any \\[\\alpha > 0\\]  and the support becomes tighter as \\[\\alpha\\]  increases. Figure 1a shows the response on a 1D input x for the RBF neuron \\[y = \\exp\\left( - \\middle| x - 2 \\middle| {}_{2} \\right)\\]  and the compact support neuron \\[y = f_{\\alpha}(x,2,0,1)\\]  from Equation (2) for \\[\\alpha \\in \\left\\{ 0,0.8,1 \\right\\}\\] . Observe that the standard neuron with ReLU activation \\[y = \\max(x,0)\\]  is obtained as \\[y = f_{\\alpha}(x,2,0,1)\\]  for \\[\\alpha = 0\\] . Figure 1b shows on a 2D example the support of \\[f_{\\alpha}\\left( \\mathbf{x},\\mathbf{w},b,R \\right)\\]  from (2) for several values of \\[\\alpha \\in \\lbrack 0,1\\rbrack\\] , where \\[\\mathbf{x} = \\left( x_{1},x_{2} \\right)\\] , \\[\\mathbf{w} = (0,2)\\] , \\[b = 0\\]  and \\[R = 1\\] ."
                    ],
                    "subsections": []
                },
                {
                    "title": "2. The Compact Support Neural Network",
                    "paragraphs": [
                        "For a layer containing only compact support neurons (CSN), the weights can be combined into a matrix \\[\\mathbf{W}^{T} = \\left( \\mathbf{w}_{1},\\cdots,\\mathbf{w}_{K} \\right)\\] , the biases into a vector \\[\\mathbf{b} = \\left( b_{1},\\cdots,b_{K} \\right)\\]  and the radii into a vector \\[\\mathbf{r} = \\left( R_{1},\\cdots,R_{K} \\right)\\]  and the CSN layer can be written as: \\[\\mathbf{f}_{\\alpha}\\left( \\mathbf{x},\\mathbf{W},\\mathbf{b},\\mathbf{r} \\right) = \\rho\\left( \\alpha\\left\\lbrack \\mathbf{r} - \\mathbf{b} - \\mathbf{x}^{T}\\mathbf{x} - {Tr}\\left( \\mathbf{W}\\mathbf{W}^{T} \\right) \\right\\rbrack + 2\\mathbf{W}\\mathbf{x} + \\mathbf{b} \\right),\\]  where \\[\\mathbf{f}_{\\alpha}\\left( \\mathbf{x},\\mathbf{W},\\mathbf{b},\\mathbf{r} \\right) = \\left( f_{1}\\left( \\mathbf{x} \\right),\\cdots,f_{K}\\left( \\mathbf{x} \\right) \\right)^{T}\\]  is the vector of neuron outputs of that layer. This formulation enables the use of standard neural network machinery (e.g., PyTorch) to train a CSN. In practice usually no bias term is used (i.e., \\[\\mathbf{b} = 0\\] ), except in low dimension experiments. The radius parameter \\[\\mathbf{r}\\]  is trainable.",
                        "The simplest compact support neural network (CSNN) has two layers: a hidden layer containing compact support neurons (2) and an output layer, which is a standard fully connected layer without bias, as shown in Figure 2a. A Batch Normalization layer without trainable parameters can be added to normalize the data as described below. A LeNet network with a CSN layer before the last layer is shown in Figure 2b.",
                        "Normalization. It is desirable that \\[\\parallel \\mathbf{x} \\parallel\\]  be approximately 1 on the training examples so that the value of the radius R does not depend on the dimension d of \\[\\mathbf{x}\\] . These goals can be achieved by standardizing the variables to have zero mean and standard deviation \\[1/\\sqrt{d}\\]  on the training examples. This way \\[{\\parallel \\mathbf{x} \\parallel}^{2} \\sim 1\\]  when the dimension d is large (under assumptions of normality and independence of the variables of \\[\\mathbf{x}\\] ). Our experiments on three real datasets indicate that indeed \\[\\parallel \\mathbf{x} \\parallel \\sim 1\\]  when the inputs \\[\\mathbf{x}\\]  are normalized this way. To achieve this goal, a Batch Normalization layer without trainable parameters is added before the CSN layer.",
                        "Training. Similar to the RBF network, training a neural network with such neurons with \\[\\alpha = 1\\]  is difficult because the loss function has many local optima. To make matters even worse, the compact support neurons have small support when \\[\\alpha\\]  is close to 1, and consequently the loss function has flat regions between the local minima.",
                        "This is why another training approach is taken. Using Equation (4), a CSNN can be trained by first training a regular NN (\\[\\alpha = 0\\] ) and then gradually increasing the shape parameter \\[\\alpha\\]  from 0 towards 1 while continuing to update the NN parameters. Observe that whenever \\[\\alpha > 0\\] , the NN has compact support, but the support becomes smaller as \\[\\alpha\\]  becomes closer to 1. The training procedure is described in detail in Algorithm 1. Algorithm 1 Compact Support Neural Network (CSNN) Training\u2003Input: Training set \\[T = \\left\\{ \\left( \\mathbf{x}_{i},y_{i} \\right) \\in \\mathbb{R}^{p} \\times \\mathbb{R} \\right\\}_{i = 1}^{n}\\] , \u2003Output: Trained CSNN.\u00a01:\u00a0\u00a0Train a regular NN \\[\\mathbf{f}\\left( \\mathbf{x} \\right) = \\mathbf{L}\\rho\\left( 2\\mathbf{W}\\mathbf{g}\\left( \\mathbf{x} \\right) + \\mathbf{b} \\right)\\]  where \\[\\mathbf{W},\\mathbf{L}\\]  are the last two layer weight matrices and \\[\\mathbf{g}\\left( \\mathbf{x} \\right)\\]  is the rest of the NN.\u00a02:\u00a0\u00a0Freeze \\[\\mathbf{g}\\left( \\mathbf{x} \\right)\\] , compute \\[\\mathbf{u}_{i} = \\mathbf{g}\\left( \\mathbf{x}_{i} \\right),i = 1,\\cdots,n\\] , their mean \\[\\mu\\]  and standard deviation \\[\\sigma\\] .\u00a03:\u00a0\u00a0Obtain normalized versions \\[\\mathbf{v}_{i}\\]  of \\[\\mathbf{u}_{i}\\]  as \\[\\mathbf{v}_{i} = \\left( \\mathbf{u}_{i} - \\mu \\right)/\\sqrt{d}\\sigma,i = 1,\\cdots,n.\\] \u00a04:\u00a0\u00a0for e= 1 to \\[N^{epochs}\\] \u00a0do\u00a05:\u00a0\u00a0\u00a0\u00a0\u00a0Set \\[\\alpha = e/N^{epochs}\\] \u00a06:\u00a0\u00a0\u00a0\u00a0\u00a0Use the examples \\[\\left( \\mathbf{v}_{i},y_{i} \\right)\\]  to update \\[\\left( \\mathbf{W},\\mathbf{L},\\mathbf{b},\\mathbf{r} \\right)\\]  based on one epoch of \\[{\\mathbf{f}\\left( \\mathbf{v} \\right) = \\mathbf{L}\\rho}\\left( \\alpha\\left\\lbrack \\mathbf{r} - \\mathbf{v}^{T}\\mathbf{v} - {Tr}\\left( \\mathbf{W}\\mathbf{W}^{T} \\right) - \\mathbf{b} \\right\\rbrack + 2\\mathbf{W}\\mathbf{v} + \\mathbf{b} \\right)\\mspace{-1540mu}\\] \u00a07:\u00a0\u00a0end for",
                        "In practice, the training is stopped at an \\[\\alpha \\leq 1\\]  where the training and validation errors still take acceptable values, e.g., a validation error less than the validation error for \\[\\alpha = 0\\] . However, the larger the value of \\[\\alpha\\] , the tighter the support is around the training data and the better the generalization. The whole network can then be fine-tuned using a few more epochs of backpropagation."
                    ],
                    "subsections": []
                },
                {
                    "title": "3. Datasets Used",
                    "paragraphs": [
                        "Real data experiments are conducted on classifiers trained on three datasets: MNIST [16], CIFAR-10 and CIFAR-100 [17]. The OOD (out of distribution) detection is evaluated using the respective test sets as in-sample data and other datasets as OOD data, such as the test sets of EMNIST [18], FashionMNIST [19] and SVHN [20], and the validation set of ImageNet [21]. For MNIST, a grayscale version of CIFAR-10 is also used as OOD data, obtained by converting the 10,000 test images to grayscale and resizing them to \\[28 \\times 28\\] ."
                    ],
                    "subsections": []
                },
                {
                    "title": "4. Neural Network Architecture",
                    "paragraphs": [
                        "For MNIST, a 4-layer LeNet convolutional neural network (CNN) backbone is used, with two \\[5 \\times 5\\]  convolution layers with 32 and 64 filters, respectively, followed by ReLU and \\[2 \\times 2\\]  max pooling, and two fully connected layers with 256 and 10 neurons. For the other two datasets, a ResNet-18 [22] backbone is used, with 4 residual blocks with 64, 128, 256 and 512 filters, respectively.",
                        "For the CSNN, two architectures, illustrated in Figure 2, will be investigated. The first is a small one (called CSNN), illustrated in Figure 2a, which takes the output of the last convolutional layer of the backbone as input, normalized as described in Section 2.2 using a batch normalization layer without any learnable affine parameters. The second one is a full network (called CSNN-F), illustrated in Figure 2b, where the backbone (LeNet or ResNet) is part of the backpropagation, and a Batch Normalization layer (BN) without any learnable parameters is used between the backbone and the CSN layer.",
                        "All experiments were conducted on an MSI GS-60 Core I7 laptop with 16GB RAM and Nvidia GTX 970M GPU, running the Windows 10 operating system. The CSNN and CSNN-F networks were implemented in PyTorch 1.90."
                    ],
                    "subsections": []
                },
                {
                    "title": "5. Training Details",
                    "paragraphs": [
                        "For all datasets, data augmentation with padding (3 pixels for MNIST, 4 pixels for the rest) and random cropping is used to train the backbones. For CIFAR-100, random rotation up to 15 degrees is also used.No data augmentation is used when training the CSNN and CSNN-F.",
                        "The CSNN was trained for 510 epochs with \\[R = 0.01\\] , of which 10 epochs at \\[\\alpha = 0\\] . The Adam optimizer is used with a learning rate of \\[0.001\\]  and weight decay of \\[0.0001\\] . SGD obtained similar results. The CSNN-F was trained with SGD with a learning rate of \\[0.001\\]  and weight decay of \\[0.0005\\] . Its layers were initialized with the trained backbone and the trained CSNN. Then \\[\\alpha\\]  was kept fixed for two epochs and increased by \\[0.005\\]  every epoch for 4 more epochs.",
                        "Training the CSNN from \\[\\alpha = 0\\]  to \\[\\alpha = 1\\]  for 510 epochs takes less than an hour. Each epoch of the CSNN-F takes less than a minute with the LeNet backbone and about 3 min with the ResNet-18 backbone. The CSNN-F was obtained by merging the corresponding CSNN head with the ResNet or LeNet backbone and training them together for 6 epochs."
                    ],
                    "subsections": []
                },
                {
                    "title": "6. Ood Detection",
                    "paragraphs": [
                        "The out of distribution (OOD) detection is performed similarly to the way it is performed in a standard CNN. For any observation, the maximum value of the network\u2019s raw outputs is used as the OOD score for predicting whether the observation is OOD or not. If the observation is in-distribution, its score will usually be large, and if it is OOD, it will usually be close to zero or even zero. The ROC (receiver operating characteristic) curve based on these scores for the test set of the in-distribution data (as class 0) and one OOD dataset (as class 1) will give us the AUROC (Area under the ROC). If the two distributions are not separable (have concept overlap), some of the OOD scores will be large, but for the OOD observations that are away from the area of overlap they will be small or even zero."
                    ],
                    "subsections": []
                },
                {
                    "title": "7. Methods Compared",
                    "paragraphs": [
                        "The OOD (out of distribution) detection results of the CSNN and CSNN-F are compared with the Adversarial Confidence Enhanced Training (ACET) [3], Deterministic Uncertainty Quantification (DUQ) [5], a standard CNN and a standard RBF network. The RBF network has the same architecture as the CSNN-F, but with an RBF layer instead of the CSN layer, and has a learnable \\[\\sigma\\]  for each neuron. Also shown are results for an ensemble of five or 10 CNNs trained with different random initializations; however, these methods are more computationally expensive and are not included in our comparison. The ACET results are taken directly from [3], and the DUQ, CNN and ensemble results were obtained using the DUQ authors\u2019 PyTorch code. The parameters for the CNN, RBF and ensemble were tuned to obtain the smallest average test error. For DUQ, multiple models were trained with various combinations of the length scales \\[\\sigma \\in \\left\\{ 0.05,0.1,0.2,0.3,0.5,1.0 \\right\\}\\]  and gradient penalty \\[\\lambda \\in \\left\\{ 0,0.05,0.1,0.2,0.3,0.5,1.0 \\right\\}\\]  and the combination with the best test error-AUROC trade-off was selected. For the CSNN methods, for each dataset, the classifier was selected to correspond to the largest \\[\\alpha\\]  where the test error takes a value comparable to the other methods compared."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Results",
            "paragraphs": [
                "This section presents theoretical results on the gradient of the proposed CSNN and its universal approximation properties, plus experimental results on a number of public datasets."
            ],
            "subsections": [
                {
                    "title": "1. Gradient Bound",
                    "paragraphs": [
                        "The authors of the Deterministic Uncertainty Quantification (DUQ) [5] paper claim that gradient regularization is needed in deep networks to enforce a local Lipschitz condition on the prediction function that limits how fast the output will change away from the training examples. Thus, it is of interest to see whether this Lipschitz condition is satisfied for the proposed CSNN.",
                        "The following result proves that indeed the Lipschitz condition [5] is satisfied for the CSN layer:",
                        "Thus, a small gradient can be enforced everywhere by penalizing the R and \\[{\\parallel \\mathbf{w} \\parallel}^{2}\\]  to be small using weight decay, or by making \\[\\alpha\\]  close to 1, or both (all assuming \\[b = 0\\] )."
                    ],
                    "subsections": []
                },
                {
                    "title": "2. Universal Approximation For The Compact Support Neural Network",
                    "paragraphs": [
                        "It was proved in [23,24] that standard two-layered neural networks have the universal approximation property, in the sense that they can approximate continuous functions or integrable functions with arbitrary accuracy under certain assumptions. Similar results have been proved for RBF networks in [25,26]; thus, it is of interest whether such results can be proved for the proposed compact support neural network. In this section, the universal approximation property of the compact support neural networks will be proved.",
                        "Let \\[L^{\\infty}\\left( \\mathbb{R}^{d} \\right)\\]  and \\[L^{p}\\left( \\mathbb{R}^{d} \\right)\\]  and be space of functions \\[\\left. f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R} \\right.\\]  such that they are essentially bounded and, respectively, their p-th power \\[f^{p}\\]  is integrable. Denote the \\[L^{p}\\]  and \\[L^{\\infty}\\]  norms by \\[{||} - {||}_{p}\\]  and \\[{||} - {||}_{\\infty}\\] , respectively.",
                        "The family of networks considered in this study consists of two-layer neural networks, which can be written as:\\[q\\left( \\mathbf{x} \\right) = \\sum\\limits_{i = 1}^{H}{\\beta_{i}f\\left( \\mathbf{x},\\mathbf{w}_{i},b_{i} \\right)}\\]  where H is the number of hidden nodes, \\[\\beta_{i} \\in R\\]  are the weights from the i-th hidden node to the output nodes, \\[f\\left( \\mathbf{x},\\mathbf{w}_{i},b_{i} \\right)\\]  is the representation of the hidden neuron with weights \\[\\mathbf{w}_{i} \\in \\mathbb{R}^{n}\\]  and biases \\[b_{i} \\in \\mathbb{R}\\] .",
                        "The neurons used in the hidden layer can be either regular neurons or radial-basis function (RBF) neurons. The regular neuron can be written as:\\[f\\left( \\mathbf{x},\\mathbf{w},b \\right) = g\\left( \\mathbf{w}^{T}\\mathbf{x} + b \\right)\\]  where g is an activation function such as the sigmoid or ReLU. An RBF neuron has the following representation:\\[f\\left( \\mathbf{x},\\mathbf{w},b \\right) = g{(\\frac{\\left| \\middle| \\mathbf{x} - \\mathbf{w} \\middle| \\right|}{b})}\\]  where the exponential function \\[g(x) = \\exp( - x)\\]  is used for activation in RBF networks.",
                        "The studies on regular neurons [23,24] showed that if the activation function g used in the hidden layer is continuous almost everywhere, locally essentially bounded, and not a polynomial, then a two-layered neural network can approximate any continuous function with respect to the \\[{||} - {||}_{\\infty}\\]  norm.",
                        "Universal approximation results for RBF networks are quite limited according to [25,26], which proved that if the RBF neuron used in the hidden layer is continuous almost everywhere, bounded and integrable on \\[R^{n}\\] , the RBF network can approximate any function in \\[L^{p}\\left( \\mathbb{R}^{n} \\right)\\]  with respect to the \\[L^{p}\\]  norm with \\[1 \\leq p \\leq \\infty\\] . More exactly, in [25], the following statement is proved:",
                        "The above result will be used to prove the following statement for the CSNN:",
                        "Observe that universal approximation results for \\[\\alpha = 0\\]  have already been proven in [23,24] and that the standard RBF kernel can be obtained in Theorem 3 by taking \\[g(x) = \\exp(x)\\] , \\[\\alpha = 1\\]  and \\[R_{0} = 0\\] ."
                    ],
                    "subsections": []
                },
                {
                    "title": "3. Two-Dimensional Example",
                    "paragraphs": [
                        "This experiment is on the moons 2D dataset, where the data are organized on two intertwining half-circle-like shapes, one containing the positives and one the negatives. The data are scaled so that all observations are in the interval \\[\\lbrack 0,1\\rbrack^{2}\\]  (shown as a white rectangle in Figure 3. The out of distribution (OOD) data are generated starting with \\[100 \\times 100 =\\]  10,000 samples on a grid spanning \\[\\lbrack - 0.5,1.5\\rbrack^{2}\\]  and removing all samples at a distance at most \\[0.1\\]  from the moons data, obtaining 8763 samples.",
                        "A two-layer CSNN is used, with 128 CSN neurons in the hidden layer, as illustrated in Figure 2a. The CSNN is trained on 200 training examples for 2000 epochs with \\[R = 0.02\\]  and \\[\\alpha\\]  increasing from 0 to 1 as \\[\\alpha_{i} = \\min\\left( 1,\\max\\left( 0,(i - 100)/1500 \\right) \\right),i = 1,\\cdots,2000.\\]",
                        "The confidence map for the trained classifier is shown in Figure 3d. One can see that the confidence is 0.5 (white) almost everywhere except near the training data, where it is close to 1 (black). This assures us that the method works as expected, shrinking the support of the neurons to a small domain around the training data. One can also see that the support is already reasonably small for \\[\\alpha = 0.6\\] , and it becomes tighter and tighter as \\[\\alpha\\]  becomes closer to 1.",
                        "The training/test errors vs. \\[\\alpha\\]  are shown in Figure 4. Also shown is the AUROC (area under the ROC curve) for detecting the OOD data described above against the test set. Observe that the training and test errors for \\[\\alpha = 0\\]  are quite large because the standard 2-layer NN with 128 neurons cannot fit these data well enough and decrease as the neuron support decreases in which the model is better capable of fitting the data.",
                        "It is known [3,27] that the output of a ReLU-based neural network is piecewise linear, and the domains of linearity are given by the activation pattern of the neurons. The activation pattern of the neurons contains the domains where the set of active neurons (with nonzero output) does not change. These activation pattern domains are polytopes, as shown in Figure 5a, for a two-layer NN with 32 neurons. The activation domains for a CSNN are intersections of circles, as illustrated in Figure 5b, with the domain where all neurons are inactive shown in white. The corresponding confidence map is shown in Figure 5c.",
                        "In real data applications, one does not need to go all the way to \\[\\alpha = 1\\]  since even for smaller \\[\\alpha\\] , the support is still bounded and if the instance space is high dimensional (e.g., 512 to 1024 in the real data experiments below), the volume of the support of the CNN will be extremely small compared to the instance space, making it unlikely to have high confidence on out-of-distribution data."
                    ],
                    "subsections": []
                },
                {
                    "title": "4. Real Data Experiments",
                    "paragraphs": [
                        "In Figure 6, the train/test errors vs. \\[\\alpha\\]  for the CSNN on the three datasets are shown. Also shown are the area under the ROC curve (AUROC) for OOD detection on CIFAR-10 or CIFAR-100. Observe that all curves on the real data are very smooth, even though they are obtained from one run, not averaged. One could see that the training and test errors stay flat for a while and then start increasing from a certain \\[\\alpha\\] , which depends on the dataset. At the same time, the AUROC stays flat and slightly increases, and there is a range of values of \\[\\alpha\\]  where the test error is low and the AUROC is large.",
                        "Different values of \\[\\alpha\\]  make different trade-offs between test error and AUROC. In practice, \\[\\alpha\\]  should be chosen as large as possible where an acceptable validation error is still obtained to have the smallest support possible. For example, one could choose the largest \\[\\alpha\\]  such that the validation error at \\[\\alpha\\]  is less than or equal to the validation error at \\[\\alpha = 0\\] .",
                        "The results are shown in Table 1. All results except the ACET results are averaged over 10 runs, and the standard deviation is shown in parentheses. The results of the best non-ensemble method are shown in bold and the second best in red."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Discussion",
            "paragraphs": [
                "From Table 1, one could see that the trained RBF has difficulties in fitting the in distribution data well, with much larger test errors on CIFAR-10 and CIFAR-100 than the standard CNN. The most probable cause is that the training is stuck in a local optimum, because the training errors were also observed to be large (not shown in the table).",
                "Further comparing the test errors, Table 1 reveals that the proposed CSNN-F method obtains the smallest test errors on in-distribution data among the non-ensemble methods compared. These findings strengthen the claim that the CSNN can fit the in-distribution data as well as a standard CNN, a claim that is also supported by the universal approximation statement from Theorem 3.",
                "Comparing the detection AUROC on different OOD datasets, the proposed CSNN and CSNN-F methods obtain the best results on MNIST and CIFAR-100, and ACET and SNGP obtain the best results on CIFAR-10. However, the test errors of ACET are the highest among all methods by a large margin.",
                "One should be aware that there usually is a trade-off to be made between small test errors and good OOD detection. ACET makes this trade-off in favor of a high AUROC by training with adversarial samples, while the other methods are trying to obtain small test errors, comparable with a standard CNN. Observe that the test errors of the CSNN-F approach are smaller than the CSNN, and the AUROCs are comparable.",
                "Compared to ACET, both CSNN and CSNN-F obtain smaller test errors on all three datasets and better average AUROC on two out of three datasets. Compared to DUQ, the CSNN and CSNN-F obtain comparable test errors and better average AUROC on all three datasets. The RBF network cannot obtain a small training or test error in most cases, and the test errors and OOD detection results are poor and have a large variance.",
                "Comparing the training time, the CSNN methods are about four times faster than training a 5-ensemble, eight times faster than a 10-ensemble and about three times faster than DUQ.",
                "Overall, compared to the other non-ensemble OOD detection methods evaluated, the proposed methods obtain smaller test errors and better OOD detection performance (except ACET, which has large test errors). Training the whole deep network together with the CSNN results in smaller test errors and improved OOD detection performance.",
                "In the synthetic experiment in Figure 3, the train and test errors were close to 0 for \\[\\alpha = 1\\]  because there are neurons close to all observations. However, in the real data applications where the representation space is high dimensional, the training, test and validation errors might first decrease a little bit but ultimately increase as \\[\\alpha\\]  approaches 1. For example, one could see the test errors vs. \\[\\alpha\\]  for the synthetic dataset in Figure 4 and for the real datasets in Figure 6. This is due to the curse of dimensionality, which makes all distances between observations relatively large and the CSNN centers will also be far from the observations, thus the neurons will have a larger radius to cover the training data.",
                "It is worth noting that in contrast to the weights of a standard neuron, the weights of the compact support neuron exist in the same space as the neuron inputs, and they can be regarded as templates. Thus, they have more meaning, and one could easily visualize the type of responses that make them maximal, using standard neuron visualization techniques such as [28]. Furthermore, one can also obtain samples from the compact support neurons, e.g., for generative or GAN models."
            ],
            "subsections": []
        },
        {
            "title": "Conclusions",
            "paragraphs": [
                "This paper presented four contributions. First, it introduced a novel neuron formulation that is a generalization of the standard projection-based neuron and the RBF neuron as two extreme cases of a shape parameter \\[\\alpha \\in \\lbrack 0,1\\rbrack\\] . It obtains a novel type of neuron with compact support by using ReLU as the activation function. Second, it introduced a novel way to train the compact support neural network of an RBF network by starting with a pretrained standard neural network and gradually increasing the shape parameter \\[\\alpha\\] . This training approach avoids the difficulties in training the compact support NN and the RBF networks, which have many local optima in their loss functions. Third, it proves the universal approximation property of the proposed neural network, in that it can approximate any function from \\[L^{p}\\left( \\mathbb{R}^{d} \\right)\\]  with arbitrary accuracy, for any \\[p \\geq 1\\] . Finally, through experiments, it shows that the proposed compact support neural network outperforms the standard NN and the RBF network and even usually outperforms existing state-of-the-art OOD detection methods, both in terms of smaller test errors on in-distribution data and larger AUROC for detecting OOD samples.",
                "The OOD detection feature is important in safety critical applications such as autonomous driving, space exploration and medical imaging.",
                "The results have been obtained without any adversarial training or ensembling, and adversarial training or ensembling could be used in the proposed framework to obtain further improvements.",
                "In real data applications, the compact support layer was used as the last layer before the output layer. This ensures that the compact support is involved in the most relevant representation space of the CNN. However, because the CNN still has many projection-based layers to obtain this representation, it means that the corresponding representation in the original image space does not have compact support, and erroneous high-confidence predictions are still possible. Architectures with multiple compact support layers that have even smaller support in the image space are subject to future study."
            ],
            "subsections": []
        }
    ]
}