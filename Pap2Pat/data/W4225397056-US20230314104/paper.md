# I. Introduction

P oportional navigation (PN) can be shown to be equivalent to a guidance law that minimizes the zero e ort miss (ZEM) [1], with the ZEM being the predicted miss distance in the case that neither the missile nor the target accelerates for the remainder of the engagement. Consequently, target maneuvers can increase miss distance against a missile employing PN guidance. However, if the target acceleration can be estimated, the PN guidance law can be augmented using an additional term that is a function of estimated target acceleration, allowing improved performance against maneuvering targets. This augmented proportional navigation (APN) guidance law can be shown to be optimal for targets employing a step acceleration maneuver [2] and can increase accuracy for other target maneuvers as well. Other formulations of PN give improved performance against weaving targets, provided the missile navigation system can estimate the weave frequency. However, accurate estimation of target acceleration is di cult for arbitrary target maneuvers, and there is the possibility that the estimate can diverge, resulting in large miss distances [3]. For this reason, there has been considerable research e ort devoted to guidance laws that perform well against maneuvering targets, but do not require estimates of target acceleration.

In [4] the authors augment true proportional navigation [5] with an additional term that is the product of a time-varying constant and the estimated target acceleration, with the additional term derived through capturability analysis. However, the experiments only tested the guidance law with a relatively large ratio of missile to target acceleration capability ( 50:1 in Figure 12). In another approach [6] a guidance law that does not require an estimate of target acceleration is developed, and the authors show that the guidance law requires less acceleration than PN for a single target maneuver, but do not demonstrate e ectiveness over a range of target maneuvers. Switched bias proportional navigation (SBPN), described in [3], uses sliding mode theory to derive a bias term that is added to the commanded acceleration from PN, with the bias term only depending on the sign of the line of sight rotation rate. SBPN results in an acceleration profile close to that of APN, without requiring an estimate of target acceleration. Other proposed guidance laws use geometric guidance [7,8], parameterizing the missile trajectory in the Fresnet-Serret reference frame. Importantly, [7] assumes knowledge of the target acceleration vector, and [8] only considers low speed targets.

In contrast, we propose a new approach to improving the performance of any guidance law against maneuvering targets. Specifically, we optimize a line of sight curvature policy c that maps navigation system outputs to a Euler 321 attitude parameterization ) LOSC . ) LOSC is then mapped to a direction cosine matrix, which rotates the observed line of sight (LOS) unit vector pointing from the missile to the target. By varying ) LOSC as a function of navigation system outputs, c can arbitrarily curve the LOS during an engagement. We parameterize c as a deep neural network with recurrent layer optimized using reinforcement meta-learning (meta-RL) [9].

RL has been demonstrated to be e ective in optimizing integrated and adaptive G&C systems that generate direct closed-loop mapping from navigation system outputs to actuation commands. Applications where RL has been e ectively applied to GN&C include asteroid close proximity operations [10,11], planetary landing [12][13][14], exoatmospheric intercept [15], endoatmospheric intercept [16], and hypersonic vehicle guidance [17,18]. In the RL framework, an agent instantiating a policy learns how to complete a task through episodic simulated experience with an environment. The policy is implemented as a deep neural network that maps observations to actions u = c \ (o), and in our work is optimized using a customized version of proximal policy optimization (PPO) [19]. In this work we include a recurrent layer in the policy and value function networks. This allows making the LOS curvature a function of the history of observations o.

We optimize the LOS curvature policy in an environment where the curved LOS is passed to a proportional navigation guidance system, creating the PN-LOSC guidance law. We test PN-LOSC over a wide range of target maneuvers, a large altitude range, and using a simple aerodynamic drag model for the missile and target. Missile and target maximum acceleration are a function of dynamic pressure, which would be the case for vehicles maneuvering using control surface deflections. In our experiments we demonstrate that PN-LOSC achieves improved accuracy with less control as compared to both PN and APN against maneuvering targets with an acceleration capability approaching that of the missile. Importantly, although we demonstrate the LOS curvature approach using PN guidance, a LOS curvature policy can be optimized in conjunction with any guidance law that takes inputs derived from the LOS unit vector pointing from the missile to the target.

The paper is organized as follows. In Section II we present the engagement scenarios, LOS curvature model, guidance system model, and the equations of motion. Next in Section III.A we give a brief summary of the RL framework. This is followed by Section III.B, where we formulate the RL optimization problem for the air-to-air homing phase scenario described in Section II. In Section IV we optimize and test the PN-LOSC policy, and compare the results to PN and APN benchmarks.

# II. Problem Formulation

## Fig. 1 Engagement

We consider a skewed head on engagement scenario. Due to the high target acceleration capability, which exceeds the level that a human pilot could withstand, a realistic scenario would be a fighter launching a missile to intercept a supersonic cruise missile that can actively maneuver to avoid threats. Referring to Fig. 1 , the missile position vector, missile velocity vector, target position vector, and target velocity vector are shown as r M , v M , r T , v T . We can also In this figure, the illustrated vectors are not in general within the x-z plane define the relative position and velocity vectors r TM = r T r M and v TM = v T v M . The elevation angle \ ⇢ is the angle between r TM and its projection onto the x-y plane. We randomly generate the target's initial velocity vector such that v T lies within a cone with axis r TM and half apex angle \ v T . A collision triangle is then defined in a plane that is not in general aligned with the coordinate frame shown in Fig. 1, and is illustrated in Fig. 2. Here we define the required lead angle ! for the missile's velocity vector v M as the angle that will put the missile on a collision triangle with the target in terms of the target velocity v T , line-of-sight angle W, and the magnitude of the missile velocity as shown in Eqs. (1a) through (1c).

## Fig. 2 Planar Heading Error

This formulation is easily extended to a three dimensional engagement using the following approach: 1) define a plane normal as vC ⇥ ,

2) rotate v T and , onto the plane 3) calculate the required planar missile velocity (Eq. (1a)) 4) rotate this velocity back into the original reference frame Thus in R 3 we define a heading error (HE) as the angle between the missile's initial velocity vector and the velocity vector associated with the lead angle required to put the missile on a collision heading with the target. Note that due to the missile aerodynamic forces and target acceleration, this is far from a perfect collision triangle, and the true heading error is generally greater than HE.

The simulator randomly chooses between a target bang-bang, weave, and jinking maneuver with equal probability, with the acceleration applied orthogonal to the target's velocity vector. The maneuvers have varying acceleration levels and random start time, duration, and switching time. At the start of each episode, with probability 0.5 the maneuvers in that episode use the target's maximum acceleration capability, and with probability 0.5 the acceleration is sampled uniformly between 0 and the maximum. We assume the target uses aerodynamic control surfaces (no thrust vector control). Consequently, the maximum target acceleration is reduced taking into account dynamic pressure. Specifically, we assume that the target can achieve the acceleration shown in Table 1 only at @ MAX > , the dynamic pressure corresponding to its maximum speed at sea level, but we reduce this maximum acceleration by the ratio of

, where

Sample target maneuvers are shown Fig. 3, note that in some cases the maneuver period is considerably shorter or longer, with the longest periods being twice the time of flight.

We can now list the range of engagement scenario parameters in Table 1. During optimization and testing, these parameters are drawn uniformly between their minimum and maximum values, except as noted. The generation of heading error is handled as follows. We first calculate the optimal missile velocity vector that puts the missile on a collision triangle with the target as described previously. We then uniformly select a heading error ⇢ between the bounds given in Table 1, and randomly perturb the direction of the missile's velocity vector direction such that arccos(v M • v M ? ) < ⇢, where v M ? is the perturbed missile velocity vector.  

## B. Radome and Seeker Model

We use a similar radome model to that used in [16], but simplified for the 3-DOF simulator used in this work. Here the look angle is defined as the angle between the ground truth inertial frame line of sight (LOS) vector , = r TM kr TM k and the missile's velocity unit vector vM = v M kv M k , with the look angle calculated as \ ! = arccos (, • vTM ). This definition of look angle is di erent than in a 6-DOF environment, where the look angle is the angle between the missile centerline axis and ,. However, the dynamic look angle in our 3-DOF implementation coupled with our look angle dependent refraction model does create a parasitic attitude loop [20], reducing guidance system performance. The radome refraction angle \ ' is defined as the angle between the ground truth LOS , and the apparent LOS ,, i.e., \ ' = arccos (, • ,). We assume a symmetrical radome, where the radome refraction angle \ ' is a function of look angle \ ! . The model first calculates the azimuthal (\ D ) and elevation (\ E ) refraction errors as shown in Eqs. (3a) through (3b), where D , E , : D , and : E are sampled uniformly within the bounds given in Table 2 at the start of each simulation episode. We then create the refracted body frame LOS unit vector as shown in Eq. (2a), where C(q R ) is the DCM corresponding to the 321 Euler rotation

) is a stochastic Euler 321 rotation (to model Gaussian white noise on the LOS measurement), and N (`, f, =) denotes an = dimensional normally distributed random variable with mean `and standard deviation f. We use f LOS = 1 mrad. "LAG" indicates a first order low pass noise filter with time constant 0.02s. We assume a strap down seeker implementation, and therefore do not model seeker gimbal lag.

A plot of refraction angle \ ' as a function of the look angle \ ! is shown in Fig. 4 for the case of D = E = 10 mrad and various values of :. Note that the radome slope m\ ' m\ ! is given by the slope of the curves in the figure.  

## C. LOS Curvature

The LOS curvature policy c : o 7 ! u maps observations o to actions u, with the observations given in Eq. ( 4).

The action u is then scaled and interpreted as a Euler 321 attitude, as shown in Eq. ( 5), where we use : = 2 .

) LOSC 2 SO(3) = :u (5) ) LOSC is then used to construct the direction cosine matrix (DCM) C() LOSC ), and compute the shaped LOS direction vector as , LOSC = C() LOSC ) ,. Thus, by varying ) LOSC during the engagement, the LOS curvature policy can arbitrarily curve , LOSC . The signal , LOSC is then used to construct a LOS rotation vector

, where r TM LOSC = , LOSC A. Both ⌦ LOSC and , LOSC are used by the guidance law described in Section II.D. c is implemented as a deep recurrent neural network and optimized using reinforcement meta-learning, as described in Sections III.A and III.B. Both c and the value function used to optimize c contain a recurrent network layer, allowing actions to be generated using the history of observations. In theory, this allows the c to infer properties of the target maneuvers.

## D. Guidance Law

The guidance law implements the true proportional navigation (TPN) guidance law [21], which is shown in Eqs. (6a), with the commanded acceleration adjusted so that it is perpendicular to v M in Eqs. (6b) and (6c), where rTM = r TM kr TM k, and ⌦ is the LOS rotation rate [21].

The APN benchmark uses the TPN guidance law described in Eqs. (6a) and (6b), except that the APN benchmark replaces Eq. (6a) with Eq. (7a).

In Section IV the benchmark PN and APN guidance laws use : = 0 in Eq. ( 5), i.e., there is no LOS curvature. With : = 2 , we obtain the PN-LOSC guidance law. A system diagram of PN-LOSC guidance is illustrated in Fig. 5. The simulation environment used to optimize and test the PN, APN, and PN-LOSC guidance laws instantiates this system. We use # = 3 for both PN and APN guidance, noting that this is optimal for APN.

## Fig. 5 System Diagram

Eqs. 8a and 8b model the lag from the flight control system (FCS) and actuator. Here we define 0 M REF as the magnitude of the maximum achievable lateral acceleration at sea level and 1000 m/s. We set 0 " REF = 74g, which corresponds to the steady state acceleration at those conditions with a 20 degree horizontal and vertical fin deflection in our 6-DOF simulator [16], and 6 = 9.81 m/s 2 . 0 M is then clipped based o of dynamic pressure, where d(⌘) is the atmospheric density at missile altitude ⌘ T and + = kv M k, and again clipped to remain below the load constraint of 0 M MAX = 40g. In Eq. (8a), "F-LAG" denotes a first order low pass filter with time constant 0.08s that we use to represent the flight control system time constant, and "A-LAG" denotes a first order low pass filter with time constant of 0.02s that is used to model actuator lag.

## E. Equations of Motion

The missile inertial frame position r M and velocity unadjusted for drag ṽM are then updated by integrating Eqs. (9a) and (9b). The missile speed + M = kv M k is adjusted for drag by integrating Eq. (9c), where we use

The missile velocity is then computed as shown in Eq. ( 10)

The target is modeled as shown in Eqs. (11a) through (11b), where a T COM is the commanded target acceleration assuming the maneuvers described in Section II.A. The target maximum speed is 600m/s, which appears in the denominator of Eq. (11a).

The target inertial frame position r T and velocity unadjusted for drag ṽT are then updated by integrating Eqs. (12a) and (12b). The target speed + T = kv T k is adjusted for drag by integrating Eq. ( 12c), where we use ranges of : T from 1/8 to 1/3 and ranges of cd0 T from 0.125 to 0.4 for the experiments that randomize target drag; these parameters are both set to zero for the experiments where we do not model target drag. We use

The target velocity is then computed as shown in Eq. ( 13)

The equations of motion are updated using fourth order Runge-Kutta integration. For ranges greater than 80 m, a timestep of 20 ms is used, and for the final 80 m of homing, a timestep of 0.2 ms is used in order to more accurately (within 0.4m) measure miss distance; this technique is borrowed from [22]. The policy and guidance system inputs are updated every 20 ms.

# III. Methods

## A. Reinforcement Learning Framework

In the reinforcement learning framework, an agent learns through episodic interaction with an environment how to successfully complete a task using a policy that maps observations o to actions u. The environment initializes an episode by randomly generating a ground truth state, mapping this state x to an observation, and passing the observation to the agent. The agent uses this observation to generate an action that is sent to the environment; the environment then uses the action and the current ground truth state to generate the next state and a scalar reward signal A (x, u). The reward and the observation corresponding to the next state are then passed to the agent. The process repeats until the environment terminates the episode, with the termination signaled to the agent via a done signal. Trajectories collected over a set of episodes (referred to as rollouts) are collected during interaction between the agent and environment, and used to update the policy and value functions. The interface between agent and environment is depicted in Fig. 6.

Meta-RL di ers from generic reinforcement learning in that the agent learns over an ensemble of environments. These environments vary the engagement scenarios, dynamics, aerodynamic coe cients, radome parameters, and other factors. Optimization within the meta-RL framework results in an agent that can quickly adapt to novel environments, often with just a few steps of interaction with the environment. Similar to [23], we implement meta-RL by including a recurrent layer in the policy and value function. For a given trajectory over observations and actions, the recurrent layer will evolve di erently for di erent target maneuvers, allowing the policy to infer the nature of the maneuver. By optimizing over an ensemble of target behavior models, the agent learns to adapt, using the recurrent layer's hidden state to infer the current target behavior model.

## Fig. 6 Environment-Agent Interface

In this work, we implement RL using proximal policy optimization (PPO) [19] with both the policy and value function implementing recurrent layers in their networks. The PPO algorithm has demonstrated state-of-the-art performance for many reinforcement learning benchmark problems. PPO approximates the Trust Region Policy Optimization method [24] by accounting for the policy adjustment constraint with a clipped objective function. The objective function used with PPO can be expressed in terms of the probability ratio ? : ()) given by,

The PPO objective function is shown in Equations (15a) through (15c). The general idea is to create two surrogate objectives, the first being the probability ratio ? : ()) multiplied by the advantages c w (o : , u : ) (see Eq. ( 16)), and the second a clipped (using clipping parameter n) version of ? : ()) multiplied by c w (o : , u : ). The objective to be maximized ()) is then the expectation under the trajectories induced by the policy of the lesser of these two surrogate objectives.

This clipped objective function has been shown to maintain a bounded Kullback-Leibler (KL) divergence [25] with respect to the policy distributions between updates, which aids convergence by ensuring that the policy does not change drastically between updates. Our implementation of PPO uses an approximation to the advantage function that is the di erence between the empirical return and a state value function baseline, as shown in Equation 16, where W is a discount rate and A the reward function. 

Here the value function + c w is learned using the cost function given by

In practice, policy gradient algorithms update the policy using a batch of trajectories (roll-outs) collected by interaction with the environment. Each trajectory is associated with a single episode, with a sample from a trajectory collected at step : consisting of observation o : , action u : , and reward A : (o : , u : ). Finally, gradient ascent is performed on ) and gradient descent on w and update equations are given by w + = w V w r w !(w)| w=w (18)

where V w and V ) are the learning rates for the value function, + c w (o : ), and policy, c ) (u : |o : ), respectively. In our implementation of PPO, we adaptively scale the observations and servo both n and the learning rate to target a KL divergence of 0.001.

## B. RL Problem Formulation

In this 3-DOF air-to-air missile environment, an episode terminates when the closing velocity E 2 = r TM • v TM r TM turns negative. The agent observation o was given in Eq. ( 4) from Section II.C, and the agent action u is interpreted according to Eq. ( 4) from Section II.C. Each rollout consists of 60 episodes, and we optimize for 90,000 episodes.

The reward function maps observations and actions to a scalar reward signal A, and is shown below in Eqs. (20a) through (20d). Eq. (20a) penalizes the LOS curvature, encouraging the agent to curve the LOS only when it results in higher terminal rewards, as given by Eqs. (20b) and (20c), where "done" indicates the last step of an episode. We use a discount rate of 0.95 for all rewards except the terminal reward, which uses a discount rate of 0.995. Other hyperparameters are U = 0.01, V = 10, A lim = 1m, n = 20, and f LOSC = 1.

The policy and value functions are implemented using four layer neural networks with tanh activations on each hidden layer. Layer 2 for the policy and value function is a recurrent layer implemented using gated recurrent units [26]. The network architectures are as shown in Table 3, where = hi is the number of units in layer 8, obs_dim is the observation dimension, and act_dim is the action dimension. The policy and value functions are periodically updated during optimization after accumulating trajectory rollouts of 60 simulated episodes. 

# IV. Experiments

## A. RL Optimization of PN-LOSC

Optimization uses the initial conditions and vehicle parameters given in the problem formulation (Section II). The optimization history is plotted in Fig. 7, which shows the mean, mean less one standard deviation, and minimum  

## C. Results with No Radome Refraction

As look angle dependent radome refraction results in a false indication of target motion, it is possible that at least some of the performance advantage of the PN-LOSC guidance system is attributable to increased robustness to radome refraction. The lower control e ort as compared to APN supports this hypothesis, which we investigate by testing all three guidance systems with no target drag and 30g maximum target acceleration capability, but with D and E set to zero in the radome model (Section II.B). The results are shown in Table 7, where as expected we find that the performance of all three guidance systems improves. Importantly, we see that the improvement is greatest for the PN and APN systems, and that the accuracy of PN-LOSC is similar to that of APN, although PN-LOSC requires considerably less control e ort. This suggests that the performance of the PN-LOSC guidance system is due to two factors. The first factor is an increased robustness to radome refraction, whereas the second factor is an improved ability to react to target maneuvers as compared to PN guidance. Moreover, the robustness to radome refraction is likely in at least part due to the PN-LOSC guidance system responding more intelligently to perceived target acceleration. This is supported by the lower control e ort as compared to APN, and the LOS shaping and missile acceleration profile for the weave maneuver in Fig. 9. Since the parasitic attitude loop can be attenuated by increasing guidance system lag, we experimented with increasing the filter time constant to 0.05s. However, the increase in flight control response time had a negative overall impact on performance. 

# V. Conclusion

We presented a novel method for dynamically curving the line of sight input to a guidance law using a policy optimized with reinforcement meta-learning. The PN-LOSC guidance law was then formulated by combining proportional navigation with the line of sight curvature policy. In the experiments we compared the performance of PN-LOSC to both proportional navigation and augmented proportional navigation against highly maneuvering targets, and demonstrated that PN-LOSC has improved accuracy and requires less control e ort, without requiring an estimate of target acceleration. Additional experiments reveal that the performance of PN-LOSC can be attributed to two factors, the first being increased robustness to radome refraction, and the second a more intelligent response to target maneuvers. Although we demonstrated the combination of line of sight curvature with proportional navigation, the approach could also be coupled with other guidance laws. Importantly, LOS curvature could also be used to satisfy look angle and impact angle constraints, as well as constraints on load and heating rate. Future work will investigate the use of line of sight curvature in a higher fidelity six degrees-of-freedom model.

## B. Policy Testing and Comparison to Benchmarks

Here we test the optimized PN-LOSC guidance system, and compare performance to a PN and APN benchmark. Each test consists of running 5000 episodes, under di erent levels of maximum target acceleration capability 0 T MAX and using two di erent target aerodynamic drag models. The randomized drag model would correspond to an unpowered maneuvering target on a ballistic trajectory, whereas the case of no target drag would correspond to a powered target such as a cruise missile holding a constant speed. Note that without target drag, the target speed is constant, and that a reduction in target speed reduces achievable acceleration (see Eq. 11a). Consequently, all three guidance systems perform better with the randomized target drag model.

The test results are given in Tables 4 and5, and target acceleration statistics are given in Table 6. In all cases we see that the highest performance with least control e ort is obtained with the PN-LOSC guidance law. Next in miss distance performance is APN, followed by PN. Importantly, APN is only optimal (in requiring the least control e ort) for target step maneuvers, and we see that APN actually requires more control e ort than PN, probably due to the types of target maneuvers we consider. It is worth noting that with regards to accuracy, the benefit of PN-LOSC is greatest for miss distances of less than 100cm. Finally, note that in a more realistic environment, the navigation system will provide an imperfect estimate of target acceleration. This would likely widen the performance gap between APN and PN-LOSC. Sample trajectories for the PN-LOSC guidance law, with no target drag and 0 T MAX = 30g, are shown in Figs. 8 through 10. The left top subplot illustrates the evolution of the curvature parameter \ LOSC , the right top subplot plots the LOS rotation rate Eq. (6a), and the bottom sub plots show the evolution of the missile and target acceleration over the engagement. The weave maneuver plot is particularly interesting, as it appears that the LOS curvature increases as range to target decreases. By attenuating the response to the target maneuver at longer ranges, less control e ort is required. Since control e ort increases drag, which reduces missile speed and acceleration capability, the PN-LOSC guidance law ends the engagement (on average) with higher missile acceleration capability.

