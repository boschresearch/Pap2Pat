# I. INTRODUCTION

Mobile devices are increasingly being used for entertainment (gaming, video streaming): the coexistence of these new services with the "Internet of Things" (IoT) and Machine-to-Machine (M2M) communications means that wireless applications may quickly become starved for bandwidth. Millimeter wave bands can provide the much needed linear increase in throughput, but pose the challenge of high speed sampling, required to sense the spectrum, for many relatively low rate IoT applications, which are likely to benefit from opportunistic decentralized spectrum access.In order to achieve efficient usage of the spectrum, when a large portion is potentially available, we need to overcome the bottleneck of Nyquist sampling, which can be prohibitive, especially for low power wireless devices.

The literature offers two classes of solutions to eliminate this bottleneck: a static sub-Nyquist sampling front-end or a tunable narrowband coupled with active sensing strategies.

The static sub-Nyquist sampling approach is based on the representation of the signal as a nonlinear Union of Subspaces (UoS) [1]. A common framework to cover several acquisition and reconstruction approaches under the umbrella of the L. Ferrari and A. Scaglione {Lorenzo.Ferrari,Anna.Scaglione@asu.edu} are with the School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, 85281, USA.

This work was supported in part by the National Science Foundation CCF-247896 grant.

UoS model was defined in [2], which named these analog to digital conversion techniques Xampling architectures. Xampling architectures preprocess the signal in the analog domain, and then sample at a lower rate compared to what the Nyquist theorem dictates. The aim is to reduce the complexity and energy cost for the Analog-to-Digital Converter (ADC) hardware. The downside is the increased complexity in the reconstruction of the underlying signal. Our work focuses on multiband signals, whose UoS representation is a finite union of subspaces with infinite, but countable dimensions, in the space spanned by orthogonal sinc functions. Examples of Xampling architectures for multiband signals are in e.g. [3]- [6]. There are other Compressive Spectrum Sensing (CSS) algorithms in the literature that are related. Typically, they start directly from a discrete time model (see e.g. [7], [8]) where the receiver has a fixed number of measurements, forming an underdetermined system of equations, whose solution is a sparse vector with support equal to the spectrum occupancy.

The second set of approaches is fully adaptive and consists in selecting opportunistically, and in a cognitive fashion, a small section of the spectrum at a time, relying on an analog front-end able to swiftly switch between small sub-bands. The problem choosing optimally the band to explore was studied by several authors, see e.g. [14]- [16] and our previous work [17] that also focuses on single band tests. Note that tests that alias the spectrum create correlation in the resources, and this is why our formulation departs from the Partially Observable Markov Decision Processes (POMDPs) models used in previous works for single band tests. More recently, other authors have proposed and studied sensing strategies that would reduce the required number of CSS measurements to the sparsity of the signal. The work in [18] has a predetermined sensing phase duration, while [19] introduces an analog preprocessing which guarantees satisfactory detection performance, irrespective of the sparsity of the signal, as long as the measurement phase is proportional to the average occupancy. Our contribution and motivation is discussed next.

# A. Contributions

Our main contribution is a new dynamic optimization framework married with the design of the active sub-Nyquist spectrum sensing frontend.

We provide two arguments to study alternatives to Xampling (or to the aforementioned CSS methods) previously studied. First, in the spectrum sensing problem, the objective is the detection of the idle channels, not the signal reconstruction: this suggests that the Xampling complexity may still exceed what is really necessary for this task, as previously discussed in [9]. Additionally, most of the standard results in Compressive Sensing (CS), that bound the 2 -norm of the estimation error, do not directly express the detection performance. The architecture studied in this paper has the advantage of being sequential, requiring incoherent observations and being robust to time inaccuracies in the sampling hardware, as opposed to e.g. the multi-coset approach in [6]. For the spectrum sensing detection problem, the additive noise at the receiver plays an important role in the performance of interest. Hence, rather than focusing on reconstruction in noiseless scenarios, in this paper we directly tackle the so called noise folding problem in the design [10]. Noise folding gives a Signal-to-Noise Ratio (SNR) deterioration approximately linear in the number of bands that are aliased prior to sampling [10]. This can cause poor performance for several Xampling approaches at low SNR. As discussed in [11], low density measurement matrices represent an effective countermeasure to noise folding. Additionally sparse matrices enable belief propagation techniques (i.e. message passing) for signal recovery (or detection in our context) that lead to state of the art performance, in spite of the poor conditioning of the sensing matrices.

Second, to the best of our knowledge, receivers in the Xampling family that are in the literature (see e.g. [3]-[6], [9], [12], [13]) do not fall in an active sensing framework, as they are not married to an online optimization of the measurement strategy. Hence, that can have a limitation: if the spectrum is not sufficiently sparse, neither the signal reconstruction, nor the detection of its presence in a certain band, can be accurate, even in the absence of noise. In fact, for general non-sparse signals, in a noiseless setting, [9] proved that half the Nyquist rate is necessary (see also [12], [13] for related discussions).

In contrast to Xampling architectures used for multi-band signals (e.g. the Modulated Wideband Converter (MWC) in [5], the Multirate Asynchronous Sub-Nyquist Sampler (MASS) in [25] and also [24]), our CSS sequential architecture only needs a single mixer, with a programmable analog waveform and Low Pass Filter (LPF). In our optimization framework, the Cognitive Receiver (CR) needs to select what group of sub-bands (generally non-contiguous) to sense at each test. A test corresponds to a sample, obtained after folding different sub-bands of the wide-spectrum signal. Compared to other multi-band signals receivers, the hardware in our architecture is simpler, since we use a single non-coherent receiver and a single sampling device that collects energy measurements sequentially, sampling at a fraction of the Nyquist rate.We use a time-dependent utility function to optimize the trade-off between sensing and exploitation.

It is important to remark that, similarly to [14]- [17] and [24], the optimum action will not generally attempt the full recovery of all the white spaces. In fact, the optimum decision may be conservative and sense a very limited portion of the spectrum. Furthermore, since scanning one sub-band at a time is a possible action of our active spectrum sensing strategy, our method subsumes previous techniques to scan the spectrum optimally, without mixing it. We refer to this approach as that of Direct Inspection (DI) and discuss it in Section IV-A.

Our work is more closely related with the stochastic op-timization schemes that extend the framework in [14]- [16], and optimize a CSS action based on previous observations [20]- [24]. With the exception of [24], the common goal in these papers is the recovery of the full support of a given vector. Typically, the techniques proposed are shown to be able to cope with lower SNR in the signal reconstruction with low complexity. What these optimizations do not capture is the fact that, in cognitive spectrum sensing applications, a timely decision is also desirable, to have enough time to exploit the spectrum. In fact, our method is also adaptive with respect to the time horizon K, the number of resources N , the prior probabilities on the states of each resources, and the parameters that characterize the utility function (i.e. reward/penalty for good/bad decisions). Interestingly, from our performance analysis, it is clear that sparse and adaptive sensing matrix designs outperform dense sensing matrices, as well as those that are sparse, but static. There are two main reasons for this: 1) through belief propagation algorithms, they achieve near-optimum detection performance; 2) they mitigate the aforementioned noise folding phenomena. We also emphasize in the paper that our model is applicable not only when the utility comes from finding empty entries (e.g. spectrum sensing), but also when one is interested in finding the occupied ones (e.g. in a RADAR application). More specifically, the paper is organized as follows: Section II is dedicated to the signal model and the analog front-end of our detector, and in Section III we formulate the optimization problem. Then, in Section IV we study the optimal dynamic design, for the Direct Inspection (DI) case (IV-A), where there is no mixing of sub-bands (also known as scanning receiver), and a Group Testing (GT) case (IV-B), where we introduce the possibility of mixing different bands. We will show that, even if finding the optimal policy is exponentially complex in the number of resources, we can characterize the approximation factor for a greedy procedure. Section V is dedicated to alternative detection approaches: linearization of Maximum Likelihood (ML) estimate and covariance estimation via LASSO relaxation. Numerical results to sustain our claims are presented in Section VI.

Notation We use bold lower-case to represent vectors, bold upper-case for matrices and calligraphic letters to indicate sets. With s A we indicate the entries i ∈ A of vector s, and with y 2

A we represent the weighted 2 -norm y T Ay. For any set function f (A) we define the marginal increment for adding element a, as

## II. XAMPLING DETECTOR

In the context of spectrum sensing for cognitive radio, in addition to the payload, each transmission includes large amounts of control signals overhead. It is then natural to assume that the activity of the Primary Users (PUs), in a certain spectrum, will persist for several sampling periods (see Fig. 1). However, assuming this interval lasts T = KT s , the sensing mechanism should provide the fastest decision it can. The goal of the proposed cognitive receiver is to sequentially sense the spectrum for the first portion of the interval and 

# A. Sensing Architecture and Observation Model

We assume that the complex envelope of the analog signal we are exploring is a multicomponent signal, with overall bandwidth equal to W = N R s . The components are indexed by i ∈ N , where N {1, 2, . . . , N }. The elements of the binary vector s = {s i : i ∈ N } ∈ {0, 1} N indicate presence (1) or absence (0) of a component (i.e. Primary User (PU) communication signal) over the i-th sub-band. During the interval 0 ≤ t < T = KT s the received signal is:

with w(t) ∼ N (0, N 0 δ(τ )) being Additive White Gaussian Noise (AWGN). The components of the received signals x i (t) correspond to each PU source, modeled as band-limited random processes with bandwidth R s ; they are equal in the mean square sense to the following process:

Rather than having a filter bank architecture as in [24], to further reduce the hardware complexity, we base our scheme on a sequential non-coherent sampling architecture. As the diagram in Fig. 2 shows, the sequential receiver we propose first modulates the received signal over the period (k -1)T s ≤ t < kT s with a signal g k (t), synthesized by selecting an appropriate input to the L VCOs. More specifically, if we denote by A k ⊂ N the subset of |A k | ≤ L sub-band mixed in the sample for the kth test, we have:

a ki e j2πRs(i-1)t , 

where, as shown in (5), only |A k | ≤ L tones are activated, and the phase φ ku in (4)-( 5) accounts for the delay in generating the tone at the u-th frequency included in A k plus the oscillator phase, while √ b ki is the amplitude. The optimal choice of the set A k is the subject of Section III. Correspondingly, for our incoherent detector, the sensing vector coefficients b ki (that is, the kth row of the sensing matrix B), associated to the kth sample, are:

irrespective of the random phase φ ku of the VCO tones activated. Clearly, if L N , B is a sparse matrix. Then, after convolving the modulated signal with an ideal low-pass filter, with impulse response sinc(πR s t), the receiver samples the output c(t) at times kT s , k = 1, . . . , κ. This operation is equivalent 1 to an orthogonal projection, as shown below:

b ki e jφi Y ki (7) where Y ki represent the orthogonal projections over the period (k -1)T s ≤ t < kT s of y(t) over the following signals:

Note that the signals e j2πRs(i-1)t sinc (π (R s tk + 1))

i,k∈Z form a orthogonal basis, and that (1) is equivalent to:

where w i [k] ∼ CN (0, n i ). If we model x i [k] also as i.i.d.

x i [k] ∼ CN (0, ϕ i ), we get that for a given state s:

where ϕ is a vector collecting the average, unknown a priori, received signal power from the existing communications. The receiver samples for k = 1, . . . , κ are:

and therefore, assuming φ i 's are independent and uniformly distributed in [0, 2π), they are also conditionally zero mean Gaussian random variables:

It follows that the information for the detection of the PU communications is embedded in the variance of the sample (which is the energy received during the k-th period). Sufficient statistics for our problem are:

which are exponentially distributed, i.e. y[k] ∼ Exp(θ[k]). 2Remark 1: The signals are subject to linear distortion due to a multi-path channel. For the case of Rayleigh narrowband fading and digital modulations, such as PSK with constant amplitudes, the model is correct, but it is only an approximation in other cases. Also, it would be appropriate to include a certain correlation among the samples x i [k] in the case of a frequency selective channel. We do not consider it, given the generalization is straightforward and the opportunistic strategy is a viable sub-optimum solution for that case as well. Finally it is useful to remark that, irrespective of the statistics of the received signal from the PUs, the variance of the samples c[k] will have the same expression, which makes the energy detector a viable heuristic in general.

Our work will investigate the design of:

1) the κ × N measurement matrix B, whose rows are the vectors b k (exploration phase) 2) a set of N decision rules δ = {δ i ∈ {0, 1} : i = 1, 2, . . . , N } over the unknown states s i of the resources (at the end of the exploration phase).

Notice that the design of B includes:

• the measurement vectors b k for each test at time k = 1, 2, . . . , κ (matrix rows), • the sensing (exploration) time κ to acquire information on the states s i via the observations y[k] (number of rows).

# B. Hardware Considerations

From the sufficient statistics y[k] in (15), it follows that our incoherent receiver can be implemented by combining different oscillators that do not require to be synchronized or phaselocked (see Fig. 2). Furthermore, the switching frequency of the oscillator is also R s , i.e. the single channel bandwidth, which is much slower than the limit of state-of-the art hardware. In the proposed receiver, the input is mixed with the signal g k (t), that folds the spectrum present in specific sub-bands onto the center frequency of the receiver, during what we can refer to as a sub-Nyquist carrier sensing phase. The samples are spaced by intervals of duration T s = 1/R s which is a factor 1/N smaller than the total spectrum.

Naturally, the mapping of the signal in general will be imperfect and, like in any ADC, calibration is necessary [26], [27]. For most ADCs the assumption is that this calibration is done during an initial training phase, in which a known input signal can be used to estimate the equivalent matrix B.

As far as the proposed architecture is concerned, the circuit diagram of Fig. 2 assumes a settling time for the VCOs much smaller than T s , i.e. the sampling period for the single channel sub-band. If this assumption does not hold, one should use a LPF with a smaller bandwidth and collect the samples c[k] at an even slower rate than R s , to wait for the VCOs to settle. This modification would not alter the statistical characterization of the samples, derived in the previous subsection. The drawback of taking samples less often is that (assuming the same occupancy coherence time) one would have accrued less information than what is available in the received signal, and would have less than K slots to decide. Given that our strategy is derived as a function of K, this would not invalidate our findings. Another possibility would be to replace the L tunable VCOs with N oscillators at constant frequencies, corresponding to the N possible bands of the signal. Using N oscillators would increase the power consumption and cost of the circuit but would significantly reduce the switching time between two measurements. Hence, this would be the natural choice if one wants to exploit a dense sensing matrix. Instead, the use of a bank of VCOs is preferable if the matrices are sparse because a small number of VCOs can synthesize the mixing signal. The switching would be in fact performed by a multiplexer, that would take the sum of the up to L tones selected by the vector b k .

In general, since we focus on the detection of the signal, with reasonably good device components we expect that calibration will either be far less demanding or unnecessary, if one accepts loss in sensitivity. In fact, the binary coefficients for the vector b can be set to ones and zeros, as discussed in IV-B1. Controlling the gains is unnecessary for the system to work, and it is preferable to not add tunable gains, as they can be another possible source of uncertainty and complexity in the system. Finally, imperfect tuning of the VCOs will reduce the SNR, either by spreading or misplacing the center frequency of the components of interest, but not fundamentally impair the detection.

## III. OPTIMIZATION FRAMEWORK

During the times devoted to sensing k = 1, 2, . . . , κ < K the player has the possibility to dynamically and adaptively design each measurement, by selecting a sensing vector b

that indicates what subset of the entries of s to probe (that is, the b ki 's will be non-zero only on the channels that are actively sensed). To capture the optimization between sensing (exploration) and exploitation of a subset of the N sub-bands, we model a total utility proportional to the time left for exploitation (Kκ). The detector acquires information about the entries s i via observations with a p.d.f. parameterized by an unknown vector. Together with the optimal design of B, the detector also optimizes the binary decision vector δ, once the observations have been collected, on the state of the sub-bands. We denote the type I (false alarm) and type II (miss) errors probability with α i , β i respectively, i.e. α i = P (δ i = 1|s i = 0) and β i = P (δ i = 0|s i = 1). We assume the state entries s i are mutually independent Bernoulli random variables with known prior probabilities, given by a vector ω = [ω 1 , ω 2 , . . . , ω N ], where ω i = P (s i = 0). A practical guideline to initialize the ω i 's is to set them to be uniform, and equal to a conservative estimate of the expected fraction of busy channels. In addition, past tests results could potentially be used to update the beliefs. Let us consider a reward r i > 0 for correctly detecting an empty sub-band and a penalty ρ i < 0 for failing to detect a busy sub-band, the utility can be written as

We anticipate however, that the framework proposed can be extended to cover applications where the utility comes from an action on the entries detected as busy, e.g. for a RADAR application, where the entries correspond to spatial directions. Thus we use SS case (Spectrum Sensing) or R case (RADAR) to refer to the case where utility is generated by detection of empty or busy entries (also referred as resources), respectively. Extending the definition in ( 16)

Remark 2: In our framework there are two possible actions over a resource: the null action that always brings zero utility, and the other one that brings a random utility, which depends on the actual channel state. This is motivated by the emphasis we place on the time-dependent utilization of the resources, which we assume occurs only in one of the binary states (based on the application of interest). Note that a more general formulation for (17) with 4 different rewards/penalties (for the possible cases (s i , δ i )) would not alter the structure of the problem, nor invalidate our results, i.e. there exists a unique mapping from our model to such case.

Finding the optimal policy corresponds to solve the following optimization problem maximize

### IV. DYNAMIC DESIGN OF SENSING MATRICES A. Direct Inspection (DI) case

In the DI case, we limit b k to have only one non-zero entry i, i.e. b ki = 0, b kj = 0 ∀j = i. This means that there is an underlying hypothesis testing:

In this context, it is known that the signal energy is a sufficient statistic for the test and that energy detection is optimal. Assuming no prior knowledge over the ϕ i 's in case of existing communication, we only need to design the test threshold, which we set in order to maximize the utility defined in (17).

Notice that, assuming a minimum average received signal power ϕ min > 0 in case of existing transmission, makes the test meaningful also for values of γ i < 1. Assumption 1: To simplify the decision problem, we will assume every resource has to be sensed before being declared empty/busy. This can be enforced as a standard/protocol rule or numerically guaranteed by setting ∀i ∈ N , ω i < ρi ρi+ri (SS case) / ω i > ri |ρi|+ri (R case). It is clear that the optimality3 of the test completely characterizes the set of decision rules δ for the sensed resources, while Assumption 1 gives us the decision rules for the non-sensed resources. This implies that for the DI case, the optimization in ( 18) can be expressed solely in terms of B. It is also known that for this type of test, where there is uncertainty in a parameter of the alternative hypothesis, one does not know the exact miss probability β; thus we will use an upper-bound, which will reflect in a lower bound for the achievable utility. Since this test is part of the DI strategy, we add the superscript DI to the test error probabilities α i and β i and have:

What we can guarantee, since ϕ i ≥ ϕ min is that

Remark 3: The test performance for the DI case does not depend on b ki , therefore, for the DI case no further optimization is needed over the sensing matrix B, other than selecting the non-zero entries.

Under Assumption 1, we can rewrite the optimization problem in (18) for the DI case as maximize

We then introduce the following Lemma Lemma 1: U DI (A) is a normalized, non-monotone, nonnegative sub-modular function of A. Proof See Appendix A Lemma 1 implies that there are diminishing returns in augmenting sets by adding a certain action to bigger and bigger sets. The maximization of a non-monotonic sub-modular function is generally NP-hard, but the case of interest is not as difficult. In fact, by sorting the resources i so that:

the set of size i, A i = {1, . . . , i} will be such that for any set

Therefore, what remains is to find the best set size i such that

The maximum in ( 28) is attained for

where

In fact, given the function is sub-modular, as soon as this condition is attained, it is maintained for i + 2, i + 3 etc., given that the marginal returns continue to decrease. This maximization is greedy and stops when the marginal reward becomes negative.

# B. A Group Testing Approach

We now allow each test to mix different sub-bands, i.e. the vector b k to have more than one non-zero entry. As outlined in the Introduction, aliasing of the spectrum comes with an associated noise folding phenomenon. Its impact is particular severe in a non coherent scheme as ours. In fact, the samples are collected sequentially and not in parallel, which means that we do not have multiple observations of the same value but only sequential observations tied to the same underlying random process.

To mitigate the noise folding effects, and reduce the hardware complexity, our focus is on low density measurement matrices. Our goal is to develop a relatively simple dynamic strategy for choosing a sensing matrix, whose utility can be expressed in closed form, and can potentially outperform the DI alternative. A common approach for recovery with low density measurement matrices is to use belief propagation via message passing 4 , whose most well known application is Low Density Parity Check (LDPC) optimum error correction decoding. For LDPC (and CS methods), performance guarantees come as asymptotic bounds on the 2 -norm, but little is known for optimal design in the finite regime. A difficulty in the design arises from the inherent multi-hypothesis testing problem associated with sensing several resources at the same time. This is why, to develop our dynamic design, we look at a Group Testing (GT) approach, which allows us to consider a binary hypothesis test for each measurement. In this way, the complexity of the analysis is relatively low, and we can derive the expected performance for any sensing matrix, under mild assumptions. Prior to providing more details, a remark regarding related group-testing approaches is in order:

Remark 4: In the context of group testing, little is known in presence of measurement errors that depend on the group size, which is the scenario this work considers, as the remainder of the paper will detail. Asymptotic results on the target rate for measurement-dependent noise, using an information-theoretic approach, are given in [28], where the noise is modeled as independent additive Bernoulli with bias dependent on the test size. Hence, the false-alarm and missed-detection probabilities of each single test, are symmetric. An additional noise, called dilution effect was considered in [29], where each resource could independently flip from 1 to 0 before the grouped test, and information-theoretic bounds were provided. In our model the false alarm and miss-detection probabilities are dependent on the optimization of the test threshold, therefore the noise is not independently added (nor an independent dilution can be considered). Furthermore, the strategy derived depends on the finite horizon for K, i.e. our results are not asymptotic. The same considerations apply to similar information-theoretic approaches in [30]- [32].

From the sensing matrix B, let us define the sets

Note that at times we use B as an argument in functions that, strictly speaking, are just functions of the sets A k just defined. For each test we define a binary group test as follows 5 :

) 4 In our model, an uninformative prior can be assigned to the ϕ i 's to run the belief propagation message-passing algorithm on the obtained measurements 5 We envision that such test would be useful for a downlink transmission in which the Access Point (AP) may want to allow multiple communications at the same time and can alert the SUs over a narrowband signaling channel to access the spectrum.

Remark 5: It is important to highlight that the two hypotheses pertain exclusively the group of sub-bands explored in test (i.e. A k ), not the whole spectrum. Also note that this grouptesting approach pertains the design of the sensing matrix and detection algorithm and not the underlying observation model. The different approaches we compare ourselves against later, use detection strategies that are multi-hypothesis tests.

The test can be written as:

for which we can derive:

The decision declares that resource i is busy (H 1 is true) if the majority of the tests, where resource i is involved, is positive, else it accepts the null hypothesis H 0 for resource i. Thus:

where the functions π j (i, b, γ), j = 0, 1 are only defined when b i = 0. These functions represent the probabilities of declaring H 1 in a group-test defined by b with threshold γ and given s i = j, j = 0, 1. Notice that the error probabilities α, β refer to each binary hypothesis testing defined in (30). The notation for β i (b, γ; s i ) indicates the probability of having a missed-detection conditioned on the state s i of one of the resources. It then follows that

where F P BD (k; n, p) indicates the CDF of a Poisson Binomial Distribution parameterized by p ∈ [0, 1] n . At this point, one can replace (36)-(37) in (17), to then solve the optimization in (18), where the equivalence between the decision rules δ and the selection of the thresholds γ is essentially the same as for the DI case. Notice that, in order for (36)-(37) to hold, each of the tests must be independent, conditioned on the state of the resource i. This is true if the sensing matrix (in the language used for LDPC codes) does not have length-4 cycles (i.e. two different measurements do not mix more than one sub-band in common) 6 .

The optimization remains extremely complex due to the complexity of the decision space for B and the sum of an exponentially growing number of terms for the probabilities defined in (36)-(37). Nevertheless, it gives a method to evaluate the objective of our optimization for any sensing matrix B, where the optimization over γ can be numerically solved. In fact, (36)-(37) are monotonic functions of the probabilities π 0 , π 1 defined in (34)-( 35), which are monotonic in the γ k 's, and therefore a unique solution for γ exists. Next, we introduce additional constraints to (18), in particular on the structure of B, in order to evaluate whether a GT strategy could be superior to the DI approach.

Remark 6: Note that an ML or a MAP estimator, for a rank-deficient sensing matrix, do not provide optimality guarantees in terms of minimum error probability or minimum Bayesian risk. Nevertheless, for the same sensing matrix, we expect the MAP estimator to outperform the binary grouptesting hypothesis in (30) by simply adding more degrees of freedom to the decision δ in the κ-th dimensional space of the observations. Therefore, the evaluation of the objective in (18) via (36)-(37) provides a benchmark for the utility obtainable with a more refined detection method.

1) The pairwise tests case: We start by considering matrices B that have the following property: each resource is sensed only one time, either directly inspected or mixed with another resource, and no test mixes more than 2 resources, i.e.

Let us discuss the test that mixes entries i and j. According to the strategy derived at the beginning of the section, one can use (34)-( 35)-( 36)-(37) to write out the per-time instant utility obtainable after the decision. First, from (30), we note that, without prior knowledge over ϕ i , ϕ j other than the threshold ϕ min , the best choice to minimize α is to set b i = b j (we refer to this false alarm probability as α ij ). Therefore, similar to the DI case, one can consider binary coefficients for b k , i.e. b ki = 0 → b ki = 1. This will hold true also for the extension of L > 2 and will give implementation advantages as discussed in II-B.

A missed detection event in (30) can occur for three different states of the resources i, j; we upper-bound the corresponding missed detection probabilities by always considering θ = θ min and refer to this bound as β ij,max . We then obtain:

where the threshold for this test γ ij has been set to maximize (38), i.e.

Let us then consider a graph where each resource is a vertex and the edge weight u ij between two vertices ij is the utility (per time instant) u GT ij just defined (the weight of the loops u GT ii are given by u DI i in ( 26)). We can then translate our problem into a particular instance of a max-cut problem: picking a subset of the edges and form a subgraph, where each edge represents a test, to maximize the objective in (18). Formally, we can write maximize

where

and deg E (i) is the nodal degree of node i induced by the undirected graph G = (N , E). It is possible to map the constraint on the nodal degree in the objective of (40), by adding a penalty for the violation of such constraint. This guarantees the optimal solution will be equivalent to (40), i.e. no set of edges that violates the constraint can improve the objective, and any feasible set of edges would have the same objective in the two problems. We rewrite our optimization as maximize

where

and M is a positive constant. Lemma 2: For M > 0 the objective in (42) is a nonmonotone sub-modular function of E and it is possible to find M * > 0 such that for any M > M * the two optimizations (40)-(42) are equivalent. Proof See Appendix B We now discuss the extension of this result for L > 2, to develop a general algorithm that leverages the sub-modularity of the optimum design problem in (40).

2) Extension to L > 2: If we mix more than 2 channels, instead of just edges or self loops to indicate the tests, we could have cycles of length up to L. The nodal degree in (42) will then be interpreted as the number of cycles a node belongs to, and the set of edges will be replaced with the set of cycles. We then replace the set E of edges with the set C of possible cycles, and use c to indicate the generic cycle (which could be a self-loop, an edge or a cycle with length 3 or greater). With these substitutions, the proof of sub-modularity in Lemma 2 naturally extends to this case as well. In light of the constraint |B i | ≤ 1 we will have that no node can be in two cycles.

To visualize this concept, in Fig. 3 we show two possible sets of cycles of length up to 4. On the right, we have a set of tests that respect our constraint: there is a test that only considers one resource and three tests that combine 2, 3, and 4 resources respectively, but no resource is considered in two different tests. On the left, instead, a resource is considered in two tests: one where it is combined with other 3 resources, and one where it is inspected directly; such configuration is therefore not acceptable. Algorithm 1: Greedy Maximization of U GT (C )

3) The factor approximation of the greedy algorithm: Having proven the sub-modularity of (42) in Lemma 2, it is natural to resort to a greedy procedure, however it is important to highlight that the objective in (42) does not respect the non-negativity property. To the best of our knowledge, there is no known procedure in the literature on approaching the maximization of a general sub-modular non monotone function, if the minimum value is not known: no constant approximation factor guarantee can therefore be given in general. Nevertheless, due to the particular structure of our problem, it is possible to find a factor approximation for the output of the greedy procedure, described in Algorithm 1.

Lemma 3: Algorithm 1 guarantees a α-constant factor approximation of the optimal solution for (42), where:

Proof See Appendix C.

The set C indicates the set of cycles that are not adjacent (share a node) with any of the cycle in C . Note that

so, as long as the number of tests |C |, added in the greedy maximization, is less than the time horizon K, we have arg max

This relation indicates that, in the greedy procedure, edges are added in decreasing order of utility, respecting the constraint on the nodal degree. Also, from (45) it is easy to find that the optimal |C | will never exceed K-1

## 2

. In the greedy procedure in Algorithm 1, there is a constant number of operations per query, which indicates the overall complexity of the algorithm is dominated by the sorting of all possible cycles' utilities. In the worst case, sorting n values require O(n 2 ) operations, thus the complexity will be given by O

O N 2L , i.e. polynomial in N and exponential in L.

# C. Additional applications of the stochastic optimization

We would like to highlight the analysis for the factor approximation of the greedy strategy transcends the spectrum sensing application discussed in detail in this paper. In fact, group-testing has been applied to a number of disparate contexts to model the outcome of sequential tests. As long as one has a way to define the per-time utility derived from each test as in (38), and an overall utility as in (41), then our results can be applied. Classes of problems that could be formulated in a similar way include job scheduling for data centers, design of parity checks for rateless coding, dynamic advertisement (promoting an offer that bundles two products/services together) etc.. Obviously, in all these cases, the statistics of the observations would be radically different.

## V. ALTERNATIVE APPROACHES

In the previous sections, we have provided methods that find a low density measurement matrix. As will be apparent in our numerical results, the noise folding phenomenon justifies the use of sparse sensing matrices. They are also ideal when one wants to use belief propagation to the decision problem. However, for the sake of comparison here we look at alternative support recovery methods, which can be applied to any measurement matrix B, and that can be mapped into previous solutions, as the MWC in [5], [9].

# A. ML estimate

Let us assume that κ measurements have been collected, by mixing a set A ⊆ N of sub-bands. One could ignore the prior ω i and derive the ML estimate for ϕ. The log-likelihood function is:

where the linearization corresponds to the Taylor expansion of the likelihood function around the observations mean (recall ( 14)-( 15)). A possible approach consists in solving the following LASSO problem:

(48) with C = diag(y) denoting the covariance of the observations, and λ A the vector of weights for the weighted 1 -norm. The first penalty term in the objective enhances sparsity, while the second term comes from the ML estimate in (47). To incorporate the information of the prior beliefs ω i , one can set λ i = γ i from (20), ∀i ∈ A, to favor the estimates ϕ i > 0 for entries with lower thresholds γ i . Alternatively, one can also set λ i = λ ∀i ∈ A. Note that, compared to the nonsequential sampling models (i.e. those using a filterbank), the application of the LASSO (see Section V) in this context is an approximation. The random demodulator in [33] (similar to our scheme in terms of architecture) is an Xampling ADC converter for signals that are sum of harmonics with constant amplitude, i.e. each subspace, in the UoS representation, has finite dimension. This is not the case we are interested in, and approximating our signal as a sum of harmonics would require sampling at a much higher rate than R s . For our multiband signal model, instead, rather than having observations that are noisy linear combination of a sparse input, the p.d.f. of the samples depends on those same linear combinations.

# B. Covariance estimate

A similar approach is to estimate the covariance of the samples, and write the correspondent linear equations system, as derived in [9], using the analog front-end of the MWC, introduced in [5]. This leads to write a system

where the sensing matrix B (A in their work) is a M × N matrix, with M being the number of analog channels and N the number of spectral bands, whose occupancy is desired to be detected. The authors estimate the covariance vector (diagonal of the covariance matrix)

(where the x i have been introduced in ( 9)) by taking multiple samples in multiple frames (in their work K samples per frame in P frames). Note, however, than in their work the sampling frequency per branch, called f s , needs to be larger than the single component bandwidth R s , to justify the approximation of a multi-band signals as sum of harmonics with constant amplitudes over a single frame. Also, in light of this, the different frames considered for the estimate of z, cannot be consecutive in time, since the x i 's would be correlated. However, for the sake of comparison, to calculate the utility of the scheme in [9], we will ignore this limitation in our numerical tests (c.f. Section VI) as well as their need of sampling at a faster rate than our method, pretending their scheme can take κ (using their notation κ = KP ) independent consecutive measurements, and can do so at the lower rate R s . One can then use (48) for the recovery of the sparse vector ϕ from the linear system in (49), replacing C with I, and A with N , since their scheme mixes the whole spectrum.

## VI. SIMULATION RESULTS

In this section, we showcase the ability of our approach to dynamically switch between a DI receiver (scanning receiver) and a GT approach, based on the expected occupancy (the vector of priors ω), the time available K, the minimum SNR threshold SN R min = ϕmin w and the number of resources N . In the context of spectrum sensing (SS case), the parameters r i and ρ i can be mapped into a maximization of the overall weighted network throughput (see [17]): the reward r i can be proportional to the achievable rate over the channel i in the absence of PU communications, i.e. r i ∝ log(1 + SN R i,S ) (where the suffix S indicates the secondary communication), whereas the penalty ρ i can be made proportional to the loss in rate caused to the primary communication, due to the interference added by the secondary. For the cognitive radio application, the concept of exploitation of the resource is tied to the definition of utility function chosen in (17), which is expressed in bits/s/Hz7 . The longer the time available to transmit, the larger is the number of bits that can be transmitted over that band. For the other case, i.e. when the reward comes from detecting correctly resources that are busy (for example a RADAR application), it is not immediately clear why the utility would be proportional to the number of remaining time instants. To interpret this, we model the action upon declaration of s i = 1 as a Bernoulli trial which accrues a reward r i if such action is successful (i.e. the target is actually hit) and this happens with a certain probability p i for each attempt. The number of attempts T i necessary to hit the target will then be geometrically distributed. One can then find that the expected reward is equal to

for small p i , which would motivate having an expected utility that increases linearly with time. The ρ i associated with this case would model an intervention cost, whose main purpose would be to limit the false alarm rate. It is important to highlight, however, that the time dependency in the optimization objective prevents our formulation to return a standard Constant False Alarm Rate (CFAR) detection method. Nevertheless, our model can apply to electronic warfare (tentatives of create jamming), wake-up radio, and other problems where the action (and the associated utility) is on the channels that are declared busy. For all the figures we refer to L = 2, 3 as the maximum number of resources per test allowed in our greedy procedure in Algorithm 1. Theoretically, the optimal value for U GT monotonically increases with L, since increasing L introduces additional degrees of freedom. However, in our simulations we used the greedy solution and, as proved in our Lemma 3, the approximation factor of the greedy maximization is potentially worse for higher values of L, as the following numerical results will show.

We indicate with "Group Testing" the utility obtained with our GT approach. The "MAP Estimator" is the estimator that knows the true values ϕ i , uses the same matrix B of the GT approach, but then decides on each resource, based on the posterior for ω i , using belief-propagation.

1) SS case vs RADAR: Even though, in light of the symmetry in the definition of the threshold γ i , one can switch the r i 's and ρ i 's to go from SS case to R case and find the same trends, even for the combined tests, to avoid confusion, we highlight the difference in the two scenarios, in the first simulation we present in Fig. 4.

For the experiment in Fig. 4, we set K = 30, N = 60 and r i = r, ρ i = ρ and ω i = ω, SN R i = SN R min (10dB) ∀i ∈ N we have that for ω equal to ρ ρ+r or r ρ+r for SS case or R case, respectively. These are the threshold values given in Assumption 1, to guarantee no resource can give positive utility if not tested. As we can see, in both scenarios the utility increases with the ratio ρ r , since the prior probability, that favors a positive utility, increases as well. However, the gain for the GT approach over the DI, happens in complementary ranges: when ρ r > 1 for spectrum sensing application, and when ρ r < 1 in the RADAR problem. When the penalty increases with respect to the reward, the GT approach for spectrum sensing will be conservative and not transmit in any of the channels in a group that tested positively; nevertheless, as the priors ω i increase, it is possible to find multiple empty sub-bands with just one test and gain in utility compared to the DI. For the RADAR application, when the penalty increases with respect to the reward, there is a disadvantage in declaring as busy all the elements in the test, even if the prior ω decreases. Clearly this limits the benefit of combined tests, whereas when ρ r decreases, there is a gain since one element, found busy in the pool, guarantees higher reward. Apart from this asymmetry, both cases show the same trends in utility over number of available resources N , and the value of SN R min . Hence, we will only consider the SS case in the next simulations.

2) Utility for different N : In Fig. 5 we plot the utility (normalized over K 2 ) over the ratio K N for two different horizons, i.e. K = 10 and K = 30 and SN R min = 10dB. We can see that, only for K N 0.75, the GT approach outperforms all competing options whereas, when the horizon increases, almost no benefit comes from mixing resources. This suggests that there is enough time to test them independently with high accuracy. For this experiment, we set ω i ∼ 0.7, ρi ρi+ri , where r i = log(1+SN R i,S ) and ρ i = 5r i with SN R i,S dB ∼ U( [10,20]). The SN R for the test, ϕi ni , is generated uniformly between 10 and 20 dB; recall that the only information used in our algorithm is the minimum SN R value, i.e. in this case 10dB. In the regime considered, the DI is approximately constant since it is easy to show U DI,OP T ≤ K 2 4 u max irrespective of N . 3) Utility for different SN R min : In this set of experiments we studied how the utility behaves versus the minimum SN R in each active sub-band. In this case the SN R was drawn uniformly between SN R min dB and SN R min dB + 10, and once again only the value of SN R min was used in the optimization, which is shown in the abscissa of the figures. Matching our intuition, we can see how the GT approach outperforms the DI only when SN R min is sufficiently high, and also that the gain in utility is larger for K = 10 than for K = 30. In fact, for this experiment the number of resources has been fixed to N = 20 and, as previously highlighted, increasing K for fixed N diminishes the benefit of combining resources in a test. We also plotted the utility obtainable with the approximate ML estimate obtained via Compressive Sensing, described in Section V. For this case, to illustrate the noise-folding issue, we used a dense matrix that has the same aspect ratio of the one found via GT approach (i.e. that scans the same set of resources for the same number of tests). To show reasonable results, only for the ML estimate via CS, we actually took the mean of y[k] over 10 samples. We can see that, despite having more measurements, such approach gives a much lower utility than DI as well as the proposed GT, due to the negative effect of noise folding. For K = 30, we also compared our approach with the performance obtained using belief propagation in a loopy network and an LDPC matrix (see [11] for details). Considering N = 20 resources, and an expected sparsity approximately equal to 4, we chose a regular LDPC matrix with a row weight of 5 (20/4 as suggested in [11]) and a column weight of 3, resulting in 12 tests. The LDPC has not been implemented for K = 10, since the regularity constraints would have given either a diagonal matrix (same as DI), or a relatively dense matrix. The absence of any optimization in the choice of which and how many resources to test produces a utility which, for low SN R, is lower than the DI approach proposed. For high enough SN R, the LDPC design can outperform the DI approach, but still gives a utility lower than our GT strategy with L = 2. This highlights the benefit of having an active sub-Nyquist receiver compared to a static offline selection of the parameters.

4) Detection performance vs MWC: In this last section, we compare our approach to the performances of another incoherent power spectrum sensing approach proposed in [9], which was mentioned in Section V-B. We considered 150 independent bands with an expected 5% occupancy, e.g. ω i = 0.95 ∀i ∈ N . For the MWC scheme there are M = 30 analog channels (in expectation twice the support of the double-sided bandwidth occupancy). We recall that our definition of SNR is the worst case per sub-band when the signal is present, e.g. SN R = ϕmin n , where we considered for simplicity n i = n ∀i ∈ N , whereas the SNR considered in [9] is i∈N siϕi N n . As indicated in [9], for a fixed sensing time it is preferable to choose approximately the same number of frames  and samples per frame. This is the case in the simulations in Fig. 7, where κ indicates the product of the two quantities KP . For the utility parameters, in light of Assumption 1, we considered r i = 1 and ρ i = 19, ∀i ∈ N . We remark however, that the ROC curves in Fig. 7 for the MWC power spectrum sensing, do not depend on these parameters: the different points in the curve are obtained by changing λ (λ i = λ ∀i ∈ N ) in (48). Notice that, for the same sampling frequency in each branch, the scheme in [9] collects M times the observations we collect per unit of time, hence the two points for the optimized strategy correspond to: 1) an unfair comparison with our scheme, where we keep 1 M observations (indicated with R s ), and 2) a fair comparison where we assume our scheme can collect the same number of observations, sampling at M • R s , hence obtaining a factor of M SNR gain per test 8 . We note that at 10 dB (Fig. 7a), the MWC scheme needs approximately 8 • M = 240 times more observations to outperform our proposed approach in the unfair comparison, while at 20 dB our approach offers better detection performances, even when we let MWC collecting 200 • M more observations. What is remarkable is how much higher is the gain of our approach at higher SNR. This is due to the effect of the SN R on the covariance estimate, which is required in the power spectrum sensing algorithm in [9]. In fact, while extending the number of samples per frame K can mitigate the noise-folding problem, improving the accuracy of the covariance estimate requires a larger number of frames P , despite good SN R. For instance, for a Gaussian random variable with zero mean and variance σ 2 , it is relatively straightforward to find that the ML estimate for the variance, has itself variance equal to 2σ 4 P (for P observations). This implies that the LASSO-recovery step does not keep to improve for higher SN R, but rather by averaging more, i.e., for higher P .

## VII. CONCLUSIONS

In this work, we proposed a new framework to optimize the performances of an opportunistic spectrum access strategy with sub-Nyquist sampling, and described the connection between such strategy and the design of the front-end sampling architecture. For the problem proposed, we characterized the factor approximation of the greedy strategy and showed, via numerical results, how our dynamic design for the sensing matrix guarantees better performances than other static approaches, namely: the linearization of an ML estimate using a dense CS-sensing matrix, Belief Propagation using a regular Fig. 7: ROC curve for the power spectrum sensing algorithm in [9] with MWC front-end, and comparison with the detection performances of our optimized sensing strategy Low-Density Parity-Check sensing matrix, and the Xampling power spectrum sensing strategy proposed in [9].

# A. Proof of Lemma 1

To prove the submodularity of U DI (A) we show that, to prove the submodularity property:

which is true by assumption on the function u DI i defined in (26). Since u DI i = 0 for α i = 1, β i = 0, we have u DI i ≥ 0 ∀i ∈ N for the optimized α DI i , β DI i,max .

# B. Proof of Lemma 2

The function is the sum of two terms, to prove the first one is sub-modular one can follow the same steps in Appendix A. For the second term, it is enough to show that, for any i, -Υ(deg E (i)) is a sub-modular function of E. The function is clearly sub-modular since is a concave function of the nodal degree, and from this we can conclude the second term is a positive sum of sub-modular functions, hence sub-modular.  To prove the equivalence of the two optimizations in (40)-(42), we first note that for any E that satisfies the constraints in (40), the second term of the objective in (42) is equal to 0 the two are equal. It follows that we simply need to verify that no set of edges, that violate the constraint on the nodal degree, would be the optimal solution for (42). To show this, we note that any infeasible set of edges (according to (40)) can be made feasible by removing some edges. For M large enough, i.e. M > K max ij u ij it is relatively straightforward to verify that such removal of edges would improve the objective, preventing an infeasible solution for (40) to be optimal for (42), and this concludes the proof.

# C. Proof of Lemma 3

We want to prove

where α =

2 } and L eff ≤ L is the largest test size returned by the greedy algorithm. We also rewrite

To prove the claim we look at the graph obtained by the union of the cycles in the optimal and the greedy solution. Since in each of the solutions, no node can be in two cycles, it follows that in the obtained graph, no node can be in more than two cycles. Let us start by assuming there is a cycle C with associated utility u C in the optimal solution that does not share any node with the greedy solution. This means ). This means we can replace a cycle in C G with this isolated cycle, to form a set of cycle C G whose objective is lower than C G by greedy search. In light of (54), we can iterate this process by always picking the cycle to be replaced, in such a way that all the cycles in the optimal solution share at least one node with the set of cycles in C G . Now we have that all the cycles in the optimal solution share at least one node with a cycle in C G .

If, instead, one has that no cycle C in the optimal solution is isolated, and that there are isolated cycles in the greedy solution, then the set C G is formed by removing these cycles from C G , lowering the objective (by submodularity and greedy search). One would then again obtain that all the cycles in the optimal solution share at least one node with the set of cycles in C G . We now iteratively remove cycles from C G and C OP T , while bounding the loss in performance and therefore obtain the factor approximation we want to prove. We can remove cycles from C G in decreasing order of utility and since we know that for each cycle C of length L in C G there are at most L different cycles in the optimal solution that share a node with C , by greedy search we have that L • u C is greater than the utility given by the cycles in the optimal solution that are adjacent to cycle C . Let us then define Ĉ G as the minimal subset of C G that can cause the removal of all the cycles in the optimal solution when iterating the procedure just described, i. 

this concludes the proof. We have used the fact that d ≤ L eff and that the function d(Kd) has its maximum in d = K 2 . Fig. 8 shows an example of the iterative procedure to obtain the bound just derived. 

