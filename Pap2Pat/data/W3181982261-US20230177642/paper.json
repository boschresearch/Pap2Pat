{
    "id": "https://semopenalex.org/work/W3181982261",
    "authors": [
        "Vladimir M. Shalaev",
        "Alexander V. Kildishev",
        "Alexandra Boltasseva",
        "Xiaohui Xu",
        "Zachariah O. Martin",
        "Simeon Bogdanov",
        "Demid Sychev",
        "Zhaxylyk A. Kudyshev"
    ],
    "title": "Machine learning assisted quantum super-resolution microscopy",
    "date": "2023-08-10",
    "abstract": "One of the main characteristics of optical imaging systems is spatial resolution, which is restricted by the diffraction limit to approximately half the wavelength of the incident light. Along with the recently developed classical super-resolution techniques, which aim at breaking the diffraction limit in classical systems, there is a class of quantum super-resolution techniques which leverage the non-classical nature of the optical signals radiated by quantum emitters, the so-called antibunching super-resolution microscopy. This approach can ensure a factor of [Formula: see text] improvement in the spatial resolution by measuring the n -th order autocorrelation function. The main bottleneck of the antibunching super-resolution microscopy is the time-consuming acquisition of multi-photon event histograms. We present a machine learning-assisted approach for the realization of rapid antibunching super-resolution imaging and demonstrate 12 times speed-up compared to conventional, fitting-based autocorrelation measurements. The developed framework paves the way to the practical realization of scalable quantum super-resolution imaging devices that can be compatible with various types of quantum emitters.",
    "sections": [
        {
            "title": "Results",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Machine learning assisted antibunching super-resolution microscopy",
                    "paragraphs": [
                        "The antibunching SRM technique relies on the detection of quantum correlations in the signal radiated by quantum emitters, which allows for a gain in the spatial resolution of a factor of ffiffiffi n p by measuring n-th order autocorrelation function 22 . This fact can be understood by conducting a Gedanken experiment first proposed by Hell et al. 32 . In the case of a hypothetical emitter that emits photons by pairs, an improvement in resolution can be theoretically obtained by sending each of the two photons to a separate camera. Since the two cameras will record two independent point-spread function (PSFs) estimates, the spatial resolution can be improved by a factor of ffiffiffi 2 p via simple multiplication. However, instead of requiring the emitter to emit pairs of photons, one can acquire the same amount of information by assessing an absence of the two-photon correlation in single photon emission by measuring the second-order autocorrelation function. Furthermore, one can achieve an arbitrarily high improvement in resolution by measuring higher-order correlations in the emission of a single photon emitter. In the most general form, the intensity distribution of the super-resolved image based on antibunching SRM G n \u00f0 \u00de \u00f0x,y\u00de can be obtained via retrieving spatial distributions of the n-th order autocorrelation function at zero time delay g n \u00f0 \u00de \u00f0x,y,\u03c4 = 0\u00de and the number of detected photons e N\u00f0x,y\u00de 22 :",
                        "here h e N\u00f0x,y\u00dei is the average number of detected photon from a given point x,y \u00f0 \u00de of the sample; \u03c7 i is a function of the product",
                        ", where i max is the number of ordered combinations, fulfilling the condition P l k = 1 j k = n. For example, for n = 2 case, Eq. (2) takes the following simple form 22 :",
                        "The most commonly used approach for retrieving the g 2 \u00f0 \u00de \u00f00\u00de value is a Hanbury-Brown-Twiss (HBT) interferometry measurement, composed of a beam-splitter directing the emitted light to two singlephoton detectors connected to a correlation board (Fig. 1a). The correlation board registers events consisting of pairs of detector clicks. It then arranges these events into a histogram as a function of the time delay \u03c4 between the clicks, which can be used for the post-processing via Levenberg-Marquardt fitting:",
                        "Here, a j ,t j ,j = 1,2 are the fitting parameters related to the internal dynamics of the emitters. Figure 1b shows the main steps of the fittingbased approach for the realization of the antibunching SRM technique. The area of interest is divided into n \u00d7 m pixels, and autocorrelation histograms are acquired at each pixel. The autocorrelation measurement is performed for several minutes. The L-M fitting is done over all of the HBT histograms and the corresponding g 2 \u00f0 \u00de x,y,0 \u00f0 \u00de map is retrieved. Finally, the resolved image is calculated via Eq. ( 2) (Fig. 1d).",
                        "In our demonstration, we use single nitrogen-vacancy (NV) centers in nanodiamonds dispersed on a coverslip glass substrate as single photon emitters. These emitters typically yield between 10 4 and 10 5 counts per second on each of the single-photon detectors in the HBT setup (when in focus) and exhibit fluorescence lifetimes between 10 and 100 ns. During the scan, when the emitters are partially out of focus, the fluorescence counts drop significantly. Consequently, in order to assess g 2 \u00f0 \u00de \u00f00\u00de via Levenberg-Marquardt (L-M) fitting with an uncertainty varying between \u00b10.01 to \u00b10.05, autocorrelation histogram acquisition times of 1 min are required per pixel. In the pulsed excitation regime, the fitting is not required to retrieve g 2 \u00f0 \u00de \u00f00\u00de as long as the pump repetition period is much longer than the emitter's fluorescence lifetime. However, this requirement becomes somewhat impractical when the emitter lifetime is long as in the case of NV centers. The developed ML approach addresses the aforementioned problem by rapidly estimating the g 2 \u00f0 \u00de x,y,0 \u00f0 \u00devalues based on sparse HBT measurement. The main framework of the developed approach is shown in Fig. 1c. A CNN regression network is trained on a set of \"sparse\" autocorrelation data with short acquisition times (see the Methods section). Once trained, the CNN network estimates the g 2 \u00f0 \u00de \u00f00\u00de values, requiring an acquisition time of less than 10 s."
                    ],
                    "subsections": []
                },
                {
                    "title": "Machine learning assisted autocorrelation function measurement",
                    "paragraphs": [
                        "The main building block of our ML assisted antibunching SRM technique is the CNN based regression model, used for retrieving g \u00f02\u00de \u00f00\u00de values. In this section, we highlight the structure of the CNN, its training and testing, as well as compare its performance against conventional L-M fitting. The training dataset for sparse second-order autocorrelation histograms consists of measurements performed on a set of 40 randomly dispersed nanodiamonds with NV centers on a coverslip glass substrate. Figure 2a shows the schematics of the HBT setup used for these measurements. Two avalanche detectors (D1, D2) with 30 ps jitter are connected to a pulse correlator with a 4 ps internal jitter. The co-detection events are recorded over a range of 500 ns and collected into 215 equally sized time bins. For each of the 40 emitters, hundreds of sparse autocorrelation histograms with 1 s acquisition time are collected, until the total number of co-detection events in their sum allows a precise ground truth (g \u00f02\u00de \u00f00\u00de) estimation via L-M fitting with fitting uncertainty varying between \u00b10.01 to \u00b10.05. The estimated ground truth value is then assigned as a label to the entire set of 1 s histograms. We then formed all the possible combinations of 1 to 10 of these 1 s histograms to obtain training data that emulated histograms with acquisition times from 1 s to 10 s. Such a data augmentation process assumed that the emission is a process with no memory over times exceeding 1 s and allowed us to significantly extend the training dataset. More information on the training dataset collection process and augmentation is described in the Methods section. Additional details on the CNN structure and training can be found in Supplementary Section 1, Table S1 and Figure S1.",
                        "Figure 2b shows the structure of the CNN used for g \u00f02\u00de \u00f00\u00de regression. The CNN consists of one input layer, three hidden convolutional layers, one max-pooling layer followed by dropout, three fully connected layers, and one output node containing the regression result. The input layer had 215 nodes corresponding to the number of bins in the input histogram. The feature learning part of the CNN is optimized to capture the salient features of the autocorrelation datasets, while the regression part is trained to predict g \u00f02\u00de \u00f00\u00de values based on these extracted features. All the hidden layers were comprised of 260 filters. The third hidden layer's output is connected with the max-pooling layer, followed by the dropout layer. The kernel size of the filters (4) is chosen to be the same for each layer. Importantly, the CNN takes the total number of two-photon detection events N events in the histogram as an additional input. N events is concatenated to the output of the feature learning part and used as a regularization term during the training process. The 5s-10s histograms acquired on pixels where the contribution of the quantum emission to the total counts is negligible, feature N events < 4, while the histograms on areas close to the quantum emitter locations feature N events = 65 on average. To populate the \"dark\" pixels, the CNN regression network is implicitly biased to produce g \u00f02\u00de \u00f00\u00de = 1, on the datasets with N events < 4 counts. Supervised training of the CNN regression model was performed using the augmented dataset of 5s-10s sparse HBT histograms and the corresponding ground truth labels. The training process is realized by performing adamax gradient descent optimization using the Keras library 33 for 100 epochs with mean absolute percentage error loss function. 80% of the dataset is used for training, while the remaining 20% are used for validation and testing.",
                        "The performance of the trained CNN regression model is assessed via calculating the mean absolute percentage error (MAPE) and the coefficient of determination (r 2 ) on the 5 s histogram datasets. Figure 2c shows the regression plot of the L-F fitting performed on 5 s HBT histograms.",
                        "Markers show the average value of the prediction, while error bars show the standard deviation over the set of 5 s histograms belonging to the same emitter. Due to the sparsity of the HBT measurement, the L-M fitting expectedly cannot ensure precise fitting of the data, which results in MAPE = 32%, r 2 = 70% and root mean square error (RMSE) of 0.215.",
                        "In contrast, the CNN regression model ensures very precise predictions of the g \u00f02\u00de \u00f00\u00de values based on 5 s HBT histograms (Fig. 2d). Due to the ability of the CNN network to learn hidden correlations between signature features of the sparse datasets and the ground truth labels, the CNN regression model shows excellent performance on the sparse dataset and ensures low MAPE (5%), a high coefficient of determination of 93% and RMSE of 0.0018. The CNN performance is also robust against the reduction of the acquisition time. We analyze the performance of both approaches on 5 s, 6 s, and 7 s HBT datasets. The performance of the direct fitting ensures 30% and 27% MAPE when applied to 6 s and 7 s HBT measurements, respectively. The CNN regression model ensures performance that is much more robust than L-M fitting. It ensures 3.92% MAPE on 6 s HBT datasets and reaches up to 3.58% MAPE when applied to 7 s datasets. "
                    ],
                    "subsections": []
                },
                {
                    "title": "Experimental realization of machine learning assisted antibunching super-resolution microscopy",
                    "paragraphs": [
                        "The benchmarking of the ML-assisted regression of autocorrelation data enables the experimental demonstration of the ML-assisted antibunching SRM. The experiment is realized on a sample of randomly dispersed nanodiamonds with NVs on a glass substrate. In this demonstration, the objective is scanned using a piezo-stage with sub-10 nm resolution over the 775\u00d7775 nm 2 region of interest, which is divided into 1024 (32\u00d732) pixels and contains one nanodiamond with a single NV center. Autocorrelation measurements are performed on each pixel in 1 s time increments with a 7 s total acquisition time per pixel. Along with the autocorrelation data, the corresponding photoluminescence (PL) map is retrieved (Fig. 3a).",
                        "The cross-section of the diffraction-limited image, taken along the blue dashed line (Fig. 3a), is shown in Fig. 3b. Gaussian fitting of the intensity distribution yields a full width half maximum (FWHM = 2 ffiffiffiffiffiffiffiffiffiffiffiffiffi 2ln\u00f02\u00de p \u03c3) of 310 nm. We therefore treat the PSF of the optical microscope as a Gaussian distribution with a FWHM of ~300 nm (see Supplementary Figure S2 for setup PSF characterization). By L-M fitting the 5 s sparse histograms of each pixel, the g \u00f02\u00de \u00f0x,y,0\u00de map is retrieved. Due to the sparsity of the HBT histograms, the L-M fitting expectedly leads to a noisy reconstruction of the g \u00f02\u00de \u00f0x,y,0\u00de distribution (Fig. 3c). Figure 3d shows the corresponding reconstructed image of G \u00f02\u00de \u00f0x,y\u00de (Eq. 3). The cross-section of the obtained image and corresponding fitting with the same \u03c3 value as of the original PL image are shown in Fig. 3e. Here we can see that the g \u00f02\u00de \u00f0x,y,0\u00de obtained via L-M fitting leads to a noisy, blurred image without any gain in spatial resolution, which is a direct consequence of the inaccurate retrieval of the g \u00f02\u00de \u00f0x,y,0\u00de In contrast, the CNN-based antibunching SRM ensures the expected \u221a2 gain in resolution on sparse 7 s HBT scan. Figure 3 (f,g) show g \u00f02\u00de \u00f0x,y,0\u00de distribution retrieved via using the pre-trained CNN (f) and corresponding super-resolved image (g). Here, we can see that MLbased framework ensures precise reconstruction of the g \u00f02\u00de \u00f0x,y,0\u00de map, and as a result achieves a ffiffiffi 2 p gain in the spatial resolution of the reconstructed image. Gaussian fitting of the cross-section distribution of the resolved image shows that ML assisted approach ensures a FWHM of 219 nm, which corresponds to \u03c3 CNN = \u03c3= ffiffiffi 2 p (Fig. 3h). Up until now, we have considered an acquisition time of 7 s per pixel. However, the robustness of the regression model indicates that the developed approach can be efficiently applied to more sparse datasets. Figure 4a-c shows the reconstructed images based on 5 s, 6 s, and 7 s HBT scans, respectively, and Fig. 4d compares their crosssections, which appear stable against the reduction of the acquisition time. It is worth noting that the fitting-based approach requires at least 1 min of HBT measurement per pixel for precise retrieval of the g \u00f02\u00de \u00f00\u00de values, as it has been observed during dataset collection process (Section 2). This time requirement significantly depends on the properties of the single-photon emitters, e.g. quantum purity, lifetime, and emission rate, and can be significantly longer in the case of low emission rates of the emitter. Here, the developed ML-assisted antibunching approach ensures up to 12 times speed-up compared with the fitting-based approach.",
                        "The developed ML-assisted SRM is also capable of resolving closely spaced quantum emitters (Fig. 5). Figure 5a-c shows the PL distribution, CNN-based retrieved g \u00f02\u00de \u00f0x,y,0\u00de map and the resolved image of the two NVs separated by ~600 nm distance. By comparing the original PL distribution and the resolved image, the expected ffiffiffi 2 p improvement in the spatial resolution is observed. By performing the Gaussian fitting of the cross-section (taken along the dashed line in Fig. 5a), one can retrieve the FWHM values of each of the lobs, which are equal to ~465 nm (Fig. 5d). By performing the same fitting on the resolved image, ffiffiffi 2 p narrowing of the emission features (FWHM = 330 nm) by the CNN based approach is confirmed. We also use a Monte-Carlo simulation (parameters are tabulated in Table S2) to confirm enhanced resolution of two closely spaced emitters (Figures S3 andS4), as well as three closely spaced emitters (Figure S5). See Supplementary Section 3 for further details on the simulation and its results. "
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Discussion",
            "paragraphs": [
                "The proposed ML assisted regression technique allows for a significant speed-up of quantum SRM imaging. Specifically, the performance of the CNN-assisted SRM is demonstrated on nanodiamonds that contain single NV centers as quantum emitters. In the microscopy of quantum light sources, the developed ML-assisted super-resolution framework ensures a speed-up of 12 times compared to the conventional L-M fitting-based approach for retrieving the second-order autocorrelation value at zero delays, g 2 \u00f0 \u00de 0 \u00f0 \u00de. The proposed approach can be extended to rapid measurements of higher-order autocorrelation functions, which opens up the way to the practical realization of scalable quantum super-resolution imaging systems. It is worth noting that the single-to-noise ratio of quantum imaging scanning microscopy decreases with the greater density of markers for the case of higherorder autocorrelation measurements. This makes it difficult to extend quantum imaging scanning microscopy to higher order correlations. While the approach developed in this work opens the way to speed-up the antibunching-based microscopy in general, it is an open question if the ML-assisted techniques can be used for overcoming the aforementioned issues. This interesting avenue can be a subject of future studies."
            ],
            "subsections": []
        },
        {
            "title": "Methods",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Experimental setup",
                    "paragraphs": [
                        "The sample with nanodiamonds containing NV centers was prepared by cleaning a coverslip glass substrate with solvents, treating it with ultraviolet radiation for an hour, and drying a 5 \u03bcL droplet of a sonicated nanodiamond solution (20 nm average size, Adamas Nano) on the coverslip surface. Optical characterization was performed using a custom-made scanning confocal microscope with a 100 \u03bcm pinhole based on a commercial inverted microscope body (Nikon Ti-U). To locate the emitters, objective scanning was performed using a P-561 piezo stage driven by an E-712 controller (Physik Instrumente). Immersion microscopy was performed using an oil objective with a numerical aperture (NA) of 1.49. The optical pumping in the CW experiments was administered by a continuous wave 532 nm laser (RGB Photonics). Power on the order of 1 mW (measured before entering the optical objective) was used to pump the NV centers. This excitation power is above the NV center saturation power, and was used in order to obtain sufficient photon counts in the 1 s autocorrelation histograms for the machine learning algorithm to extract g (2) (0) values. The excitation beam was reflected off a 550 nm long-pass dichroic mirror (DMLP550L, Thorlabs), and a 550 nm long-pass filter (FEL0550, Thorlabs) was used to filter out the remaining pump power. Two avalanche detectors with a 30 ps time resolution and 35% quantum efficiency at 650 nm (PDM, Micro-Photon Devices) were used for single-photon autocorrelation measurements. Time-correlated photon counting was performed by a \"start-stop\" acquisition card with a 4 ps internal jitter (SPC-150, Becker & Hickl). The total histogram span was set to 500 ns and the co-detection events were collected into 215 time bins."
                    ],
                    "subsections": []
                },
                {
                    "title": "Training dataset",
                    "paragraphs": [
                        "In order to train the regression network, autocorrelation measurements were performed on a set of 40 emitters. For each emitter, autocorrelation datasets were acquired in series of 1-second-long intervals. These \"sparse\" datasets acquired for each emitter were compounded into a \"full\" dataset, from which the g 2 \u00f0 \u00de 0 \u00f0 \u00de value was attained using the L-M fitting algorithm. Autocorrelation measurements on each emitter were performed by repeating acquisitions for one second, until accumulating about 300 co-detection events per bin in total. To extract an estimate of the autocorrelation at zero delays, the complete autocorrelation histograms were fitted according to a three-level emitter model (Eq. 3) using the Levenberg-Marquardt (L-M) method. L-M fitting is realized by using non-linear least squares to fit a function, g 2 \u00f0 \u00de \u03c4 \u00f0 \u00de, to data. The main goal of the fit is to determine parameters (a 1 ,a 2 ,t 1 ,t 2 ) that minimizes the mean absolute difference between data and the function. It is worth noting that the position of the zero-th time-bin is assumed to be known.",
                        "The training dataset at this point consisted of 9416 sparse HBT histograms. The emitters in the dataset covered a broad range of g 2 \u00f0 \u00de 0 \u00f0 \u00de values from 0.1 to 0.884, while the total number of counts of the 1 s HBT histograms was in the range of 1.2 to 61.",
                        "The 1 s HBT histograms were used for data augmentation. Specifically, we formed all the possible combinations of 1 to 10 of these 1 s histograms to obtain training data that emulated histograms with acquisition times from 1 s to 10 s. This was done via bin-wise summation of the histograms. Such data augmentation processes assumed that the emission is a process with no memory over times exceeding 1 s and allowed us to significantly extend the training dataset.  Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/ licenses/by/4.0/."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Acknowledgements",
            "paragraphs": [
                "This work is supported by the U.S. Department of Energy (DOE), Office of Science through the Quantum Science Center (QSC), a National Quantum Information Science Research Center (A.B., V.M.S., Z.M., D.S.), and the Office of Biological and Environmental Research under Award Number DE-SC0023167 (S.I.B.), DARPA/DSO Extreme Optics and Imaging (EXTREME) Program (HR00111720032) (A.V.K.), National Science Foundation award 2015025-ECCS (V.M.S., Z.M., X.X.) and Purdue's Elmore ECE Emerging Frontiers Center \"The Crossroads of Quantum and AI\" (A.B., Z.A.K., O.Y.)."
            ],
            "subsections": []
        },
        {
            "title": "Data availability",
            "paragraphs": [
                "The datasets generated and/or analyzed during the current study are available from the corresponding author on request."
            ],
            "subsections": []
        },
        {
            "title": "Code availability",
            "paragraphs": [
                "The code used for the ML-assisted measurements and simulations is available from the corresponding author on request."
            ],
            "subsections": []
        },
        {
            "title": "Author contributions",
            "paragraphs": [
                "Z.A.K., D.S., S.B., V.M.S. conceived the experiment. A.V.K., V.M.S., and A.B. supervised the project. Z.A.K. developed and wrote deep learning regression codes, performed neural network training, produced, and optimized the regression results. D.S., Z.M., S.B., and X.X. performed HBT measurements on NV centers. X.X. and P.G.C. performed atomic force microscopy measurements on nanoparticles. O.Y. performed analysis of the evolution of the CNN during training. All authors interpreted the data. Z.A.K., S.B., and A.B. wrote the manuscript."
            ],
            "subsections": []
        },
        {
            "title": "Competing interests",
            "paragraphs": [
                "The authors declare no competing interests."
            ],
            "subsections": []
        }
    ]
}