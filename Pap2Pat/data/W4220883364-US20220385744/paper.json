{
    "id": "https://semopenalex.org/work/W4220883364",
    "authors": [
        "Matej Pavlovi\u010d",
        "Marko Vukoli\u0107",
        "Chrysoula Stathakopoulou"
    ],
    "title": "State machine replication scalability made simple",
    "date": "2022-03-28",
    "abstract": "Consensus, state machine replication (SMR) and total order broadcast (TOB) protocols are notorious for being poorly scalable with the number of participating nodes. Despite the recent race to reduce overall message complexity of leader-driven SMR/TOB protocols, scalability remains poor and the throughput is typically inversely proportional to the number of nodes. We present Insanely Scalable State Machine Replication, a generic construction to turn leader-driven protocols into scalable multi-leader ones. For our scalable SMR construction we use a novel primitive called Sequenced (Total Order) Broadcast (SB) which we wrap around PBFT, HotStuff and Raft leader-driven protocols to make them scale. Our construction is general enough to accommodate most leader-driven ordering protocols (BFT or CFT) and make them scale. Our implementation improves the peak throughput of PBFT, HotStuff, and Raft by 37x, 56x, and 55x, respectively, at a scale of 128 nodes.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Considerable research effort has recently been dedicated to scaling state-machine replication (SMR) and total-order broadcast (TOB) or consensus protocols, fundamental primitives in distributed computing. By scaling, we mean maintaining high throughput and low latency despite a growing number of nodes (replicas) \ud835\udc5b. Driven by the needs of blockchain systems, particular focus lies on deterministic Byzantine faulttolerant (BFT) protocols in the eventually or partially synchronous model.",
                "In this model, the classical Dolev/Reischuk (DR) [14] lower bound requires \u03a9(\ud835\udc5b 2 ) worst case message complexity, which was a focal complexity metric of many subsequent protocols including recent HotStuff [33]. However, we claim message complexity to be a rather poor scalability metric, demonstrated by the fact that HotStuff and other leader-driven protocols scale inversely proportionally to the number of nodes, despite some of them matching the DR lower bound. This is because in leader-driven protocols, the leader has at least \ud835\udc42 (\ud835\udc5b) bits to send, even in the common case, yielding \ud835\udc5b -1 throughput scalability.",
                "A recent effort to overcome the single leader bottleneck by allowing multiple parallel leaders (Mir-BFT [31]) in the classical PBFT protocol [10] demonstrates high scalability in practice. Despite certain advantages of PBFT, e.g., being highly parallelizable and designed not to require signatures on protocol messages, among the many (existing and future) TOB solutions, there are none that fit all use cases. HotStuff [33] is the first protocol with linear message complexity both in the common case and in leader replacement, making it suitable for highly asynchronous or faulty networks. On the other hand, other protocols, e.g., Aliph/Chain [19], have optimal throughput when failures are not expected to occur often. Finally, crash fault-tolerant (CFT) protocols such as Raft [28] and Paxos [24] tolerate a larger number of (benign) failures than BFT protocols for the same number of nodes.",
                "Our work takes the Mir-BFT effort one step further, introducing Insanely Scalable SMR, hereinafter referred to as ISS, the first modular framework to make leader-driven TOB protocols scale. ISS is a classic SMR system that establishes a total order of client requests with typical liveness and safety properties, applicable to any replicated service, such as resilient databases or a blockchain ordering layer (e.g., as in Hyperledger Fabric [3]).",
                "Notably, and unlike previous efforts [31][6], ISS achieves scalability without requiring a primary node to periodically decide on the protocol configuration. ISS achieves this by introducing a novel abstraction, Sequenced Broadcast (SB), which requires each instance of an ordering protocol to terminate after delivering a finite number of messages. This allows nodes in ISS to decide on the configuration independently and deterministically, without requiring additional communication and without relying on a single primary node. This in turn allows for more flexible and fair leader selection policies. Moreover, it guarantees better resilience against an adaptive adversary that can corrupt the primary node, which changes slowly and can be known in advance with a deterministic round robin rotation [31] [6].",
                "ISS implements SMR by multiplexing multiple instances of SB which operate concurrently on a partition of the domain of client requests. We carefully select the partition to maintain safety and liveness, as well as to prevent redundant data duplication, which has been shown to be detrimental to performance [31]. This is qualitatively better than related modular efforts [20,27] which do not provide careful partitioning and load balancing and hence cannot achieve the same scalability and robustness at the same time.",
                "ISS maintains a contiguous log of (batches of) client requests at each node. Each position in the log corresponds to a unique sequence number and ISS agrees on the assignment of a unique request batch to each sequence number. Our goal is to introduce as much parallelism as possible in assigning batches to sequence numbers while avoiding request duplication, i.e., assigning the same request to more than one sequence number. To this end, ISS subdivides the log into non-overlapping segments. Each segment, representing a subset of the log's sequence numbers, corresponds to an independent instance of SB that has its own leader and executes concurrently with other SB instances. The SB abstraction, moreover, facilitates the reasoning about multiplexing the outputs of multiple instances into a single log, while staying very close to classic definitions of broadcast and thus being easily implementable by existing algorithms.",
                "To prevent the leaders of two different segments from concurrently proposing the same request, and thus wasting resources, while also preventing malicious leaders from censoring (i.e., not proposing) certain requests, we adopt and generalize the rotating bucketization of the request space introduced by Mir-BFT [31]. ISS assigns a different bucketsubset of client requests-to each segment. No bucket is assigned to more than one segment at a time and each request maps (through a hash function) to exactly one bucket. ISS periodically changes the bucket assignment, such that each bucket is guaranteed to eventually be assigned to a segment with a correct leader.",
                "To maintain the invariant of one bucket being assigned to one segment, all buckets need to be re-assigned at the same time. ISS therefore uses finite segments that it groups into epochs. An epoch is a union of multiple segments that forms a contiguous sub-sequence of the log. After all log positions within an epoch have been assigned request batches, and thus no requests are \"in-flight\", ISS advances to the next epoch, meaning that it starts processing a new set of segments forming the next portion of the log.",
                "We implement and deploy ISS on a wide area network (WAN) spanning 16 different locations spread around the world, demonstrating ISSs performance using two different BFT protocols (PBFT [10] and HotStuff [33]) and one CFT protocol (Raft [28]). On 128 nodes ISS improves the performance of the single leader counterpart protocols, PBFT, Hot-Stuff, and Raft, by 37x, 56x, and 55x, respectively.",
                "The rest of this paper is organized as follows. Section 2 presents the theoretical foundation of our work. It models the systems we study (Section 2.1), introduces the SB abstraction (Section 2.2) and describes how we multiplex SB instances with ISS (Sections 2.3 and 2.4). Sections 3 and 4 respectively describe the details of ISS and its implementation. In Section 5 we prove that multiplexing SB instances with ISS implements SMR and in Section 6 we evaluate the performance of ISS. In Section 7 we discuss related work. We conclude in Section 8."
            ],
            "subsections": []
        },
        {
            "title": "Theoretical Foundations",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "System Model",
                    "paragraphs": [
                        "We assume a set N of node processes with |N | = \ud835\udc5b. At most \ud835\udc53 of the nodes in N can fail. We further assume a set C of client processes of arbitrary size, any of which can be faulty. Each process is identified by its public key, provided by a public key infrastructure. Unless mentioned otherwise, we assume Byzantine, i.e., arbitrary, faults. Therefore, we require \ud835\udc5b \u2265 3\ud835\udc53 + 1. We further assume that nodes in N are computationally bounded and cannot subvert standard cryptographic primitives.",
                        "Processes communicate through authenticated point-topoint channels. We assume a partially synchronous network [15] such that the communication between any pair of correct processes is asynchronous before an unknown time \ud835\udc3a\ud835\udc46\ud835\udc47 , when the communication becomes synchronous.",
                        "Nodes in N implement a state machine replication (SMR) service to clients in C. To broadcast request \ud835\udc5f , a client \ud835\udc50 triggers an \u27e8SMR-CAST|\ud835\udc5f \u27e9 event. A client request is a tuple \ud835\udc5f = (\ud835\udc5c, \ud835\udc56\ud835\udc51), where \ud835\udc5c is the request payload, e.g., some operation to be executed by some application, and \ud835\udc56\ud835\udc51 a unique request identifier. The request identifier is a tuple \ud835\udc56\ud835\udc51 = (\ud835\udc61, \ud835\udc50) where \ud835\udc61 is a logical timestamp and \ud835\udc50 a client identifier, e.g., a client public key. Two client requests \ud835\udc5f = (\ud835\udc5c, \ud835\udc56\ud835\udc51), \ud835\udc5f \u2032 = (\ud835\udc5c \u2032 , \ud835\udc56\ud835\udc51 \u2032 ) are considered equal, we write \ud835\udc5f = \ud835\udc5f \u2032 and we refer to them as duplicates, if and only if \ud835\udc5c = \ud835\udc5c \u2032 \u2227 \ud835\udc56\ud835\udc51 = \ud835\udc56\ud835\udc51 \u2032 .",
                        "Nodes assign a unique sequence number \ud835\udc60\ud835\udc5b to \ud835\udc5f and eventually output an \u27e8SMR-DELIVER|\ud835\udc60\ud835\udc5b, \ud835\udc5f \u27e9 event such that the following properties hold: SMR1 Integrity: If a correct node delivers (\ud835\udc60\ud835\udc5b, \ud835\udc5f ), where \ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc50 is a correct client's identity, then client \ud835\udc50 broadcast \ud835\udc5f . SMR2 Agreement: If two correct nodes deliver, respectively, (\ud835\udc60\ud835\udc5b, \ud835\udc5f ) and (\ud835\udc60\ud835\udc5b, \ud835\udc5f \u2032 ), then \ud835\udc5f = \ud835\udc5f \u2032 . SMR3 Totality: If a correct node delivers request (\ud835\udc60\ud835\udc5b, \ud835\udc5f ), then every correct node eventually delivers (\ud835\udc60\ud835\udc5b, \ud835\udc5f ).",
                        "SMR4 Liveness: If a correct client broadcasts request \ud835\udc5f , then some correct node eventually delivers (\ud835\udc60\ud835\udc5b, \ud835\udc5f )."
                    ],
                    "subsections": []
                },
                {
                    "title": "Sequenced Broadcast (SB)",
                    "paragraphs": [
                        "Sequenced Broadcast (SB) is a variant of Byzantine total order broadcast [8] with explicit sequence numbers and an explicit set of allowed messages.",
                        "SB is instantiated with a failure detector instance as a parameter. We assume an eventually strong failure detector in an environment with Byzantine faults denoted as \ud835\udc46 (\ud835\udc4f\ud835\udc67), as defined by Malkhi and Reiter [25].",
                        "The failure detector D of the class \ud835\udc46 (\ud835\udc4f\ud835\udc67) detects quiet nodes. Intuitively, a quiet node is the equivalent to a crashed node in the BFT model, accounting for non-crash faults that are indistinguishable from crashes. For the exact definition we refer the reader to Malkhi and Reiter's work [25].",
                        "A failure detector of the \ud835\udc46 (\ud835\udc4f\ud835\udc67) class guarantees the following two properties: Strong Completeness: There is a time after which every quiet node is permanently suspected by every correct node. Eventual Weak Accuracy: There is a time after which some correct node is never suspected by any correct node.",
                        "We can now define Sequenced Broadcast as follows.",
                        "Let \ud835\udc40 be a set of messages and \ud835\udc46 \u2286 N a set of sequence numbers. Only one sender node \ud835\udf0e \u2208 N can broadcast messages (we hereon write sb-cast) by invoking \u27e8SB-CAST|\ud835\udc60\ud835\udc5b, \ud835\udc5a\u27e9 with (\ud835\udc60\ud835\udc5b, \ud835\udc5a) \u2208 \ud835\udc46 \u00d7 \ud835\udc40. \u27e8SB-DELIVER|\ud835\udc60\ud835\udc5b, \ud835\udc5a\u27e9 is triggered at a correct node \ud835\udc5d when \ud835\udc5d delivers (we hereon write sb-delivers) message \ud835\udc5a with sequence number \ud835\udc60\ud835\udc5b.",
                        "If a correct node suspects that \ud835\udf0e is quiet, all correct nodes are allowed to sb-deliver a special nil value \ud835\udc5a = \u22a5 \u2209 \ud835\udc40. If, however, \ud835\udf0e is trusted by all correct nodes, all correct nodes are guaranteed to sb-deliver non-nil messages \ud835\udc5a \u2260 \u22a5.",
                        "SB is explicitly initialized with an \u27e8SB-INIT\u27e9 event. We assume a failure detector list at each correct node which is initially empty. It is only after the invocation of \u27e8SB-INIT\u27e9 that suspecting \ud835\udf0e can lead to the \u22a5 value being delivered. An instance of \ud835\udc46\ud835\udc35(\ud835\udf0e, \ud835\udc46, \ud835\udc40, \ud835\udc37) has the following properties: SB1 Integrity: If a correct node sb-delivers (\ud835\udc60\ud835\udc5b, \ud835\udc5a) with \ud835\udc5a \u2260 \u22a5 and \ud835\udf0e is correct then \ud835\udf0e sb-cast (\ud835\udc60\ud835\udc5b, \ud835\udc5a). SB2 Agreement: If two correct nodes sb-deliver, respectively, (\ud835\udc60\ud835\udc5b, \ud835\udc5a) and (\ud835\udc60\ud835\udc5b, \ud835\udc5a \u2032 ), then \ud835\udc5a = \ud835\udc5a \u2032 . SB3 Termination: If \ud835\udc5d is correct, then \ud835\udc5d eventually sb-delivers a message for every sequence number in \ud835\udc46, i.e., \u2200\ud835\udc60\ud835\udc5b \u2208 \ud835\udc46 : \u2203\ud835\udc5a \u2208 \ud835\udc40 \u222a {\u22a5} such that \ud835\udc5d sb-delivers (\ud835\udc60\ud835\udc5b, \ud835\udc5a). SB4 Eventual Progress: If some correct node sb-delivers (\ud835\udc60\ud835\udc5b, \u22a5) for some \ud835\udc60\ud835\udc5b \u2208 \ud835\udc46, then some correct node suspected \ud835\udf0e after having invoked \u27e8SB-INIT\u27e9.",
                        "The key difference from TOB is that SB is invoked for an explicit set of sequence numbers \ud835\udc46 and messages \ud835\udc40. Moreover, SB is invoked with a \ud835\udc46 (\ud835\udc4f\ud835\udc67) failure detector and correct nodes deliver messages from set \ud835\udc40 and the special \u22a5 value such that SB terminates for all sequence numbers. The latter is guaranteed by the \u22a5 value and \ud835\udc46 (\ud835\udc4f\ud835\udc67) completeness; if \ud835\udf0e is quiet it will eventually be suspected by all correct nodes.",
                        "Our technical report [32] shows that SB is implementable with consensus, Byzantine reliable broadcast (BRB), and \ud835\udc46 (\ud835\udc4f\ud835\udc67). On a high level, the dedicated sender \ud835\udf0e reliably broadcasts a message for each sequence number in \ud835\udc46 and all correct nodes run consensus for each sequence number in \ud835\udc46 to decide if they can deliver a message broadcast by \ud835\udf0e or if \ud835\udf0e is quiet. In the first case they sb-deliver the message and in the second case they sb-deliver \u22a5. Since both BRB and \ud835\udc46 (\ud835\udc4f\ud835\udc67) are implementable with consensus, SB can also be implemented with consensus."
                    ],
                    "subsections": []
                },
                {
                    "title": "Multiplexing Instances of SB with ISS",
                    "paragraphs": [
                        "ISS multiplexes instances of SB to implement SMR. Each node maintains a log of ordered messages which correspond to batches of client requests. Each position in the log corresponds to a sequence number signifying the offset from the start of the log. The log is partitioned in subsets of sequence numbers called segments. Each segment corresponds to one instance of SB. Nodes obtain requests from clients and, after mapping them to a log position using an instance of SB, deliver them together with the assigned sequence number.",
                        "ISS proceeds in epochs identified by monotonically increasing integer epoch numbers. Each epoch \ud835\udc52 is associated with a set of segments. The union of those segments forms a set \ud835\udc46\ud835\udc5b(\ud835\udc52) of consecutive sequence numbers. Epoch 0 (the first epoch) starts with sequence number 0. The mapping of sequence numbers to epochs is a function known to all nodes with the only requirements being that it is monotonically increasing and that there are no gaps between epochs. More formally, \ud835\udc5a\ud835\udc4e\ud835\udc65 (\ud835\udc46\ud835\udc5b(\ud835\udc52)) + 1 = \ud835\udc5a\ud835\udc56\ud835\udc5b(\ud835\udc46\ud835\udc5b(\ud835\udc52 + 1)). Epoch length can be arbitrary, as long as it is finite. For simplicity, we use a fixed, constant epoch length.  Epochs are processed sequentially, i.e., ISS first agrees on the assignment of request batches to all sequence numbers in \ud835\udc46\ud835\udc5b(\ud835\udc52) before starting to agree on the assignment of request batches to sequence numbers in \ud835\udc46\ud835\udc5b(\ud835\udc52 + 1).",
                        "Within an epoch, however, ISS processes segments in parallel. Multiple leaders, selected according to a leader selection policy, concurrently propose batches of requests for different sequence numbers in \ud835\udc46\ud835\udc5b(\ud835\udc52). To this end, ISS assigns a different leader node to each segment in epoch \ud835\udc52. We refer to the set of all nodes acting as leaders in an epoch as the leaderset of the epoch. The numbers of leaders and segments in each epoch always match. We denote by \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56) the subset of \ud835\udc46\ud835\udc5b(\ud835\udc52) for which node \ud835\udc56 is the leader. This means that node \ud835\udc56 is responsible for proposing request batches to sequence numbers in \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56). No node other than \ud835\udc56 can propose batches for sequence numbers in \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56). Let \ud835\udc3f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc60 (\ud835\udc52) be nodes that are leaders in epoch \ud835\udc52. We associate sequence numbers with segments in a round-robin way, namely, for",
                        "In principle, any assignment of sequence numbers to segments is possible and leads to a correct algorithm. We choose the round-robin assignment because it uniformly distributes sequence numbers among instances. Therefore, in a fault-free execution, it is the least likely to create \"gaps\" in the log, which minimizes the end-to-end request latency.",
                        "In order not to waste resources on duplicate requests, we require that a request cannot be part of two batches assigned to two different sequence numbers. We enforce this at three levels: (1) within a segment, (2) across segments in the same epoch, and (3) across epochs.",
                        "Within a segment, we rely on the fact that a correct leader will propose (and a correct node, as follower, will accept) only batches with disjoint sets of requests for each sequence number within a segment. Across segments, we partition the set of all possible requests into buckets using a hash function and enforce that only requests from different buckets can be used for different segments within an epoch. We denote by B the set of all possible buckets. We assign a subset of B to each segment, such that each bucket is assigned to exactly one segment in each epoch. We denote by \ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56) \u2286 B the set of buckets assigned to leader \ud835\udc56 in epoch \ud835\udc52. For simplicity, we say that we assign a bucket to a leader \ud835\udc56 when assigning a bucket to a segment for which \ud835\udc56 is the leader. Across epochs, ISS prevents duplication by only allowing a node to propose a request batch in a new epoch once it has added all batches from the previous epoch to the log. If a request has been delivered in a batch in the previous epoch, a correct leader will not propose it again (see also Section 3.3). Also, a correct node, as follower, will not accept a proposal which includes a previously delivered request.",
                        "In summary, a segment of epoch \ud835\udc52 with leader \ud835\udc56 is defined by the tuple (\ud835\udc52, \ud835\udc56, \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56), \ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56)). For a set of buckets \ud835\udc35 \u2286 B , we denote with \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc52\ud835\udc60 (\ud835\udc35) the set of all possible batches consisting of valid (we define request validity precisely later in Section 3.7) requests that map to some bucket in \ud835\udc35. For each segment (\ud835\udc52, \ud835\udc56, \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56), \ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56)), we use an instance \ud835\udc46\ud835\udc35(\ud835\udc56, \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc52\ud835\udc60 (\ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56)), \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56)), \ud835\udc37) of Sequenced Broadcast where \ud835\udc37 is a failure detector. We say that leader \ud835\udc56 proposes a batch \ud835\udc4f \u2208 \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc52\ud835\udc60 (\ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56)) for sequence number \ud835\udc60\ud835\udc5b \u2208 \ud835\udc46\ud835\udc52\ud835\udc54(\ud835\udc52, \ud835\udc56) if \ud835\udc56 sb-casts \ud835\udc4f with a sequence number \ud835\udc60\ud835\udc5b at the corresponding instance of SB. A batch \ud835\udc4f commits with \ud835\udc60\ud835\udc5b (and is added to the log at the corresponding position) at node \ud835\udc5b when, for the corresponding instance of SB, \ud835\udc5b sb-delivers \ud835\udc4f with \ud835\udc60\ud835\udc5b.",
                        "During epoch \ud835\udc52, all nodes that are leaders in \ud835\udc52 simultaneously propose batches for sequence numbers in their corresponding segments. ISS multiplexes all segments into the single common log as shown in Figure 1. Each node thus executes |\ud835\udc3f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc60 (\ud835\udc52)| SB instances simultaneously, while being a leader for at most one of them.",
                        "Epoch \ud835\udc52 ends and epoch (\ud835\udc52 + 1) starts when all sequence numbers in \ud835\udc46\ud835\udc5b(\ud835\udc52) have been committed. Nodes keep the old instances active until all corresponding sequence numbers become part of a stable checkpoint. This is necessary for ensuring totality (even for slow nodes which might not have actively taken part in the agreement)."
                    ],
                    "subsections": []
                },
                {
                    "title": "Assigning Buckets to Segments",
                    "paragraphs": [
                        "ISS partitions the request hash space into buckets which it assigns to leaders/segments and changes this bucket assignment at epoch transitions. At any point in time, a leader can assign sequence numbers only to requests from its assigned buckets. This approach was first used in Mir-BFT [31] to counter request duplication and censoring attacks.",
                        "During an epoch, the assignment of buckets to leaders is fixed. To ensure liveness, each bucket must repeatedly be assigned to a correct leader. To this end, ISS re-assigns the buckets on every epoch transition as follows. For epoch \ud835\udc52, we start by assigning an initial set of buckets to each node (leader or not) in a round-robin way. Let \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56) \u2286 B be the set of buckets initially assigned to each node \ud835\udc56, 0 \u2264 \ud835\udc56 < \ud835\udc5b in epoch \ud835\udc52. We consider the buckets in B to be numbered, with each bucket having an integer bucket number \ud835\udc4f \u2208 {0, . . . , |B| -1}. In the following we refer to buckets using \ud835\udc4f. \ud835\udc56\ud835\udc5b\ud835\udc56\ud835\udc61\ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc56) = {\ud835\udc4f \u2208 B | (\ud835\udc4f + \ud835\udc52) \u2261 \ud835\udc56 mod \ud835\udc5b} However, not all nodes belong to \ud835\udc3f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc60 (\ud835\udc52). Let \ud835\udc52\ud835\udc65\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52) be the set of buckets initially assigned to non-leaders.",
                        "We must re-distribute those extra buckets to the leaders of epoch \ud835\udc52. We do this in a round robin way as well. Let \ud835\udc59 (\ud835\udc52, \ud835\udc58), 0 \u2264 \ud835\udc58 < |\ud835\udc3f\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc52\ud835\udc5f\ud835\udc60 (\ud835\udc52)| be the \ud835\udc58-th leader (in lexicographic order) in epoch \ud835\udc52. The \ud835\udc35\ud835\udc62\ud835\udc50\ud835\udc58\ud835\udc52\ud835\udc61\ud835\udc60 (\ud835\udc52, \ud835\udc59 (\ud835\udc52, \ud835\udc58)) of the \ud835\udc58-th leader in \ud835\udc52 are thus defined as follows.",
                        "An example bucket assignment is illustrated in Figure 2.",
                        "With this approach, all buckets are assigned to leaders and every node is eventually assigned every bucket at least through the initial bucket assignment. ISS ensures liveness as long as, in an infinite execution, there is a correct node that (1) eventually stops being suspected forever by every correct node, and ( 2) is assigned each bucket infinitely often. ( 1) is satisfied by the properties of the eventually strong failure detector. ( 2) is satisfied by the bucket re-assignment and the leader selection policy described in Section 3.4.",
                        "Figure 3 shows how ISS multiplexes SB instances to build a totally ordered log. "
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "ISS Algorithm Details",
            "paragraphs": [
                "In this section we present the ISS algorithm in detail. The main high-level algorithm that produces a totally ordered log is described in Algorithm 1. For better readability, certain auxiliary functions and the functions related to epoch initialization are presented separately in Algorithms 2 and 3, respectively. The notation \u27e8\ud835\udc56, \ud835\udc52 |\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc60\u27e9 corresponds to an event \ud835\udc52 of instance \ud835\udc56 with arguments \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc60.",
                "As described in the previous sections, ISS proceeds in epochs, each epoch multiplexing multiple segments into a final totally ordered log. We start with epoch number 0 (line 16) and an empty log (line 21). All buckets are initially empty (line 25). Whenever a client submits a new request (line 30), ISS adds the request to the corresponding bucket (line 32).",
                "We assume access across all epochs to a module \ud835\udc37 that implements an eventually strong failure detector (Section 2.2).",
                "Algorithm 1 Main ISS algorithm for node \ud835\udc5d "
            ],
            "subsections": [
                {
                    "title": "Ordering Request Batches",
                    "paragraphs": [
                        "ISS orders requests in batches, a common technique which allows requests to be handled in parallel, amortizes the processing cost of protocol messages, and, thereby, improves throughput. During an epoch, every node \ud835\udc59 that is the leader of a segment \ud835\udc60 proposes request batches for sequence numbers assigned to \ud835\udc60 (line 37). \ud835\udc59 does so by sb-casting the batches using the instance of SB associated with \ud835\udc60. Every node then inserts the sb-delivered (sequence number, batch) pairs at the corresponding positions in its copy of the log (line 41). We say the node commits the batch with the corresponding sequence number since, once inserted to the log, the assignment of a batch to a sequence number is final. Proposing Batches. Each node maintains local data structures of buckets queues, which store the received and not yet proposed or delivered requests corresponding to the respective bucket. To propose a request batch for sequence number \ud835\udc60\ud835\udc5b, \ud835\udc59 first constructs the batch using requests in the bucket queues corresponding to the buckets assigned to \ud835\udc60. To implement efficient request batching while preserving low latency, \ud835\udc59 waits until at least one of the following conditions is fulfilled: (1) the bucket queues assigned to \ud835\udc60 contain enough requests (more than a predefined \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc52) (line 67), or (2) a predefined time elapses since the last proposal (line 68). Under low load, this condition sets an upper bound on the pending1 latency of requests waiting to be proposed, even if the batch is filling slowly.",
                        "\ud835\udc59 then constructs a \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e using up to \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e\ud835\udc46\ud835\udc56\ud835\udc67\ud835\udc52 requests (line 69), removes those requests from their bucket queues and proposes the batch by invoking SB-CAST(\ud835\udc60\ud835\udc5b, \ud835\udc4f\ud835\udc4e\ud835\udc61\ud835\udc50\u210e) on the SB instance associated with \ud835\udc60 (line 70).",
                        "Every leader also keeps track of the values it proposed for each sequence number (line 71). This, as we explain later, is important in the case of asynchrony.",
                        "Assembling the Final Log. Whenever any instance of SB (belonging to any segment) delivers a value associated with a sequence number (line 40) at some node \ud835\udc5b, \ud835\udc5b inserts the delivered value at position \ud835\udc60\ud835\udc5b of the log (line 41). If a request batch \u2260 \u22a5 has been delivered (line 42), \ud835\udc5b removes the contained requests from their corresponding bucket queues (line 44) to avoid proposing them again in a later epoch.",
                        "Note that bucket queues are local data structures at each node, and thus each node manages its bucket queues locally. Nodes add all requests they obtain from clients to their local bucket queues, but only propose batches constructed from the queues corresponding to the buckets assigned to their segments. Consequently, a node \ud835\udc5b delivers batches containing requests mapping to other buckets than those \ud835\udc5b uses for proposing. Therefore, to avoid request duplication across epochs, each node must remove all delivered requests from its local bucket queues.",
                        "If the special value \u22a5 has been delivered by SB and, at the same time, \ud835\udc5b itself had been the leader proposing a batch for \ud835\udc60\ud835\udc5b (line 46), \ud835\udc5b \"resurrects\" all requests in the batch it had proposed (line 47) by returning them to their corresponding bucket queues (line 92). This scenario can appear in the case of asynchrony / partitions, where a correct leader is suspected as faulty after having proposed a batch. Such a leader must return the unsuccessfully proposed requests in their bucket queues and, if batches with those requests are not committed by other nodes in the meantime, retry proposing them in a later epoch where it is again leader of a segment with those buckets.",
                        "A node considers the ordering of a request finished when it is part of a committed batch with an assigned sequence number \ud835\udc60\ud835\udc5b and the log contains an entry for each sequence number \ud835\udc60\ud835\udc5b \u2032 \u2264 \ud835\udc60\ud835\udc5b.",
                        "Each request is delivered with a unique sequence number \ud835\udc60\ud835\udc5b \ud835\udc5f denoting the total order of the request. \ud835\udc60\ud835\udc5b \ud835\udc5f is derived from the sequence number of the batch in which the request is delivered and the position of the request in the batch. Let S \ud835\udc60\ud835\udc5b be the number of requests in a batch delivered with sequence number \ud835\udc60\ud835\udc5b and let \ud835\udc5f be the \ud835\udc58 \ud835\udc61\u210e request in this batch. For each such request \ud835\udc5f , ISS outputs \u27e8SMR-DELIVER|\ud835\udc60\ud835\udc5b \ud835\udc5f , \ud835\udc5f \u27e9 where:"
                    ],
                    "subsections": []
                },
                {
                    "title": "Advancing Epochs",
                    "paragraphs": [
                        "ISS advances from epoch \ud835\udc52 to epoch \ud835\udc52 + 1 when the log contains an entry for each sequence number in \ud835\udc46\ud835\udc5b(\ud835\udc52) (line 50). This will eventually happen for each epoch at each correct node due to SB Termination. Only then does the node start processing messages related to epoch \ud835\udc52+1 and starts proposing batches for sequence numbers in \ud835\udc46\ud835\udc5b(\ud835\udc52 + 1) (lines 51 and 52).",
                        "Requiring a node to have committed all batches in epoch \ud835\udc52 before proposing batches for \ud835\udc52 +1 prevents request duplication across epochs. When a node transitions from \ud835\udc52 to \ud835\udc52 + 1, no requests are \"in flight\"-each request has either already been committed in \ud835\udc52 or has not yet been proposed in \ud835\udc52 + 1."
                    ],
                    "subsections": []
                },
                {
                    "title": "Selecting Epoch Leaders",
                    "paragraphs": [
                        "In order to guarantee that each request \ud835\udc5f submitted by a correct client is ordered (liveness), we must ensure that, eventually, there will be a segment in which \ud835\udc5f is committed. As implied by the specification of SB, this can only be guaranteed if a correct leader \ud835\udc5d proposes a batch containing \ud835\udc5f and the failure detector does not suspect \ud835\udc5d until \ud835\udc5f is committed. The choice of epoch leaders is thus crucial.",
                        "ISS selects leaders according to a leader selection policy, a function known to all nodes that, at the end of each epoch \ud835\udc52, determines the set of leaders for epoch (\ud835\udc52 + 1).",
                        "In order to guarantee liveness of the system, the leader selection policy must ensure, for each bucket \ud835\udc4f, that, in an infinite execution, \ud835\udc4f will be assigned infinitely many times to a segment with a correct leader that is not suspected by the failure detector. Weak eventual accuracy (see Section 2.2) guarantees that there exists such a leader. Different leader selection policies pose different trade-offs with respect to performance. For simplicity, we adopt the policy of BFT-Mencius [27] because it trivially ensures liveness by always keeping enough correct nodes in the leaderset."
                    ],
                    "subsections": []
                },
                {
                    "title": "Checkpointing and State Transfer",
                    "paragraphs": [
                        "ISS implements a simple checkpointing protocol. Every node \ud835\udc56, in each epoch \ud835\udc52, when the log contains an entry for each sequence number in \ud835\udc46\ud835\udc5b(\ud835\udc52), broadcasts a signed message \u27e8CHECKPOINT, \ud835\udc5a\ud835\udc4e\ud835\udc65 (\ud835\udc46\ud835\udc5b(\ud835\udc52)), \ud835\udf07 (\ud835\udc52), \ud835\udf0e \ud835\udc56 \u27e9, where \ud835\udf07 (\ud835\udc52) is the Merkle tree root of the digests of all the batches in the log with sequence numbers in \ud835\udc46\ud835\udc5b(\ud835\udc52). Upon acquiring a strong quorum of 2\ud835\udc53 + 1 matching CHECKPOINT messages with a valid signature against the sender node's public key, node \ud835\udc56 creates a stable checkpoint \u27e8STABLE-CHECKPOINT, \ud835\udc5a\ud835\udc4e\ud835\udc65 (\ud835\udc46\ud835\udc5b(\ud835\udc52)), \ud835\udf0b (\ud835\udc52)\u27e9, where \ud835\udf0b (\ud835\udc52) is the set of 2\ud835\udc53 + 1 signatures on the CHECKPOINT messages. At this point, \ud835\udc56 can garbage collect all segments of epoch \ud835\udc52.",
                        "When a node \ud835\udc56 has fallen behind, for example when \ud835\udc56 starts receiving messages for a future epoch, \ud835\udc56 performs a state transfer, i.e., it fetches the missing log entries along with their corresponding stable checkpoints which prove the integrity of the data.",
                        "ISS checkpointing is orthogonal to any checkpointing and state transfer mechanism pertaining to the SB implementation because SB instances must terminate independently."
                    ],
                    "subsections": []
                },
                {
                    "title": "Membership Reconfiguration",
                    "paragraphs": [
                        "A detailed membership reconfiguration protocol is outside the scope of this paper. However, we outline a solution. Thanks to SB 3 (Termination), all correct nodes eventually deliver a value for each sequence number of an epoch. Moreover, thanks to SMR 2 (Agreement), all correct nodes assemble the same log at the end of the epoch. Therefore, the log at the end of the epoch can be used to deterministically make decisions for the next epoch, including decisions about nodes and clients joining/leaving the set of system processes. Such a decision can be based, for example, on a flagged reconfiguration request proposed by a manager process [30] which becomes part of the log."
                    ],
                    "subsections": []
                },
                {
                    "title": "Request Handling",
                    "paragraphs": [
                        "A request \ud835\udc5f = (\ud835\udc5c, \ud835\udc56\ud835\udc51) with payload \ud835\udc5d and identifier \ud835\udc56\ud835\udc51 = (\ud835\udc61, \ud835\udc50) sent from a client to a node is wrapped in a signed message. Our implementation represents the client identifier \ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc50 with an integer associated with the client's \ud835\udc50 public key. The signature is calculated over the request identity \ud835\udc5f .\ud835\udc56\ud835\udc51 and payload \ud835\udc5f .\ud835\udc5c to guarantee integrity and authenticity.",
                        "Similarly to Mir-BFT [31], clients can submit multiple requests in parallel within a client watermark window, i.e., a contiguous set for the per-client request sequence number \ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc61. ISS advances all clients' watermark widows at the end of each epoch.",
                        "Each correct node, upon receiving a valid request, adds the request, based on its identifier, to the corresponding bucket queue. A request is considered valid if: (1) it has a valid signature (2) the public key corresponds to a client in the client set \ud835\udc36 of the system, and (3) is within the client watermarks. Bucket queues are idempotent, i.e., each correct node adds a request to the corresponding bucket queue exactly once. Moreover, the bucket queue implementation maintains a FIFO property to guarantee liveness with the oldest request always being proposed first.",
                        "Requests are uniformly distributed to buckets using modulo as a hash function. With |B| denoting the total number of buckets and || denoting concatenation, each request \ud835\udc5f maps to a bucket \ud835\udc4f = \ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc50 ||\ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc61 mod |B|.",
                        "We exclude the payload of the request from the bucket mapping function to prevent malicious clients from biasing the uniform distribution. In a permissioned system the client cannot assume different identities and may only bias the outcome of the hash function by choosing the request sequence number. However, we limit the available sequence numbers for each client, and, therefore, their ability to bias the distribution, the client watermarking mechanism. Request execution. ISS is oblivious to the payload of the requests for general applicability. Execution is not part of ISS; however, it can be coupled with any application that requires a total order of requests. Moreover, request execution against a state machine is straight-forward. A request, as part of a batch, is considered part of the log (and can be, therefore, executed) once all previous batches are added to the log (see Section 3.2). Therefore, a request can be executed as soon as it is delivered by ISS. This does not require the epoch, in which the request is added to the log, to finish."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "ISS Implementation",
            "paragraphs": [
                "We implement ISS in Go, using gRPC for communication with TLS on nodes with two network interfaces: one for client-to-node and one for node-to-node communication.",
                "Our implementation is highly concurrent: multiple threads are handling incoming client requests, verifying request signatures, sending / receiving messages, and executing various sub-protocols such as checkpointing and fetching missing protocol state. Each SB instance also executes in its own thread. A separate thread orchestrates all of the above.",
                "In the rest of this section, we discuss our implementation of the ordering subprotocols (4.1), the interaction between ISS and its clients (4.2), and crucial technical aspects for achieving robustness and high performance (4.3)."
            ],
            "subsections": [
                {
                    "title": "SB Implementation",
                    "paragraphs": [
                        "In this section we discuss (1) how we implement SB with different leader-driven consensus protocols and (2) adaptations in the leader-driven protocols critical for ISS performance.",
                        "All protocol implementations adhere to the following common design principles: 1.We initialize the protocol such that the the first protocol leader is the segment leader (dedicated SB sender) and all other nodes of the system participate as followers. 2. After a leader change, any new leader (including the segment leader if it becomes leader again), only proposes \u22a5 values for any sequence number not initially proposed by the segment leader. 23. A follower accepts a proposal only if (a) all requests in the batch are valid according to Section 3.7, (b) no request in the batch has previously been proposed in the same epoch or committed in a previous epoch (c) all requests belong to the buckets of the segment, and (d) either the segment leader sb-casts the proposal, or the proposal is \u22a5.",
                        "4.1.1 PBFT. We follow the PBFT protocol as described by Castro and Liskov [10], with a few adaptations.",
                        "Our implementation need not deal with timeouts at the granularity of single requests, as PBFT does. To prevent censoring attacks (and thus ensure liveness), a PBFT replica initiates a view change if any request has not been committed for too long. Since ISS prevents censoring attacks by bucket re-assignment, it is sufficient for us to make sure to commit some batch before a timeout fires and reset this timer when committing any batch. In the absence of incoming requests, the primary periodically proposes an empty batch to prevent a view change. Moreover, for simplicity, we implement viewchange with signatures according to Castro and Liskov [9]. 4.1.2 HotStuff. We implement chained HotStuff according to Yin et al. [33] with BLS [7] threshold signatures using DEDIS library for Go [1].",
                        "In our implementation, each batch corresponds to a Hot-Stuff command, and each segment sequence number corresponds to a HotStuff view. Each segment is implemented as a new HotStuff instance with a new root certificate \ud835\udc44\ud835\udc36 0 . To ensure that all sequence numbers can be delivered, i.e., to ensure that we can always \"flush\" the pipeline of chained HotStuff, we extend the segment with 3 dummy sequence numbers corresponding to dummy empty batches which are not added to the ISS log. Figure 4 demonstrates an example of a segment with 3 sequence numbers.",
                        "4.1.3 Raft. Briefly, in Raft, nodes set a random timer within a configurable range, which they reset every time they receive a message from the elected leader. If the timer fires, the node advances to a new term (similar to PBFT view) and enters an election phase as a candidate leader. An elected leader periodically sends append-entry requests for new values, possibly also empty, as a heartbeat. The leader collects responses according to which it might resend to the followers any value they declare as missing. We implement Raft according to [28] with minor adaptations. We fix the first leader to be the leader of the segment, skipping the election phase. Until the end of the segment, the leader periodically sends append-entry requests containing batches. The leader continues to send empty append-entry requests until the end of the epoch to guarantee that enough nodes have added all the batches of the segment to their log."
                    ],
                    "subsections": []
                },
                {
                    "title": "Interaction with Clients",
                    "paragraphs": [
                        "Clients submit requests to ISS by sending signed request messages to nodes. When a node delivers a request \ud835\udc5f as described in Section 3.2, it sends a response message to the client that submitted \ud835\udc5f . When the client obtains a quorum of responses, it considers the request delivered.",
                        "To guarantee Liveness (Section 2.1, SMR4), a client must ensure that at least one correct leader eventually receives the request. A trivial solution is to send the request to all nodes. However, ISS implements an optimistic a leader detection mechanism to help the clients find the correct leader for each request faster and to better load balance request processing among the nodes. At any point in time, the bucket to which a request belongs is assigned to a single segment. Thus, the client only needs to send its request to the node currently serving as a leader for the corresponding segment.",
                        "ISS keeps the clients updated about the assignment of buckets to leaders. At each epoch transition, all nodes send a message with the assignment for the next epoch to all clients. A client accepts such a message once it receives it from a quorum of nodes. The client submits all subsequent requests for this epoch to the appropriate node. Moreover, it resubmits all requests submitted in the past that have not yet been delivered. This guarantees that all correct nodes will eventually receive the request, which ensures liveness.",
                        "To make sure that, in most cases, a leader already has a request when it is that leader's turn to propose it, the client sends its request to two additional nodes that it projects to be assigned the corresponding bucket in the next two epochs.",
                        "4.3 Important Technical Aspects 4.3.1 Rate-limiting Proposals in PBFT. PBFT's ability to send proposals in parallel is instrumental for achieving high throughput. However, as soon as a load spike or a temporary glitch in network connectivity occurs (as it regularly does on the used cloud platform), the leader can end up trying to send too many batches in parallel. Due to limited aggregate bandwidth, all those batches will take longer to transfer, triggering view change timeouts at the replicas.",
                        "We address this issue by setting a hard limit on the rate of sending batches \"on the wire\", allowing (the most part of) a batch to be transmitted before the transfer of the next batch starts. This measure limits peak throughput but is effective at protecting against unnecessary view changes."
                    ],
                    "subsections": [
                        {
                            "title": "Concurrency Handling.",
                            "paragraphs": [
                                "A naive approach to handling requests, where each client connection is served by a thread that, in a loop, receives a request, locks the corresponding bucket queue, adds the request, and unlocks the bucket queue, is detrimental to performance. We attribute this to cache contention on the bucket queue locks.",
                                "Still, access to a bucket does have to be synchronized, as adding (on reception) and removing (on commit) requests must happen atomically. At the same time, an efficient lockfree implementation of a non-trivial data structure such as a bucket queue could be a research topic on its own.",
                                "We thus dedicate a constant, limited number of threads (as many as there are physical CPU cores) to only adding requests to bucket queues, such that each bucket queue is only accessed by one thread, removing most of the contention. The network-handling threads pass the received requests to the corresponding bucket-adding threads using a lock-free data structure optimized for this purpose (a Go channel). 4.3.3 Deployment, Profiling, and Analysis. ISS comes with tools for automating the deployment of hundreds of experiments across hundreds of geo-distributed nodes on the cloud and for analyzing their outputs. They profile (using pprof) the execution at each node, pinpointing lines of code that cause stalling or high CPU load. For example, the abovementioned cache contention was pointed to by the profiler. They also plot various metrics over time, such as the size of proposed batches, commit rate, or CPU load. Automatic exploration of the multi-dimensional parameter space proved essential for understanding the inner workings of the system.",
                                "Using hundreds of cloud machines with hourly billing also incurs significant cost. Automatically commissioning cloud machines only for the time strictly necessary to run our experiments and releasing those resources as soon as possible most likely saved thousands of dollars."
                            ],
                            "subsections": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "Correctness",
            "paragraphs": [
                "We now prove that multiplexing SB instances with ISS implements an SMR service as defined in Section 2.1.",
                "SMR1 Integrity: If a correct node delivers (\ud835\udc60\ud835\udc5b, \ud835\udc5f ), where \ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc50 is a correct client's identity, then client \ud835\udc50 broadcast \ud835\udc5f .",
                "Proof. A correct node only delivers request \ud835\udc5f if it is inserted in the log as part of a committed batch \ud835\udc4f (line 55). In turn, \ud835\udc4f is added in the log only upon an event \u27e8\ud835\udc60\ud835\udc4f, SB-DELIVER|\ud835\udc60\ud835\udc5b \ud835\udc4f , \ud835\udc4f\u27e9, where \ud835\udc60\ud835\udc4f is a \ud835\udc46\ud835\udc35(\ud835\udf0e, \ud835\udc40, \ud835\udc46, \ud835\udc37) instance (line 40) and \ud835\udc40 the set of all possible valid batches in the buckets of the segment. A correct node only invokes \u27e8\ud835\udc60\ud835\udc4f, SB-DELIVER|\ud835\udc60\ud835\udc5b \ud835\udc4f , \ud835\udc4f\u27e9 with a batch \ud835\udc4f in the set of valid batches \ud835\udc40. A validity condition is that every request of the batch has a valid signature (see Section 3.7). Since \ud835\udc5f .\ud835\udc56\ud835\udc51.\ud835\udc50 is the only process able to produce a valid signature, \ud835\udc50 must have broadcast \ud835\udc5f . \u25a1 SMR2 Agreement: If two correct nodes deliver, respectively, (\ud835\udc60\ud835\udc5b, \ud835\udc5f ) and (\ud835\udc60\ud835\udc5b, \ud835\udc5f \u2032 ), then \ud835\udc5f = \ud835\udc5f \u2032 .",
                "Proof. Let \ud835\udc5f be in batch \ud835\udc4f committed with \ud835\udc60\ud835\udc5b \ud835\udc4f and \ud835\udc5f \u2032 in a batch \ud835\udc4f \u2032 committed with \ud835\udc60\ud835\udc5b \ud835\udc4f \u2032 . For \ud835\udc5f and \ud835\udc5f \u2032 to have the same sequence number, by Equation ( 1) and by the same log established by SB2 (Agreement), \ud835\udc60\ud835\udc5b \ud835\udc4f = \ud835\udc60\ud835\udc5b \u2032 \ud835\udc4f . Since \ud835\udc4f and \ud835\udc4f \u2032 are delivered with the same sequence number, they belong to the same segment \ud835\udc46 and, thus, also in the same set \ud835\udc40 for which an instance \ud835\udc46\ud835\udc35(\ud835\udf0e, \ud835\udc40, \ud835\udc46, \ud835\udc37) was initialized. Thus, by SB2, \ud835\udc4f = \ud835\udc4f \u2032 , and by Equation ( 1 Proof. Let us assume by contradiction that \ud835\udc5f is never delivered by any correct node. This implies that every correct node puts \ud835\udc5f in their bucket queue (by the correct client re-transmitting \ud835\udc5f forever, see Section 3.7). Eventually, after some time \ud835\udc61, by Lemma 5.2 there will be at least one correct, unsuspected node \ud835\udc56 in the leaderset forever.",
                "Let \ud835\udc45(\ud835\udc5f ) be the set of all requests received by \ud835\udc56 before receiving \ud835\udc5f . Let \ud835\udc5f map to a bucket b. If \ud835\udc45(\ud835\udc5f ) = 0, i.e., \ud835\udc5f is the oldest request of \ud835\udc56, either node \ud835\udc56 delivers \ud835\udc5f by time \ud835\udc61 or \ud835\udc5f remains in \ud835\udc56's corresponding bucket queue. By bucket reassignment, \ud835\udc56 will invoke SB-CAST infinitely many times with batches (messages) containing requests from bucket b. Therefore, eventually, \ud835\udc56 sb-casts a batch \ud835\udc4f which contains \ud835\udc5f and by Lemma 5.3 \ud835\udc56 delivers \ud835\udc4f. Let \ud835\udc60\ud835\udc5b be the sequence number with which \ud835\udc4f is sb-delivered. By SB3 (Termination), all correct nodes sb-deliver and add in their log all sequence numbers in the segment of \ud835\udc60\ud835\udc5b. Therefore, by the ISS algorithm (line 55), they deliver all requests in \ud835\udc4f, including \ud835\udc5f .",
                "We can prove by induction on the size of \ud835\udc45(\ud835\udc5f ) that \ud835\udc5f is delivered by some correct node. A contradiction to \ud835\udc5f not being delivered. \u25a1"
            ],
            "subsections": []
        },
        {
            "title": "Evaluation",
            "paragraphs": [
                "Our implementation is modular, allowing easy switching between different protocols implementing SB. We use 3 wellknown protocols for ordering requests: PBFT [10] (BFT), HotStuff [33] (BFT), and Raft [28] (CFT). We evaluate the impact ISS has on these protocols by comparing its performance to their respective original single-leader versions. In addition, we compare ISS to Mir-BFT [31] which also has multiple leaders. We do not compare, however, to other multileader protocols that do not prevent request duplication (e.g., Hashgraph [23], Red Belly [13], RCC [20], OMADA [16], BFT-Mencius [27]). The codebase of these protocols is unavailable or unmaintained. Moreover, Mir-BFT evaluation demonstrates that the performance of this family of protocols deteriorates as the number of nodes increases in the presence of duplicate requests. For the same reason, we also do not compare to trivially running multiple instances of the single leader protocols."
            ],
            "subsections": [
                {
                    "title": "Experimental Setup",
                    "paragraphs": [
                        "We perform our evaluation in a WAN which spans 16 datacenters across Europe, America, Australia, and Asia on IBM cloud. All processes run on dedicated virtual machines with 32 x 2.0 GHz VCPUs and 32GB RAM running Ubuntu Linux 20.04. All machines are equipped with two network interfaces, public and private, rate limited for repeatability to 1 Gbps; the public one is for request submission and the private one is for node-to-node communication. Clients submit requests with 500 byte payload, the average Bitcoin transaction size [2].",
                        "Each node runs on a single virtual machine. Each node setup is uniformly distributed across all datacenters, except for the 4-node setup which spans 4 datacenters, distributed across all 4 continents. We use 16 client machines, also uniformly distributed across all datacenters, each running 16 clients in parallel which communicate independently with the nodes using TLS. We evaluate throughput, i.e., the number of requests the system delivers per second, and end-to-end latency, i.e., the latency from the moment a client submits a request until the client receives \ud835\udc53 + 1 responses."
                    ],
                    "subsections": []
                },
                {
                    "title": "ISS Configuration",
                    "paragraphs": [
                        "After a preliminary evaluation of each protocol, we established a meaningful set of parameters. We do not claim that this set is optimal. Our choice of parameters allows us, however, to demonstrate that ISS makes all three protocols scalable, which is the key contribution of this work. Table 1 summarizes the set of parameters of our evaluation. For Raft and PBFT we maintain a fixed batch rate. This translates to \ud835\udc42 (1/\ud835\udc5b) proposals per leader and \ud835\udc42 (\ud835\udc5b) message complexity per bottleneck node 3 . The choice of a fixed batch rate prevents throughput from dropping due to super-linear message complexity. On the other hand, it introduces higher end-to-end latency as the number of nodes grows, since the batch timeout increases.",
                        "The epoch length is kept short: 256 batches per epoch for a batch rate of 32 batches per second yield an epoch duration of approximately 8 seconds in a fault free execution. Shorter epoch length maintains lower latencies in case a fault occurs because bucket re-distribution is executed faster. However, a fixed epoch length yields a shorter segment length as the number of nodes increases. Too short segments for HotStuff and Raft translate to a significant overhead of the dummy/empty batches at the end of the segment. We, therefore, chose a larger minimum segment size for those two protocols.",
                        "Batch timeouts should in general be kept small to prevent increasing end-to-end latency. In Raft, however, by design, a leader node re-sends proposals until it has received an acknowledgment from the followers. A very short batch timeout would result in sending proposals too soon and therefore repeating previous proposals. This has a negative impact on throughput because the bandwidth is consumed by unnecessary duplicate proposals. To avoid this phenomenon, we opted for a minimum batch timeout longer than the approximated network round trip. To prevent rate-limiting Raft throughput due to the long batch timeouts, we allowed a large batch size.",
                        "HotStuff, on the other hand, is a latency bound protocol as sending a proposal first requires assembling a quorum certificate which depends on the previous proposal. We opted, therefore, for a batch timeout of 0 to allow the leader to send proposals as fast as possible. Similarly to Raft, we allowed a large batch size to prevent rate-limiting the throughput."
                    ],
                    "subsections": []
                },
                {
                    "title": "Failure-Free Performance",
                    "paragraphs": [
                        "Figure 5 shows the overall throughput scalability of PBFT, HotStuff, and Raft, with and without ISS, as well as that of Mir-BFT.",
                        "We evaluate the scalability of ISS with up to 128 nodes, uniformly distributed across all 16 datacenters. Mir-BFT is evaluated on the same set of datacenters on machines with the same specifications. For an apples-to-apples, comparison, we disabled Mir-BFT optimizations (signature verification sharding and light total order broadcast). Such optimizations could be implemented on top of ISS yielding even better performance. However, this goes beyond the scope of this work. For all protocols we run experiments with increasing the client request submission rate until the throughput is saturated.",
                        "In Figure 5 we report the highest measured throughput before saturation. We observe that ISS dramatically improves the performance of the single leader protocols as the number of nodes grows (37x, 56x and 55x improvement for PBFT, HotStuff, and Raft, respectively, on 128 nodes). This improvement is due to overcoming the single leader bandwidth bottleneck. Moreover, as the number of nodes grows, ISS-PBFT outperforms Mir-BFT. While in theory, in a fault-free execution, we would expect the two protocols to perform the same, we attribute this improvement to the more careful concurrency handling in the ISS implementation.",
                        "ISS-PBFT maintains more than 58 kreq/s on 128 nodes. Its performance, though, drops compared to smaller configurations. We attribute this to the increasing number of messages each node processes, which, with a fixed batch rate (Table 1), grows linearly with the number of nodes. We further observe that throughput increases for Raft and HotStuff ISS implementations with the number of nodes, approaching that of ISS-PBFT. While PBFT's watermarking mechanism allows the leader to propose batches in parallel, HotStuff, as explained in Section 6.2, is latency-bound. However, running multiple independent protocol instances with ISS helps improve the overall throughput. Raft, on the other hand, suffers from the redundant re-proposals. While this drawback is mostly hidden in fast LANs with negligible latency, it manifests strongly in a WAN. With more nodes in the leaderset the batch timeout increases and re-proposals are reduced. In Figure 6 we observe that ISS latency grows with the number of nodes. This is due to our choice of a fixed batch rate in order to reduce message complexity and sustain high throughput with an increasing number of nodes."
                    ],
                    "subsections": []
                },
                {
                    "title": "ISS Under Faults",
                    "paragraphs": [
                        "In this section we fix PBFT as the protocol multiplexed with ISS and study its performance under crash faults and Byzantine stragglers in a WAN of 32 nodes. The PBFT view change timeout is set to 10 seconds.   Figure 7 shows the impact of crash faults on latency. We see that latency converges towards that of a fault-free execution as we increase the duration of the experiment. This is due to our leader selection policy removing the faulty node from the leaderset once detected. Note that, regardless of their number, epoch-end failures have a stronger impact on latency (as they delay requests in all bucket queues) than epoch-start failures (affecting only the faulty nodes' bucket queues).   We compare the ISS performance under crash faults to Mir-BFT. In Figure 9 we study a run of Mir-BFT on 32 nodes with a single epoch-start crash fault. Mir-BFT stops processing all proposals during epoch changes, unlike ISS where segments make progress independently. This results in any crash fault having an impact similar to that of the epoch-end fault for ISS. Moreover, Mir-BFT relies on an epoch primary for liveness. Every time the crashed node becomes epoch primary, it causes an ungraceful epoch change. In Figure 9 this happens around \ud835\udc61 = 600 s. The phenomenon repeats periodically, unlike in ISS, where the faulty leader is permanently removed. Finally, ISS crash fault recovery is more lightweight, since it concerns only the batches of a single segment. Figure 10 shows the impact of an increasing number of stragglers. ISS with PBFT reaches from 15% of its maximum throughput with one straggler to 10% of its maximum throughput with 10 stragglers. This, though, translates to maintaining more than 11.4 and 7.9 kreq/s, respectively, on 32 nodes. Mean latency before saturation increases from 14\ud835\udc65 with one up to 29\ud835\udc65 with 10 stragglers. Figure 11 shows how throughput is affected over time with a total submission rate of 16.4kreqs/s. The performance degradation is due to the \"holes\" in the log temporarily created by the stragglers. Request delivery progresses as fast as the slowest straggler, hence the spikes in the graph. When the straggler's batch is finally committed, one more batch per leader can be delivered as well (due to the interleaved batch sequence numbers). This is inherent to any SMR protocol [4] until the straggler is removed from the leaderset. Straggler resistance in ISS depends on the underlying SB implementation. A more sophisticated leader selection policy could dynamically detect and remove stragglers from the leaderset. ISS facilitates such dynamic detection by comparing the progress of SB instances, similarly to RBFT [5] but without the need for redundant instances. Alternatively, SB instances could implement Aardvark's [11] straggler detection mechanism with reducing timeouts. This is promising future work. "
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Consensus under Byzantine faults was first made practical by Castro and Liskov [9][10] who introduced PBFT, a semipermanent leader-driven protocol. The quadratic message complexity of PBFT across all replicas triggered vigorous research towards protocols with linear message complexity. Ramasamy and Cachin [29] replace reliable broadcast in the common case (fault-free execution) with echo broadcast, achieving common case message complexity \ud835\udc42 (\ud835\udc5b) per delivered payload. Echo broadcast is also exploited in [22] [18] to achieve linear common case message complexity. Recently, HotStuff [33] introduced a 4th communication round to the 3 message rounds of reliable broadcast, to achieve linear message complexity also for the recovery phase (view-change) of the protocol. Regardless of the improvement of message complexity, all aforementioned protocols have a single leader at a time, permanent or not, limiting throughput scalability.",
                "Mencius [26] introduced multiple parallel leaders, running instances of Paxos [24], to achieve throughput scalability and low latency in WAN under crash fault assumptions. BFT-Mencius [27] was the first work to introduce parallel leaders under Byzantine faults. BFT-Mencius introduced the Abortable Timely Announced Broadcast communication primitive to guarantee bounded delay after GST. However, BFT-Mencius partitions requests among instances by deterministically assigning clients to replicas, which cannot guarantee load balancing. Moreover, this opens a surface to duplication performance attacks, since malicious clients and replicas can abuse the suggested denial of service mitigation mechanism.",
                "Guerraoui et al. [19] also introduced an abstraction which allows BFT instances to abort. The paper uses the abstraction to compose sequentially different BFT protocols, which allows a system to choose the optimal protocol according to network conditions. Our work, on the other hand, composes TOB instances in parallel to achieve throughput scalability.",
                "Mir-BFT [31] is the multi-leader protocol which eliminates request duplication ante broadcast, effectively preventing duplication attacks. Later FnF [6] suggested improved leaderset policies for throughput scalability under performance attacks. FnF adopts Mir-BFT's request space partitioning mechanism for duplication prevention. Similarly, Dandelion [21] leverages the same mechanism to combine Algorand [17] instances. However, Mir-BFT and FnF multiplex PBFT and SBFT [18] instances, respectively, leveraging a single replica in the role of epoch primary. ISS not only eliminates the need for an epoch primary but also provides a modular framework to multiplex any single leader protocol that can implement SB.",
                "Parallel to this work, several works attempt multiplexing BFT instances to achieve high throughput (Redbelly [12], RCC [20], Omada [16]). However, similarly to BFT-Mencius, clients are assigned to primaries, and, after a timeout, a client can change primary to guarantee liveness, again allowing duplication attacks."
            ],
            "subsections": []
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "In this work we introduced ISS, a general construction for efficiently multiplexing instances of leader-based consensus protocols. ISS leverages request space partitioning to prevent duplication, similarly to Mir-BFT, but re-assigns the partitions without the need of a replica to act as a primary, even in the case of faults. To achieve this, we introduced Sequenced Broadcast, a novel abstraction which allows periodically terminating and synchronizing the otherwise independent consensus instances. Our evaluation shows that our careful engineering along with the multi-leader paradigm results in scalable performance for three single-leader protocols (PBFT [10], HotStuff [33], and Raft [28]), outperforming their original designs by an order of magnitude at scale. that decides which instance is responsible for which part of the log, when to execute a checkpoint protocol and which client requests are to be ordered by which ordering instance. The decisions of the manager must be consistent across all peers.",
                "The log is a sequence of entries. Each entry has a sequence number (sn) defining its position in the log, and contains a batch of requests. The log is logically partitioned into segments -parts of the log attributed to a single instance of an ordering protocol. It is the manager's task to create these segments and to instantiate the ordering protocol for each created segment.",
                "The set of all possible client requests is partitioned (based on their hashes) into subsets called buckets. The manager assigns a bucket to each segment it creates. The ordering protocol instance only creates batches of requests using the assigned bucket. It is the manager's task to assign buckets in a way ensuring that no two segments that are being ordered concurrently are assigned the same bucket. This is required to prevent request duplication.",
                "The manager observes the log and creates new segments as the log fills up. When the manager creates a new segment, it triggers the orderer that orders the segment. Ordering a segment means committing new entries with the sequence numbers of that segment. Periodically, the manager triggers the checkpointer to create checkpoints of the log. The manager observes the created checkpoints and issues new segments as the checkpoints advance, respecting the watermark window."
            ],
            "subsections": []
        },
        {
            "title": "A.2 Description & Requirements",
            "paragraphs": [
                "A.2.1 How to Access. The code used to produce the results of the experiments is publicly available under Hyperledger Labs 45 . Importantly, the artifact does not reside in the main branch of the repository. It can be found in the research-iss branch. For completeness we mention that main branch started as a production implementation of MirBFT which has started shifting towards ISS.",
                "A.2.2 Hardware Dependencies. We performed our evaluation on a WAN on IBM cloud. All processes ran on a dedicated virtual machine with 32 x 2.0 GHz VCPUs and 32GB RAM running Ubuntu Linux 20.04.",
                "Our WAN spanned in 16 datacenters across Europe, America, Australia, and Asia. In detail, machines were deployed in the following locations: San Jose, Osaka, Amsterdam, Sydney, London, Washington D.C., Chennai, Tokyo, Paris, Dallas, Frankfurt, Milan, Mexico City, Toronto, Montreal, Seoul. For each deployment (16,32,64, 128 nodes) the virtual machines where uniformly distributed across all data centers. For the smaller deployments (4, 8 nodes) the machines were located on the first 4 and 8 locations of the list respectively. Finally, all machines were equipped with two network interfaces, public and private, rate limited for repeatability to 1 Gbps. A.2.3 Software Dependencies. Go 16+, Python 3 A.2.4 Benchmarks. None."
            ],
            "subsections": []
        },
        {
            "title": "A.3 Set-up",
            "paragraphs": [
                "In the README6 file of the root directory of the project exist high level information about the architecture of the code. Detailed information on how to deploy the code can be found under the deployment directory 7 . In a nutshell, the deployment provides scripts that automatically deploy a network of nodes and clients on IBM cloud or locally, runs a set of experiments, analyzes the results and produces a result summary.",
                "For an IBM cloud deployment, the user first needs to set up an account with IBM cloud and register an ssh key with this account. The repository has scripts to help the user set up and initialize the ibmcloud cli.",
                "To run a set of experiments the user needs only needs to describe the desired set of experiments in a configuration script. Then the user runs the main script deploy.sh. After the result summary is generated, the user can find it in a .csv file, or process it with a plotting scrip to visualize the results."
            ],
            "subsections": []
        },
        {
            "title": "A.3.1 Major Claims.",
            "paragraphs": [
                "\u2022 We observe that ISS dramatically improves the performance of the single leader protocols as the number of nodes grows: 37x, 56x and 55x improvement for PBFT, HotStuff, and Raft respectively on 128 nodes (Figure 5). \u2022 ISS is robust under crash faults: the impact of the crash fault is limited within the epoch it occurs. Latency remains in the order of seconds even for the affected by the crash fault requests (Figures 7 and8).",
                "A.3.2 Experiments. ISS configuration includes a wide set of parameters. While the most important are grouped in Table 1, in the artifact we provide the configuration scripts that reproduce the main claims of the paper. Each script generates the configuration for a set of experiments. In the rest of this section we describe the set of experiments each configuration file produced and we link them to the main claims of the main paper.",
                "For each of the experiments listed in Table 2 8 we performed experiments on 4,8,16,32,64, and 128 nodes with 16 clients for increasing client request rate. By fixing the number of nodes to 32 we produced the Latency-Throughout pots (Figure 6). We further produced latency throughput plots for each number of nodes -not included in the paper. For each latency -throughput plot the data point at which throughput stops increasing and latency starts increasing significantly represents the maximum throughput. By collecting the maximum throughput data points for all node and protocol configurations we produced Figure 5. Table 2. Failure free performance.",
                "The script generate-crash-faults.sh is used to generate experiments for ISS-PBFT with [1,2] failures at the [beginning, end] of the epoch relevant to Figures 7 and8. Finally, script generate-straggler-0.5.sh is used to generate experiments for ISS-PBFT with a straggler relevant to Figures 10 and 11. "
            ],
            "subsections": []
        },
        {
            "title": "A Artifact Appendix",
            "paragraphs": [
                "In this document we provide a guide on how to reproduce the results of the main paper."
            ],
            "subsections": []
        },
        {
            "title": "A.1 Abstract",
            "paragraphs": [
                "ISS is a modular framework for implementing, deploying and testing a state-machine replication service. The main task of such a service is maintaining a totally ordered log of client requests. This implementation uses multiple instances of an ordering protocol (an SB implementation) and multiplexes their outputs into the final log. The ordering protocol instances running on each peer are orchestrated by a manager module"
            ],
            "subsections": []
        }
    ]
}