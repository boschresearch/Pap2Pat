# RELATED WORK

Anti-discrimination laws in the United States regulate two discriminatory behaviors based on protected attributes. These doctrines of discrimination are disparate treatment and disparate impact. Disparate treatment protects individuals who are affected by algorithmic decision making against explicit discrimination. The standard practice to conform to disparate treatment in Machine learning techniques used for decision making is to exclude protected attributes from inputs. This is easy to achieve in a fraud detection model -one would just need to not use any sensitive attribute while training the model. In contrast, disparate impact addresses outcome discrimination; it recognizes liability for practices with uneven impacts on different protected groups. While there is no single fairness measure that captures absolute impact parity, equal opportunity, demographic parity, and equalized odds [2,4,10,13] have received considerable attention. Efforts from scholars revolve around heuristics to ensure these fairness measures. While these techniques are catered to ensure fairness for any Machine learning technique, they are not always practical to implement in an industry setting. To expound, consider demographic parity, which requires that the outcome rates across sensitive attributes be the same. In the context of fraud detection, for a sensitive attribute "country", this would mean that acceptance or rejection rates of online transactions for each country should be the same. This may not be ideal since we observe that the ground truth fraud rates from different countries are significantly different (Refer to Figure 2). This could possibly be explained by the jurisdiction in each of these countries in handling online fraud (stricter penalties would mean lower fraud rates and vice versa). If we were to sanitize our fraud detection model to ensure demographic parity, one would either end up allowing lots of fraudsters or declining lots of genuine users both of which are not an ideal outcome for a firm.  Another approach, equalized odds, aims to achieve the same false positive and true positive rates across sensitive attributes. While this is better than the demographic parity for fraud detection, the measure enforces equal importance to both false positives and false negatives. This is again not ideal for a firm given the cost of type-1 and type-2 errors are different for electronic1 vs physical goods. Lastly, Equalized opportunity, is a relaxed version of Equalized odds that only requires equal true positive rates across sensitive attributes. This again is insufficient since we would not want to deny a genuine customer and hence would like to balance false positive rates as well. Furthermore, while getting to equal rates is possible in theory, it is less practical with daily fluctuations in traffic and fraud patterns. Additionally, from an implementation standpoint, another major critique about the current techniques is that they require multiple sanitizations of the ML model to achieve fairness. This largely arises since majority of these techniques assume binary sensitive attributes. Such constraints are not viable in a fraud detection system that usually deal with high arity attributes and require real time inference. Further, to the best of our knowledge, none of these methods perform sanitization of a machine learning model in a post-hoc fashion across multiple protected attributes. We tackle these drawbacks in our work by introducing a relaxed Equalized odds fairness measure and a one-shot fairness heuristic to achieve the proposed fairness measure. Further, we design the proposed heuristic to have the flexibility to pick either or both constraints of the Equalized Odds definition (similar FPR or similar TPR or both) [10]. Finally, and importantly, we also extend the proposed fairness measure and heuristic to handle multiple attributes.

# METHOD

The classical definition of Equality of Odds posits that true positive rates and false positive rates across a protected attribute are the same. It reflects a fundamental idea of fairness, that qualified individuals should be given equal opportunity (true positive rate -TPR) to access a desirable outcome while also requiring equal false positive rates (FPR, ensuring no bias to the positive class) regardless of their demographics or any other sensitive attributes. However, this strict requirement of equality in most settings requires non-deterministic thresholds as pointed out in previous research [2,4,10,13] making it less appealing from a practical standpoint. Further, the current heuristics to achieve such fairness involves multiple sanitizationsone per each categorical attribute value (Ex: one for each country) of a machine learning model to ensure the fairness criterion is met. We deal with these issues by first, relaxing the notion of the standard Equalized odds. Specifically, we relax the equality constraint of FPR, TPR and enforce a weaker, yet practical constraint. Second, we propose a fairness heuristic that sanitizes the outputs of a classification model to conform to the relaxed equalized odds measure. Note: We describe the details assuming a single attribute and later extend this to multiple attributes in Section 3.3.

## Relaxed Equalized Odds

Say 𝐷 = {𝑑 1 , 𝑑 2 , . . . , 𝑑 𝐾 } is a protected attribute with arity 𝐾, we say that a fraud detector model 𝐹 2 satisfies relaxed equalized odds with respect to attribute 𝐷 if the false positive and true positive rates of the respective attribute values

where 𝜇 and 𝜎 are the average and standard deviation respectively across the 𝐾 attribute values.

The above definition while weaker, captures the core philosophy of the Equalized odds in a high arity(𝐾) 3 setting -similar opportunity and similar false alarms across a sensitive attribute.

## Fairness Heuristic

Following a similar notation as above, say we would like to sanitize a fraud detector 𝐹 with respect to the protected attribute 𝐷, our fairness heuristic has the following key steps.

### Choice of constraints.

We provide an end user the capability to further relax the traditional equalized odds definition to conform to either similar FPRs or TPRs or both (i.e., Eq 1 or Eq 2 or both). This would be useful for a business where false positives of a fraud detector are more costly compared to true positives. Said differently, if it is costlier for a business to accept a fraudulent customer compared to declining a genuine customer (false negative), then they might choose to conform to similar FPRs, while also ensuring that the overall FPR is low. In a similar way, a business might choose to have equal opportunity (TPR) across a protected attribute. This choice decides the 𝑠𝑒𝑙𝑒𝑐𝑡𝑖𝑜𝑛 𝑚𝑒𝑡𝑟𝑖𝑐 in our heuristic. 𝑠𝑒𝑙𝑒𝑐𝑡𝑖𝑜𝑛 𝑚𝑒𝑡𝑟𝑖𝑐 = 𝐹 1; if Eq (1) and Eq (2), = 𝐹 0.5;, if only Eq (1) and = 𝐹 2; if only Eq (2). The choice of the selection metric is done based on the importance of the FPR and TPR to the end user. F1 treats false positives and false negatives (true positives) equally while 𝐹 0.5 and 𝐹 2 weight false positives and false negatives higher respectively.

### Threshold grid initialization.

The core idea of our heuristic is to calibrate the decision thresholds of the model 𝐹 across the different attribute values D to conform to Eq (1) and/or Eq (2). To do this, we initialize a linear grid of possible threshold values denoted as 𝐺 𝑡ℎ𝑟𝑒𝑠ℎ . A choice of 𝐺 𝑡ℎ𝑟𝑒𝑠ℎ could be {0.6, 0.61, . . . , 0.9}. Note that depending on the threshold value, both FPR and TPR of the model 𝐹 would change.

### Performance computation.

For all the values in 𝐺 𝑡ℎ𝑟𝑒𝑠ℎ , per attribute value in 𝐷, we compute model 𝐹 's performance metrics. These comprise of the metric that the end user chooses to conform to in the fairness measure (say FPR in our running example). At the end of this step, we would have 𝑓 𝑝𝑟 This would mean that for a protected attribute value 𝑑 𝑖 , 𝐷 = {𝑑 1 , 𝑑 2 , . . . , 𝑑 𝐾 }, some of the threshold choices are pruned since their false positive rates are statistically higher than the average. Denote such pruned set as {𝑓 𝑝𝑟 𝐹 𝑔 } 𝑝𝑟𝑢𝑛𝑒𝑑 . Select : From {𝑓 𝑝𝑟 𝐹 𝑔 } 𝑝𝑟𝑢𝑛𝑒𝑑 , we next select the threshold per each attribute value 𝑑 𝑖 where the 𝑠𝑒𝑙𝑒𝑐𝑡𝑖𝑜𝑛 𝑚𝑒𝑡𝑟𝑖𝑐 (F-0.5 in our running example) is maximized. This results in a choice of {(𝑑 1 , 𝑔 1 ), ..., (𝑑 𝐾 , 𝑔 𝐾 )}. Denote this choice of thresholds as the model 𝐹 𝑠𝑒𝑙𝑒𝑐𝑡 We check if 𝐹 𝑠𝑒𝑙𝑒𝑐𝑡 conforms to the fairness constraints, i.e., Eq (1) in our running example 𝜇 (𝑓

If not, we repeat the pruning and selection step with the updated {𝑓 𝑝𝑟 𝐹 𝑔 } 𝑝𝑟𝑢𝑛𝑒𝑑 until the constraint is satisfied. First, we note that the final 𝐹 𝑠𝑒𝑙𝑒𝑐𝑡 conforms to the fairness measure and preserves model fidelity by selecting the best available model based on the selection metric. Second, from Step 3 and Step 4, we note that our fairness heuristic only requires aggregate information such as FPR, TPR, F-1 score about the protected attribute 𝐷 and does not require information about the features (protected or unprotected) or their feature values used to train the fraud detector model. Hence, one could perform this sanitization in a differentially private manner by adding appropriate noise to the aggregate information. Third, the reliance of the output on solely the outcomes makes the fairness heuristic model agnostic. Fourth, the proposed heuristic is a one-shot search heuristic that conforms the model 𝐹 to the protected attribute 𝐷 rather than performing 𝐾 sanitizations for each attribute of 𝐷. Finally, we note that the grid heuristic discussed above is guaranteed to converge to a solution if there is indeed a solution where the FPRs/TPRs are equal for the sub-groups.

## Multiple Attribute Extension

The fairness heuristic detailed above handles a single protected attribute and conforms to equalized fairness in that attribute. However, for most practical applications, we would require a machine learning model to conform to fairness across multiple attributes. For instance, in the case of a fraud detection model, one would like to sanitize the model to not be biased against a country, currency that they use. The current state of the art fairness heuristics deals with this issue while training [6] or by processing the features before learning [14]. Supplementing this line of literature, we extend the proposed fairness heuristic which performs sanitization post inference to handle multiple attributes.

A naive way to handle multiple attributes (consider two for example) say

Note that such calibration of thresholds would conform to a strict notion of the Relaxed Equalized Odds ensuring the FPR and TPRs are equal across every sub-population. While this is certainly an attractive property to have, the computational complexity is exponential. A weaker notion would be to ensure that the FPR and TPR values satisfy the constraints in Eq (1) and (2) per attribute independently rather than all enumerations.

Expounding the two concepts with an example of two attributes country and currency with attribute values US, IN and USD, INR respectively. In the stronger notion, we want FPRs and TPRs across the eight sub populations {(𝑈 𝑆, 𝑈 𝑆𝐷), (𝑈 𝑆, 𝐼 𝑁 𝑅), (𝐼 𝑁 , 𝑈 𝑆𝐷), (𝐼 𝑁 , 𝐼 𝑁 𝑅), (𝑈 𝑆), (𝑈 𝑆𝐷), (𝐼 𝑁 ), (𝐼 𝑁 𝑅)} to conform to Eq (1) and ( 2). In the weaker notion, we want them to conform to four sub populations {(𝑈 𝑆), (𝐼 𝑁 ), (𝑈 𝑆𝐷), (𝐼 𝑁 𝑅)}. While the former is a stronger fairness notion and scales exponentially with the number of attributes, the latter is weaker and scales linearly. We propose heuristics for both such notions of fairness giving the decision maker the flexibility to opt to either of those depending on the number of attributes the model needs to be sanitized on. We recommend the stronger notion for 10 or a smaller number of attributes and the univariate, weaker notion for greater than that. In both cases, we reduce this combinatorial computation by first pruning the attribute space by identifying attributes that encompass similar sub populations.

### Attribute

Pruning. If two attributes are dependent i.e., capture similar sub populations with their attribute values (for instance country and currency), it is redundant to calibrate the thresholds for both. We define dependence between attributes as the Chisquare statistic between pairs of attributes. Based on the statistic, we compute the 𝑝-value to infer if two attributes are statistically independent of each other (p-value <= 0.01) and drop the attributes that are highly dependent. This step prunes the space of protected attributes while only leaving independent attributes to calibrate.

### Strong Multiple Attribute Fairness : Attribute Value Pruning.

In this regime, we want to ensure that the FPRs and TPRs of every sub-population conform to Eq (1) and ( 2). While reducing the attribute space partly addresses the computation issue discussed earlier, we are still left with potentially multiple attributes with high arity. For instance, say we have m protected attributes after pruning with cardinalities 𝐾 1 , 𝐾 2 , . . . , 𝐾 𝑚 , we would be looking at possibly 𝐾 1 𝐾 2 . . . 𝐾 𝑚 subspaces to calibrate the thresholds on. However, we note that to yield a reliable estimate of the performance metrics fpr,tpr (Step 3 in the heuristic) or the selection metrics (Step 4.b.), as a rule of thumb [8], a subspace would at least need to have 100 transactions (data points). Hence, we prune away such subspaces using a scalable implementation of the Frequent Pattern Tree [9] data structure. Let 𝐶 = {𝑐 1 , 𝑐 2 , . . . , 𝑐 𝑃 } be the total number of subspaces left with each at least having 100 transactions. An example of a 𝑐 1 would be {𝑐𝑜𝑢𝑛𝑡𝑟𝑦 = 𝑈 𝑆, 𝑠𝑒𝑥 = 𝑀𝑎𝑙𝑒, 𝑟𝑎𝑐𝑒 = 𝐻𝑖𝑠𝑝𝑎𝑛𝑖𝑐}. The resulting clusters 𝐶 are then calibrated with the grid of thresholds to conform to the Relaxed Equalized Odds fairness measure in a similar fashion to the single attribute case where the performance table looks like {𝑓 𝑝𝑟 𝐹 𝑔 }, 𝑔 ∈ 𝐺.

### Weak Multiple Attribute Fairness .

In this regime, we want to ensure that the FPRs and TPRs per attribute conform to Eq (1) and Eq (2). Operationally, this can be achieved by changing the Step 3) in the fairness heuristic to compute the performance table across all attributes and their attribute values. That is, for m protected attributes, construct 𝑓 𝑝𝑟

} 𝑔 ∈ 𝐺 𝑡ℎ𝑟𝑒𝑠ℎ , where 𝑓 𝑝𝑟 𝐹 𝑔 is the FPRs of the model 𝐹 at threshold g across all univariate enumerations of the protected attribute values. This is followed with the Pruning and Selection step as discussed earlier to find thresholds per attribute value for all the m attributes.  We showcase the efficacy of the proposed approach on several publicly available data sets -criminal recidivism 4 , incomeprediction , health prediction 5 , and a proprietary commercial data set that is comprised of online transactions. For baselines, we consider Equalized Odds [10] and Calibrated Equalized Odds [13]. In the experiments, we set 𝑛 = 2 and investigate single protected attribute with high arity, multiple protected attributes, and also consider the case of 2 protected groups as investigated by earlier studies.

Case Study : Fraud Detection We first discuss our motivating example, online fraud detection 6 . The aim of the prediction is to assess whether an online transaction initiated by an individual is fraudulent. The prediction model omits all protected attributes as input to conform to the disparate treatment doctrine of fairness measure. However, as evident from 3 and 4 (red color bars) we observe that model exhibits a bias towards the demographic attribute country. A similar trend is observed when we look at the FNRs. As we motivate earlier, from a business perspective, it is useful to have flexibility to sanitize these metrics across attributes  We employ the proposed heuristic on the predictions made by the fraud detector by conforming to both the constraints Eq (1) and Eq (2). From Figures 3,4 (green color bars), we observe that the final 𝐹 𝑠𝑒𝑙𝑒𝑐𝑡 with custom thresholds per country considerably reduces the bias towards certain countries. Further, we also note that the FPRs and FNRs across countries conform to the relaxed equalized odds fairness measure -mean across the countries lies withing two standard deviations.

Next, we consider the exercise on the fraud detector model trained with XGBoost to sanitize across with two protected attributes country and currency. From Figure 5, we observe that we can achieve similar FPRs across the attribute values of currency. From a separate experiment (not presented for brevity), we also observe that sanitizing the model on currency inherently sanitizes it against country given their high dependence.  Model benchmarks: Next, we compare the proposed heuristic to Equalized [10] and Calibrated Equalized Odds [13]. These techniques are aimed at achieving fairness across two groups (E.g. Country is/not 1), unlike the proposed heuristic which aims to achieve fairness across all sub groups with custom thresholds per country. In Table 1, rows 1 and 2, we report FPR and FNR for Country is/not is Country 6 7 (Class 0 and Class 1). and Country is/not is Country 22 and observe that we are able to achieve similar metrics compared to the benchmarks.

# Method

Income-Prediction This dataset from UCI Machine Learning Repository contains 14 demographic and occupational features for various people, with the goal of predicting whether a person's income is above $50, 000. In this scenario, we seek to achieve predictions with equalized cost across genders (single protected attributes, two groups -Male and Female). In this dataset, we consider a scenario where the primary concern is ensuring equal generalized false negative rates across genders, which would help job recruiters prevent gender discrimination in salary estimates. Hence, we choose our fairness constraint to require relaxed equalized false negative rates across groups. In row 3, Table 1, we observe that the proposed heuristic achieves similar FNRs for the two groups -0.77 (African American) and 0.61 (not African American). Also, note that we are able to achieve comparable (similar FNRs) or better performance (better FPRs) compared to Calibrated Equalized Odds, designed specifically to enforce equal FNRs across the two groups. Equalized Odds on the other hand, ensures equal FPRs and FNRs across the two groups, trading off the FPR for a lower FNR across groups (0.46 and 0.34) compared to the proposed heuristic (0.61 and 0.77).

Criminal Recidivism Finally, we examine the proposed heuristic in t he context of criminal recidivism. As noted by several studies earlier, in this dataset, African Americans, receive a disproportionate number of false positive predictions as compared with Caucasians when automated risk tools were used. Hence, we aim to equalize the generalized false positive rate. In row 4, Table 1, we observe that the proposed heuristic achieve a) similar FPRs for the two groups -0.4 (African American) and 0.38 (not African American) b) similar FNRs (0.60 and 0.59 respectively). Note that we are able to achieve comparable (similar FNRs) and slightly better better FPRs compared to Equalized Odds, designed to enforce equal FNRs and FPRs across the two groups. When compared to the Calibrated Equalized Odds, where the weighted combination of error rates are matched (equal FPRs and FNRs as discussed in [13]), we observe that the proposed heuristic performs comparably in terms of both FPRs and FNRs across two groups.

# CONCLUSION

We propose a fairness measure relaxing the FPR and TPR equality conditions in the popular equal odds fairness regime [10]. To conform to the proposed fairness, we design an iterative, modelagnostic, grid-based heuristic that calibrates the outcomes per sensitive attribute value (for e.g., different countries). Through a detailed case study of our motivating application, fraud detection, we show that the proposed heuristic is able to achieve fairness across multiple values of a single protected attribute, multiple protected attributes. We compare our work to current fairness techniques and show comparable performance across several public data sets.

