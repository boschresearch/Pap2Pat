{
    "id": "https://semopenalex.org/work/W4282813729",
    "authors": [
        "\u0141ukasz Pawela",
        "Zbigniew Pucha\u0142a",
        "Ryszard Kukulski"
    ],
    "title": "On the probabilistic quantum error correction",
    "date": "2022-06-10",
    "abstract": "Probabilistic quantum error correction is an error-correcting procedure which uses postselection to determine if the encoded information was successfully restored. In this work, we deeply analyze probabilistic version of the error-correcting procedure for general noise. We generalized the Knill-Laflamme conditions for probabilistically correctable errors. We show that for some noise channels, we should encode the information into a mixed state to maximize the probability of successful error correction. Finally, we investigate an advantage of the probabilistic error-correcting procedure over the deterministic one. Reducing the probability of successful error correction allows for correcting errors generated by a broader class of noise channels. Significantly, if the errors are caused by a unitary interaction with an auxiliary qubit system, we can probabilistically restore a qubit state by using only one additional physical qubit.",
    "sections": [
        {
            "title": "I. INTRODUCTION",
            "paragraphs": [
                "Quantum error correction (QEC) is an encoding-decoding procedure that protects quantum information from errors arising due to quantum noise. Similarly, as in classical computations, this procedure is essential to develop fully operational quantum computers [1]. The theory of QEC, initialized by the work of Shor [2], covers a wide range of coding techniques: Calderbank-Shor-Steane codes [3-5], stabilizer codes [6], topological codes [7], subsystem codes [8], entanglement-assisted quantum error-correcting codes [9, 10], quantum low-density parity-check (LDPC) codes [11], quantum maximum distance separable codes [12] and many more (for a review see [13]).",
                "In this work, we study a particular QEC procedure called probabilistic quantum error correction (pQEC) [14-16]. To outline how pQEC procedure works, let us present an example of classical probabilistic error correction. Consider the scenario, when the encoded data is harmed by a single bit error, that is with the probability p \u2208 [0, 1] an arbitrary bit will be flipped. To secure a one bit of information, we use two physical bits. If we expect that p \u2264 2 3 , then we can encode 0 \u2192 00 and 1 \u2192 11. If we receive information 00 at the decoding stage, we are certain the encoded message was 0 (and 1 for 11). We dismiss the cases 01 and 10 as they do not give conclusive answers. Otherwise, if p > 2 3 it would be beneficial to use encoding 0 \u2192 00 and 1 \u2192 01 with the accepting states 10 and 11. It is worth mentioning, that to secure a one bit of information perfectly, it is necessary to use three physical bits, for example 0 \u2192 000, 1 \u2192 111.",
                "Let us return to the quantum case. The heart of pQEC procedure is the probabilistic decoding operation [17,18]. This operation uses a classical postselection to determine if the encoded information was successfully restored. The clear drawback is that the procedure may fail with some probability. In such case, we should reject the output state and ask for a retransmission [19]. In the context of QEC, probabilistic decoding operations have found application in stabilizer codes [20] especially for iterative probabilistic decoding in LDPC codes [11,21,22], error decoding [23,24] or environment-assisted error correction [25]. Moreover, it was noted that they have a potential to increase the spectrum of correctable errors [15] and are useful when the number of qubits is limited [14]. It is also worth mentioning, they were used with success in other fields of quantum information theory, e.g. probabilistic cloning [26], learning unknown quantum operations [27] or measurement discrimination [28].",
                "Despite the fact that pQEC procedure has been studied in the literature for a while, there is lack of a formal description of its application for a general noise model. In this work, we fill this gap. Inspired by celebrated Knill-Laflamme conditions [29], we provide conditions (Theorem 1) to check, when probabilistic error correction is possible. We discover that optimal error-correcting codes are not always generated with the usage of isometric encoding operations. We give an explicit example of noise channels family (Section V), such that to maximize the probability of successful error correction we need to encode the quantum information into a mixed state. Moreover, we discuss the advantage of pQEC procedure over the deterministic one with a formal statement in Theorem 7. We show in Theorem 13 how *"
            ],
            "subsections": []
        },
        {
            "title": "II. PRELIMINARIES A. Mathematical framework",
            "paragraphs": [
                "In this section, we will introduce the notation and recall necessary basic facts of quantum information theory. We will denote complex Euclidean spaces by symbols X , Y, . . .. The set of linear operators M : X \u2192 Y will be written as M(X , Y) and M(X ) := M(X , X ). The identity operators will be denoted by 1l X \u2208 M(X ). For any operator M \u2208 M(X , Y) we will consider its vectorization |M \u2208 Y \u2297 X , which is defined as",
                "where |i are elements of computational basis. In the space M(X ), we distinguish the set of positive semi-definite operators P(X ), the space of Hermitian operators H(X ) and the set of unitary operators U(X ). We use the convention that for non-invertible operator M , by M -1 , we denote its Moore-Penrose pseudo-inverse [30]. We consider the set of quantum states D(X ), that is, the set of positive semi-definite operators with unit trace. We say that a quantum state \u03c1 is a pure state if rank(\u03c1) = 1, otherwise, if rank(\u03c1) > 1, we say that \u03c1 is a mixed state. The maximally mixed state will be denoted by \u03c1 * X := 1 dim(X ) 1l X . We also consider transformations between linear operators. We denote by I X : M(X ) \u2192 M(X ) the identity map. Let us define the set of quantum subchannels sC(X , Y) [31]. A quantum subchannel \u03a6 \u2208 sC(X , Y) is a linear map \u03a6 : M(X ) \u2192 M(Y), which is completely positive [30,Theorem 2.22], i.e.",
                "(\u03a6 \u2297 I X )(Q) \u2208 P(Y \u2297 X ) for any Q \u2208 P(X \u2297 X )",
                "and trace non-increasing tr(\u03a6(\u03c1)) \u2264 1 for any \u03c1 \u2208 D(X ).",
                "In particular, the subchannel \u03a6 which is trace preserving, i.e.",
                "tr(\u03a6(\u03c1)) = 1 for any \u03c1 \u2208 D(X )",
                "will be called a quantum channel. We denote by C(X , Y) the set of quantum channels \u03a6 : M(X ) \u2192 M(Y). We will also use the following notation, sC(X ) := sC(X , X ) and C(X ) := C(X , X ).",
                "In this work, we will consider the following representations of subchannels:",
                "\u2022 Kraus representation: Each subchannel \u03a6 \u2208 sC(X , Y) can be defined by a collection of Kraus operators (K i ) r i=1 \u2282 M(X , Y), such that \u03a6(X) = r i=1 K i XK \u2020 i for X \u2208 M(X ) and r \u2208 N. The operators K i satisfy the condition r i=1 K \u2020 i K i \u2264 1l X . We say that the subchannel \u03a6 is given in a canonical Kraus representation (K i ) r i=1 , if it holds that tr(K \u2020 j K i ) \u221d \u03b4 ij and K i = 0 for each i \u2264 r. To represent the subchannel \u03a6 by its Kraus representation (K i ) r i=1 , we introduce the notation K : M(X , Y) \u00d7r \u2192 sC(X , Y) given by \u03a6 = K ((K i ) r i=1 ). \u2022 Choi-Jamio\u0142kowski representation: Each subchannel \u03a6 \u2208 sC(X , Y) can be uniquely described by its Choi-Jamio\u0142kowski operator J(\u03a6) \u2208 M(Y \u2297 X ), which is defied as J(\u03a6) := (\u03a6 \u2297 I X )(|1l X 1l X |). The rank of J(\u03a6) is called the Choi rank and it determines the minimal number r of Kraus operators K i needed to describe \u03a6 in the Kraus form \u03a6 = K ((K i ) r i=1 ). Therefore, if the Kraus representation (K i ) r i=1 is canonical, then r = rank(J(\u03a6)).",
                "\u2022 Stinespring representation: By the Stinespring Dilatation Theorem any subchannel \u03a6 \u2208 sC(X , Y) can be defined as \u03a6(X) = tr 2 AXA \u2020 for X \u2208 M(X ), where A \u2208 M(X , Y \u2297 C r ) and tr 2 is the partial trace over the second subsystem C r . The minimal dimension r of the auxiliary system is equal to the Choi rank. In particular, for \u03a6 \u2208 C(X ), the Stinespring representation of \u03a6 can be written in the form \u03a6(X) = tr 2 U (X \u2297 |\u03c8 \u03c8|)U \u2020 , where |\u03c8 \u03c8| \u2208 D(C r ) and U \u2208 U(X \u2297 C r )."
            ],
            "subsections": []
        },
        {
            "title": "III. PROBABILISTIC QUANTUM ERROR CORRECTION",
            "paragraphs": [
                "To inspect pQEC procedure, first, we should state conditions which determine when given noise channel is probabilistically correctable. For deterministic QEC, such conditions have been known for a long time and in the literature as the Knill-Laflamme conditions [29]. Let E = K ((E i ) i ) \u2208 C(Y) be a given noise channel. Then, according to the Knill-Laflamme Theorem, E is perfectly correctable for X if and only if",
                "for all i, j and some isometry operator S \u2208 M(X , Y). In the following theorem we generalize the above, to cover probabilistically correctable noise channels.",
                "Theorem 1 (Equivalent conditions for pQEC).",
                "The following conditions are equivalent:",
                "(A) There exist error-correcting scheme (S, R) \u2208 sC(X , Y) \u00d7 sC(Y, X ) and p > 0 such that",
                "(D) There exist S * \u2208 M(X , Y) and R * \u2208 M(Y, X ) such that",
                "and there exists i 0 , for which it holds R * E i0 S * = 0.",
                "Moreover, if point (A) holds for S = K ((S k ) k ) and R = K ((R l ) l ), then R \u2208 P(Y) from points (B) and (C) can be chosen to satisfy R = l R \u2020 l R l . It also holds that R l E i S k \u221d 1l X for any i, k, l. The proof of Theorem 1 is presented in Appendix A 2. Let us discuss the meaning of the conditions stated in Theorem 1. The condition (B) presents a general form of probabilistically correctable noise channels E. Such channels, after applying post-processing \u221a R behave as mixed isometry operations. They hide parts of an initial quantum information on orthogonal subspaces. The condition (C) may be used to calculate the maximum value of the probability p of successful error correction. For r = rank(J(E)) and d = dim(X ), s = dim(Y) we can introduce the optimization procedure: maximize: tr(M )",
                "Moreover, one may get the form of a recovery subchannel R based on R, S = K ((S k ) k ) and M obtained from this optimization in the following way (see Appendix A 2):",
                "1. Let M = U \u2020 DU be the spectral decomposition of M ."
            ],
            "subsections": []
        },
        {
            "title": "For each",
            "paragraphs": [
                "4. The recovery subchannel is given as",
                ".",
                "Finally, the condition (D) gives us a simple method to check if E = K ((E i ) r i=1 ) is probabilistically correctable for X . Let us compare the point (D) with Knill-Laflamme conditions. The latter, is a constraint satisfaction problem with r 2 quadratic constrains S \u2020 E \u2020 j E i S \u221d 1l X for the variable S \u2208 M(X , Y), which satisfies S = 0. The parameters",
                "In comparison, the conditions in the point (D) represent a constraint satisfaction problem with r bilinear constrains RE i S \u221d 1l X for the variables S \u2208 M(X , Y) and R \u2208 M(Y, X ). Additionally, it must hold RE i0 S = 0 for some i 0 \u2208 {1, . . . , r}. In this problem, the parameters E i are arbitrary operators from M(Y), which satisfy span im(E \u2020 i ) : i = 1, . . . , r = Y (although a stronger condition holds i E \u2020 i E i = 1l Y , we will see in Section VI, it is more convenient to use the weaker version)."
            ],
            "subsections": []
        },
        {
            "title": "IV. REALIZATION OF PQEC PROCEDURE",
            "paragraphs": [
                "In this section, we will investigate the form of error-correcting scheme (S, R) which provides the maximal probability of successful error correction. For perfectly correctable noise channels, the encoding S can be realized by the isometry channel. This observation meaningfully reduces the complexity of finding error-correcting schemes -it is enough to consider a vector representation of pure states. Inspired by that, we ask if a similar behavior occurs in the probabilistic quantum error correction. The following proposition gives us some insight in the form of encoding and decoding.",
                "Proposition 2. For a given channel E \u2208 C(Y), let us fix an error-correcting scheme (S, R) \u2208 sC(X , Y) \u00d7 sC(Y, X ) such that RES = pI X , for some p > 0. Then, the following holds: The proof of Proposition 2 is presented in Appendix A 3. We may use Proposition 2 (A) to state a realization of pQEC procedure (see Figure 1). For a given noise channel E \u2208 C(Y) let (S, R) \u2208 C(X , Y) \u00d7 sC(Y, X ) be an error-correcting scheme for which RES = pI X , where p > 0. The encoding channel S can be realized using the Stinespring representation given in the form S(X) = tr 2 U S XU \u2020 S . The state is then sent through E. The decoding subchannel R \u2208 sC(Y, X ) can be realized by implementing the channel",
                "In summary, the output of the whole procedure consists of a quantum state \u03c3 \u2208 D(X ) and a classical label i \u2208 {0, 1}. If the label i = 0 is obtained, we know that \u03c3 \u221d RES(\u03c1) = p\u03c1, and hence, the output state can be accepted. Otherwise, if i = 1, the output state \u03c3 \u221d \u03a8ES(\u03c1) should be rejected, as in general it may differ from \u03c1.",
                "In Proposition 2 (C), we observed that using non-isometric channels S or formal subchannels R for perfectly correctable noise channels provides no advantage. Moreover, according to Theorem 1 (D), to predict if a noise channel is probabilistically correctable, we may consider only single Kraus encoding operations. However, among all conditions presented in Proposition 2 there is no condition, which in general allows us to restrict our attention to an isometry channel realization of S. Indeed, there is a class of noise channels E for which, in order to maximize the probability p of successful error correction, we need to consider a general channel realization of S. Paraphrasing, to obtain the best performance, we have to encode the initial state |\u03c8 \u03c8| \u2208 D(X ) into the mixed state S(|\u03c8 \u03c8|). In Section V we will present a family of noise channels for which it is necessary to use mixed state encoding."
            ],
            "subsections": []
        },
        {
            "title": "V. NEED FOR MIXED STATE ENCODING",
            "paragraphs": [
                "In this section, we provide an example of a parametrized family of noise channels {E R } R for which the mixed state encoding improves the probability of successful error correction. In our example we assume that X = C 2 and Y = C 4 . For each R \u2208 P(C 4 ) satisfying R \u2264 1l C 4 let us define a noise channel E R \u2208 C(C 4 ) given by the equation",
                "We define the optimal probability p 0 of successful error correction as",
                "We also define the optimal probability p 1 of successful error correction restricted to the pure state encoding:",
                "Our claim, which we will present later, is that there exists a family of operators R for which p 0 (R) > p 1 (R).",
                "We start with the following lemma, where we show the optimal error-correcting scheme (S, R) and a simplified version of the maximization problem p 0 (R).",
                "Lemma 3. Let R \u2208 P(C 4 ) and R \u2264 1l C 4 . Define \u03a0 R as a projector on the support of R. For E R defined in Eq. (11) we have the following simplified form of the maximization problem p 0 (R):",
                "An optimal scheme (S, R) which achieves the probability p 0 (R), that is RE R S = p 0 (R)I C 2 , can be taken as",
                "where P is an argument maximizing p 0 (R) in Eq. ( 14). Moreover, if there exists another optimal scheme ( S, R), that is RE R S = p 0 (R)I C 2 , then rank(J(S)) \u2264 rank(J( S)).",
                "The proof of Lemma 3 is presented in Appendix A 4. Let us separately consider two cases: rank(R) < 4 and rank(R) = 4. The first one will be discussed briefly as it will not support our claim.",
                "Corollary 4. Let us take R \u2208 P(C 4 ) such that R \u2264 1l C 4 and rank(R) < 4. Define \u03a0 R as a projector on the support of R. For the noise channel defined in Eq. (11) we have p 0 (R) = p 1 (R). Moreover, it holds",
                ") where R -1 denotes Moore-Penrose pseudo-inverse.",
                "The proof of Corollary 4 is presented in Appendix A 5. In the case when the operator R is invertible, the situation is more interesting. Let us focus on p 0 (R) obtained in Eq. ( 14). As \u03a0 R = 1l C 4 , the equation \u03a0 R (P \u2297 X)\u03a0 R = P \u2297 X is always satisfied. We can take P = tr(P )\u03c1, for \u03c1 \u2208 D(C 2 ). The inequality tr(P )",
                "\u221e . Hence, we get",
                "To calculate p 1 (R) it will be sufficient to add the constraint S = K ((S)). According to Lemma 3 the optimal S is of the form",
                "Then, we have",
                "Proposition 5. Let us define an unitary matrix U \u2208 U(C 4 ) which columns form the magic basis [32]",
                "Let us also define a diagonal operator D(\u03bb) := diag \u2020 (\u03bb), which is parameterized by a 4-dimensional real vector \u03bb = (\u03bb 1 , \u03bb 2 , \u03bb 3 , \u03bb 4 ), for which it holds 0 < \u03bb i \u2264 1. For R = U D(\u03bb)U \u2020 and the noise channel E R defined in Eq. (11) we have",
                "The proof of Proposition 5 is presented in Appendix A 6. We can clearly see that in the case rank(R) = 4, there are operators R, for which the mixed state encoding improves the probability of successful error correction over the pure state encoding, p 0 (R) > p 1 (R). In general, the maximization problem in Eq. ( 17) intuitively supports the inequality",
                "so it is possible, that the minimal value of it will be achieved for some mixed state \u03c1. We observed such behavior in Proposition 5 for R given in the spectral decomposition R = U D(\u03bb)U \u2020 . The introduced family of noise channels is parameterized by a 4-dimensional vector \u03bb = (\u03bb 1 , . . . , \u03bb 4 ), such that \u03bb i \u2208 (0, 1]. For almost all such \u03bb we have p 0 (R) > p 1 (R). The only exception is the 3-dimensional subset defined by the relation",
                "which describes the situation, when the pure state encoding match the mixed state encoding, p 0 (R) = p 1 (R). In an extremal case, e.g.",
                "The family of parameters R introduced in Proposition 5 is not the only one for which the minimum value of",
                "Therefore, the value of p 0 (R) is one-to-one related with the maximum value of the output min-entropy of the channel \u03a6 (see for instance [33]). Especially, we can see, if the image of the Bloch ball under \u03a6 is a three dimensional ellipsis and contains the maximally mixed state \u03c1 * 2 in its interior, then the mixed state encoding provides benefits.",
                "Finally, the noise channel E R defined for R from Proposition 5 is perfectly correctable for X = C 2 if and only if R = 1l C 4 . Interestingly, this suggests that perfectly correctable noise channels may constitute only a small subset of probabilistically correctable noise channels. This behavior will be the object of our investigation in the next section."
            ],
            "subsections": []
        },
        {
            "title": "VI. ADVANTAGE OF PQEC PROCEDURE",
            "paragraphs": [
                "The goal of this section is to show that pQEC procedure corrects a wider class of noise channels than the QEC procedure based on Knill-Laflamme conditions Eq. ( 6). For any Euclidean spaces X , Y let us define two families of noise channels; these which are probabilistically correctable for X as \u03be(X , Y), and these which are correctable perfectly for X as \u03be 1 (X , Y):",
                "We begin our analysis with some observations. Proposition 6. For any X , Y we have the following properties:",
                "The proof of Proposition 6 is presented in Appendix A 7. We see that if dim(X ) = dim(Y), then there is no need to consider pQEC procedure. The situation changes if we encode the initial information into a larger space, dim(Y) > dim(X ). In the following theorem, we will show that \u03be 1 (X , Y) \u03be(X , Y) for dim(Y) > dim(X ). Theorem 7. Let X and Y be Euclidean spaces for which dim(X ) < dim(Y). Then, the set \u03be 1 (X , Y) is a nowhere dense subset of \u03be(X , Y).",
                "The proof of Theorem 7 is presented in Appendix A 8."
            ],
            "subsections": []
        },
        {
            "title": "A. Choi rank of correctable noise channels",
            "paragraphs": [
                "Intensity of a noise channel E can be connected with its Choi rank r = rank(J(E)). Given E in the Stinespring form, the Choi rank describes the dimension of an environment system which unitarily interacts with the encoded information. If the interaction is the weakest (r = 1) we deal with unitary noise channels, which are always perfectly correctable. The strongest interaction (r = dim(Y) 2 ) is a property of hardly correctable noise channels. For example, the maximally depolarizing channel E(Y ) = tr(Y )\u03c1 * Y , which can not be corrected, has the maximal Choi rank. In the following theorem, we investigate the maximum Choi rank of probabilistically correctable noise channels \u03be(X , Y) and compare it with the maximum Choi rank for \u03be 1 (X , Y). Theorem 8. Let X and Y be some Euclidean spaces such that dim(Y) \u2265 dim(X ). The following relations hold:",
                "The proof of Theorem 8 is presented in Appendix A 9. In Proposition 6 we showed that if dim(X ) = dim(Y), then the pQEC procedure gives us no advantage. Indeed, the only reversible noise channels, in this case, are unitary noise channels. In the language of Choi rank, that means, if the Choi rank of a noise channel is equal to one, then it can be corrected. We can ask, what is the maximum value of r \u2208 N, such that all noise channels which Choi rank is less or equal r, can be corrected perfectly or probabilistically, respectively. Formally speaking, for any X and Y we define the following quantities:",
                "The quantity r 1 (X , Y) for a general noise model was studied in [34,35]. The authors of [34] calculated a lower bound for r 1 (X , Y) by using a technique of noise diagonalization along with Tverberg's theorem. They obtained the following result",
                "It implies that 4 dim(Y) dim(X ) \u2264 r 1 (X , Y). On the other hand, by using the Quantum packing bound [35] we may gain some insight of the upper bound for r 1 (X , Y). If we assume that we are allowed to use only non-degenerated codes, then for perfectly correctable E we have a bound of the form rank(J(E)) \u2264 dim(Y) dim(X ) . In the next part of this section, we will improve the upper bound of r 1 (X , Y) without putting any additional assumptions. We also will estimate the behavior of r(X , Y). In the particular case X = C 2 and Y = C 4 , we will also show that r 1 (X , Y) < r(X , Y).",
                "Let us start with the following simple, but important properties, required to study r(X , Y). We will notice, that for a constant Choi rank of the noise, it is easier to construct error-correcting scheme, if the dimension of Y is large.",
                "Directly from Lemma 9 we receive the monotonicity of r(X ,",
                "There exist two projectors \u03a0 1 , \u03a0 2 \u2208 P(Y ), such that rank(\u03a0 1 ) = rank(\u03a0 2 ) = dim(Y) and for F = K ((\u03a0 2 E i \u03a0 1 ) i ) we have rank(tr 1 (J(F))) = dim(Y). Hence, if there exists a scheme (S, R) such that 0 = RFS \u221d I X , then E \u2208 \u03be(X , Y ). Eventually, we have"
            ],
            "subsections": []
        },
        {
            "title": "B. Schur noise channels",
            "paragraphs": [
                "In this subsection, we restrict our attention to a particular family of noise channels whose Kraus operators are diagonal in the computational basis. In the literature, these channels are referred to as Schur channels [30,Theorem 4.19]. We use them to study an upper bound for r(X , Y) and r 1 (X , Y).",
                "Lemma 10. Let X and Y be Euclidean spaces such that dim(Y) \u2265 dim(X ). Then, there exists a Schur channel",
                "The proof of Lemma 10 is presented in Appendix A 10. The bounds obtained in Lemma 10 are asymptotically tight for Schur noise channels with dim(Y) \u2192 \u221e. To prove the tightness of the bound for perfectly correctable noise channels, we may use the construction provided in [34]. Hence, if we take a Schur channel",
                "In the following proposition we will prove the tightness for probabilistically correctable Schur noise channels.",
                "Proposition 11. Let X and Y be Euclidean spaces and dim(X ) \u2264 dim(Y). For any Schur channels",
                "The proof of Proposition 11 is presented in Appendix A 11. In the case of Schur channels we have a clear separation between probabilistically and perfectly correctable noise channels."
            ],
            "subsections": []
        },
        {
            "title": "C. From bi-linear to linear problem",
            "paragraphs": [
                "In general, the difficulty of finding error-correcting schemes (S, R) comes from bi-linearity of the problem Eq. (10). However, there is a particular class of noise channels, for which we can easily rewrite the bi-linear problem as a linear one. In this subsection, we will focus our attention on noise channels E \u2208 C(Y), such that rank(E(1l Y )) = dim(X ). Note, that this assumption implies dim(X )rank(J(E)) \u2265 dim(Y).",
                "Let E = K ((E i ) i ) and let \u03a0 be the projector on the image of E(1l Y ). Consider an associated channel",
                ") is an isometry operator with the image on the subspace defined by \u03a0. It is clear that E is probabilistically correctable for a given space X if and only if there exists a scheme (S, R), such that 0 = RFS \u221d I X . Hence, according to Theorem 1 we need to find S * \u2208 M(X , Y), R * \u2208 M(X ), such that R * F i S * = c i 1l X and c i0 = 0 for some i 0 . Interestingly, we can combine together an action of S * , R * as just the action of some pre-processing S * \u2208 M(X , Y), that is",
                "Therefore, we obtained a linear problem equivalent to Eq. ( 10). In the following proposition we will investigate consequences of a such simplification.",
                "Proposition 12. Let X and Y be some Euclidean spaces and dim(X ) \u2264 dim(Y).",
                "(B) There exists a noise channel E \u2208 C(Y) such that rank(E(1l Y )) = dim(X ) and rank(J(E)) \u2265 dim(Y) dim(X ) dim(X ) 2 -1 , for which we have E \u2208 \u03be(X , Y).",
                "The proof of Proposition 12 is presented in Appendix A 12. Eventually, it is worth mentioning that the QEC procedure based on Knill-Laflamme conditions works well with this class of noise channels. Consider the situation dim(X )rank(J(E)) = dim(Y). Then, if E \u2208 C(Y) and rank(E(1l Y )) = dim(X ), it holds E \u2208 \u03be 1 (X , Y). To see this, take the Kraus decomposition of E = K ((E i )) and notice that operators E i are orthogonal pieces of some unitary operator."
            ],
            "subsections": []
        },
        {
            "title": "D. Correctable noise channels with bounded Choi rank",
            "paragraphs": [
                "In this subsection we will study the behavior of r(X , Y) and r 1 (X , Y). We will state a lower and a upper bound for both quantities.",
                "Theorem 13. Let X and Y be some Euclidean spaces such that dim(Y) \u2265 dim(X ). Then, we have",
                "The proof of Theorem 13 is presented in Appendix A 13. Unfortunately, according to this theorem, there is no clear separation of r(X , Y) and r 1 (X , Y) for arbitrary X and Y. The improvement of these bounds will be investigated in the future.",
                "For now, we will calculate explicitly r(X , Y) and r 1 (X , Y) for X = C 2 and Y = C 3 , C 4 .",
                "Proposition 14. For all E \u2208 C(C 4 ) satisfying rank(J(E)) \u2264 2 we have E \u2208 \u03be(C 2 , C 4 ).",
                "The proof of Proposition 14 is presented in Appendix A 14. By using Theorem 13 and Proposition 14 we get the following advantage of pQEC protocol for X = C 2 and Y = C 4 .",
                "Corollary 15. For X = C 2 and Y = C 4 we have",
                "In particular, it holds"
            ],
            "subsections": []
        },
        {
            "title": "E. Random noise channels",
            "paragraphs": [
                "In the last subsection, we will show the advantage of pQEC procedure for randomly generated noise channels. We will follow the procedure of sampling quantum channels considered in [36][37][38].",
                "Let r \u2208 N and let (G i ) r i=1 \u2282 M(Y) be a tuple of random and independent Ginibre matrices (matrices with independent and identically distributed entries drawn from standard complex normal distribution). Define",
                "We define a random channel E r \u2208 C(Y) given as",
                "This sampling procedure induces the measure P on C(Y) whose support is defined on {E \u2208 C(Y) : rank(J(E)) \u2264 r}.",
                "Theorem 16. Let E r \u2208 C(Y) be a random quantum channel defined according to Eq. (32). Then, the following two implications hold",
                "The proof of Theorem 16 is presented in Appendix A 15. To answer this question, observe that the channel E satisfies rank(J(E)) \u2264 2. In Proposition 14 we noticed that such channels are probabilistically correctable for a given input space C 2 , if dim(Y) = 4 (in fact, from monotonicity for dim(Y) \u2265 4). Therefore, to correctly transfer a qubit state through E, we may define an error-correcting scheme with only two physical qubits.",
                "We provide the following pQEC procedure based on Proposition 14.",
                "Algorithm 17: Probabilistic QEC qubit code Input: E \u2208 C(C 4 ) such that rank(J(E)) \u2264 2. Output: pQEC procedure with success probability p > 0.",
                "Run the QEC procedure presented in Figure 2 for |\u03c8 , U S , U R , V R . 10 Let \u03c3 exp be the output state of the procedure presented in Figure 2. Use the post-processing of the measurements' output (i, j) according to the following table:",
                "Figure 2: The circuit representing the pQEC procedure. We have access to two physical qubits. The first qubit is in the state |\u03c8 . This state will be encoded. The second state we set |0 . We implement two-qubit, encoding unitary operator U S . Then, the encoded state U S (|\u03c8 \u2297 |0 ) is affected by the noise channel E. After that, we start the decoding procedure. We implement two-qubit unitary rotation U R . We measure the second qubit in the standard basis and obtain a classical label i \u2208 {0, 1}. We prepare a third qubit in the state |0 and implement two qubit unitary rotation V R . We measure the third qubit in the standard basis and obtain a classical label j \u2208 {0, 1}. If (i, j) = (0, 0) we accept the output state, otherwise, we reject it."
            ],
            "subsections": []
        },
        {
            "title": "VIII. GENERALIZATION OF PQEC PROCEDURE",
            "paragraphs": [
                "Let us denote by \u03a5 an arbitrary family of noise channels, that is \u03a5 \u2282 C(Y). In this section, we ask if there exists error-correcting scheme (S, R), such that all noise channels E \u2208 \u03a5 we have RES = p E I X , for some p E \u2265 0. Note, that p E may differ for different noise channels E, hence, we shall introduce a quantity to \"globally\" control the effectiveness of (S, R). We propose the following approach.",
                "Let \u00b5 be some probability measure defined on the set \u03a5. We assume that noise channels E \u2208 \u03a5 are probed according to \u00b5. The scheme (S, R) will be a valid error-correcting scheme for \u03a5 and \u00b5 if in average, the probability of successful error correction is non zero, that is",
                "Without loss of the generality we may assume that \u03a5 is convex. Additionally, we assume that the support of \u00b5 is equal to \u03a5. Usually, we can take \u00b5 as the flat measure, representing the maximal uncertainty in the process of probing random noise channels E from \u03a5. Let us define the average noise channel of \u03a5 with respect to \u00b5",
                "We will show that we can correct all noise channels from the family \u03a5, whenever \u0112 is probabilistically correctable for X . We put this statement as the following proposition.",
                "Proposition 18. Let \u03a5 \u2282 C(Y) be a nonempty and convex family of noise channels. Define \u00b5 to be a probability measure defined on \u03a5 and assume that the support of \u00b5 is equal to",
                "The following conditions are equivalent:",
                "The proof of Proposition 18 is presented in Appendix A 16."
            ],
            "subsections": []
        },
        {
            "title": "IX. DISCUSSION",
            "paragraphs": [
                "In this work, we analyzed pQEC procedure for a general noise model. We established the conditions to check if a given noise channel is probabilistically correctable. Moreover, we showed that mixed state encoding should be taken into account when maximizing the probability of successful error correction. Finally, we pointed the advantage of the probabilistic error-correcting procedure over the deterministic one. We saw a clear separation especially for a correction of Schur noise channels and random noise channels. We obtained the maximum value of Choi rank of probabilistically correctable noise channels. We also provide a method how to probabilistically correct noise channels with bounded Choi rank.",
                "There are many directions for further study that still remain to be explored. It would be interesting to strengthen Theorem 13 and show the separation between r(X , Y) and r 1 (X , Y) by improving the proposed proof technique in Appendix A 13. We obtained such separation for X = C 2 and Y = C 4 in Corollary 15. Another promising direction is to propose tools for the numerical analysis of pQEC protocols, based on Theorem 1. Such tools would help us estimate the value of r(X , Y) and gain an insight into probabilistically correctable noises that require mixed state encoding. Last but not least, we would like to calculate the worst-case probability of successful error correction for a given noise intensity r \u2264 r(X , Y). For example, as we showed in Proposition 14, the errors caused by a unitary interaction with an auxiliary qubit system (r = 2), can be corrected by using only two physical qubits (dim(Y) = 4). We can ask, how many times in average the procedure presented in Algorithm 17 needs to be repeated. and there exists i 0 , for which it holds R * E i0 S * = 0.",
                "Moreover, if point (A) holds for S = K ((S k ) k ) and R = K ((R l ) l ), then R \u2208 P(Y) from points (B) and (C) can be chosen to satisfy R = l R \u2020 l R l . It also holds that R l E i S k \u221d 1l X for any i, k, l. Proof. In order to show that (A) \u21d0\u21d2 (B) \u21d0\u21d2 (C), in all implications presented below, we will use the same encoding S = K ((S k ) k ) \u2208 sC(X , Y). Hence, to simplify the proof, we introduce the notation of F := ES given in the form",
                "We will check that R is a subchannel. First, from the definition of R, it follows that R is completely positive. Second, from the assumption (B), operators \u03b1 -1 i A i A \u2020 i \u2208 P(Y) are orthogonal projectors and hence",
                "It means that R \u2208 sC(Y, X ). Finally, it holds",
                "Hence, we get S \u2020 k0 S k0 = p k0 1l X . Define S = 1",
                "Therefore, for any |\u03c8 \u03c8| \u2208 D(X ) we get",
                "Observe that RES = I X . The rest of the proof follows from (B)."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Lemma 3",
            "paragraphs": [
                "Lemma 3. Let R \u2208 P(C 4 ) and R \u2264 1l C 4 . Define \u03a0 R as a projector on the support of R. For E R defined as",
                "we have the following simplified form of the maximization problem p 0 (R):",
                "An optimal scheme (S, R) which achieves the probability p 0 (R), that is RE R S = p 0 (R)I C 2 , can be taken as",
                "where P is an argument maximizing p 0 (R) in Eq. (A25). Moreover, if there exists another optimal scheme ( S, R), that is RE R S = p 0 (R)I C 2 , then rank(J(S)) \u2264 rank(J( S)).",
                "Proof. Let us investigate the form of an optimal scheme (S, R) that maximize the probability p of successful error correction, RE R S = pI C 2 . First, one can note that R must be of the form",
                "We obtain pI C 2 = RE R S = RF. From Theorem 1 we have R k F i \u221d 1l C 2 and there are k 0 , i 0 such that R k0 F i0 = 0. Hence, for each k we have R k \u221d F -1 i0 . That implies the operation R can be written as R(X) = RX R \u2020 . Now, consider another scheme (S , R ), where",
                "Therefore, the scheme (S , R ) is also optimal and rank(J(S )) \u2264 rank(J(S)).",
                "To sum up, from now, we will consider the optimal scheme (S, R), where R(Y ) = tr 1 (Y (|0 0| \u2297 1l C 2 )). The equation RE R S = pI C 2 can be rewritten as",
                "for any X \u2208 M(C 2 ). According to Theorem 1 we have",
                "Without loss of the generality we may consider S such that \u03a0 R S(X)\u03a0 R = S(X) (one can note that rank(J(S)) will not increase). Hence, the equation",
                "Therefore, basing on Eq. (A28) we can express the probability p 0 (R) as:",
                "= max tr(P ) :",
                "(A29)"
            ],
            "subsections": []
        },
        {
            "title": "Proof of Corollary 4",
            "paragraphs": [
                "Corollary 4. Let us take R \u2208 P(C 4 ) such that R \u2264 1l C 4 and rank(R) < 4. Define \u03a0 R as a projector on the support of R. For the noise channel defined as",
                "we have p 0 (R) = p 1 (R). Moreover, it holds",
                "Proof. The proof is based on Lemma 3. Let us investigate the value of p 0 (R). We will consider three cases depending on rank(R).",
                "In the first case, we assume that rank(R) \u2208 {0, 1}. Then, for P satisfying \u03a0 R (P \u2297 X)\u03a0 R = P \u2297 X we have",
                "Hence, we obtain rank(P ) \u2264 1 2 which implies P = 0. In this case p 0 (R) = 0. In the second case, we assume that rank(R) = 2. Using the same argumentation for P as in the first case, we get rank(P ) \u2264 1. We can write P = |x x| for |x \u2208 C 2 . Note that, if P = 0, then from the equality \u03a0 R |x, y = |x, y for |y \u2208 C 2 we get \u03a0 R = |\u03c8 \u03c8| \u2297 1l C 2 , for |\u03c8 = 1",
                "x |x . Therefore, if for all",
                "\u221e . In the third case, we assume that rank(R) = 3. Again, P can be written in the form ",
                "6. Proof of Proposition 5",
                "Proposition 5. Let us define an unitary matrix U \u2208 U(C 4 ) which columns form the magic basis",
                "Let us also define a diagonal operator D(\u03bb) := diag \u2020 (\u03bb), which is parameterized by a 4-dimensional real vector \u03bb = (\u03bb 1 , \u03bb 2 , \u03bb 3 , \u03bb 4 ), for which it holds 0 < \u03bb i \u2264 1. For R = U D(\u03bb)U \u2020 and the noise channel E R defined as",
                "we have",
                "Proof. First, we calculate p 0 (R). Let |x = (x 0 , x 1 ) . Then, we have",
                "). Eventually, we obtain the following upper bound",
                "That means, p 0 (R) \u2264 4 tr(R -1 ) -1 . To saturate this bound, we take the maximally mixed state \u03c1 = \u03c1 * 2 and by using Eq. (A36) we calculate",
                "Therefore, we showed that p 0 (R) = 4 tr(R -1 ) -1 .",
                "In the case of p 1 (R), to calculate the largest eigenvalue of tr",
                "One may calculate that the largest eigenvalue minimized over \u03b1 is given by",
                "It turns out, there are only two situations when this expression is minimized:",
                "\u2022 For |x 0 | = 0 and |x 1 | = 1 (or equivalently |x 0 | = 1 and |x 1 | = 0), we obtain",
                "Hence, the optimal value p 1 (R) equals (A) Take E = K ((E i ) r i=1 ) \u2208 \u03be 1 (X , Y), where r = rank(J(E)). From Proposition 2 there exist S = K ((S)) \u2208 C(X , Y) and R \u2208 C(Y, X ) such that RES = I X . According to Theorem 1 it holds",
                "If r < r, then let us define A i = 0 for i = r + 1, . . . , r. There exists the Kraus decomposition E = K ((E i ) r i=1 ) such that A i = E i S for each i \u2264 r. For A i = 0 images of A i are orthogonal and rank(A i ) = d. Hence, r d \u2264 s which is equivalent to r \u2264 k. For i > r it holds 1l Y \u2297 S |E i = 0. Note that the Kraus operators E i are linearly independent and it holds ",
                "where",
                ", where r = rank(J(E)). According to Theorem 1 (D) there exist S * \u2208 M(X , Y) and R * \u2208 M(Y, X ) such that R * E i S * \u221d 1l X , and there exists i 0 for which it holds R * E i0 S * = 0. We may assume that R * \u221e \u2264 1 and S * \u221e \u2264 1. Hence, according to Theorem 1 (B) we get",
                "If r < r, then let us define A i = 0 for i = r + 1, . . . , r. There exists the Kraus decomposition",
                "Let \u03a0 be the projector on the support of R \u2020 * R * . Observe that rank(\u03a0) = d. Then, for each i \u2264 r we have \u03a0A i = A i and for i \u2264 r we have rank(A i ) = d. The relation A \u2020 j A i \u221d \u03b4 ij 1l X implies that there exists exactly one A i = 0, hence, r = 1.  ",
                "for E a defined in Eq. (A58). Observe that F i are linearly independent. We have that l-1 i=0",
                "Now, we introduce a Schur channel F = K (F i ) l-1 i=0 \u2208 C(Y). Assume indirectly that F \u2208 \u03be 1 (X , Y). Then, according to Proposition 2 and Theorem 1 there exists S \u2208 M(X , Y), which satisfies S \u2020 S = 1l X and M \u2208 M(C l ), such that S \u2020 F \u2020 j F i S = M ji 1l X . Therefore, we get Proof. Let \u2206 \u2208 C(Y) be the maximally dephasing channel, that is \u2206(Y ) = i |i i|Y |i i|. Let us fix r such that r < dim(Y) dim(X )-1 . We will show that if E = K ((E i )) \u2208 C(Y), such that E i = \u2206(E i ) for each i and rank(J(E)) \u2264 r, then E \u2208 \u03be(X , Y). Observe that the thesis is true in two particular situations:"
            ],
            "subsections": []
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "paragraphs": [
                "This work was supported by the project \"Near-term Quantum Computers: challenges, optimal implementations and applications\" under Grant Number POIR.04.04.00-00-17C1/18-00, which is carried out within the Team-Net programme of the Foundation for Polish Science co-financed by the European Union under the European Regional Development Fund."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Theorem 1",
            "paragraphs": [
                "Theorem 1. Let E = K ((E i ) i ) \u2208 C(Y). The following conditions are equivalent:",
                "(A) There exist error-correcting scheme (S, R) \u2208 sC(X , Y) \u00d7 sC(Y, X ) and p > 0 such that RES = pI X .",
                "(A1) (B) There exist S = K ((S k ) k ) \u2208 sC(X , Y) and R \u2208 P(Y), such that R \u2264 1l Y , for which it holds"
            ],
            "subsections": []
        },
        {
            "title": "A2)",
            "paragraphs": [
                "(C) There exist S = K ((S k ) k ) \u2208 sC(X , Y), R \u2208 P(Y), such that R \u2264 1l Y and a matrix M = [M jl,ik ] jl,ik = 0, for which it holds",
                "(D) There exist S * \u2208 M(X , Y) and R * \u2208 M(Y, X ) such that",
                "where we introduced p := i \u03b1 i > 0.",
                "(A) =\u21d2 (B)",
                "Define \u03a0 R to be the projector on the support of R. One can show that R k \u03a0 R = R k for each k. We define R = K R k k , where",
                "we get pI X = RF = R \u2022 K ( \u221a RF i ) i . As we have p > 0, it follows that K ( \u221a RF i ) i = 0. Hence, there exists a canonical decomposition",
                "From the relationship between Kraus representations, it follows that A i satisfy \u03a0 R A i = A i . Then, by Choi-Jamio\u0142kowski isomorphism we have",
                "Therefore, from the extremality of the point |1l X 1l X | in P(X \u2297 X ) we have",
                "for any i, k. On the one hand we get",
                "and on the other hand",
                "The above conditions provide that A \u2020 j A i = c ji 1l X , for some c ji \u2208 C. Then, for i = j we have 0 = tr(A \u2020 j A i ) = c ji dim(X ) and eventually",
                "From the relationship between Kraus decompositions K ( \u221a RF i ) i and K ((A i ) i ), there exists isometry operator U , such that",
                "Therefore, it holds",
                "That implies M \u2265 0. Take the spectral decomposition M = U \u2020 DU and define",
                "Finally, A i = 0 if and only if D ii > 0 and by the fact M = 0 we conclude the set {A i : A i = 0} is not empty.",
                "Using the Choi-Jamio\u0142kowski isomorphism we get",
                "Therefore, from the extremality of the point |1l X 1l X | in P(X \u2297 X ) we obtain R l E i S k \u221d 1l X . There exist l 0 , i 0 , k 0 such that R l0 E i0 S k0 = 0. We can take S * = S k0 and R * = R l0 .",
                "(D) =\u21d2 (A) There exist q 0 , q 1 > 0 for which S := q 0 K ((S * )) \u2208 sC(X , Y) and R := q 1 K ((R * )) \u2208 sC(Y, X ). One may note that 0 = RES \u221d I X ."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Proposition 2",
            "paragraphs": [
                "Proposition 2. For a given channel E \u2208 C(Y), let us fix an error-correcting scheme (S, R) \u2208 sC(X , Y) \u00d7 sC(Y, X ) such that RES = pI X , for some p > 0. Then, the following holds:",
                "Using Theorem 1 one can show that there exists k 0 for which rank(S k0 ) = dim(X ). Hence, S is invertible. Define S \u2208 C(X , Y), R \u2208 sC(Y, X ) given by the equations",
                "From Theorem 1 there exists k 0 such that RES k0 = p k0 I X , for some p k0 > 0. For any |\u03c8 \u03c8| \u2208 D(X ) it holds then",
                "and there exists i 0 for which it holds R * E i0 S * = 0. It implies that R * and S * are invertible, so for all i we have"
            ],
            "subsections": []
        },
        {
            "title": "Proof of Theorem 7",
            "paragraphs": [
                "Theorem 7. Let X and Y be Euclidean spaces for which dim(X ) < dim(Y). Then, the set \u03be 1 (X , Y) is a nowhere dense subset of \u03be(X , Y).",
                "Proof. First, we will prove that \u03be",
                "As dim(X ) < dim(Y), there exists |y y| \u2208 D(Y) such that y|A 1 = 0. Let us define a sequence of channels",
                "given by",
                "One can note that lim n\u2192\u221e E n = E. We take S n = S and R n = K (A \u2020 1 ) for n \u2208 N and obtain",
                "Hence, we obtain E n \u2208 \u03be 1 (X , Y)."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Theorem 8",
            "paragraphs": [
                "Theorem 8. Let X and Y be some Euclidean spaces such that dim(Y) \u2265 dim(X ). The following relations hold:",
                "We may assume that rank(J(E)) = r. Therefore, there exists a projector \u03a0 \u2208 P(Y), such that rank(\u03a0) = r and \u2206(\u03a0) = \u03a0, and for which the operators \u03a0E i \u03a0 are linearly independent. Let us consider the operation",
                ". By the recurrence and Theorem 1 for operation F there exist S * \u2208 M (X , Y) and R * \u2208 M (Y, X ), such that R * \u03a0 \u22a5 E i \u03a0 \u22a5 S * = c i 1l X and c i0 = 0 for some i 0 . Let |s \u2208 C(Y) be the flat superposition. As \u03a0E i \u03a0 are diagonal and linearly independent, there exists the vector |r such that r|\u03a0E i \u03a0|s = c i . We may define an encoding operator S * by adding a column \u03a0|s to the operator \u03a0 \u22a5 S * . In the same manner, we may construct R * by adding a row r|\u03a0 to the operator R * \u03a0 \u22a5 . It is easy to check that S * , R * satisfy Theorem 1 (D), so E \u2208 \u03be(X , Y).",
                "12. Proof of Proposition 12 Proposition 12. Let X and Y be some Euclidean spaces and dim(X ) \u2264 dim(Y).",
                ", where r = rank(J(E)). Assume that rank(E(1l Y )) = dim(X ) and r < dim(Y) dim(X ) dim(X ) 2 -1 . We can consider the equivalent form of the problem by taking the associated channel",
                "As rank(F ) = dim(Y), the subspace {(F \u2297 1l X )|S : |S } has the dimension dim(Y) dim(X ). On the other hand, the subspace {|c \u2297 |1l X : |c } has the dimension r. Therefore, as long as",
                "there exists non-zero solution S \u2208 M(X , Y) and |c \u2208 C r , such that",
                "we obtain E \u2208 \u03be(X , Y).",
                "(B) In the part (A) of the proof we showed that",
                "Therefore, in this proof, we will construct appropriate operator F , such that the latter condition holds. It would imply that the associated channel E is not probabilistically correctable. Formally, the operator F should be an isometry operator, but by Lemma 9, it is enough to define F such that rank(F ) = dim(Y). Let d = dim(X ), s = dim(Y) and fix r \u2208 N, such that r \u2265 sd d 2 -1 . We start with the case s = kd for k \u2208 N. Consider the decomposition F = r-1 i=0 |i \u2297 F i , where F i \u2208 M(Y, X ). For i = 0, . . . , k -1 we define",
                "\u2282 M(X ) be a basis of M(X ). For each i = k, . . . , r -1 we define",
                "Observe, that rank(F ) = s. Let us take S which satisfies F i S \u221d 1l X for each i. Basing on the equations with indices i = 0, . . . , k -",
                "hence, all entries c j are zeroed. It implies S = 0.",
                "The case s = kd + l for l = 1, . . . , d -1 is more technically engaging than the previous case but it is based on the same idea. It will be only briefly discussed. For i = 0, . . . , k -1 we can define F i similarly as in the previous case, that is",
                "where the image of N is contained in span(|j : j \u2265 l). Here, the operator S which satisfy F i S \u221d 1l X has the form S \u223c |c \u2297 1l X for some |c = k j=0 c j |j . We can choose N such that d(d -l) entries c j will be zeroed if F k S \u221d 1l X . Finally, operators F i for i = k + 1, . . . , r -1 has the analogous form as Eq. (A66) -each nullify (d 2 -1) entries. In total, the number of entries c j which can be zeroed is not less than k + 1. Indeed, it holds",
                "Therefore, S = 0, which ends the proof."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Theorem 13",
            "paragraphs": [
                "Theorem 13. Let X and Y be some Euclidean spaces such that dim(Y) \u2265 dim(X ). Then, we have",
                "Proof. The inequality 4 dim(Y) dim(X ) \u2264 r 1 (X , Y) follows directly from [34]. The inequalities",
                "follow from Lemma 10 and Proposition 12, respectively. Now, we will show that dim(Y) dim(X )-1 -1 \u2264 r(X , Y). Take arbitrary E \u2208 C(Y) such that rank(J(E)) 2 (dim(X ) -1) < dim(Y). We will show E \u2208 \u03be(X , Y). Let us denote r = rank(J(E)). Consider a Kraus representation E = K (E j ) r j=1 and define the following set",
                "Observe that dim(Y) \u2208 A and if some s \u2208 A, then sr \u2265 dim(Y). Define s 0 = min(A) and consider a corresponding projector \u03a0 s0 \u2208 P(Y), such that rank(\u03a0 s0 ) = s 0 and rank(E \u2020 (\u03a0 s0 )) = dim(Y). Let us take a orthonormal collection of vectors |v i , where i = 1, . . . , s 0 for which we have",
                "From the assumption s 0 = min(A), for any i we get rank(E \u2020 (\u03a0 s0 -|v i v i |)) < dim(Y). Therefore, we may define vectors 0",
                "Observe that for each i, there exists E j for which v i |E j |w i = 0. Let us define F j = [ v a |E j |w b ] a,b=1,...,s0 for j = 1, . . . , r. Note, that F j are diagonal operators and it holds j F \u2020 j F j > 0. From r 2 (dim(X ) -1) < dim(Y) and s 0 r \u2265 dim(Y) we have",
                "Utilizing Proposition 11, Lemma 9 and Theorem 1 there exist S * \u2208 M(X , C s0 ) and R * \u2208 M(C s0 , X ), such that R * F j S * \u221d 1l X and there exists j 0 , for which it holds R * F j0 S * = 0. That implies E \u2208 \u03be(X , Y)."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Proposition 14",
            "paragraphs": [
                "Proposition 14. For all",
                "we may write the singular decomposition of E 0 , E 1 in the form:",
                "In order to show that E \u2208 \u03be(C 2 , C 4 ) we will use Theorem 1 (D). We will prove that there exist",
                "for some c 0 , c 1 \u2208 C satisfying (c 0 , c 1 ) = (0, 0). Let us introduce the following notation",
                "Note that vectors |x i are orthogonal (the same holds for |y i ) and for each i = 0, . . . , 3 we have |x i = 0 or |y i = 0. We may write S * and R * in the following form",
                "for some vectors |S 0 , |S 1 , |R 0 , |R 1 \u2208 C 4 . The rest of the prove will be divided into three cases.",
                "In the first case, we assume there exists i 3 \u2208 {0, . . . , 3} such that vectors |x i3 , |y i3 are linearly independent. Define indices i 0 , i 1 , i 2 \u2208 {0, . . . , 3} as the remaining labels, such that {i 0 , . . . , i 3 } covers the whole set {0, . . . , 3}. Let (a 0 , a 1 , a 2 ) \u2208 C 3 be a normalized vector orthogonal to vectors ( y i3 |x i0 , y i3 |x i1 , y i3 |x i2 ) \u2020 and ( x i3 |y i0 , x i3 |y i1 , x i3 |y i2 ) \u2020 . Take |S 1 = |i 3 and |S 0 = a 0 |i 0 +a 1 |i 1 +a 2 |i 2 . Define |x = a 0 |x i0 +a 1 |x i1 +a 2 |x i2 and |y = a 0 |y i0 + a 1 |y i1 + a 2 |y i2 . We obtain ",
                "Take |R 1 = b0 |x i3 + b1 |y i3 . Eventually, we may check that it holds",
                "In the second case, we assume that there exists a pair of vectors |y i0 , |y i1 for i 0 = i 1 , such that |y i0 = |y i1 = 0. Then, the vectors |x i0 , |x i1 are orthonormal. We simply define",
                "In the third case, for all i \u2208 {0, . . . , 3} vectors |x i , |y i are not linearly independent and there is at most one zero vector |y i3 for some i 3 \u2208 {0, . . . , 3}. Define indices i 0 , i 1 , i 2 \u2208 {0, . . . , 3} as the remaining labels, such that {i 0 , . . . , i 3 } covers the whole set {0, . . . , 3}. Define the matrix",
                "In the first sub-case we assume that rank(M ) = 1.",
                "In the second sub-case we assume that rank(M ) = 2. Define indices j 1 , j 2 \u2208 {0, 1, 2}, such that",
                "Define j 0 \u2208 {0, 1, 2} as the remaining label, such that {j 0 , j 1 , j 2 } covers the whole set {0, 1, 2}. Take |S 0 = |i j0 , |R 0 = |y ij 0 and define",
                "We may take",
                "15. Proof of Theorem 16",
                "Theorem 16. Let E r \u2208 C(Y) be a random quantum channel defined according to Eq. (32). Then, the following two implications hold",
                "be a tuple of random and independent Ginibre matrices and Q",
                "|i i| and consider the set",
                "One can observe that P((G i ) r i=1 \u2208 A) = 1. Let E r \u2208 C(Y) be a random channel defined according to Eq. ( 32) for",
                "Utilizing Lemma 9, Proposition 12 and Theorem 1 (D) for \u1ebc = K ((\u03a0G i ) r i=1 ) \u2208 sC(Y), there exist S, R, such that R\u03a0G i S \u221d 1l X and R\u03a0G i0 S = 0 for some i 0 . Eventually, E r \u2208 \u03be(X , Y). Now, for a given r \u2208 N let us define B = {E r : E r \u2208 \u03be 1 (X , Y)}. From the assumption P(B) = 1, we obtain that B is a dense subset of {E \u2208 C(Y) : rank(J(E)) \u2264 r}. Imitating the proof of Theorem 7, we get that if E \u2208 C(Y) and rank(J(E)) \u2264 r, then E \u2208 \u03be 1 (X , Y). That implies r \u2264 r 1 (X , Y). By using Lemma 10 we obtain the desired inequality."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Proposition 18",
            "paragraphs": [
                "Proposition 18. Let \u03a5 \u2282 C(Y) be a nonempty and convex family of noise channels. Define \u00b5 to be a probability measure defined on \u03a5 and assume that the support of \u00b5 is equal to \u03a5. Let \u0112 = \u03a5 E\u00b5(dE) \u2208 C(Y) and fix (S, R) \u2208 sC(X , Y) \u00d7 sC(Y, X ). The following conditions are equivalent:",
                "(A) For each E \u2208 \u03a5 there exists p E \u2265 0 such that RES = p E I X and \u03a5 p E \u00b5(dE) > 0.",
                "(B) It holds that 0 = R \u0112S \u221d I X ."
            ],
            "subsections": []
        },
        {
            "title": "Proof. (B) =\u21d2 (A)",
            "paragraphs": [
                "Let us assume that R \u0112S = pI X for p > 0. There exists a k dimensional affine subspace L such that \u03a5 \u2282 L and int L (\u03a5) = \u2205. Take arbitrary E 0 \u2208 \u03a5. There exist E 1 , . . . , E k \u2208 \u03a5 such that convex hull of points E 0 , . . . , E k is a k-dimensional simplex \u2206 k . For any state |\u03c8 \u03c8| \u2208 D(X ) it holds (A81)",
                "Inside \u2206 k each E can be uniquely represented as k i=0 q i (E)E i , where (q i (E)) k i=0 is a probability vector which depends on E. Hence,"
            ],
            "subsections": []
        },
        {
            "title": "(|\u03c8 \u03c8|).",
            "paragraphs": [
                "(A82)",
                "There exists small ball B around E 0 , such that for each channel E \u2208 B \u2229 \u2206 k it holds q 0 (E) \u2265 1 2 . Hence, \u2206 k q 0 (E)\u00b5(dE) \u2265 1 2 \u00b5 (B \u2229 \u2206 k ) > 0, where in the last inequality we used the fact that the support of \u00b5 is equal to \u03a5. Therefore, it holds that for any |\u03c8 \u03c8| \u2208 D(X ) we have RE 0 S(|\u03c8 \u03c8|) \u221d |\u03c8 \u03c8| and from Lemma 19 there exists p E0 \u2265 0 such that RE 0 S = p E0 I X . The instant relation \u03a5 p E \u00b5(dE) = p > 0 ends the proof."
            ],
            "subsections": []
        }
    ]
}