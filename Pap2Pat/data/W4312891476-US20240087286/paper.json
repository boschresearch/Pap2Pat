{
    "id": "https://semopenalex.org/work/W4312891476",
    "authors": [
        "Chen Wang",
        "Federico Tombari",
        "Alireza Zareian",
        "Joshua L. Moore",
        "Huseyin Coskun"
    ],
    "title": "GOCA: Guided Online Cluster Assignment for Self-supervised Video Representation Learning",
    "date": "2022-01-01",
    "abstract": "Clustering is a ubiquitous tool in unsupervised learning. Most of the existing self-supervised representation learning methods typically cluster samples based on visually dominant features. While this works well for image-based self-supervision, it often fails for videos, which require understanding motion rather than focusing on background. Using optical flow as complementary information to RGB can alleviate this problem. However, we observe that a na\u00efve combination of the two views does not provide meaningful gains. In this paper, we propose a principled way to combine two views. Specifically, we propose a novel clustering strategy where we use the initial cluster assignment of each view as prior to guide the final cluster assignment of the other view. This idea will enforce similar cluster structures for both views, and the formed clusters will be semantically abstract and robust to noisy inputs coming from each individual view. Additionally, we propose a novel regularization strategy to address the feature collapse problem, which is common in cluster-based self-supervised learning methods. Our extensive evaluation shows the effectiveness of our learned representations on downstream tasks, e.g., video retrieval and action recognition. Specifically, we outperform the state of the art by 7% on UCF and 4% on HMDB for video retrieval, and 5% on UCF and 6% on HMDB for video classification (Code available at https://github.com/Seleucia/goca ).",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "The pursuit of understanding human activities in videos is a fundamental problem in computer vision. Representation learning methods with supervised training strategies showed promising results on various tasks, such as action understanding [12,20,28,43,54], action detection and localization [13,95], and action proposal [32,56]. It is fair to say that large-scale, manually labeled video datasets such as Kinetics [45], AVA [35], and Epic [22] substantially contributed to that success. In spite of those promising results, these algorithms can usually only recognise activities if they have access to a semantically labelled dataset. The cost and challenges of collecting large-scale, manually labelled videos hinder further improvements in activity understanding. On the other hand, Fig. 1: An abstract illustration of the proposed idea. The filled and empty shapes represent RGB and OF views respectively. Each shape represents a different type of activity, while colors represent intra-class variations. In this toy example, the initial clustering is wrong for both views. RGB fails because it clusters based on color (e.g. irrelevant background information), while OF fails since it confuses square with circle (due to the lower resolution of details). By combining the two types of information, clustering can be achieved correctly on both views. the internet is a virtually unlimited source of unlabeled videos (e.g. YouTube). Therefore, designing a representation learning strategy that does not rely on manual labelling is fundamentally important.",
                "Self-supervised learning (SSL) aims to address this issue, by designing pretext tasks that only rely on input, and training networks to solve those tasks. Recent advances in SSL for image understanding [11,15,34,36,38,70,79,96] have achieved excellent performance in various downstream tasks. Motivated by this success, several papers brought these ideas to the video domain [23,29,68,76]. Although these methods show promising results to some extent, they rely solely on an RGB stream. As demonstrated by [4,6,37], this is not sufficient to learn a strong temporal representation.",
                "Johansson's classical psychology work [44] shows that humans can recognize activities by only watching a few bright dots depicting the movement of the main body joints. This intuitive work motivated researchers to use optical flow as a representation of motion for activity understanding [12,30,71,72], and they have achieved significant improvements over RGB-only models in the supervised learning literature. Inspired by this success, many recent SSL works [33,37,80] have explored using optical flow (OF) to advance SSL beyond RGB-only baselines. Han et al. (CoCLR) [37] used OF to retrieve positive samples for the infoNCA [63], which led to significant improvements. Nevertheless, CoCLR did not utilize OF for training the backbone and hence may not have realized the full potential of learning motion representations. VICC [80] adopted the online cluster assignment [11] to videos by considering OF as another view of RGB, and minimized the distance between RGB and OF features during online clustering. This method obtained SOTA results on various datasets. However, enforcing similarity between RGB and OF features can be detrimental, especially if one information source is noisy, as is usually the case for OF due to camera motion. Furthermore, these mod-els [37,80] require a complicated training strategy that successively updates one model while freezing the parameters of the other model, which prevents end-to-end training.",
                "In this paper, we introduce the guided online cluster assignment algorithm (GOCA) to address the aforementioned problems. Specifically, for a given video with RGB and OF representations, we first compute initial cluster assignments for only using RGB or OF separately, and then we use these assignments as priors for each other to compute a final assignment that is guided by both views, as illustrated in Fig. 1. After we obtain our final assignment, we train a backbone network by minimizing a cross entropy loss between the final cluster assignment of different augmentations of the same video (as used in [11]). The proposed idea has several benefits compared to the state of the art [11,37,80]. First, it constructs more robust clusters during training due to prior information, which is particularly important when one information source is noisy. Second, allowing RGB and OF to share information by means of sharing cluster assignment encourages the two views to form similar cluster structure, which leads to more semantically abstract representations. Third, both RGB and OF backbones are trained jointly and information flows both ways during training, which is beneficial for both backbones due to the complementary nature of these views. Fourth, compared to the CoCLR method [37], OF is utilized more explicitly in our formulation, which leads to stronger spatio-temporal representations. Finally, the proposed approach circumvents complicated training strategies [37,33,80] and allows simple and end to end training.",
                "We also propose a novel prototype regularization method to address the feature collapse problem, where all features are mapped to a single point. This is a common problem in SSL and was partially addressed in SwAV [11] via equipartition constraint. However, it requires careful tuning1 of the parameter \u03bb (See Eq. ( 5)) of the Sinkhorn algorithm, which makes it hard used in practice [9,16,17,69]. We address this problem by constructing cluster prototypes which are maximally distant from each other. We achieve this by locating the N prototypes in the \u03a6 dimensional space such that they divide the space equally. Despite the simplicity of the proposed idea, it yields consistent performance improvements.",
                "To sum up, our contributions are: 1) we introduce a novel Guided Online Cluster Assignment (GOCA) algorithm that aims to learn stronger spatio-temporal representations by utilizing the complementary information of RGB and OF; 2) we mathematically prove that guidance based clustering can be achieved efficiently with the Sinkhorn algorithm; 3) we propose a prototype regularization strategy that addresses the common feature collapse problem; 4) we perform extensive evaluations of our method using two backbones (S3D [85] and R(2+1)D [81]) on four different evaluation regimes (See Sec. 4). The proposed model outperforms the state of the art in almost all experiments. Furthermore, we present ablation studies to show the effect of each contribution and the key parameters of our method. Optimal assignment (OA) and optimal assignment with prior (OAP) are defined in Eq. ( 6) and Eq. ( 7), respectively. L p is defined for a batch of videos in Eq. ( 13)",
                "Self-supervised representation learning. The success of image-based SSL methods [25,26,38,48,62,96,70,79,94] has inspired their application in the video domain, resulting in several recently proposed methods in that domain. Early approaches for video representation learning [1,24,75,82] typically relied on the idea of predicting the future frames given the past frames, which requires carefully training a deep generative model. Other works focus on designing proxy tasks to exploit temporal information. In a pioneering work, Misra et al. [60] designed a pretext task that predicts whether the order of video frames is correct or wrong. Follow-up methods achieved better performance by designing new pretext tasks such as predicting clip order [31,49,60,86], pace [8,18,83,88], or the arrow of time [66,84]. To achieve better generalization, aside from designing novel pretext tasks, some recent works focus on using instance-based contrastive learning approaches [23,29,39,52,64,68]. Since videos often contain multimodal data, many recent works used audio [2,3,4,5,6,46,65] or language [2,3,59,77] as complementary information to improve results. Unlike those works, we focus on learning representations purely on visual input, namely RGB and optical flow (OF). Note that we compute OF from RGB videos in an unsupervised way.",
                "Clustering based self-supervised learning. Early works in this direction adopted clustering to representation learning by simply applying clustering to obtain pseudo-labels [4,10,87].",
                "The common approach has two alternating steps, as it performs clustering offline, rather than concurrently with training as described in Sec. 3.1. One of the main limitations of that approach is that such naive implementations may form degenerate clusters where all samples are clustered to the same point. Asano et al. [6,7] tackle this problem by adding an equipartition constraint on the number of samples per cluster and converting the pseudo-label generation step into an optimal assignment problem. They solve the resulting problem using the Sinkhorn-Knopp algorithm [21]. Although the equipartition constraint achieves promising results, it is still inefficient due to the offline clustering. Recently, Caron et al. [11] (closest work to ours) eliminated offline label generation via an online clustering algorithm which allows them to train end-to-end on large-scale datasets. Although it works well for images, we observe that adapting this idea to videos is not as effective as with images. As seen in [4,6] and our results, RGB alone is not sufficient to form representative clusters for videos. We tackle this problem by using OF as complementary information to guide clustering."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [
                "Fig. 2 shows an overview of the proposed method. For a given video with RGB and OF views, we first compute two different augmentations for each view, and then pass them through the RGB and OF backbones, followed by a shared projection head and a shared prototype layer in order to compute cluster assignments."
            ],
            "subsections": [
                {
                    "title": "Preliminaries",
                    "paragraphs": [
                        "State-of-the-art clustering based SSL methods (SwAV [11], VICC [80], Sela [90], Selavi [7]) are built on top of the Cuturi et. al. [21] formulation. The Cuturi et. al. [21] formulation is used to find optimal assignments from samples to cluster centers (or prototypes) under constraints. Formally, consider a given minibatch of M videos X = {x 1 , ..., x M }, and N prototypes P = {p 1 , ..., p N } represented by trainable vectors. The idea of [11] is to compute two random augmentations X t and X s , and compute their feature vectors ",
                        "where",
                        "This loss function minimizes the distance of two different augmentations by comparing them according to their assignment. Ideally, the two augmentations should be assigned to the same prototype, due to identical semantic content.",
                        "Computing optimal assignment: The optimal assignment (OA) D can be found by solving the following optimization:",
                        "where C \u2208 R M \u00d7N is a distance matrix from a batch of feature vectors to prototypes and \u27e8D, C\u27e9 = tr(C \u22a4 D). U represents all possible assignments from our features F to prototypes P . Formally:",
                        "where",
                        "The constraint on U ensures all the prototypes are selected. Eq. ( 3) can be solved with linear programming. Since this is computationally expensive, it is infeasible to carry it out during training for every batch. Cuturi et al. [21] address this issue by adding entropy regularization and solving the resulting problem with the Sinkhorn algorithm:",
                        "(",
                        "where h represents the entropy. The above formulation is a strictly convex and smoothed version of Eq. ( 3). This allows us to efficiently approximate D. Specifically, D has a unique solution for any given \u03bb 1 , which is in the form of:",
                        "where K = e \u03bb1C , and u, v are non-negative unique vectors. The proof of this statement can be found in [21]. According to the Sinkhorn theorem [27], if K has only positive elements, the unknowns u and v can be determined via the Sinkhorn algorithm.",
                        "There are two main problems with this conventional formulation: First, it does not allow utilizing the complementary information, which is particularly important in the video, since it has been shown that using RGB alone encourages the model to focus on the background and ignore motion cues [6]. We address this problem by rewriting Eq. ( 3) such that we can fuse two information sources to obtain the cluster assignment. Second, in the conventional formulation [11,80], the prototypes are randomly initialized and trained with the rest of the model without any restriction, which can lead to a degenerate solution where all prototypes collapse into a single point, and consequently the feature vectors too [9,16,17,69]. To overcome this problem, we propose to use a novel prototype regularizer, which encourages prototypes to be maximally far apart."
                    ],
                    "subsections": []
                },
                {
                    "title": "Guided Online Cluster Assignment (GOCA)",
                    "paragraphs": [
                        "Our goal is to train backbone networks \u03b8 RGB and \u03b8 OF while sharing information between the two views. The single-view approach of [10,11,90] does not allow combining the two information sources. A straightforward way is to concatenate the F RGB and F OF features, but we have observed that this approach yields a very small improvement over RGB-only training on various downstream tasks (see Experiments 4.1). Another way is to force the similarity of OF and RGB features as in [80]. Even though this idea obtains better results on downstream tasks, it leads to a loss of information by forcing each view to only maintain mutual information. Hence, it does not reach the maximum potential performance of correctly combining the two views (Tab. 1).",
                        "In contrast, we propose a principled way to combine two information sources. We use each information source as a prior to the other. We assign D \u2032 OF and D \u2032 RGB as the initial prototype assignment (computed based on Eq. ( 5)), and D OF and D RGB as the final. To this end, we use the D \u2032 OF as the prior to D RGB , and D \u2032 RGB as the prior to D OF We implement this idea by modifying Eq. ( 5) in a way that it takes the prior into account. Our optimal assignment with prior (OAP) optimization problems for D RGB and D OF takes this form:",
                        "Where C RGB and C OF are distance matrices from RGB and OF features to prototypes, KL(\u2022|\u2022) represents the Kullback Leibler divergence between two assignment matrices, \u03bb 1 and \u03bb 2 are hyper-parameters. Note that we use the same prototypes P for OF and RGB features. These two optimization problems can be solved via the following lemma:",
                        "Lemma 1. D RGB and D OF have a unique solution for \u03bb 1 and \u03bb 2 in the form of:",
                        "Proof We will prove only for D RGB , but the same proof can be used for OF as well. Let L(D RGB , \u03b1, \u03b2) be the Lagrangian form of the Equation( 6) with dual variables \u03b1 \u2208 R M and \u03b2 \u2208 R N for the two equality constraints in U (see Eq. ( 4)):",
                        "for all (i, j) we set (\u2202L/\u2202d RGBij = 0) and solve for d RGBij",
                        "When we choose",
                        "), we can see that it is strictly positive since it is the element-wise exponential, therefore according the Sinkhorn's theorem [73], D \u03bb1,\u03bb2",
                        "RGB has unique solution for any given \u03bb 1 and \u03bb 2 . Thus, u RGB and v RGB vectors in Eq. ( 8) can be computed with Sinkhorn algorithm. In our formulation, since the final cluster assignment relies on both views, formed clusters will be robust to noise and semantically abstract. The supplemental includes the details of the proof."
                    ],
                    "subsections": []
                },
                {
                    "title": "Prototype Regularization",
                    "paragraphs": [
                        "It has been shown that SSL models suffer from feature collapse, i.e. when all features are mapped to the same representation [14,34,70,93]. Even though this problem was partially addressed in [11] by using equipartition constraint as in Eq. (3), in practice, \u03bb needs to be carefully tuned [9,16,17,69]. More specifically, we observe that a higher \u03bb leads to numerical issues, and lower values tend to cause feature collapse.",
                        "Alternatively, we introduce a regularization term that encourages prototypes to be maximally far apart. We achieve this by utilizing the idea of hyperspherical prototypes [58]. Formally, we divide the \u03a6-dimensional hyperspherical space equally into N prototypes. For instance, for a 2-dimensional hyperspherical space (circle), this can be easily done by placing N prototypes with a 2\u03c0 N angle difference. Even though this is easy to do for a 2-dimensional space, there is no exact solution for 3 or more dimensions [78]. We solve this problem by finding an approximate solution using gradient descent. Specifically, consider the N prototypes that are represented with a linear layer, W \u2208 R N \u00d7\u03a6 in our network (Fig. 2). Instead of training W with the rest of the network end-to-end, we train it separately, once before the main training phase, by minimizing the following loss under the following constraint:",
                        "where I and w i represents the identity matrix and i-th row in W , respectively. This loss minimizes the similarity of maximally similar prototypes. To apply the constraint, we continuously re-project our prototypes to the hypersphere during training via l2 normalization. This can be seen as a simple and quick initialization step, which takes only 5 minutes on a Nvidia GTX 1060 GPU."
                    ],
                    "subsections": []
                },
                {
                    "title": "Training procedure",
                    "paragraphs": [
                        "After initializing (and fixing) the prototype layer as described in Eq. ( 11), we train the network by minimizing the following loss function:",
                        "where",
                        "Here b \u2208 {RGB, OF } and d bi i-th row in D b . D b represents optimal assignment matrix. We compute these matrices using Lemma 1, while our prototypes are obtained as described in Sec. 3.3. Note that l and g are defined in Eq. ( 2). Since RGB and OF features only interact when computing optimal assignment, this formulation encourages cluster similarity without enforcing RGB and OF features to be strictly similar, hence, not leading to information loss."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experimental Results",
            "paragraphs": [
                "In this section, we first describe datasets, metrics, and training details. We present our ablation results to show the importance of our method design choices in Sec. 4.1. Then, we compare our approach with SOTA in Sec. 4.2 for retrieval task. Then, we show our cluster analysis results in Sec. 4.3. Finally, we show classification results in Sec. 4.4."
            ],
            "subsections": []
        },
        {
            "title": "Datasets:",
            "paragraphs": [
                "We conduct our experiments on four different datasets: Kinetics (K400) [45], UCF [74], HMDB [47], and Diving-48 [51]. We follow the same training protocols as other self-supervised learning approaches [6,37,68,80]. K400 training set contains 240k videos. UCF, HMDB, and Diving-48 contain 13k, 7k, and 17k videos respectively. Evaluation Metrics: We evaluate our model performance on the action retrieval and action classification tasks. To evaluate action retrieval, we compute Recall at K \u2208 {1, 5, 10, 20}, similar to earlier works [6,37,68,80]. More specifically, if the correct class is within the K nearest neighbours we consider it a correct result. For action classification, we consider top-1 accuracy for Linear Classification and Fine Tuning experimental setups (See Sec. 4.4). We report all numbers in terms of percentage. Data Augmentations: Following [37,68,80], we use horizontal flipping, random cropping, Gaussian blurring, and color jittering for augmentation. We also use a multitemporal-resolution idea which is analogous to multi-crop in SwAV [11]. In this augmentation strategy, we sample shorter length clips alongside longer ones. We observe this makes convergence faster, but does not affect the final accuracy."
            ],
            "subsections": []
        },
        {
            "title": "Backbones and Training details:",
            "paragraphs": [
                "We conduct our experiments with two widely used backbones: S3D [85] and R(2+1)D+18 [81]. We use two datasets (K400 [45] and UCF [74]) for self-supervised training. Optical flow (OF) is computed using the TV-L1 algorithm [92] and pre-processed as in [12,37,80]. We follow earlier works [29,68,80] and use SGD+LARS [91] optimizer with a learning rate of 4.8 that is increased during the first 10 warm-up epochs and then is decreased to 0.0048 with cosine learning rate decay. All models are pre-trained on 64 V100 GPUs (10 samples per GPU) for 500 epochs as in [6,29,68,80]. Each clip contains 32 consecutive frames. During the test, we turn off all augmentations and use a standard 3 (spatial) \u00d7 10 (temporal). We always denote with a \" + \" suffix the case of RGB and OF being used during testing. Following the earlier works [37,67,80] and we use 32 and 16 frames for S3D and R(2+1)D, respectively during the training of fine-tuning and linear-classification experiments."
            ],
            "subsections": [
                {
                    "title": "Ablation study",
                    "paragraphs": [
                        "In these experiments, we demonstrate the effectiveness of each proposed component in terms of recall values. All models pre-trained on the UCF training set and evaluated on the UCF and HMDB test sets with S3D backbone. Effect of view merging strategies For a better understanding of the effectiveness of the proposed approach, we design 3 different baselines: SView, Avg, and Sep. SView: We train OF and RGB separately with identical but independent backbones, projection heads, and prototypes. This baseline can be considered a trivial extension of SwAV [11] to videos. Avg: We train OF and RGB jointly, by passing OF and RGB from different backbones and then feeding their average into the projection head and prototypes. Sep: We train the model jointly but this time OF and RGB information do not interact. We use separate backbones but share the projection head and prototypes. Our model differentiates from this only at the assignment computing stage. We train all our baselines by minimizing the loss described in Eq. (1). Finally, to better compare our method (GOCA) with the single-view (SView) baseline, we also evaluate it in single-view settings by only using one of the backbones during the test. To combine RGB and OF representations during the test, we simply take the average of the two.",
                        "Tab. 1 shows our results on the UCF and HMDB test sets. In single-view settings (first 6 rows), SView with RGB obtains very poor results, which can be attributed to an over-emphasis on the background scene and ignoring the motion, which is common for RGB-only models. Our results confirm that motion-only models can perform better than RGB, as observed in [37,80]. Furthermore, GOCA outperforms both baselines even in single-view testing, which confirms that the proposed joint training approach is beneficial for each individual backbone as well. We can also see that enforcing the similarity of the two views as in VICC does not improve OF results compared to single-UCF HMDB Method Train Test R@1 R@5 R@1 R@  view training. In contrast, GOCA significantly improves the results for both views, by aligning the two representations in a smarter, more implicit manner.",
                        "When we combine RGB and OF during testing (last 4 rows), we observe that naively merging features at training (Avg) performs better than RGB-only by 3.5% while significantly worse than OF-only training (-11.2%). This might be due to the fact that during training, RGB information dominates the gradients, and the OF backbone can not be fully trained. The performance of Sep indicates that joint training for RGB and OF can improve the results when we do not naively merge features. Finally, the proposed model, GOCA, further improves the results, which verifies the efficacy of the proposed guided cluster assignment idea. Effect of \u03bb 1 and \u03bb 2 . Our next ablation study is observing the impact of the \u03bb 1 and \u03bb 2 parameter values in Eq. ( 6) and Eq. ( 7). These parameters control the effect of the uniformity assumption and prior distribution. Tab. 2 shows how recall at 1 varies depending on the \u03bb 1 and \u03bb 2 parameters. We train GOCA for 200 epochs and evaluate it on the UCF dataset. We can observe that the proposed approach is robust to \u03bb 1 and \u03bb 2 , especially in the range of [0.02, 0.03]. For all other experiments, we set \u03bb 1 = 0.02 and \u03bb 2 = 0.03. Table 3: Effect of prototype regularization on recall values Effect of prototype regularizer. Our prototype regularizer idea guarantees that prototypes are maximally far apart from each other. Fig. 3 shows t-SNE plot of 1000 prototypes with and without regularization. As we can see, the proposed method locates the prototypes maximally far apart from each other, on the other hand, in the unregularized case, prototypes are quite closely grouped. As we discuss in Sec. 3.3, maximally far apart prototypes prevent the feature collapse and allow us stable training. Another benefit is that clusters formed around prototypes are also will be far apart from each other which is particularly important for retrieval tasks. We verify this benefit by comparing the recall accuracy with and without using prototype regularization. Tab. Tab. 3 shows the consistent effectiveness of the proposed method for both models on both datasets.",
                        "UCF HMDB Method DS Backbone Res Modl R@1 R@5 R@10 R@20 R@1 R@5 R@10 R@20",
                        "Selavi   [8,89], direction [66], order [86]). These loss functions can be combined with our loss functions as well.",
                        "Comparison with the state-of-the-art Given the large body of self-supervised video understanding works published thus far, we only selected recent (from 2019) publications. To the best of our ability, we conducted a fair comparison. However, we still observe small variations in terms of input resolution and fine-tuning details in the literature, which makes it extremely hard to perform perfectly fair comparisons. Furthermore, we encourage the readers to study the supplementary material for more in-depth experimental results. As noted in [29], we observe that during the Fine-Tuning experiment, backbone networks tend to overfit validation datasets (UCF and HMDB), therefore we perform extensive retrieval and cluster analysis experiments to show our contributions' influence (On these experiments there is no supervised training on validation datasets).   "
                    ],
                    "subsections": []
                },
                {
                    "title": "Retrieval Results",
                    "paragraphs": [
                        "Tab. 4 and Tab. 5 show the retrieval results for Recall@K (R@K). In this experiment, we follow the standard protocol defined in [23,36,37,42,57,67,80] and perform the evaluation on the frozen features that were computed from a pre-trained model. At Tab. Tab. 4, the first part of the table (above the double line) includes models that are all pre-trained on K400 [45], while evaluated on the UCF and HMDB datasets. We can see a significant improvement (5% and 8% on UCF and HMDB respectively, averaged over all K values). Surprisingly, the proposed approach even outperforms models [6,61] that use an additional view (Audio) by a large margin. Below the double line, we show models that are pre-trained on the UCF training set, using two different backbone architectures, S3D and R(2 + 1)D, separated by a horizontal line. On the S3D, we achieve an improvement of 5.7% on UCF and 4.0% on HMDB at K = 1. On the R(2 + 1)D, we can see a nearly 6% increase across all recall values for UCF and 5% for HMDB.",
                        "Our RGB-only model also achieves state-of-the-art results and outperforms baselines by a large margin on both datasets for both backbones. These findings verify our intuition that effectively utilizing OF can result in strong spatio-temporal representations.  We observe that Time-Equ* [41] method also performs well however, this work uses speed [8,89], direction [66], and order [86] as auxiliary loss as well. These loss functions can be combined with our method also. Time-Equ* [41] obtains 52.1% and 21.4% at at K = 1 on UCF and HMDB respectively, without these auxiliary loss function. We also evaluate our pre-trained model on motion centric dataset [51]. Tab. 5 show our result for Diving-48 [51]. We can see that proposed model improves the baselines for the motion centric dataset as well and obtain SOTA results. Fig. 4 shows our retrievals results. GOCA retrieves videos from the same semantic categories and it fails only for one case where it confuses Disk Throwing with Hammer Throwing. This is a quite hard example because these two activities have very similar motions and the only difference is the quite small object in the person's hand. Higher resolution input images would help to solve this problem. We can see that SModel-RGB fails to retrieve relevant videos and consistently retrieves based on the background."
                    ],
                    "subsections": []
                },
                {
                    "title": "Cluster Analysis",
                    "paragraphs": [
                        "Ideally, video representations should form semantic clusters in order to facilitate activity recognition. To evaluate the clustering quality of our learned representations, we use two metrics that measure the correlation between clusters and semantic labels (ground truth). To this end, we perform K-means clustering on the our representations that are extracted from the UCF101 test set. Then we use the given labels of the test set to de-termine a label for each cluster via majority voting. We assign each cluster's label to all its members and compare those assigned labels to given labels. Specifically, we compute the accuracy of the assigned labels (Acc), as well as the F 1 score, which is the harmonic mean of recall and precision. We also compute the Normalized Mutual Information (NMI), which measures the mutual information between clusters and ground truth labels, divided by the sum of their entropy. Due to the randomness of K-means, we repeat experiments 50 times and take the average for each metric. As shown in Tab. 6, GOCA significantly improves the cluster quality in terms of all metrics, both with and without OF. This verifies the high quality semantic clustering ability of our method."
                    ],
                    "subsections": []
                },
                {
                    "title": "Classification Tasks",
                    "paragraphs": [
                        "Linear Classification We follow the earlier works of [37,67,80] for the linear classification experiments. After the self-supervised training on the K400 dataset, we discard the projection head and prototypes and replace them with a linear layer. Then we train the linear layer on the training set of each downstream dataset (UCF and HMDB) with frozen backbone. The results are shown in the first section of Tab. 7, and demonstrate that the proposed model significantly outperforms CoCLR [37] on both datasets by 5% and 6%, respectively. Notably, when we combine RGB and OF, we achieve state-of-theart results on the HMDB dataset, even though the other methods [36,68] benefit from a higher input resolution. For the case of UCF dataset, our model with the S3D marginally outperforms the other methods, achieving 80.1% on UCF and 50.0% on HMDB.",
                        "Fine-Tuning We follow the standard protocol from [37,67,80], where we train the full backbone on the downstream tasks. We summarize the results in the second section of Tab. 8. On the S3D backbone, our method improves CoCLR+ [37] by 0.5% and 2.9%, respectively. In addition, when using only RGB as input, we achieve improvements of 1.4% and 8.8% on both datasets. Moving to the UCF dataset, our approach obtains state-of-art results with R(2+1)D; while, with S3D, we are slightly worse than ViCC+ [80]. However, on the HMDB dataset, our results outperform ViCC+ [80] by a large margin of 6.6% and 2.6% when using RGB and RGB+OF. Note that CVRL [68] and \u03c1 BYOL [29] use higher input resolution (224) than ours (112), which leads to better performance, but needs significantly more computation. In fact, when we compare to CVRL at a resolution of 112, we significantly outperform CVRL [68]."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "In this paper, we presented a novel self-supervised learning approach for videos. We showed that the proposed guided online clustering idea and the prototype regularization approach both substantially improve the performance of our learned representations on both activity retrieval and action classification tasks. We believe that our work establishes a new direction for SSL research on multi-view and multi-modal data. Although we conduct our experiments using RGB and optical flow, the proposed idea can be applied to fuse other modalities such as RGB+Audio or RGB+Text, which we will explore in our future work. Furthermore, our method simplifies the training procedure for multi-modal SSL on videos (CoCLR and VICC require multi-stage training)."
            ],
            "subsections": []
        }
    ]
}