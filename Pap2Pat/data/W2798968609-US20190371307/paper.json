{
    "id": "https://semopenalex.org/work/W2798968609",
    "authors": [
        "Lin Zhao",
        "Zhe Feng"
    ],
    "title": "Improving Slot Filling in Spoken Language Understanding with Joint Pointer and Attention",
    "date": "2018-01-01",
    "abstract": "We present a generative neural network model for slot filling based on a sequence-to-sequence (Seq2Seq) model together with a pointer network, in the situation where only sentence-level slot annotations are available in the spoken dialogue data. This model predicts slot values by jointly learning to copy a word which may be out-of-vocabulary (OOV) from an input utterance through a pointer network, or generate a word within the vocabulary through an attentional Seq2Seq model. Experimental results show the effectiveness of our slot filling model, especially at addressing the OOV problem. Additionally, we integrate the proposed model into a spoken language understanding system and achieve the state-of-the-art performance on the benchmark data.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Slot filling is a key component in spoken language understanding (SLU), which is usually treated as a sequence labeling problem and solved using methods such as conditional random fields (CRFs) (Raymond and Riccardi, 2007) or recurrent neural networks (RNNs) (Yao et al., 2013;Mesnil et al., 2015).",
                "Although these models have achieved good results, they are learned on the datasets with wordlevel annotations, e.g., with the BIO tagging schema as in ATIS (Hemphill et al., 1990). Manual annotations at word level require big effort and some corpora has only sentence-level annotations available, e.g., the utterance \"... moderately priced restaurant\" has a slot-value pair annotation of \"pricerange=moderate\". As such datasets lack explicit alignment between the annotations and the input words, some systems rely on handcrafted rules to find the alignments in order to automatically create word-level labels to learn the sequence model (Zhou and He, 2011;Henderson, 2015), but finding such alignments is non-trivial. For example, it was shown in (Henderson, 2015) that when applying the manually created word aliases to the speech recognition hypotheses, only around 73% of alignments can be found due to the noise, and a CRF model trained on such noisy data performs particularly worse than some other methods. In addition it is time-consuming to adapt the manual rules or aliases to new domains. Some other work avoids this issue by regarding slot filling as a classification task (Henderson et al., 2012;Williams, 2014;Barahona et al., 2016), where an utterance is classified into one or more slot-value pairs. This, however, brings other challenges. One is that some types of slots may have a large or even unlimited number of possible values, so the classifiers may suffer from the data sparsity problem when the training data is limited. Another is the OOV problem caused by unknown slot values (e.g., restaurant name, street name), which is impossible to predefine and is very common in real-world spoken dialogue applications.",
                "To address these challenges, we present a neural generative model for slot filling on unaligned dialog data, specifically for slot value prediction as it has more challenges caused by OOV. The model uses Seq2Seq learning to predict a sequence of slot values from an utterance. Inspired by the ability of pointer network (Ptr-Net) (Vinyals et al., 2015) at addressing OOV problems, we incorporate Ptr-Net into a standard Seq2Seq attentional model to handle OOV slots. It can predict slot values by either generating one from a fixed vocabulary or selecting a word from the utterance. The final model is a weighted combination of the two operations.",
                "To summarize, our main contributions are: \u2022 We use a neural generative model for slot filling on the data without word-level annotations which has received less attention.",
                "\u2022 We adopt the pointer network to handle the OOV problem in slot value prediction, which achieves good performance without any manually-designed rules or features."
            ],
            "subsections": []
        },
        {
            "title": "Background of Pointer Network",
            "paragraphs": [
                "Ptr-Net is a variation of the standard Seq2Seq model with attention. At each decoding step, it selects a position from the input sequence based on the attention distribution instead of generating a token from the target vocabulary. Given the input X = {x 1 , ..., x T }, the output y t at time step t is predicted by:",
                "where a t i is the attention weight of position i at step t. The advantage of Ptr-Net is that it can make better predictions on unknown or rare words. It has been successfully applied to tasks such as abstractive summarization (See et al., 2017), question answering (He et al., 2017), reading comprehension (Wang and Jiang, 2016), and chunking (Zhai et al., 2017)."
            ],
            "subsections": []
        },
        {
            "title": "Model for Slot Value Prediction",
            "paragraphs": [
                "Our model for slot value prediction is a hybrid of a Seq2Seq attentional model and a Ptr-Net, similar as the one in See et al. (2017). The input is a sequence of words in an utterance, and the output is a sequence of slot values whose tokens may or may not appear in the input.",
                "The hybrid model, illustrated in Figure 1, allows us to both generate a slot value from a fixed vocabulary and pick a value from the input via pointing. The two components (Seq2Seq and Ptr-Net) share the same encoder-decoder architecture and attention scores. We adopt a single-layer bidirectional GRU (Cho et al., 2014) for the encoder, and a single-layer unidirectional GRU for the decoder. The attention is calculated as in Bahdanau et al. (2014).",
                "The slot vocabulary is set to contain only the values of enumerable slots, but not those of non-enumerable slots (e.g., values of \"restaurant name\") as we assume these are not known in advance in the real scenarios.",
                "We use the term \"extended vocabulary\" to denote the union of the slot vocabulary and all words from the input utterances. The probability distribution over the extended vocabulary is calculated as:",
                "That is, the model makes the final predictions using a weighted combination of the predictions from two individual components. At the decoding step t, the Seq2Seq component produces the probability distribution P gen for the next slot value within the vocabulary, while Ptr-Net produces the probability distribution P ptr over the input positions. p t \u2208 [0, 1] is a parameter to balance the two components. It is learned at each time step based on the decoder input d t , decoder state s t and the context vector c t as follows:",
                "where \u03c3 is a sigmoid function. w c , w s and w d are all trainable weights. "
            ],
            "subsections": []
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "In this section, we present our experimental results on DSTC2 (Dialog State Tracking Challenge) (Henderson et al., 2014), including the results of slot value prediction solely and a complete SLU system. Our models are implemented using Keras 1 with TensorFlow as backend. In all the experiments, the dimension of hidden states is 128, dimension of word embeddings is 100, dropout rate is 0. "
            ],
            "subsections": [
                {
                    "title": "Data",
                    "paragraphs": [
                        "DSTC2 consists of multi-turn dialogues between users and a dialog system, in the restaurant search domain. Each utterance is annotated with semantics including dialog-acts and slot-value pairs. For an utterance, both its transcription and 10-best hypotheses are provided. We use the top hypothesis as input throughout our experiments. The corpus has been separated into training, development and testing, containing 11,677, 3,934 and 9,890 utterances respectively."
                    ],
                    "subsections": []
                },
                {
                    "title": "A Complete SLU System",
                    "paragraphs": [
                        "For better evaluation and comparison, we integrated our model of slot value prediction into a complete SLU system, which uses a CNN classifier to obtain dialog-acts and slot types respectively after slot value prediction. For dialog act prediction, the input to the CNN model is the utterance and the output is one or more dialog acts (some utterances can have more than one dialog acts). For slot type prediction, the input is each predicted slot value together with the utterance, and the output is one of the predefined slot types.",
                        "Given the limited numbers of various dialog-acts and slot types for classification, a standard CNN model is expected to achieve good performance. Note that we can adopt other SLU frameworks as well (e.g., some joint frameworks), but given our focus in this work is to explore the hybrid Seq2Seq solutions for slot filling, we do not explore much on the SLU architecture, nor do we use any extra information (e.g., dialogue context). Despite the simplicity of our SLU system, it outperforms the prior state-of-the-art. In the whole process, neither manually designed features nor domain-specific rules are employed."
                    ],
                    "subsections": []
                },
                {
                    "title": "Baselines",
                    "paragraphs": [
                        "We compare the overall SLU performance of our system against two existing baselines on DSTC2. One baseline (Williams, 2014) uses binary SVM classifiers to predict the existence of each slotvalue pair and dialog act. The other (Barahona et al., 2016) uses CNN and LSTM jointly for classification.",
                        "For slot value prediction, since it is a sub-task of SLU and not reported in the prior work, we implemented another two models for it. One adopts CNN to classify an utterance into one or more slot values. The other uses the basic Seq2Seq attentional model (without Ptr-Net). Note that when learning this basic model, the target vocabulary is set to contain all the slot values in the training set."
                    ],
                    "subsections": []
                },
                {
                    "title": "Results of Slot Value Prediction",
                    "paragraphs": [
                        "We first report the results on slot value prediction only. We compare the results of our proposed model and our own implemented baselines in Table 1, using precision, recall and F1.",
                        "We can see that the proposed hybrid model achieves the best F1 score. Although CNN has a high precision, it suffers from the low recall. By looking into the results for each slot type, it is ob- (Williams, 2014)  served that CNN performs much poorer on nonenumerable types of slots such as \"food\" due to its high cardinality. While both our model and the basic Seq2Seq model have higher recall.",
                        "Since our assumption is that the proposed model can better handle the OOV problem, we analyze the OOV rate in the corpus to obtain more insight. By checking the percentage of slot values in the testing set that do not exist in the training, we find that the OOV problem in DSTC2 is not that severe, with a OOV ratio less than 0.1%. This could be a reason why our model does not obtain larger gain on the complete dataset. We therefore design more experiments in the next section to assess the model when the OOV problem is more severe."
                    ],
                    "subsections": []
                },
                {
                    "title": "OOV Slot Prediction",
                    "paragraphs": [
                        "We create specific datasets by re-sampling from the original corpus. In particular, let group A denote all the training utterances that contain nonenumerable slots, and group B denote the rest of the training utterances. We randomly select 5%, 10%, 15%, and 20% of group A, plus the whole set of group B. In this way, we can create training data with less non-enumerable slot values thus resulting in a higher OOV ratio. The testing set is same as before. We compare the proposed model with the baselines on these four specific datasets with different OOV ratios ( As shown in each column, on all the specific datasets, our model achieves the best performance. The CNN model performs much poorer than before in terms of the recall. We can see that by reducing the training size, the OOV ratio (indicated in the first row in the brackets) goes up, and the performance of all models decreases in general. While CNN and the basic Seq2Seq model decline 10.3% and 9.2% in F1 respectively using the smallest training set compared to using the complete one, our model is the most stable one with the least performance drop of 5.4%. The gain of our model over the other two becomes more significant with the larger OOV rate. This shows the capability of the Ptr-Net to correctly predict the OOV slots.",
                        "Overall, the results in Section 4.4 and 4.5 demonstrate the effectiveness of the proposed hybrid model for slot value prediction, especially when the training set is small and the OOV ratio is large."
                    ],
                    "subsections": []
                },
                {
                    "title": "SLU Results",
                    "paragraphs": [
                        "Table 3 compares the results of the overall SLU task by our systems (incorporated with different slot value prediction models) and prior arts. All our systems outperform the prior work, and among them the one with the proposed hybrid model achieves the best F1 score.",
                        "We also conduct the similar OOV experiments as in Section 4.5 for SLU (Table 4). Similar trend is observed as discussed before. The performance of the proposed model with 20% training data already reaches that of the best system reported in the literature with 100% training data."
                    ],
                    "subsections": []
                },
                {
                    "title": "Case Study and Error Analysis",
                    "paragraphs": [
                        "Table 5 gives some examples of slot values predicted by the proposed model and baselines. We can see that for the less frequent slots, our model can still predict the values correctly, while without the Ptr-Net, the basic Seq2Seq model tends to generate words not appearing in the input, and CNN outputs nothing in many cases, which aligns with our assumption. We analyze the cases where Ptr-Net does not perform well and find several major types of errors: 1) partial prediction (e.g., detect only \"oriental\" for \"asian oriental food\"; 2) the prediction contains repetition of correct values; 3) speech recognition error although the prediction is proper if we look at the hypothesis itself (the third example). There are also cases where all the models fail to give a completely correct prediction, yet with different behaviors (the last example)."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We adopt an attentional Seq2Seq model with Ptr-Net to predict slot values on dialogue data when only sentence-level semantic annotations are available. By switching between copying and generating words, this solution can bypass the need of word-level annotations and overcome the OOV issue which is very common in real-world spoken dialogue applications. It does not require any domain specific rules or dictionaries, and therefore can be easily adapted to new domains. Our model has achieved the state-of-the-art performance for both slot value prediction and SLU on the benchmark even with less training data."
            ],
            "subsections": []
        },
        {
            "title": "Acknowledgments",
            "paragraphs": [
                "We would like to thank Yifan He for helpful discussions and proofreading, and the anonymous reviewers for their valuable feedback."
            ],
            "subsections": []
        }
    ]
}