# Introduction

Anomaly detection and outlier detection are used for detecting data samples that are inconsistent with normal data samples. Early methods did not take the sequential structure of the data into consideration [1]. However, many real world applications involve data collected as a sequence or time series. In such data, anomalous samples are better characterized as subsequences of time series. Anomaly detection is a challenging task due to the uncertain nature of anomalies. Anomaly detection in time series and sequence data is particularly difficult since both length and occurrence frequency of potentially anomalous subsequences are unknown. Additionally, algorithmic computational complexity can be a challenge, especially for streaming data with large alphabet sizes.

In this paper, we propose a universal nonparametric model-free anomaly detection method for time series and sequence data based on a pattern dictionary (PD). Given training and test data sequences, a pattern dictionary is created from the sets of all the patterns in the training data. This dictionary is then used to sequentially parse and compress (in a lossless manner) the test data sequence. Subsequently, we interpret the number of parsed phrases or the codelength of the test data as anomaly scores. The smaller the number of parsed phrases or the shorter the compressed codelength of the test data, the more similarity between training and test data patterns. This sequential parsing and lossless compression procedure leads to detection of anomalous test sequences and their potential anomalous patterns (subsequences).

The proposed pattern dictionary method has the following properties: (i) it is nonparametric since it does not rely on a family of parametric distributions; (ii) it is universal in the sense that the detection criterion does not require any prior modeling of the anomalies or nominal data; (iii) it is non-Bayesian as the detection criterion is model-free; and (iv) as it depends on data compression, data discretization is required prior to building the dictionary. While the proposed pattern dictionary can be used as a stand-alone anomaly detection method (Pattern Dictionary for Detection (PDD)), we show how it can be utilized in the atypicality framework [2,3] for more general data discovery problems. This results in a method we call PDA (Pattern Dictionary based Atypicality), in which the proposed pattern dictionary is contrasted against a universal source coder which is the Tree-Structured Lempel–Ziv (LZ78) [4,5]. We use the LZ78 as the universal encoder since its compression procedure is similar to our proposed pattern dictionary, and it is (asymptotically) optimal [4,5].

The main contributions of this paper are as follows. First, we propose the pattern dictionary method for anomaly detection and characterize its properties. We show in Theorem 1 that using a multi-level dictionary that separates the patterns by their depth results in a shorter average indexing codelength in comparison to a uni-level dictionary that uses a uniform indexing approach. Second, we develop novel non-asymptotic lower and upper bounds of the LZ78 parser in Theorem 2 and further analyze its non-asymptotic properties. We demonstrate that the non-asymptotic upper bound on the number of distinct phrases resulting from the LZ78 parsing of an \(\left| \mathcal{X} \right|\) -ary sequence of length l can be explicitly expressed by the Lambert W function [6]. To the best of our knowledge, such characterization has not previously appeared in the literature. Then, we show in Lemma 1 that the achieved non-asymptotic upper bound on the number of distinct phrases resulting from the LZ78 parsing converges to the optimal upper bound \(\frac{l}{\log l}\)  of the LZ78 parser as \(\left. l\rightarrow\infty \right.\) . Third, we show how the pattern dictionary and LZ78 can be used together in an atypicality detection framework. We demonstrate that the achieved non-asymptotic lower and upper bounds on both LZ78 and pattern dictionary determine the range of the anomaly score. Consequently, we show how these bounds can be used to analyze the effect of dictionary depth on the anomaly score. Furthermore, the bounds are used to set the anomaly detection threshold. Finally, we compare our proposed methods with the competing methods, including nearest neighbors-based similarity [7], threshold sequence time-delay embedding [8,9,10,11], and compression-based dissimilarity measure [12,13,14,15,16,16], that are designed for anomaly detection in sequence data and time series. We conclude our paper with an experiment that details how the proposed framework can be used to construct a baseline of health against which anomalous deviations are detected.

The paper is organized as follows. In Section 2, we briefly review the relevant literature in anomaly detection (readers who are familiar with anomaly detection can skip this section). Section 3 introduces the detection framework and the notation used in this paper. Section 4 presents our proposed pattern dictionary method and its properties. In Section 5, we show how the proposed pattern dictionary can be used in an atypicality framework alongside LZ78, and we analyze the non-asymptotic properties of the LZ78 parser. Section 6 presents experiments that illustrate the proposed pattern dictionary anomaly detection procedure. Finally, Section 7 concludes our paper.

# Related Works

Anomaly detection has a vast literature. Anomaly detection procedures can be categorized into parametric and nonparametric methods. Parametric methods rely on a family of parametric distributions to model the normal data. The slippage problem [17], change detection [18,19,20,21], concept drift detection [19,20,21,22], minimax quickest change detection (MQCD) [23,24,25], and transient detection [26,27,28,29] are examples of parametric anomaly detection problems. The main difference between our proposed pattern dictionary method and the aforementioned techniques is that our method is a model-free nonparametric method. The main drawback of the parametric anomaly detection procedure is that it is difficult to accurately specify the parametric distribution for the data under investigation.

Nonparametric anomaly detection approaches do not assume any explicit parameterized model for the data distributions. An example is an adaptive nonparametric anomaly detection approach called geometric entropy minimization (GEM) [30,31] that is based on the minimal covering properties of K-point entropic graphs constructed on N training samples from a nominal probability distribution. The main difference between GEM-based methods and our proposed pattern dictionary is that former techniques are designed to detect outliers and cannot easily incorporate the temporal information regarding anomaly in a data stream. Another nonparametric detection method is sequential nonparametric testing that considers data as online stream and addresses the growing data storage problem by sequentially testing every new data samples [32,33]. A key difference between sequential nonparametric testing and our proposed pattern dictionary method is that our method is based on coding theory instead of statistical decision theory.

Information theory and universal source coding have been used previously in anomaly detection [34,35,36,37,38,39,40,41,42,43,44,45]. The detection criteria in these approaches are based on comparing metrics such as complexity or similarity distances that depend on entropy rate. An issue with these approaches is that there are many completely dissimilar sources with the same entropy rate, reducing outlier sensitivity. Another related problem is universal outlier detection [46,47]. In these works, different levels of knowledge about nominal and outlier distributions and number of outliers are incorporated. Unlike these methods, our proposed pattern dictionary approach does not require any prior knowledge about outliers and anomalies. In [48], a measure of empirical informational divergence between two individual sequences generated from two finite-order, finite-alphabet, stationary Markov processes is introduced and used for a simple universal classification. While the parsing procedure used in [48] is similar to the pattern dictionary used in this paper, there are important differences. The empirical measure proposed in [48] is a stand alone score function that is designed for two-class classification, while our measure is a direct byproduct of the LZ78 encoding algorithm designed for single-class classification, i.e., anomaly detection. In addition, the theoretical convergence of the empirical measure to the relative entropy between the class conditioned distributions, shown in [48], is only guaranteed when the sequences satisfy the finite-order Markov property, a condition that may be difficult to satisfy in practice. In [2,3], an information theoretic data discovery framework called atypicality has been introduced in which the detection criterion is based on a descriptive codelength comparison of an optimum encoder or a training-based fixed source coder, namely a data-dependent source coder introduced in [2]) with a universal source coder. In this paper, we show how our proposed pattern dictionary method can be used as a training-based fixed source coder in an atypicality framework.

Anomaly and outlier detection for time series has also been extensively studied [49]. Various time series modeling techniques such as regression [50], auto regression [51], auto regression moving average [52], auto regressive integrated moving average [53], support vector regression [54], and Kalman filters [55] have been used to detect anomalous observations by comparing the estimated residuals to a threshold. Many of these methods depend on a statistical assumption on the residuals, e.g., an assumption of Gaussian distribution, while the pattern dictionary method is model-free.

The proposed pattern dictionary method is closely related to the anomaly detection methods that are designed for sequence data. Many of these methods are focused on specific applications. For instance, detection of mutations in DNA sequences [7,56], detection of cyberattacks in computer network [57], and detection of irregular behaviors in online banking [58] are all application-specific examples of anomaly detection for discrete sequences. In the recent years, multiple sequence data anomaly detection methods have been developed specifically for graphs [59], dynamic networks [60], and social networks [61]. Chandola et al. [34] summarized many anomaly detection methods for discrete sequences and identified three general approaches to this problem. These anomaly detection formulations are unique in the way that anomalies are defined, but similar in their reliance on comparison between a test (sub)sequence and normal sequences in the training data. For example, kernel-based techniques such as nearest neighbor-based similarity (NNS) [7] are designed to detect anomalous sequences that are dissimilar to the training data. As another example, threshold sequence time-delay embedding (t-STIDE) [8,9,10,11] is established to detect anomalous sequences that contain subsequences with anomalous occurrence frequencies. The compression-based dissimilarity measure (CDM) is proposed for discord detection [12,13,14,15,16,16] to detect anomalous subsequences within a long sequence. Chandola et al. [34] also showed how various techniques developed for one problem formulation can be changed and applied to other problem formulations. While our pattern dictionary method shares similarity with NNS, CDM, and t-STIDE, our proposed method is generally applicable to any of the categories of anomaly detection identified in [34]. Furthermore, our detection criterion does not depend on the specific type of anomaly. Note that while CDM is also a compression-based method, its anomaly score is based on a dissimilarity measure that might fail to detect atypical subsequences [2]. For instance, using CDM method, a binary i.i.d. uniform training sequence is equally dissimilar to another binary i.i.d. uniform test sequence or to a test sequence drawn from some other distribution. In Section 6, the detection performance of our proposed pattern dictionary method is compared with NNS, CDM, t-STIDE, and the Ziv–Merhav method of [48].

It is worth mentioning that since the proposed pattern dictionary method is based on lossless source coding, it requires discretization of time series prior to deployment. In fact, many anomaly detection approaches require discretization of continuous data prior to applying inference techniques [62,63,64,65]. Note that discretization is also a requirement in other problem settings such as continuous optimization in genetic algorithms [66], image pattern recognition [67], and nonparametric histogram matching over codebooks in computer vision [68].

# Framework And Notation

In the anomaly detection literature for sequence data and time series, the following three general formulations are considered [34]: (i) an entire test sequence is anomalous if it is notably different from normal training sequences; (ii) a subsequence within a long test sequence is anomalous if it is notably different from other subsequences in the same test sequence or the subsequences in a given training sequence; and (iii) a given test subsequence or pattern is anomalous if its occurrence frequency in a test sequence is notably different from its occurrence frequency in a normal training sequence. In this paper, we consider a unified formulation in which we determine if a (sub)sequence is anomalous with respect to a training sequence (or training sequence database) if any of the aforementioned three conditions are met. In other words, given a training sequence or a training sequence database, a test sequence is anomalous if it is significantly different from training sequences, or it contains a subsequence that is significantly different from subsequences in the training sequence, or it contains a subsequence whose occurrence frequency is significantly different from its occurrence frequency in the training data.

## Notation

We use x to denote a sequence and \(x_{n}^{m}\)  to denote a subsequence of x: \(x_{n}^{m} = \left\{ {x_{i},i = n,n + 1,\ldots,m} \right\}\) , and \(x^{l}\)  represents a sequence of length l, i.e., \(\left\{ {x_{n},n = 1,\ldots,l} \right\}\) . \(\mathcal{X}\)  denotes a finite set, and \(\mathcal{D}\)  represents a dictionary of subsequences. Throughout this paper:All logarithms are base 2 unless otherwise is indicated.In the encoding process, we always adhere to lossless compression and strict decodability at the decoder.While adhering to strict decodability, we only care about the codelength, not the codes themselves.

# Pattern Dictionary: Design And Properties

Consider a long sequence, called the training data, \(\left\{ {x_{n},n = 1,\ldots,L} \right\}\)  of length L drawn from a finite alphabet \(\mathcal{X}\) . The goal is to learn the patterns (subsequences) of this sequence by creating a dictionary that contains all distinct patterns of maximum length (depth) \(D_{max} \ll L\)  that are embedded in the sequence. We call this dictionary a pattern dictionary \(\mathcal{D}\)  with the maximum depth \(D_{max}\)  and the set of observed patterns \(\mathcal{S}_{\mathcal{D}}\left( x_{1}^{L} \right)\) .

Since the pattern dictionary is going to be used as a training-based fixed source coder (a data-dependent source coder as defined in [2]), an efficient structure for the pattern representation that minimizes the indexing codelength is of interest. The simplest approach is to consider all the patterns of length \(1 \leq d \leq D_{max}\)  in one set \(\mathcal{S}_{\mathcal{D}}\)  and use a uniform indexing approach. This approach is called a uni-level dictionary. Another approach is to separate all the patterns by their depth (pattern length) and arrange them in \(D_{max}\)  sets \(\mathcal{S}_{\mathcal{D}}^{(1)},\mathcal{S}_{\mathcal{D}}^{(2)},\ldots,\mathcal{S}_{\mathcal{D}}^{(D_{\max})}\) , and define \(\mathcal{S}_{\mathcal{D}} = \bigcup_{d = 1}^{D_{max}}\mathcal{S}_{\mathcal{D}}^{(d)}\) , which we call a multi-level dictionary. In the following sections, we show that the latter results in a shorter average indexing codelength. It is worth mentioning that since a multi-level dictionary results in a depth-dependent indexing codelength, the average over the depth is considered. A relevant question is if the average of indexing codelength over all the patterns independent of depth should be used as an alternative. Since such pattern dictionaries are used to sequentially parse test data, patterns at smaller depth are more likely to be matched, even if they are anomalous. Thus, the average of indexing codelength over depth can better differentiate depth-dependent anomalies.

## 1. A Special Case

Suppose all the possible patterns of depth \(d \leq D_{max}\)  exist in the training sequence \(\left\{ {x_{n},n = 1,\ldots,L} \right\}\) . That is, the cardinality of \(\mathcal{S}_{\mathcal{D}}^{(d)}\)  is \(\left| \mathcal{S}_{\mathcal{D}}^{(d)} \right| = \left| \mathcal{X} \right|^{d}\)  for \(1 \leq d \leq D_{max}\) . Then, the total number of patterns is \[\begin{aligned} \left| {\mathcal{S}_{\mathcal{D}}\left( x_{1}^{L} \right)} \right| & {= \sum\limits_{d = 1}^{D_{max}}\left| {\mathcal{S}_{\mathcal{D}}^{(d)}\left( x_{1}^{L} \right)} \right|} \\  & {= \sum\limits_{d = 1}^{D_{max}}\left| \mathcal{X} \right|^{d}} \\  & {= \frac{\left| \mathcal{X} \right|\left( {\left| \mathcal{X} \right|^{D_{max}} - 1} \right)}{\left| \mathcal{X} \right| - 1}.} \end{aligned}\] Hence, a uni-level dictionary results in a uniform indexing codelength of \[\begin{aligned} L^{uni} & {= \log\left( \frac{\left| \mathcal{X} \right|\left( {\left| \mathcal{X} \right|^{D_{max}} - 1} \right)}{\left| \mathcal{X} \right| - 1} \right)} \\  & {\approx D_{max}\log\left( \left| \mathcal{X} \right| \right).} \end{aligned}\] On the other hand, a multi-level dictionary requires a two-stage description of index. The first stage is the index of the depth d (using \(\log D_{max}\)  bits), and the second stage is the index of the pattern among all the patterns with the same depth (using \(d\log\left( \left| \mathcal{X} \right| \right)\)  bits). This two-stage description of the index leads to a non-uniform indexing of codelength: the minimum indexing codelength occurring for the patterns of depth \(d = 1\)  equals to \(L_{min}^{multi} =\) \(\log D_{max} + \log\left( \left| \mathcal{X} \right| \right)\)  bits, while the maximum indexing codelength occurring for the patterns of depth \(d = D_{max}\)  equals to \(L_{max}^{multi} =\) \(\log D_{max} + D_{max}\log\left( \left| \mathcal{X} \right| \right)\)  bits. Thus, the average indexing codelength of a multi-level dictionary is given by \[\begin{aligned} L^{multi} & {= \frac{1}{D_{max}}\sum\limits_{d = 1}^{D_{max}}\left( {\log D_{max} + d\log\left( \left| \mathcal{X} \right| \right)} \right)} \\  & {= \log D_{max} + \frac{\log\left( \left| \mathcal{X} \right| \right)}{D_{max}}\sum\limits_{d = 1}^{D_{max}}d} \\  & {\approx \log D_{max} + \frac{1}{2}D_{max}\log\left( \left| \mathcal{X} \right| \right).} \end{aligned}\] Figure 1 and Figure 2 graphically compare the indexing codelength between a uni-level dictionary and a multi-level dictionary for a fixed alphabet size and a fixed \(D_{max}\) , respectively. As seen, the average indexing codelength of a multi-level dictionary results in a shorter indexing codelength.

## 2. The General Case

Given the training sequence \(\left\{ {x_{n},n = 1,\ldots,L} \right\}\) , suppose there are \(a_{d} = \left| \mathcal{S}_{\mathcal{D}}^{(d)} \right| \leq \left| \mathcal{X} \right|^{d}\)  patterns of depth \(d \leq D_{max}\)  (\(a_{1}\)  patterns of depth one, \(a_{2}\)  patterns of depth two, etc.). The following Theorem 1 shows that the average indexing codelength using a multi-level dictionary is always less than the indexing codelength of a uni-level dictionary.

Theorem 1 shows that a multi-level dictionary gives shorter average indexing codelength than a uni-level dictionary. \(\log D_{max} + \log a_{d}\)  is the indexing codelength for patterns of depth d, where \(a_{d}\)  is the total number of observed patterns of the depth d. In order to reduce the indexing codelength even further, the patterns of the same length in each set \(\mathcal{S}_{\mathcal{D}}^{(d)}\)  can be ordered according to their relative frequency (empirical probability) in the training sequence. This allows Huffman or Shannon–Fano–Elias source coding [4] to be used to assign prefix codes to patterns in each set \(\mathcal{S}_{\mathcal{D}}^{(d)}\)  separately. In this case, for any pattern \(x_{1}^{d} \in \mathcal{S}_{\mathcal{D}}^{(d)}\) , the indexing codelength becomes \[\begin{aligned} L^{multi} & {\left( x_{1}^{d} \right) = \log D_{max} + L_{\mathcal{D}}^{(d)}\left( x_{1}^{d} \right),} \end{aligned}\]  where \(L_{\mathcal{D}}^{(d)}\left( x_{1}^{d} \right)\)  is the codelength assigned to the pattern \(x_{1}^{d}\)  based on its empirical probability using a Huffman or Shannon–Fano–Elias encoder. If such encoders are used, the codelength (1) is optimal ([4] Theorem 5.8.1). Since the whole purpose of creating a pattern dictionary is to learn the patterns in the training data, assigning the shorter codelength to the more frequent patterns and assigning longer codelength to the less frequent patterns in any pattern set \(\mathcal{S}_{\mathcal{D}}^{(d)}\)  will improve the efficiency of the coded representation.

## 3. Pattern Dictionary For Detection (Pdd)

Suppose we want to sequentially compress a test sequence \(x_{1}^{l} = \left\{ {x_{n},n = 1,\ldots,l} \right\}\)  using a trained pattern dictionary \(\mathcal{D}\)  with maximum depth \(D_{max} < l\) . The encoder parses the test sequence \(x_{1}^{l}\)  into c phrases, \(x_{v_{1}}^{v_{2} - 1},x_{v_{2}}^{v_{3} - 1},\ldots,x_{v_{c}}^{l}\)  where \(v_{i}\)  is the index of the start of the ith phrase, and each phrase \(x_{v_{i}}^{v_{i + 1} - 1}\)  is a pattern in the pattern dictionary \(\mathcal{D}\) . Let \(\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right) = \left\{ {x_{v_{1}}^{v_{2} - 1},x_{v_{2}}^{v_{3} - 1},\ldots,x_{v_{c}}^{l}} \right\}\)  denote the set of the parsed phrases using pattern dictionary \(\mathcal{D}\) . The parsing process begins with setting \(v_{1} = 1\)  and finding the largest \(v_{2} \leq D_{max}\)  and \(v_{2} \leq l\)  such that \(x_{v_{1}}^{v_{2} - 1} \in \mathcal{D}\)  but \(x_{v_{1}}^{v_{2}} \notin \mathcal{D}\) . This results in the first phrase \(x_{1}^{v_{2} - 1}\) . Similarly, the same procedure is performed in order to find the largest \(v_{3} \leq D_{max}\)  and \(v_{3} \leq l\)  such that \(x_{v_{2}}^{v_{3} - 1} \in \mathcal{D}\)  but \(x_{v_{2}}^{v_{3}} \notin \mathcal{D}\) . This type of cross-parsing was first introduced in [48] in order to estimate an empirical relative entropy between two individual sequences that are independent realizations of two finite-order, finite-alphabet and stationary Markov processes. Here, we do not impose such an assumption on the sources generating the sequences. Algorithm 1 summarizes the procedure of the proposed pattern dictionary (PD) parser. After parsing the whole test sequence \(x_{1}^{l}\)  into c phrases, \(x_{v_{1}}^{v_{2} - 1},x_{v_{2}}^{v_{3} - 1},\ldots,x_{v_{c}}^{l}\) , the codelength will be \[\begin{aligned} {L\left( x_{1}^{l} \right)} & {= \sum\limits_{i = 1}^{c}L_{\mathcal{D}}\left( x_{v_{i}}^{v_{i + 1} - 1} \right) + c\log D_{max}.} \end{aligned}\]

For detection purposes, on a test sequence \(x_{1}^{l}\) , either the number of parsed phrases or the codelength can be used as anomaly scores with respect to the trained pattern dictionary \(\mathcal{D}\) . In other words, for any test sequence \(x_{1}^{l}\)  and given a pattern dictionary, if the number of parsed phrases \(\left| {\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right)} \right|\)  or the codelength \(L\left( x_{1}^{l} \right)\)  in Equation (2) are greater than a certain threshold, then \(x_{1}^{l}\)  is declared to be anomalous. While the proposed pattern dictionary technique can be used as a stand-alone anomaly detection technique, below we show how it can be used for atypicality detection [2,3] as a training-based fixed source coder (data-dependent encoder).

# Pattern Dictionary-Based Atypicality (Pda)

In [2,3], an atypicality framework was introduced as a data discovery and anomaly detection framework that is based on a central definition: “a sequence (or subsequence) is atypical if it can be described (coded) with fewer bits in itself rather than using the (optimum) code for typical sequences”. In this framework, detection is based on the comparison of a lossless descriptive codelength between an optimum encoder (if the typical model is known) or a training-based fixed source coder (if the typical model is unknown, but training data are available) and a universal source coder in order to detect atypical subsequences in the data [2,3]. In this section, we apply our proposed pattern dictionary as a training-based fixed source coder (typical encoder) in an atypicality framework. We call it pattern dictionary-based atypicality (PDA) method.

The pattern dictionary-based source coder can be considered as a generalization of the Context Tree [70,71,72] based fixed source coder that was used in [2] for discrete data. The universal source coder (atypical encoder) used here is the Tree-Structured Lempel–Ziv (LZ78) [4,5]. The primary reason for choosing LZ78 as the universal encoder is that its sequential parsing procedure is similar to the proposed pattern dictionary described in Section 4, and it is (asymptotically) optimal [4,5]. One might ask why do we even need to compare descriptive codelengths of a training-based (or optimum) encoder with a universal encoder for data discovery purposes when, as alluded to in the end of last section, a training-based fixed source coder can be a stand-alone anomaly detector. The necessity of such concurrent comparison is articulated in [2]. In fact, such a codelength comparison enables the atypicality framework to go beyond the detection of anomalies and outliers, extending to the detection of rare parts of data that might have a data structure of interest to the practitioner.

We give an example to provide further intuition for why anomaly detection can benefit from our framework that compares the outputs of a typical encoder and an atypical encoder. Consider an i.i.d. binary sequence of length L with \(P\left( {X = 1} \right) = p\)  in which there is embedded an anomalous subsequence of length \(l \ll L\)  with \(P\left( {X = 1} \right) = \hat{p} \neq p\)  that we would like to detect. If \(p = \frac{1}{2}\)  and \(\hat{p} = 1\) , the typical encoder cannot catch the anomaly while the atypical encoder can. On the other hand, if \(p = \frac{1}{3}\)  and \(\hat{p} = \frac{2}{3}\) , the typical encoder identifies the anomaly while an atypical encoder fails to do so (since the entropy for \(p = \frac{1}{3}\)  and \(\hat{p} = \frac{2}{3}\)  is the same). Note that in both cases, our framework would catch the anomaly since it uses the difference between the descriptive codelengths of these two encoders.

Recall that in Section 4, we supposed that a test sequence \(x_{1}^{l}\)  has been parsed using a trained pattern dictionary \(\mathcal{D}\)  with maximum depth \(D_{max} < l\) . This parsing results in \(\left| {\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right)} \right|\)  parsed phrases. Using Equation (2), the typical codelength of the sequence \(x_{1}^{l}\)  is given by \[\begin{aligned} {L_{T}\left( x_{1}^{l} \right)} & {= \sum\limits_{y \in \mathcal{S}_{\mathcal{D}}{(x_{1}^{l})}}L_{\mathcal{D}}(y) + \left| {\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right)} \right|\log D_{max}.} \end{aligned}\] For the atypical encoder, the LZ78 algorithm results in a distinct parsing of the test sequence \(x_{1}^{l}\) . Let \(\mathcal{S}_{LZ}\left( x_{1}^{l} \right)\)  denote the set of parsed phrases in the LZ78 parsing of \(x_{1}^{l}\) . As such, the resulting atypical codelength is [4,5] \[\begin{aligned} {L_{A}\left( x_{1}^{l} \right)} & {= \left| {\mathcal{S}_{LZ}\left( x_{1}^{l} \right)} \right|\left\lbrack {\log\left| {\mathcal{S}_{LZ}\left( x_{1}^{l} \right)} \right| + 1} \right\rbrack.} \end{aligned}\]

Since \(L\left( x_{1}^{l} \right)\)  using both LZ78 and the pattern dictionary depends on the number of parsed phrases, we investigate the possible range and properties of \(\left| {\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right)} \right| - \left| {\mathcal{S}_{LZ}\left( x_{1}^{l} \right)} \right|\) . While the LZ78 encoder is a well-known compression method which is asymptotically optimal [4,5], its non-asymptotic behavior is not well understood. In the next section, we establish a novel non-asymptotic property of an LZ78 parser, and then compare it with the pattern dictionary parser.

## 1. Lempel–Ziv Parser

We start this section with a theorem that establishes the non-asymptotic lower and upper bounds on the number of distinct phrases in a sequence parsed by LZ78.

Figure 3 illustrates the lower and upper bounds established in Theorem 2 against the sequence length for various alphabet sizes. Note that the lower bound on the number of distinct phrases is independent of the alphabet size.

While numerical experiments are not a substitute for the mathematical proof of Theorem 2 provided above, the reader may find it useful to understand the theorem in terms of a simple example. In Figure 4, Figure 5 and Figure 6, we compare the theoretical bound with numerical results of simulation for binary i.i.d. sequences. In these experiments, for each value of \(P(X = 1)\) , a thousand binary sequences are generated; then, the number of distinct phrases resulting from LZ78 parsing of each sequence is calculated, and hence, the average, minimum, and maximum of these counts are found and represented by error bars.

Next, we verify the convergence of the non-asymptotic upper bound achieved in Theorem 2 to the asymptotic upper bound of the LZ78 parser. Using a lower bound on Lambert W function \(\ln x - \ln\left( {\ln x} \right) \leq W(x)\)  [73], we write \[\begin{aligned} {W\left( {\frac{\beta}{\alpha}\frac{\ln M}{M^{1 + 1/\alpha}}} \right)} & {= W\left( {\left( {\left( {M - 1} \right)l - \frac{M}{M - 1}} \right)\frac{\ln M}{M^{\frac{M}{M - 1}}}} \right)} \\  & {\approx W\left( {c_{M}l\ln M} \right)} \\  & {\geq \ln\frac{c_{M}l\ln M}{\ln\left( {c_{M}l\ln M} \right)}} \\  & {= \ln\frac{c_{M}l}{\log\left( {c_{M}l\ln M} \right)},} \end{aligned}\] where the logarithm is base \(M = \left| \mathcal{X} \right|\)  and \(c_{M} = \frac{M - 1}{M^{M/{({M - 1})}}}\) . Hence, we can further simplify the asymptotic upper bound of \(c(l)\)  as follows \[\begin{aligned} {c(l)} & {\leq \frac{l\ln M}{W\left( {\frac{\beta}{\alpha}M^{- 1 - 1/\alpha}\ln M} \right)}} \\  & {\leq \frac{l\ln M}{\ln\frac{c_{M}l}{\log\left( {c_{M}l\ln M} \right)}}} \\  & {= \frac{l}{\log\frac{c_{M}l}{\log\left( {c_{M}l\ln M} \right)}}} \\  & {= \frac{l}{\log l + \log c_{M} - \log\log\left( {c_{M}l\ln M} \right)}} \\  & {= \frac{l}{\left( {1 - \frac{\log\log l + \hat{c_{M}}}{\log l}} \right)\log l},} \end{aligned}\]  where \(\hat{c_{M}} = \log c_{M} - \log\log\left( {c_{M}\ln M} \right)\) . Therefore, as \(\left. l\rightarrow\infty \right.\) , we have \(c(l) \leq \frac{l}{\log l}\) . This is consistent with the binary case \(M = 2\)  proved in ([4] Lemma 13.5.3) or [5]. The following Lemma extends the result of ([4] Lemma 13.5.3) to \(\left| \mathcal{X} \right|\) -ary case.

Next, we analyze the properties of the number of distinct phrases \(c(l)\)  resulting from LZ78-parsing of an \(\left| \mathcal{X} \right|\) -ary sequence \(x_{1}^{l} = \left\{ {x_{n},n = 1,\ldots,l} \right\}\)  when l is fixed. The error bar representation in Figure 4 shows the variation of \(c(l)\)  when l is fixed. A possible explanation for such variations is that the statistical distribution of the pseudorandomly generated data are different from the theoretical distribution of the generating source. To elucidate this possibility, we enforce the exact matching of the source probability mass function and the empirical probability mass function of the generated data. Figure 5 represents the number of distinct phrases \(c(l)\)  resulting from LZ78-parsing of a binary sequence of fixed length where the characteristic of the generating source and the generated data matches. As seen, there is still some variation around the average value of \(c(l)\) . We can specify a distribution-dependent bound on \(c(l)\)  when both l and the distribution of the source are fixed.

In ([75] Theorem 1), for sequences generated from a memoryless source, \(c(l)\)  is assumed to be a random variable with the following mean and variance:\[\begin{aligned} {E\left( {c(l)} \right)} & {\sim \frac{hl}{\log l},} \\ {{Var}\left( {c(l)} \right)} & {\sim \frac{\left( {h_{2} - h^{2}} \right)l}{\log^{2}l},} \end{aligned}\]  where \(h = - \sum_{a \in \mathcal{X}}p_{a}\log p_{a}\)  is the entropy rate, and \(h_{2} = \sum_{a \in \mathcal{X}}p_{a}\log^{2}p_{a}\)  with \(p_{a}\)  being the probability of symbol \(a \in \mathcal{X}\) . Note that the approximations (4) are asymptotic as \(\left. l\rightarrow\infty \right.\) . Below, we obtain a finite sample characterization of \(c(l)\) .

Consider an \(\left| \mathcal{X} \right|\) -ary sequence \(x_{1}^{l} = \left\{ {x_{n},n = 1,\ldots,l} \right\}\)  with fixed length l generated from a source with the probability mass function \(p(x)\) . Here, the notations \(x_{1}^{l}\)  and \(x^{l}\)  are used interchangeably. Let \(c\left( {l,p} \right)\)  denote the number of distinct phrases resulting from LZ78-parsing of the sequence \(x_{1}^{l}\)  of length l and the generating probability mass function is defined by \(p(x)\) . In order to find a distribution-dependent bound on the number of distinct phrases in LZ78-based parsing of \(x_{1}^{l}\) , we note that since the generating distribution is not necessarily uniform, all the strings \(x^{n}\)  for \(n < l \ll \infty\)  do not necessarily appear as parsed phrases. For instance, consider the binary case with \(P\left( {X = 1} \right) = 0.9\) . Then, it is very unlikely to have a string with multiple consecutive zeros in any parsing of a realization of the finite sequence \(x^{l}\) . As such, using the Asymptotic Equipartition Properties (AEP) ([4] Chapter 3) or Non-asymptotic Equipartition Properties (NEP) [76], we define the typical set \(\mathcal{A}_{\epsilon}^{(n)}\)  with respect to \(p(x)\)  as the set of subsequences \(x^{n} \in \mathcal{X}^{n}\)  of \(x_{1}^{l}\)  with the property \[\begin{aligned} {2^{- n{({h + \epsilon})}} \leq} & {p\left( x^{n} \right) \leq 2^{- n{({h - \epsilon})}},} \end{aligned}\]  where h is the entropy. Then, we have \[\begin{aligned} 1 & {= \sum\limits_{x^{n} \in \mathcal{X}^{n}}p\left( x^{n} \right) \geq \sum\limits_{x^{n} \in \mathcal{A}_{\epsilon}^{(n)}}p\left( x^{n} \right) \geq \left| \mathcal{A}_{\epsilon}^{(n)} \right|2^{- n{({h + \epsilon})}},} \end{aligned}\]  therefore, \(\left| \mathcal{A}_{\epsilon}^{(n)} \right| \leq 2^{n{({h + \epsilon})}}.\)  Let \(l_{k}\)  be the sum of the lengths of all the distinct strings \(x^{n}\)  in the set \(\left| \mathcal{A}_{\epsilon}^{(n)} \right|\)  of length less than or equal to k. We write, \[\begin{aligned} l_{k} & {= \sum\limits_{n = 1}^{k}n\left| \mathcal{A}_{\epsilon}^{(n)} \right|} \\  & {\leq \sum\limits_{n = 1}^{k}n2^{n{({h + \epsilon})}}} \\  & {= \frac{1}{\left( {m - 1} \right)^{2}}\left\lbrack {\left( {\left( {m - 1} \right)k - 1} \right)m^{k + 1} + m} \right\rbrack,} \end{aligned}\]  where \(m \triangleq 2^{h + \epsilon}\) . Therefore, \(l = \frac{1}{\left( {m - 1} \right)^{2}}\left\lbrack {\left( {\left( {m - 1} \right)k - 1} \right)m^{k + 1} + m} \right\rbrack\)  can be solved for k which leads into an upper bound for \(c\left( {l,p} \right)\)  as follows \[\begin{aligned} {k =} & \frac{\alpha W\left( {\frac{\beta}{\alpha}m^{- 1 - 1/\alpha}\ln m} \right) + \ln m}{\alpha\ln m} \\ {c\left( {l,p} \right)} & {\leq \sum\limits_{n = 1}^{k}\left| \mathcal{A}_{\epsilon}^{(n)} \right| = \frac{m\left( {m^{k} - 1} \right)}{m - 1}} \\  & {= \frac{2^{k{({h + \epsilon})}} - 1}{1 - 2^{- h - \epsilon}},} \end{aligned}\]  where \(\alpha = m - 1\)  and \(\beta = \left( {m - 1} \right)^{2}l - m\) . Therefore, the dependency of the \(c\left( {l,p} \right)\)  upper bound on the distribution is only through the entropy. Figure 6 depicts the upper bound on \(c\left( {l,p} \right)\)  for \(\epsilon = 0.1\) .

## 2. Pattern Dictionary Parser Versus Lz78 Parser

Given an \(\left| \mathcal{X} \right|\) -ary sequence \(x_{1}^{l} = \left\{ {x_{n},n = 1,\ldots,l} \right\}\) , let \(c_{T}(l)\)  be the number of parsed phrases of \(x_{1}^{l}\)  when the typical encoder (pattern dictionary with \(D_{max}\) ) is used, and \(c_{A}(l)\)  be the number of parsed phrases of \(x_{1}^{l}\)  when the atypical encoder (LZ78) is used. Clearly, \(\frac{l}{D_{max}} \leq c_{T}(l) \leq l\)  where the lower bound is achieved when \(\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right) = \left\{ {x_{v_{1}}^{v_{2} - 1},x_{v_{2}}^{v_{3} - 1},\ldots,x_{v_{c}}^{l}} \right\}\) , and each \(x_{v_{i}}^{v_{i} - 1} \in \mathcal{S}_{\mathcal{D}}^{(D_{\max})}\) , namely \(x_{v_{i}}^{v_{i} - 1}\)  is of length \(D_{max}\)  and exists in the dictionary. The upper bound is achieved when \(\mathcal{S}_{\mathcal{D}}\left( x_{1}^{l} \right) = \left\{ {x_{1},x_{2},\ldots,x_{l}} \right\}\)  where each \(x_{n} \in \mathcal{S}_{\mathcal{D}}^{(1)}\) . Using the result of Theorem 2 and a lower bound on the Lambert W function, \(\ln x - \ln\left( {\ln x} \right) \leq W(x)\)  [73], we have \[\begin{aligned}  & {\frac{l}{D_{max}}\left( {1 - \frac{D_{max}}{\log\frac{l}{\log\left( {l\ln\left| \mathcal{X} \right|} \right)}}} \right) \leq c_{T}(l) - c_{A}(l)} \\  & { \leq l\left( {1 - \frac{\sqrt{8l + 1} - 1}{2l}} \right).} \end{aligned}\] The above bounds have asymptotic and non-asymptotic implications. The asymptotic analysis of the bounds in (5) suggests that as \(\left. l\rightarrow\infty \right.\) , for a dictionary with fixed \(D_{max}\) , we have \(\frac{l}{D_{max}} \leq c_{T}(l) - c_{A}(l) \leq l\) . This inequality implies the asymptotic dominance of the parser using a typical encoder. This is to be expected due to the asymptotic optimality of LZ78. However, the above inequality also implies a more interesting result: if \(D_{max} > \log\frac{l}{\log\left( {l\ln\left| \mathcal{X} \right|} \right)}\)  as \(\left. l\rightarrow\infty \right.\) , then \(c_{T}(l)\)  can be smaller than \(c_{A}(l)\) . The non-asymptotic behavior of the bounds in (5) is more relevant to the anomaly detection problem. These bounds suggest that for a fixed l and \(\left| \mathcal{X} \right|\) , increasing \(D_{max}\)  has a vanishing effect on the possible range of the anomaly score. Additionally, the achieved bounds on \(c_{T}(l) - c_{A}(l)\)  provide the range of values of the anomaly score. This facilitates the search for a data-dependent threshold for anomaly detection, as the search can be restricted to this range.

## 3. Atypicality Criterion For Detection Of Anomalous

Consider the problem of finding the atypical (anomalous) subsequences of a long sequence with respect to a trained pattern dictionary \(\mathcal{D}\) . Suppose we are looking for an infrequent anomalous subsequence \(x_{n}^{n + l - 1} = \left\{ {x_{n},n = n,\ldots,n + l - 1} \right\}\)  embedded in a test sequence \(\left\{ {x_{n},n = 1,\ldots,L} \right\}\)  from the finite alphabet \(\mathcal{X}\) . Using Equation (2), the typical codelength of the subsequence \(x_{n}^{n + l - 1}\)  is \[\begin{aligned} {L_{T}\left( x_{n}^{n + l - 1} \right)} & {= \sum\limits_{y \in \mathcal{S}_{\mathcal{D}}{(x_{n}^{n + l - 1})}}L_{\mathcal{D}}(y) + \left| {\mathcal{S}_{\mathcal{D}}\left( x_{n}^{n + l - 1} \right)} \right|\log D_{max},} \end{aligned}\]  while using LZ78, the atypical codelength of the subsequence \(x_{n}^{n + l - 1}\)  is \[\begin{aligned} {L_{A}\left( x_{n}^{n + l - 1} \right)} & {= \left| {\mathcal{S}_{LZ}\left( x_{n}^{n + l - 1} \right)} \right|\left\lbrack {\log\left| {\mathcal{S}_{LZ}\left( x_{n}^{n + l - 1} \right)} \right| + 1} \right\rbrack} \\  & {+ \log^{\ast}(l) + \tau,} \end{aligned}\]  where \(\log^{\ast}(l) + \tau\)  is an additive penalty for not knowing in advance the start and end points of the anomalous sequence [2,3], and \(\log^{\ast}(l) = \log l + \log\log l + \ldots\)  where the sum continues as long as the argument to the outer log is positive. Let \(L_{A}^{{}^{\prime}} = L_{A} - \tau\) . We propose the following atypicality criterion for detection of an anomalous subsequence:\[\begin{aligned} {\vartriangle L(n)} & {= \max\limits_{l}\left\{ {L_{T}\left( x_{n}^{n + l - 1} \right) - L_{A}^{{}^{\prime}}\left( x_{n}^{n + l - 1} \right)} \right\} > \tau,} \end{aligned}\]  where \(\tau\)  can be treated as an anomaly detection threshold. In practice, \(\tau\)  can be set to ensure a false positive constraint, e.g., using bootstrap estimation of the quantiles in the training data.

# Experiment

In this section, we illustrate the proposed pattern dictionary anomaly detection on a synthetic time series, known as Mackey–Glass [77], as well as on a real-world time series of physiological signals. In both experiments, first, the real-valued samples are discretized using a uniform quantizer [78], and then, anomaly detection methods are applied.

## 1. Anomaly Detection In Mackey–Glass Time

In this section, we illustrate the proposed anomaly detection method for the case of a chaotic Mackey–Glass (MG) time series that has an anomalous segment grafted into the middle of the sequence. MG time series are generated from a nonlinear time delay differential equation. The MG model was originally introduced to represent the appearance of complex dynamic in physiological control systems [77]. The nonlinear differential equation is of the form \(\frac{dx(t)}{dt} = - ax(t) + \frac{bx\left( {t - \delta} \right)}{1 + x^{10}\left( {t - \delta} \right)},\ t \geq 0\) , where a, b and \(\delta\)  are constants. For the training data, we generated 3000 samples of the MG time series with \(a = 0.2\) , \(b = 0.1\) , and \(\delta = 17\) . For the test data, we normalized and embedded 500 samples of the MG time series with \(a = 0.4\) , \(b = 0.2\) , and \(\delta = 17\)  inside 1000 samples of a MG time series generated from the same source as the training data, resulting in a test sequence of length 1500. Figure 7 shows a realization of the training data and the test data.

The anomaly detection performance of our proposed pattern dictionary is evaluated. To illustrate the effect of the model parameter, i.e., the maximum depth \(D_{max}\) , on the detection and compression performance of the pattern dictionary, we run two experiments. First, we use a 30-fold cross-validation on the training data (resulting in 30 sequences of length 100) and calculate the number of distinct parsed phrases against \(D_{max}\) . Second, we train a pattern dictionary with various \(D_{max}\)  using the training data and then evaluate the sensitivity of detector of the anomalous subsequences in the test data using Equation (6) with \(\tau = 0\) . In this experiment, the detection sensitivity (true positive rate) is defined as the ratio of number of samples correctly identified as anomalous over the total number of anomalous samples. Figure 8 illustrates the result of both experiments. As seen, after some point, increasing \(D_{max}\)  has diminishing effect on both detection sensitivity and the number of distinct parsed phrases. Note that this behavior is to be expected as it was suggested by the bounds in (5).

Next, we compare anomaly detection performance of our proposed pattern dictionary methods, PDD and PDA, with the nearest neighbors-based similarity (NNS) technique [7], the compression-based dissimilarity measure (CDM) method [12,13,14], Ziv–Merhav method (ZM) [48], and the threshold Sequence Time-Delay Embedding (t-STIDE) technique [8,9,10,11]. In this experiment, a window of length 100 is slid over the test data and each method measures the anomaly score (as described below) of the current subsequence with respect to the training data. The anomaly is detected when the score exceeds a threshold, determined to ensure a specified false positive rate. In the following, we compute AUC (area under the curve) of the ROC (receiver operating characteristic) and Precision-Recall curves as performance measures. In the following, we provide details of the implementation.

First, the training data are used to create a pattern dictionary with \(D_{max} = 40\) , as described in Section 4. Then, for each subsequence \(x^{100}\)  (the sliding window of length 100) of the test data, the anomaly score is computed as the codelength \(L\left( x^{100} \right)\)  of Equation (2) described in Section 4.3.

Similar to PDD, first the training data are used to create a pattern dictionary with \(D_{max} = 40\) , as described in Section 4. Then, for each subsequence \(x^{100}\)  of the test data, the anomaly score is the atypicality measure described in Section 5, i.e., \(L_{T}\left( x^{100} \right) - L_{A}\left( x^{100} \right)\) , the difference between the compression codelength of the test subsequence using typical encoder (pattern dictionary) and atypical encoder (LZ78).

In this method, a cross-parsing procedure is used in which for each subsequence \(x^{100}\)  of the test data, the anomaly score is computed as the number of the distinct phrases of \(x^{100}\)  with respect to the training data.

In this method, a list \(\mathcal{S}\)  of all the subsequence of length 100 (the length of the sliding window) of the training data is created. Then, for each subsequence \(x^{100}\)  of the test data, the distance between \(x^{100}\)  and all the subsequences in the list \(\mathcal{S}\)  is calculated. Finally, the anomaly score of \(x^{100}\)  is its distance to the nearest neighbor in the list \(\mathcal{S}\) .

In this method, given the training data \(x_{train}\) , for each subsequence \(x^{100}\)  of the test data the anomaly score is \[\begin{aligned} {CDM\left( x_{train},x^{100} \right)} & {= \frac{\mathcal{L}\left( {\mathcal{C}\left( {x_{train},x^{100}} \right)} \right)}{\mathcal{L}\left( x_{train} \right) + \mathcal{L}\left( x^{100} \right)},} \end{aligned}\]  where \(\mathcal{C}\left( {y,x} \right)\)  represents concatenation of sequences y and z, and \(\mathcal{L}(x)\)  is the size of the compressed version of the sequence x using any standard compression algorithm. The CDM anomaly score is close to 1 if the two sequence are not related, and smaller than one if the sequences are related.

In this method, given \(l < 100\) , for each sub-subsequence \(x^{l}\)  of the subsequence \(x^{100}\)  of the test data, the likelihood score of \(x^{l}\)  is the normalized frequency of its occurrence in the training data, and the anomaly score of \(x^{100}\)  is one minus the average likelihood score of all its sub-subsequences of length l. In this experiment, various values of l are tested and the best performance is reported.

We compare the detection performance of the aforementioned methods by generating 200 test data sequences with different anomaly segments (the anomalous MG segments have different initializations in each test dataset). The detection results of comparisons are reported in Table 2. As seen, our proposed PDD and PDA methods outperform the rest, with ZM and CDM coming in third place. The effect of alphabet size of the quantized data (the resolution parameter of the uniform quantizer [78]) on anomaly detection performance is summarized in Table 3. Table 3 shows that our proposed PDD and PDA methods outperform in all three cases of data resolution.

Since the parsing procedure of our proposed PD-based methods and the ZM method [48] are similar, it is of interest to compare the running time of these two methods. While the cross-parsing procedure of the ZM method was introduced as an on the fly process [48], we can also consider another implementation similar to our proposed PD by creating a codebook of all the subsequences of the training data prior to the parsing procedure. As such, in order to compare the running time of the dictionary/codebook creation and parsing procedure of our PD-based methods with the aforementioned two implementations of the ZM method, we use the same MG training data of length 3000, one test dataset of length 1500 while a sliding window of length 100 is slid over it for anomaly score calculation, and the PD-based method with \(D_{max} = 40\) . Note that since a sliding window of length 100 over the test data is considered, for the codebook-based implementation of ZM, all the subsequences of the training data up to length 100 are extracted which make its codebook creation process significantly faster. Table 4 summarizes the running time comparison. As it can be seen, our PD-based method is faster in both dictionary/codebook creation and parsing process.

## 2. Infection Detection Using Physiological Signals

Finally, we apply the proposed pattern dictionary method to detect unusual patterns in physiological signals of two human subjects after exposure to a pathogen while only one of these subjects became symptomatically ill. The time series data were collected in a human viral challenge study that was performed in 2018 at the University of Virginia under a DARPA grant. Consented volunteers were recruited into this study following an IRB-approved protocol and the data was processed and analyzed at Duke University and the University of Michigan. The challenge study design and data collection protocols are described in [79]. Volunteers’ skin temperature and heart rate were recorded by a wearable device (Empatica E4) over three consecutive days before and five consecutive days after exposure to a strain of human Rhinovirus (RV) pathogen. During this period, the wearable time series were continuously recorded while biospecimens (viral load) were collected daily. The infection status can be clinically detected by biospecimen samples, but in practice, the collection process of these types of biosamples can be invasive and costly. As such, here, we apply the proposed anomaly detection framework to the measured two-dimensional heart rate and temperature time series to detect unusual patterns after exposure with respect to the normal (healthy) baseline patterns.

In the preprocessing phase, we followed the wearable data preprocessing procedure described in [80]. Specifically, we first downsample the time series to one sample per minute by averaging. Then, we apply an outlier detection procedure to remove technical noise, e.g., sensor contact loss. After preprocessing, the two-dimensional space of temperature and heart rate time series is discretized using a two-dimensional uniform quantizer [78] with step size of 5 for heart rate and 0.5 for temperature, resulting in one-dimensional discrete sequence data. The first three days of data are used as the training data, and the PDA methods with maximum depth \(D_{max} = 30\)  are used to learn the patterns in the training data. In order to detect anomalous patterns of the test data (the last five days), we used the result of Section 5.3 and the atypicality criterion of Equation (6), which requires choosing the threshold \(\tau\) . While this threshold can be chosen freely, we selected it using cross-validation on the training data. Leave-one-out cross-validation over the training data generates an empirical null distribution of the PDA anomaly score function \(L_{T} - L_{A}\) . The threshold \(\tau\)  was chosen as the upper 99% quantile of this distribution. Figure 9 illustrates the result of anomaly detection on one subject who became infected as measured by viral shedding as shown in Figure 9C. All the anomalous patterns occur when the subject was shedding the virus. Figure 10 also depicts the result of anomaly detection on one subject who had a mild infection with a low level of viral shedding, as shown in Figure 10C. Note that in this case, no anomalous patterns were detected.

# Conclusions

In this paper, we have developed a universal nonparametric model-free anomaly detection method for time series and sequence data using a pattern dictionary. We proved that using a multi-level dictionary that separates the patterns by their depth results in a shorter average indexing codelength in comparison to a uni-level dictionary that uses a uniform indexing approach. We illustrated that the proposed pattern dictionary method can be used as a stand-alone anomaly detector, or integrated with Tree-Structured Lempel–Ziv (LZ78) and incorporated into an atypicality framework. We developed novel non-asymptotic lower and upper bounds of the LZ78 parser and demonstrated that the non-asymptotic upper bound on the number of distinct phrases resulting from LZ78-parsing of an \(\left| \mathcal{X} \right|\) -ary sequence can be explicitly derived in terms of the Lambert W function, an important theoretical result that is not trivial. We showed that the achieved non-asymptotic bounds on LZ78 and pattern dictionary determine the range of the anomaly score and the anomaly detection threshold. We also presented an empirical study in which the pattern dictionary approach is used to detect anomalies in physiological time series. In the future work, we will investigate the generalization of the context tree weighting methods to the general discrete case, using the pattern dictionary since the pattern dictionary handles sparsity well and is computationally less expensive when the alphabet size is large.

