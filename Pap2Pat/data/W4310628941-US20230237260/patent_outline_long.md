# DESCRIPTION

## BACKGROUND

- introduce anomaly detection
- describe fully supervised setting
- describe one-class classifiers
- describe unsupervised setting
- describe semi-supervised setting
- limitations of semi-supervised methods
- describe distribution mismatch
- describe challenges in real-world applications

## BRIEF SUMMARY

- introduce semi-supervised anomaly detection framework
- describe ensemble of one-class classifiers
- describe pseudo-labeling
- describe training predictor
- describe partial distribution matching
- describe method for anomaly detection
- receive unlabeled data
- determine pseudo labels
- assign pseudo labels
- train machine learning model
- describe training one-class classifiers
- describe determining pseudo labels
- describe training machine learning model with labeled data

## DETAILED DESCRIPTION

- introduce SPADE framework
- motivate semi-supervised anomaly detection
- describe distribution mismatch problem
- introduce pseudo-labeling mechanism
- describe limitations of binary classifiers
- motivate one-class classifiers (OCCs)
- describe FIG. 1 data distribution
- illustrate bias of binary classifiers
- describe FIG. 2 and FIG. 3 decision boundaries
- introduce SPADE framework architecture
- describe encoder 502 functionality
- describe predictor 504 functionality
- describe pseudo-labeler 506 functionality
- describe projection head 508 functionality
- describe FIG. 5 block diagram
- describe FIG. 6 pseudo-labeler architecture
- describe OCC ensemble functionality
- describe partial distribution matching
- describe loss functions for training
- describe optimization problem for training
- illustrate benefits of SPADE
- introduce anomaly detection datasets
- describe data division and evaluation metric
- motivate evolving anomalies
- construct datasets with multiple anomaly types
- compare SPADE with baselines
- describe limitations of baselines
- simulate easy-to-label and hard-to-label samples
- evaluate SPADE on easy-to-label and hard-to-label samples
- describe positive and unlabeled (PU) setting
- evaluate SPADE in PU setting
- describe real-world fraud detection datasets
- evaluate SPADE on real-world fraud detection datasets
- describe training and test data split
- evaluate SPADE on fraud datasets with different labeling ratios
- describe block diagram of computing environment
- describe SPADE implementation
- describe server computing device
- describe user computing device
- describe data storage and retrieval
- describe instructions and data
- describe user output and input
- describe data transmission and display
- describe multiple processors and memories
- describe remote computing devices
- describe machine learning framework
- describe network communication
- describe various network configurations
- describe distributed network of devices
- summarize SPADE framework
- describe implementation of SPADE
- describe configured systems and computer-readable storage media

