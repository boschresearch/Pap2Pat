# DESCRIPTION

## STATEMENT RE: FEDERALLY SPONSORED RESEARCH/DEVELOPMENT

- state not applicable

## BACKGROUND

### Technical Field

- introduce transformer-based model

### Background

- motivate fine-grained representation

## BRIEF SUMMARY

- receive input image and text
- tokenize input text
- generate patch groups
- mask image patches and text tokens
- generate training embedding
- train cross modality transformer-based model
- align input text tokens with image patches
- mask aligned image patches and text tokens
- train with multiple loss functions

## DETAILED DESCRIPTION

- introduce vision-linguistic models
- describe limitations of existing models
- propose domain-specific transformer-based model
- describe single-stream model architecture
- introduce kaleidoscope patch strategy
- describe pre-alignment strategy
- describe alignment-guided masking strategy
- describe pre-training approach
- describe application of model in fashion product search system
- describe training system for domain-specific transformer-based model
- describe patch generator functionality
- describe alignment guided masking functionality
- describe pre-training tasks
- introduce sub-tasks
- describe rotation recognition task
- describe jigsaw puzzle solving task
- describe camouflage prediction task
- describe grey-to-color and blank-to-color modeling tasks
- evaluate fine-grained patch cross-modality transformer model

