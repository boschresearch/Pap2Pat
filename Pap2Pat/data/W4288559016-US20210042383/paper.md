# Introduction

Automatic text summarizers condense a given piece of text into a shorter version (the summary). This is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible.

Existing summarization methods can be classified into two main types, either extractive or abstractive. Extractive methods select and order text fragments (e.g., sentences) from the original text source. Such methods are relatively simpler to develop and keep the extracted fragments untouched, allowing to preserve important parts, e.g., keyphrases, facts, opinions, etc. Yet, extractive summaries tend to be less fluent, coherent and readable and may include superfluous text.

Abstractive methods apply natural language paraphrasing and/or compression on a given text. A common approach is based on the encoder-decoder (seq-to-seq) paradigm (Sutskever et al., 2014), with the original text sequence being encoded while the summary is the decoded sequence.

While such methods usually generate summaries with better readability, their quality declines over longer textual inputs, which may lead to a higher redundancy (Paulus et al., 2017). Moreover, such methods are sensitive to vocabulary size, making them more difficult to train and generalize (See et al., 2017).

A common approach for handling long text sequences in abstractive settings is through attention mechanisms, which aim to imitate the attentive reading behaviour of humans (Chopra et al., 2016). Two main types of attention methods may be utilized, either soft or hard. Soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding (Cohan et al., 2018;Gehrmann et al., 2018;Hsu et al., 2018;Nallapati et al., 2016;Li et al., 2018;Pasunuru and Bansal, 2018;Tan et al., 2017). On the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process (Chen and Bansal, 2018;Nallapati et al., 2017;Liu et al., 2018).

Compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of "Editorial Network" (EditNet) -a mixed extractive-abstractive summarization approach. A summary generated by EditNet may include sentences that were either extracted, abstracted or of both types. Moreover, per considered sentence, EditNet may decide not to take either of these decisions and completely reject the sentence.

Using the CNN/DailyMail dataset we demonstrate that, EditNet's summarization quality is highly competitive to that obtained  EditNet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor. The key idea behind EditNet is to create an automatic editing process to enhance summary quality.

# Editorial Network

Let S denote a summary which was extracted from a given text (document) D. The editorial process is implemented by iterating over sentences in S according to the selection order of the extractor. For each sentence in S, the "editor" may make three possible decisions. The first decision is to keep the extracted sentence untouched (represented by label E in Figure 1). The second alternative is to rephrase the sentence (represented by label A in Figure 1). Such a decision, for example, may represent the editor's wish to simplify or compress the original source sentence. The last possible decision is to completely reject the sentence (represented by label R in Figure 1). For example, the editor may wish to ignore a superfluous or duplicate information expressed in the current sentence. An example mixed summary generated by our approach is depicted in Figure 2 in the appendix, further emphasizing the various editor's decisions.

Editor's automatic summary: E: what was supposed to be a fantasy sports car ride at walt disney world speedway turned deadly when a lamborghini crashed into a guardrail. A: the crash took place sunday at the exotic driving experience a . A: the lamborghini 's passenger , gary terry , died at the scene b . R: petty holdings , which operates the exotic driving experience at walt disney world speedway , released a statement sunday night about the crash.

a Original extracted sentence: "the crash took place sunday at the exotic driving experience , which bills itself as a chance to drive your dream car on a racetrack".

b Original extracted sentence: "the lamborghini 's passenger , 36-year-old gary terry of davenport , florida , died at the scene , florida highway patrol said"

Ground truth summary: the crash occurred at the exotic driving experience at walt disney world speedway. officials say the driver , 24-year-old tavon watson , lost control of a lamborghini. passenger gary terry , 36 , died at the scene.

Figure 2: An example mixed summary (annotated with the editor's decisions) taken from the CNN/DM dataset

## Implementing the editor's decisions

For a given sentence s ∈ D, we now denote by s e and s a its original (extracted) and paraphrased (abstracted) versions. To obtain s a we use an abstractor, whose details will be shortly explained (see Section 2.2). Let e s ∈ R n and a s ∈ R n further denote the corresponding sentence representations of s e and s a , respectively. Such representations allow to compare both sentence versions on the same grounds.

Recall that, for each sentence s i ∈ S (in order) the editor makes one of the three possible decisions: extract, abstract or reject s i . Therefore, the editor may modify summary S by paraphrasing or rejecting some of its sentences, resulting in a mixed extractive-abstractive summary S .

Let l be the number of sentences in S. In each step i ∈ {1, 2, . . . , l}, in order to make an educated decision, the editor considers both sentence representations e s i and a s i as its input, together with two additional auxiliary representations. The first auxiliary representation is that of the whole document D itself, hereinafter denoted d ∈ R The second auxiliary representation is that of the summary that was generated by the editor so far, denoted at step i as g i-1 ∈ R n , with g 0 = 0. Such a representation provides a local context for decision making. Given the four representations as an input, the editor's decision for sentence s i ∈ S is implemented using two fully-connected layers, as follows:

In each step i, therefore, the editor chooses the action π i ∈ {E, A, R} with the highest likelihood (according to Eq. 1), further denoted p(π i ). Upon decision, in case it is either E or A, the editor appends the corresponding sentence version (i.e., either s e i or s a i ) to S ; otherwise, the decision is R and sentence s i is discarded. Depending on its decision, the current summary representation is further updated as follows:

where W g ∈ R n×n are learnable parameters, g i-1 is the summary representation from the previous decision step; and h i ∈ {e s i , a s i , 0}, depending on which decision is made.

Such a network architecture allows to capture various complex interactions between the different inputs. For example, the network may learn that given the global context, one of the sentence versions may allow to produce a summary with a better coverage. As another example, based on the interaction between both sentence versions with either of the local or global contexts (and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both.

## Extractor and Abstractor

As a proof of concept, in this work, we utilize the extractor and abstractor that were previously used in (Chen and Bansal, 2018), with a slight modification to the latter, motivated by its specific usage within our approach. We now only highlight important aspects of these two sub-components and kindly refer the reader to (Chen and Bansal, 2018) for the full implementation details.

The extractor of (Chen and Bansal, 2018) consists of two main sub-components. The first is an encoder which encodes each sentence s ∈ D into e s using an hierarchical representation1 . The second is a sentence selector using a Pointer-Network (Vinyals et al., 2015). For the latter, let P (s) be the selection likelihood of sentence s.

The abstractor of (Chen and Bansal, 2018) is basically a standard encoder-aligner-decoder with a copy mechanism (See et al., 2017). Yet, instead of applying it directly only on a single given extracted sentence s e i ∈ S, we apply it on a "chunk" of three consecutive sentences2 (s e -, s e i , s e + ), where s e -and s e + denote the sentence that precedes and succeeds s e i in D, respectively. This in turn, allows to generate an abstractive version of s e i (i.e., s a i ) that benefits from a wider local context. Inspired by previous softattention methods, we further utilize the extractor's sentence selection likelihoods P (•) for enhancing the abstractor's attention mechanism, as follows. Let C(w j ) denote the abstractor's original attention value of a given word w j occurring in (s e -, s e i , s e + ); we then recalculate this value to be C (w j ) = C(w j )•P (s) Z

, with w j ∈ s and s ∈ {s e -, s e i , s e + }; Z = s ∈{s e -,s e i ,s e + } w j ∈s C(w j )•P (s ) denotes the normalization term.

## Sentence representation

Recall that, in order to compare s e i with s a i , we need to represent both sentence versions on as similar grounds as possible. To achieve that, we first replace s e i with s a i within the original document D. By doing so, we basically treat sentence s a i as if it was an ordinary sentence within D, where the rest of the document remains untouched. We then obtain s a i 's representation by encoding it using the extractor's encoder in a similar way in which sentence s e i was originally supposed to be encoded. This results in a representation a s i that provides a comparable alternative to e s i , whose encoding is expected to be effected by similar contextual grounds.

## Network training

We conclude this section with the description of how we train the editor using a novel soft labeling approach. Given text S (with l extracted sentences), let π = (π 1 , . . . , π l ) denote its editing decisions  (Nallapati et al., 2017) 39.60 16.20 35.30 EditNet E 38.43 18.07 35.37 Refresh (Narayan et al., 2018) 40.00 18.20 36.60 Rnes w/o coherence (Wu and Hu, 2018b) 41.25 18.87 37.75 BanditSum (Dong et al., 2018) 41.50 18.70 37.60 Latent (Zhang et al., 2018) 41.05 18.77 37.54 rnn-ext+RL (Chen and Bansal, 2018) 41.47 18.72 37.76 NeuSum (Zhou et al., 2018) 41.59 19.01 37.98 BERTSUM (Liu, 2019) 43.25 20.24 39.63 Abstractive Pointer-Generator (See et al., 2017) 39.53 17.28 36.38 KIGN+Prediction-guide (Li et al., 2018) 38.95 17.12 35.68 Multi-Task(EG+QG) (Guo et al., 2018) 39.81 17.64 36.54 EditNet A 40.00 17.73 37.53 rnn-ext+abs+RL (Chen and Bansal, 2018) 40.04 17.61 37.59 RL+pg+cbdec (Jiang and Bansal, 2018) 40.66 17.87 37.06 Saliency+Entail. (Pasunuru and Bansal, 2018) 40.43 18.00 37.10 Inconsistency loss (Hsu et al., 2018) 40.68 17.97 37.13 Bottom-up (Gehrmann et al., 2018) 41.22 18.68 38.34 DCA (Celikyilmaz et al., 2018) 41 editor's decision (including when updating g i at each step i according to Eq. 2). Following (Chen and Bansal, 2018), we set m = 512 and n = 512.

We trained for 20 epochs, which has taken about 72 hours on a single GPU. We chose the best model over the validation set for testing. Finally, all components were implemented in Python 3.6 using the pytorch 0.4.1 package.

## Results

Table 1 compares the quality of EditNet with that of several state-of-the-art extractive-only or abstractive-only baselines. This includes the extractor (rnn-ext-RL) and abstractor (rnn-ext-abs-RL) components of (Chen and Bansal, 2018) that we utilized for implementing EditNet 4 . We further report the quality of EditNet when it was being enforced to take an extract-only or abstract-only decision, denoted hereinafter as EditNet E and EditNet A , respectively. The comparison of EditNet to both EditNet E and EditNet A variants provides a strong empirical proof that, by utilizing an hybrid decision approach, a better summarization quality is obtained.

Overall, EditNet provides a highly competitive summary quality, where it outperforms most baselines. Interestingly, EditNet's summarization quality is quite similar to that of NeuSum (Zhou et al., 2018). Yet, while NeuSum applies an extraction-only approach, summaries generated by EditNet include a mixture of sentences that have been either extracted or abstracted.

Two models outperform EditNet, BERTSUM (Liu, 2019) and DCA (Celikyilmaz et al., 2018). The BERTSUM model gains an impressive accuracy, yet it is an extractive model that utilizes many attention layers running in parallel with millions of parameters (Devlin et al., 2019). DCA gains a comparable quality to EditNet, it outperforms on R-2 and slightly on R-1. The contextual encoder of DCA is comprised of several LSTM layers one on top of the other with varied number of agents (hyper-tuned) that transmit messages to each other. Considering the complexity of these models, and the slow down that can incur during training and inference, we think that EditNet still provides a useful, high quality and relatively simple extension on top of standard encoder aligned decoder architectures.

On average, 56% and 18% of EditNet's decisions were to abstract (A) or reject (R), respectively. Moreover, on average, per summary, EditNet keeps only 33% of the original (extracted) sentences, while the rest (67%) are abstracted ones. This demonstrates that, EditNet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary's quality.

# Conclusions and Future Work

We have proposed EditNet -a novel alternative summarization approach that instead of solely applying extraction or abstraction, mixes both together. Moreover, EditNet implements a novel sentence rejection decision, allowing to "correct" initial sentence selection decisions which are predicted to negatively effect summarization quality. As future work, we plan to evaluate other alternative extractor-abstractor configurations and try to train the network end-to-end. We further plan to explore reinforcement learning (RL) as an alternative decision making approach.

## Dataset and Setup

We trained, validated and tested our approach using the non-annonymized version of the CNN/DailyMail dataset (Hermann et al., 2015). Following (Nallapati et al., 2016), we used the story highlights associated with each article as its ground truth summary. We further used the F-measure versions of ROUGE-1, ROUGE-2 and ROUGE-L as our evaluation metrics (Lin, 2004). The extractor and abstractor were trained similarly to (Chen and Bansal, 2018) (including the same hyperparameters). The Editorial Network (hereinafter denoted EditNet) was trained according to Section 2.4, using the ADAM optimizer with a learning rate of 10 -4 and a batch size of 32. Following (Dong et al., 2018;Wu and Hu, 2018a), we set the reward metric to be r(•) = αR-1(•) + βR-2(•) + γR-L(•); with α = 0.4, β = 1 and γ = 0.5, which were further suggested by (Wu and Hu, 2018a).

We further applied the Teacher-Forcing approach (Lamb et al., 2016) during training, where we considered the true-label instead of the

