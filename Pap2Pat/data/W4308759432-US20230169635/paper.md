# Related Works

Generative Adversarial Networks The original GAN formulation [14] creates samples that resemble those of a target distribution from a random noise vector. Since this vector is not interpretable, several approaches have been proposed to improve this aspect. Chen et al. [9] map independent dimensions of the input space to different attributes in the output, while [15] uses known labels as additional inputs to control the generation process. StyleGAN [8] proposes to disentangle content and style by using the input noise vector to generate a set of learned affine transformations that control different aspects of the image style at different stages of the network. In all those works, however, the style depends on a highly dimensional latent space that cannot be easily interpreted. Thus, generating an image with a specific set of features can only be achieved by either brute-force sampling or tuning some preexisting configuration.

Image-to-image translation Works in the image-to-image translation literature attempt to learn a mapping from an image domain to another (day to night, summer to winter. . . ). GAN-based works like Pix2Pix [2] or Pix2PixHD [7] learn this transformation using paired images in the two domains. This setup allows for explicit supervision by directly comparing the generated and expected images at a pixel or feature level, achieving very realistic results. However, obtaining paired datasets is often difficult and for certain tasks outright impossible. Cyclic-consistent architectures such as CycleGAN [1], DiscoGAN [16] or CyCADA [17] remove this constraint, enabling training with unpaired data in two different domains. Since these networks learn a uni-modal mapping by design, it is difficult to add variability or control the output. The common approach of feeding random noise does not work for these methods, as the networks tend to ignore it [2]. Multimodal image-to-image translation approaches, such as [10,5,4,6], have been recently proposed to obtain multiple outputs from a single input. They implement variability by sampling from a latent space, a parameterization not easily controllable or interpretable without starting from some preexisting configuration. StarGAN [18] learns multiple image transformations using a single model by using attribute labels for guidance. Contrary to our approach, this work does not explore the interpolation between source and target domains. Additionally, it only focuses on discrete attribute transformations, whereas we also explore parametrized transformations over a continuous domain.

Interpretable Parameterizations Recent approaches have attempted to shed light on the interpretability of the latent spaces used to condition generative models. [19] generates images disentangling content and style by conditioning the former on a semantic segmentation mask and the latter on a reference image. Approaches such as [20,21] learn a controllable transformation for the face-aging task. However, they still rely on a categorical parameterization (a discrete set of transformations) while our model smoothly interpolates along a valid range. HoloGAN [12] generates multiple viewpoints of the same image content produced by a single random latent vector. The network is specifically designed for the 3D rotation task, as it explicitly uses a rigid-body motion layer, thus it cannot be extended to non-geometric transformations. Recently [13] proposes a network that, given an image of a face, learns to estimate the parameterization required to generate a similar 3D avatar model using a rendering engine. This task specific formulation does not generalize well either. [22] modifies CycleGAN to increase the resolution of faces conditioned by a one-hot encoded attribute vector (skin color, hair, glasses. . . ). This approach is also task-specific and discrete. A similar strategy is followed by [23], where the conditioning parameters are intended as a "guidance" mechanism to improve the reconstruction, although realistic outcomes can be obtained by changing the parameters. This approach needs paired data, a requirement we wish to avoid.

Recent works focused on exploring the learned latent space and determine which dimensions alter a characteristic. [24] attempts to find the decision boundary between binary labels, and [25] learns trajectories that conform to a smooth transformation of some feature (zoom, viewpoint. . . ). [26] produces images of high quality and learns a style interpolation, however, it still needs to extract the style code from a given image.

# Method

Given a source image domain X, a target image domain Y and a set of possible parametrizations for a transformation P ⊂ R n , we aim to implement a parametrizable mapping G P : X, P → Y . We learn this by means of a neural network that generates a valid sample y ∈ Y by taking as input a 

## Preliminaries: CycleGAN

Our work builds on top of the cyclic-consistent formulation first introduced by CycleGAN [1], a GAN architecture designed to learn a mapping function G : X → Y between unpaired image domains X and Y . To achieve this objective the authors leverage an additional inverse mapping function F : Y → X and one discriminator for each image domain, i.e., D X and D Y . The optimization objective is expressed as: expressed as:

where G and F are trained to minimize L, while D X and D Y to maximize it. Our work starts from a similar formulation and extends it to produce controllable transformations by means of parametrization-aware generators and discriminators.

## Learning parametrizable transformations

Since our objective is learning parametrizable transformations, we extend the generators and discriminators of a cyclic architecture to have as additional input a conditioning vector denoted as p. We refer to those networks adding a P superscript: G P , F P , D P X , D P Y . To maximize the generality of the proposed solution we encode the parameters of the transformation in a n-dimensional vector of floats: p ∈ P ⊂ R n . In Fig. 2 we report a schematic representation of half of a training iteration for the framework. A full train iteration will include a mirrored version of the architecture swapping x and y and inverting the order of the generators and discriminators.

Generators. The generator architecture is based on a classic hourglass architecture, where a set of convolutional layers encode the image into a compressed representation that is fed to a series of residual blocks. In parallel, a stack of fully connected layers learn to transform the raw values of p to a higher dimensionality representation. At the bottleneck layer of the generator the image representation and the transformed p are mixed by concatenation to every spatial location along the feature dimension. Then a series of convolutional layers decode the combined data into the final image. Instance normalization is used after each convolution with ReLU as activation function.

To compute L cyc , at training time, we provide both generators with the same conditioning vector p. Therefore, G P learns to apply the transformation parametrized by p while F P learns to undo it.

Discriminators. Similarly to the generators, we extend the discriminator formulation of CycleGAN to take into account the transformation of the parameterization to learn. We denote these modified components as D P X and D P Y respectively. An initial convolutional layer operates solely on the image to extract a suitable deep representation. In parallel p is transformed through a series of fully connected layers in a similar fashion as in the generator. The learned representations of the image and of p are mixed by concatenation to every spatial location in the same way as the generators. Afterwards, a series of convolutional layers operate on the mixed representation to produce a real/fake classification score for every image patch. The final prediction of the network is obtained with a mean average pooling over all patches. LeakyReLU is used as activation function, with instance normalization layers after every convolution.

Optimization objective. The overall optimization goal of ParGAN extends Eq. 3 to take into account the new parametric components and to rely on mean squared classification [27]. We denote with (x, y, p) ∼ (X, Y , P ) the distribution of training examples. Each training sample is composed of a random image from X and an image from Y with its corresponding parametrization p. We formalize our objective as:

Eq. 6 defines the optimization objective composed of two GAN losses (defined in Eq. 4) and a cyclic consistency loss (defined Eq. 5) weighted by a scalar λ. The generators (G P and F P ) are optimized to minimize the objective, while the discriminators (D P X and D P Y ) to maximize it.

Soft Parametrization: As we will show in Sec. 4.3 our formulation can be used even when the training set does not provide an explicit parametrization but only a source (X) and a target domain (Y ). In this setup, we can still use our framework to produce not only target images, but also to smoothly interpolate between the two domains. First, we define a new target domain composed of X ∪ Y and associate to each sample a one-dimensional parametrization vector p. We use as parametrization the value 0 for samples from X and 1 for samples from Y . Thus our target domain becomes {(x 1 , 0) . . . (x m , 0), (y 1 , 1) . . . (y n , 1)}.

Once we have created this softly parametrized target dataset, we train the GAN system according to Eq. 6. Once G P is trained, we can use as parametrization any real value between [0, 1] to generate transformations that smoothly interpolate between the source and the target domains.

When provided with many target domains (Y 0 , . . . , Y n ), we can apply the same technique using an n-dimensional vector for p. The parametrization corresponds to a vector of zeros for samples originated from X, while samples originated from the k th target domain Y k have the k th entry set to one (one-hot encoding). In Sec. 4.4 we show that the generator not only learns to smoothly interpolate between the source and any of the target domains independently, but also, to some extent, to mix the different transformations without having direct access to references for this kind of transformations during training.

# Experimental Results

Sec. 4.1 present our experimental setup, In Sec. 4.2 we show how we learn to replicate explicitly parametrized transformations. Sec. 4.3 shows the use of a soft parametrization approach to learn a smooth interpolable transformation. Finally Sec. 4.4 shows how to combine multiple transformations in an unsupervised fashion. Due to space constraint more experimental results and a comparison to other methods are reported in the supplementary.

## Experimental Setup

We consider various datasets for experimental evaluation. Although some include paired samples (each image has a correspondence in the transformed domain), we always assume unpaired data. All results reported in this paper have been generated using unseen images from the validation set. Book covers. To experiment with the idea of learning complex data augmentations we have generated an ad-hoc dataset of book covers randomly illuminated by a single beam of light. We start from clean and perfectly lit images from [28], which we use as source domain (X). Then we simulate the effect of a beam of light illuminating the cover with the Blender rendering engine1 and use these images as target domain (Y ). Each image is parametrized by a three dimensional float vector, with the first two dimensions encoding the (x, y) coordinates of the center of the light in the image reference system, and the last the normalized intensity of the light. SYNTHIA. In order to experiment with a soft parameterization, we learn a transformation between different subsets of the SYNTHIA [29] dataset. The dataset has 5 sequences, out of which we use 01, 02, 04 and 05 for training and 06 for evaluation.

INIT. This dataset [30] consists of real-world dashcam images of driving scenes subdivided by weather conditions. We have used the sunny and night categories, with an 80:20 training/test split.

To measure quantitatively the impact of our transformation with respect to the source and target domains, we use the Fréchet Inception Distance (FID) [31] 2 and the Learned Perceptual Image Patch Similarity (LPIPS) [32] 3 . FID compares statistics of Inception-v3 activations to measure the distance between two image distributions, while LPIPS explicitly compares pairs of images and evaluates the perceptual distance between image patches.

## Explicit Parametrization

We first investigate if our model can learn an explicitly parametrized transformation when provided with a target dataset of annotated samples such as Book-Covers.

In Fig. 3 we show different images generated by ParGAN starting from the same source image but with different parametrization vectors as input. The generator correctly learns to generate realistic images augmented with a beam of light according to the requested parametrization. For example in the last row the position of the light beam is fixed and only the beam intensity varies. The picture also shows how the generator learns a smooth and disentangled transformation function even if trained with just sparse samples of the space of possible transformations parametrized by P . Our formulation is the first to provide this level of explicit and human interpretable control over the transformation process.

In Tab. 1 we report the FID and the LPIPS metrics computed between different sets of real and generated samples parametrized by different p. We keep two axes of p fixed and modify the remaining entry.

The tables show that when both distributions are parametrized by the same conditioning vector, the corresponding distances are minimal (bold value along the diagonal of the tables). In case of a parameterization mismatch, instead, the distance between the two distributions tends to increase proportionally to the distance between the two parametrizations. These results show that, for the Book-Covers dataset, our network can correctly learn the transformation parametrized by p.  

## Soft Parametrization

We now test ParGAN for the case when no explicit parametrization is provided at training time using the soft parametrization technique described in Sec. 3.2. We train a mapping between two domains and evaluate the impact of the parametrization by sampling different values for p in [0, 1]. In this scenario the input parametrization acts as a knob to control the intensity of the transformation being applied, with 0 not modifying anything (source → source mapping) and 1 applying the complete transformation (source → target). Interestingly, the generator learns to smoothly interpolate between the two states even if during training no explicit supervision for this task was provided. In Fig. 4 and Fig. 1 we show images generated using this configuration. Ideally, the distance between the source and transformed images should increase with p, as the GAN progressively introduces more changes. Likewise, the distance to the target domain should decrease. We verify this experimentally and in Tab. 2 we report the FID and LPIPS scores for the SYNTHIA summer→winter and INIT day→night tasks comparing the result images with randomly picked sets of source and target samples. The distance to the target images is often much larger than for the source domain, since in one case we are applying a complex image transformation and in the other a simple identity mapping.  

## Simultaneous learning of transformations

We test the behavior of our model when using a single source domain and multiple targets. Following the strategy outlined in Sec. 3.2 we use a soft-parameterization associating to each target domain a one-hot encoded categorical vector p.

Thus, for a two-target transformation, the source domain has label p = [0, 0], and the two target domains have p = [1, 0] and p = [0, 1] respectively.

We train in the same fashion as the previous experiments for the summer→winter+night task on the SYNTHIA dataset and report the result in Fig. 5. Each transformation is learned independently and does not conflict with the others (the generator learns a disentangled mapping). We refer to the supplementary material for a qualitative and quantitative comparison between the transformations learned jointly and independently as well as for experimental results.

# Mixing Transformations

ParGAN can learn to mix the two transformations to some extent, producing images that display features from different domains (e.g. dark sky and snow). We show some results in Fig. 6.  

# Conclusions

We have proposed ParGAN, a new model for image-to-image translation using intuitive parameterizations to control the outcome. Our formulation is able to learn not only how to replicate complex parametric transformations, but also how to smoothly interpolate between disjoint domains using only a soft categorical parameterization. Additionally, our method does not rely on prior knowledge or ad-hoc architectures. While those approaches would have likely lead to faster convergence and higher reconstruction quality, it would constrain the versatility of the method to a small range of tasks, requiring fundamental changes in order to learn different transformations. Since human interpretability is at the core of our proposal, our network always takes a parameterization of the transformation encoded in real-world magnitudes. Finally, we have shown some promising results on learning how to mix different transformations in a completely unsupervised fashion.

In the future, we wish to test our method on more parametric transformations to further verify the generality of the proposed solutions. Moreover, our method still generates some artifacts while trying to mix different transformations. We believe that this issue might be mitigated by explicitly adding an adversarial component in the loss function to target these use cases. We will explore this direction in future works.

# Broader Impact

Despite their recent development, learning-based approaches for image transformation have already found applications in various fields, such as entertainment or content generation. We believe that tasks such as object detection or semantic segmentation can benefit from our method by increasing the amount of available training data. Specially, our method can be of particular interest to reduce bias for unbalanced categories, such as day and night in the context of autonomous driving. This can help to increase accuracy on underrepresented modalities. Parametrizable methods like ours can also determine the kind of data to generate more precisely based on particular requirements. Examples are constraining the location of an augmentation to a particular area, or limiting the percentage of a global transformation to apply (such as 50% snow). In addition to data augmentation techniques, the network can also be deployed as part of image editing tools to allow end users to edit images directly.

While the quality of GAN-generated imagery keeps improving, some generated images may still present some artifacts. This is not only aesthetically unpleasant but it can also impact the success of downstream tasks inducing unwanted biases into the generated datasets. A recent work [33] also points out that images generated by cyclic architectures might contain invisible artifacts not present in the original image distribution, again a source of bias in the data set. The main way to fight these unwanted artifacts is to increase the quality of the generated images (i.e., better architectures and better adversarial loss functions) as well as to add explicit regularization to limit this type of misbehavior.

## Quantitative analysis of explicit parameterization

In order to measure the quality of our learned transformations, we measure the similarity (in terms of FID and LPIPS between a ground truth image with known transformation properties and the same book cover with varying values in p. In Tables 1 and2 we report the results when changing the values of the x and illumination axes for Book-Covers.

Gen These results complement Tab. 1 in the main paper and show how our generator can correctly apply the desired transformation in most cases. This is testified by the distances between generated and real images being usually lower when the parametrization between the two sets matches (i.e., values along the diagonal).

## Disentanglement

In Fig. 2 we show the disentangling capabilities of our model applied to the Book-Covers dataset. Each axis can be modified independently with no effect over the others, and arbitrary combinations are also feasible. We show in Fig. 3 generated images for the SYNTHIA dataset. We use the summer subset as source domain and learn multiple transformations (summer→winter, summer→fog, summer→sunset, summer→softrain).

In Fig. 4 we show qualitative results using the INIT dataset for the day→night task. The network learns to interpolate between the two domains, progressively darkening the image in the necessary areas (sky, road) and adding elements proper of the target domain (car lights, street lights).

Similarly to the Book-Covers dataset, the network can learn multiple domain transformations simultaneously. We show this in Fig. 5 for the summer→dawn+sunset+night task. A single model is trained to learn three different domain mappings. No conflicts between the domains (such as artifacts) are noticeable.

We measure the quality of these transformations numerically using the FID [31] and LPIPS [32] metrics. We evaluate the model with different values of p on a random set of images from the source domain and measure the distance to different random subsets of both source and target images. Ideally, images transformed with p = 0 have the lowest metric value when compared against the source domain, as the network simply performs an identity map.

The opposite happens when comparing with the target images. The most similar image set is that with p = 1. We report our findings on Tab. 3 for individual transformations and Tab. 4 for entangled ones. The results show how the network is able to progressively make the image more dissimilar from the source domain and more similar to the target one.

Vs  3: Average FID and LPIPS scores for multiple tasks using subsets of SYNTHIA comparing the result image with randomly picked source and target images. A darker background color denotes a lower distance between the two sets of images.  Table 4: Average FID and L-PIPS distance for summer→winter+night transformations on arbitrary configurations. The images at the top are representatives of the transformation. In bold the set with the smallest distance row-wise is highlighted.

## Transformation quality

We are interested in determining whether the addition of the conditioning vector p has a detrimental effect on the quality of the reconstruction. To this end, we measure the FID and LPIPS distances on images produced by CycleGAN [1] and our method parametrized with p = 1, with respect to the target domain. We train both models using the same data and hyperparameters. In Tab. 5 we show that the quality of our transformations is similar to that of existing formulations.

CycleGAN  5: Comparison between the original CycleGAN formulation and our approach for full domain transformation (p = 1). Both models have similar FID and LPIPS scores. The deviation from the mean measured on LPIPS is also similar for both models.

## Mixing Transformations

In Fig. 6 we also show generated images when all values of p are nonzero, i.e., when asking to mix disjoint transformations. The generator learns to mix the two transformations, to some extent, generating images with a mixture of Night and Winter that are not part of the training sets (e.g., p = [0.75, 0.75]). In general, mixing styles can be considered a "tug of war" between the transformations, where each style "pulls" to bring the image closer to its target domain. Since multiple styles are doing this simultaneously, the result image shows several independent transformations that do not affect each other unless they alter the same area of the image (e.g. adding snow on the side of the road and a night sky at the same time causes little conflict). The values of the different entries of p determines the level of compromise to reach. When two styles collide (e.g. two sky transformations) some artifacts appear in the affected region (see Fig. 6 for p = [0.5, 0.75]). As expected, the higher the value of an individual element of p is with respect to others, the more significant its presence. For example, for the p = [0.25, 0.75] case in a summer→winter+night task, the amount of snow is fairly small and the image is significantly darkened. When p 1 increases, the night style loses its dominance and the images becomes slightly lighter and snowier, but nevertheless it still shows some darkness.  

## Feature-wise transformation

An interesting effect of the interpolation is the fact that, contrary to a simple blending between two images, the network transforms the global style and different regions of the image at different steps (sky, terrain, snow, rain. . . ). In Fig. 7 we show this phenomenon for the summer→softrain and summer→winter cases.  

## Latent space analysis

As already mentioned, our method implicitly learns an interpolation between domains and a way of mixing them. We are interested in analyzing how this behavior reflects on the inner latent space learned by the network. To this end, we collect the activations of the network right after the image and the conditioning vector information are mixed for different values of p and plot them on a 3 dimensional space using PCA decomposition. In the left side of Fig. 8 we show the results for summer→winter+night+softrain when sampling different images from the same SYNTHIA sequence and applying to each one disentangled values of p ranging from 0 to 1. Smooth changes in one of the parametrization dimensions make the learned image representations move smoothly in the latent space along well specified and pretty consistent trajectories. These results agree with the recent findings of [25] that show how basic transformations are implicitly learned by generative models as trajectories on a latent space. We show that our model has similar properties while addressing an image-to-image task and providing an explicit control on how to move inside the learned trajectories, i.e., the parametrization vector.

On the right part of Fig. 8 we add to the plot the latent space vectors obtained by mixing multiple transformations (e.g. [0.25, 0.5, 1]) to the same set of images. The newly added points falls within the three trajectories described by the disjoint transformations. Therefore our model is able to remap the content of an image and the transformation to be applied in a shared latent space where individual domain to domain transformation remap to trajectories while mixing domains transformation remaps to points within these trajectories.

Figure 8: Visualizations of the latent space activations for the summer→winter+night+softrain task. Left: Latent space activactions for disentangled transformations. Right: Same disentangled transformations plus entangled ones. The color code for each dimension of p ranges from blue for p = 0 to red (p = 1). The transformations are laid out following three different trajectories (one per dimension of p, therefore one per style) and follow a clear trajectories away from p = 0. The mixed vectors are all contained in the volume spanned by the disentangled trajectories.

# Supplementary material for ParGAN: Learning Real Parametrizable Transformations

In the main body of this paper we have proposed our method for controllable image-to-image translation and showed its capabilities on different datasets. In this supplement, we show additional experiments and qualitative results in order to further analyze the properties of our method and how it compares to existing approaches.

In Sec. 1.1 we describe the datasets used to evaluate our method. Sections 1.2,1.3 and 1.4 report additional quantitative and qualitative results. In Sec. 1.5 we compare our model to a standard CycleGAN [1] to show that the addition of the parametrization does not affect the generated image quality. In Sec. 1.6 we show how our models learns to some extent to interpolate between different domain even without having ever received explicit supervision for it In Sec. 1.7 and Sec. 1.8 we provide some intuition on how the network learns parametric transformations. In Sec. 1.9 we show some interesting domain generalization result on image acquired by a phone. Finally we conclude with some implementation details, Sec. 1.10, and ablation studies on the better way of injecting p in the generator and discriminator networks Sec. 1.11.

# Summer

Winter Night Softrain Fog Sunset We explore the capabilities of our model in two different tasks: learning a transformation guided by an explicit parametrization and domain translation using soft labels. We use both real and synthetic data. In Fig. 1 we show several samples from the datasets that we used for training.

## Domain generalization

We also briefly explore the domain generalization capabilities of this method using real-world datasets for evaluation. In Fig. 9 we evaluate a pre-trained network for the summer→ night+rain task trained with images from the SYNTHIA dataset on real images acquired by a phone. The network correctly segments the different components of the image (sky, buildings) and adds the appropriate effects (clouds, lights).   6: Different configurations on how p is included in the generator and discriminator and its effects on quality. We use the same measurement strategy as in Tab. 3.

## Implementation details

Our model for soft-parameterization extends CycleGAN with a series of fully connected layers that transform the conditioning vector by upsampling it to a 64-dimensional vector. This is replicated and concatenated to the feature dimension of the output of the last ResNet block. In the Book-Covers case, we additionally use skip connections between the encoder and decoder parts of the generator, to which we also concatenate the upsampled conditioning vector, as well as multi-scale discriminators. We train for all experiments using a batch size of 1 and an image size of 512 × 512. We implement our framework on TensorFlow 1 [34] following the architecture of CycleGAN [1] 4 with the additions already discussed in Sec. 3.2. We use the Adam [35] optimizer in all our experiments with a learning rate of 0.0001 and β 1 = 0.5. Following the guidelines of the original CycleGAN architecture, we set λ = 10 and use a batch size of 1. The number of training iterations varies between different tests but roughly corresponds to 200 epochs.

## Alternatives for the concatenation of p

The conditioning vector p is transformed through a series of fully connected layers and finally concatenated at a certain step of the generators and discriminators. As additional ablation studies we analyze the effect of injecting it at different levels of the architectures and report the results in Tab. 6. We train multiple variants of our architecture for the summer→winter tasks and compare the different alternatives by measuring FID with respect to a random source images (when conditioning with p = 0) and with respect to a random target image (when conditioning with p = 1). The performances of all the alternatives are fairly close, but injecting p just once for both the generator and discriminator has the advantage of being the simpler strategy and providing the best performance. For this reason we selected this option and presented it in the main paper.

