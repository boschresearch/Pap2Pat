{
    "id": "https://semopenalex.org/work/W4372270126",
    "authors": [
        "Shinji Watanabe",
        "Kilian Q. Weinberger",
        "Ki Jin Han",
        "Ryan McDonald",
        "Yoav Artzi",
        "Felix Wu",
        "Kwangyoun Kim"
    ],
    "title": "Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages",
    "date": "2023-06-04",
    "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task \u2014 transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Self-supervised pre-trained models have recently become a core part of speech models [5,10,13,21,26,51,56,62], leading to impressive performance on a wide variety of speech tasks [41,53,58,66]. This trend mirrors recent success in natural language processing [NLP; 16,33,36,44,48,49] and computer vision [CV; 8,11,24,34,63].",
                "Most of these approaches rely on pre-training an encoder to create expressive representation of input data. If a sequential decoder is needed for downstream tasks (i.e., for generative tasks), it is often trained with task-specific supervised data. The most common approaches for automatic speech recognition (ASR) follow this encoder-decoder paradigm [6,14,22,23,45,61], regardless if they use sequence transducers [20] or sequence-to-sequence [9,12,55] architectures. Because all existing self-supervised learning approaches for speech focus on pre-training an encoder model only, when adapted to an encoder-decoder architecture, the decoder has to be either randomly initialized or borrowed from a pre-trained NLP decoder [4,60].",
                "In this paper, we propose Wav2Seq, the first self-supervised approach to jointly pre-train the encoder and decoder. We automatically induce pseudo subwords that form a compact discrete representation of spoken language. We treat these as audio transcripts in a pseudo ASR task, and use them as the targets for Seq2Seq learning (see Fig. 1). When fine-tuned on a downstream task (e.g., ASR or speech translation), the input and output embedding layers are replaced in order to adapt to natural language.",
                "We conduct extensive experiments on ASR, spoken named entity recognition (SNER), and speech-totext translation (ST) tasks. Focusing on settings with limited labeled audio data (i.e., 10h or less), our ASR results show that with a pre-trained encoder only, CTC models outperform encoder-decoder models significantly in few-example scenarios; however, Wav2Seq boosts the performance of encoder-Preprint. Under review. Figure 1: Pseudo ASR task. The Wav2Seq model is pre-trained to transcribe an audio input into a sequence of pseudo language tokens. The embedding layers are replaced during fine-tuning on real ASR or speech translation tasks. \"<sos>\" is the start-of-sequence token and \"<eos>\" is the end-of-sequence token. See Fig. 2 for how the pseudo language tokens are generated.",
                "decoder models and closes this gap. When we apply Wav2Seq as a low-cost second-stage pre-training method, models based on existing pre-trained encoders (e.g., HuBERT) achieve even better results.",
                "On the SLUE-VoxPopuli SNER benchmark, Wav2Seq initialized with HuBERT achieves the best end-to-end results. For ST tasks, we conduct experiments on four from-English language pairs and 16 to-English pairs with both low-and high-resource setups. Wav2Seq consistently outperforms models initialized with HuBERT or XLS-R pre-trained models and achieves similar BLEU scores as models trained on additional machine translation annotated text data. Pre-trained models and code are available at https://github.com/asappresearch/wav2seq."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Speech-to-Text Models Automatic speech recognition (ASR) and speech-to-text translation (ST) are two of the most commonly studied speech-to-text tasks. The former produces a monotonic mapping while the latter may involve re-ordering (e.g. translating English to German). When re-ordering exists, it is advantageous to use sequence-to-sequence (Seq2Seq) models [55] with encoder-decoder attention [7,9], such as doen in popular open-source speech translation toolkits [28,57].",
                "When the mapping is monotonic, such as with ASR, various approaches are applicable. Connectionist temporal classification [CTC; 19] is one of the simplest and efficient approaches. It only requires an encoder that generates a sequence of feature vectors where each feature vector represents the input within a time window. A linear classifier can be applied to classify each feature vector into a character and the repeated and blank characters will be removed. When decoding with an LM, beam-search can also be used. CTC is used in various systems [1,38,54] and is becoming the default approach to fine-tune a self-supervised pre-trained model such as wav2vec 2.0 [5]. The sequence transducer [20] is another architecture dedicated to monotonic mapping in ASR. Similar to Seq2Seq, it has an encoder for audio inputs and an auto-regressive decoder for text inputs; however, the decoder does not have access to the full encoded speech features. The decoder starts with using the speech feature of the first frame from the encoder and decides either to emit a text token or shift to use the speech feature of the next frame. State-of-the-art supervised speech models are based on the transducer framework [22,23].",
                "Text-to-Text Encoder-Decoder Pre-training In NLP, it has been observed that pre-trained Seq2Seq models outperforms pre-trained encoders on text generation tasks [33,49] despite being less competitive in discriminative tasks. Lewis et al. [33] introduced BART which is a Seq2Seq Transformer pre-trained on a text denoising task. Xu et al. [64] extend BART to a multilingual setup and introduce mBART. Concurrently, T5 [49] is pre-trained on a large collection of NLP tasks, showing effective task transfer behavior [47,50,67].",
                "Speech Self-supervised Learning CPC [56] and wav2vec [51] are two early approaches for selfsupervised speech representation learning based on contrastive loss. Wav2vec 2.0 [5] is the first self-supervised model to outperform purely supervised approaches on ASR. Instead of directly reconstructing masked features, it is uses a contrastive loss -distinguishing the quantized version of the correct features from several negative samples. Lai et al. [31] show that many of the weights in wav2vec 2.0 are redundant and can be pruned. Wu et al. [62] provides a deeper understanding of the efficiency of various components of wav2vec 2.0 and propose SEW-D, a more compact and efficient variant. Hsu et al. [27] found that adding a loss to intermediate layer improves the quality of wav2vec 2.0 pre-training. Rather than contrastive learning, Hsu et al. [26] propose HuBERT, a self-supervised approach based on masked language modeling. Unlike wav2vec 2.0 using online quantization, HuBERT extracts speech features and clusters them offline. The model is trained to predict the cluster indices of the features at the masked positions. WavLM [10] extends HuBERT by using a larger dataset with non-audiobook audios and adding noise to the audio during pre-training. W2v-BERT [13] further combines these two approaches (contrastive learning and masked language modeling) and sets a new state of the art in ASR. Concurrently to our paper, Baevski et al. [6] introduce data2vec which is trained to match the outputs of an exponentially moving averaged teacher (similar to BYOL [21] and DINO [8] in CV) and works for NLP and CV data as well. Ao et al. [2] introduce SpeechT5, a multimodal encoder-decoder model, which is pre-trained on both text and speech data. Unlike our approach, their decoder is trained to reconstruct the log Mel-filterbank features of the speech audio.",
                "3 Self-supervision using Pseudo Subwords ",
                "During pre-training, in contrast, transcriptions are not available, making this objective inapplicable.",
                "Our key insight is that we can use unsupervised techniques to induce a pseudo language given raw audio only, and annotate raw audio inputs with it automatically. We then apply the likelihood sequence-to-sequence objective, and train Wav2Seq, a pre-trained encoder-decoder architecture. Fig. 1 illustrates the pseudo ASR task.",
                "While our focus is the negative log-likelihood objective, the pseudo language formulation is not restricted to this objective, and is broadly compatible with other training objectives. For example, we also experiment with pre-training Wav2Seqs with a sequence transducer architecture using a transducer loss [20] (Subsubsec. 5.1.1)."
            ],
            "subsections": [
                {
                    "title": "Inducing Pseudo Languages",
                    "paragraphs": [
                        "We construct sequences of discrete tokens from speech. Because Transformer decoders require quadratic computation with respect to sequence length, we need to balance the length of these sequences (i.e., wanting them to be short) with the quality of the pre-trained model.",
                        "Fig. 2 illustrates how our pseudo subwords are generated. We extract a sequence of hidden feature vectors from an audio file using a pre-trained HuBERT [26] model. We apply average pooling to reduce the sequence length and then k-means clustering to discretize these hidden feature vectors. This results in a sequence of cluster indices, which have also been referred to as hidden units in prior literature [26].",
                        "We treat these cluster indices as characters because they have been found to correlate with the phonemes of speech audio. We observe many consecutive repetitions of cluster indices, and propose to deduplicate them (i.e., remove the duplicates). For example, \"k k t t t\" will be mapped to \"k t\". We treat the deduplicated cluster indices as the characters of pseudo subwords and refer to them as pseudo characters.",
                        "Clustering Model (e.  We apply a subword tokenization algorithm such as byte-pair encoding [BPE; 17] to further shorten the target sequence length. This process merges common co-occurring characters into newly created tokens, and is widely used in NLP tasks such as language modeling and machine translation to balance between the benefits and costs of character and word representations (i.e., vocabulary size, unit semantics, handling of unseen words, etc). For example, a word \"negotiation'' can be represented by \"ne\" \"go\" \"ti\" and \"ation\" with a subword tokenizer, which allows the model to share the embedding of the common suffix \"ation\" across different words. Similarly, common pseudo characters sequences are merged to pseudo subwords. LibriLight Similar to the training of wav2vec 2.0 large and HuBERT-large, we use LibriLight [30] to pre-train large models. LibriLight contains 60K hours of unlabelled audiobook recordings.",
                        "CoVoST-2 We use CoVoST-2 speech translation data set [58] for ST experiments. There are 21 X-to-English and 15 English-to-X language pairs. For X-to-English pairs, we use the 12 low-resource pairs with less than 10h audios and 4 high-resource pairs with more than 100h audios. All English-to-X pairs have 430h of English audio. We therefore use the four language pairs used in prior work [60] and subsample a 10h subset for each pair to simulate a low-resource setup."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "SLUE-VoxPopuli",
            "paragraphs": [
                "We use SLUE-VoxPopuli dataset [53] for spoken NER experiments. SLUE-VoxPopuli contains several subsets: fine-tune, dev, and test sets with 14.5h, 5h, and 5h of transcribed audios and their corresponding named entity labels. The audio is from recordings of European Parliament events and the transcriptions are provided by Wang et al. [59]."
            ],
            "subsections": [
                {
                    "title": "Pre-training",
                    "paragraphs": [
                        "We use the official baselines for most prior work [4,5,26], which are implemented in fairseq [40]. Our Wav2Seq is also implemented as a plugin of fairseq. We use the mini-batch k-means algorithm [52] with k-mean++ initialization [3] implementation in scikit-learn [43]. Following HuBERT's best hyper-parameters, the number of clusters C is set to 500 by default unless specified separately. We use the byte-pair encoding (BPE) [17] implementation from Huggingface's tokenizers library 1 as the subtokenization method where the vocabulary size of subword tokens V is set to 30K (or 10K for the tiny models).",
                        "Following Baevski et al. [5], we pre-trained models with a batch size of at most 87.5 seconds per GPU on 8 GPUs and an update frequency of 4 to simulate 32-GPU training as Baevski et al. [5]. We pre-train the models for 100K updates (or 25K updates in second-stage pre-training) unless specified otherwise. We use the same hyper-parameters as prior work [5,62]. The peak learning rate is set to 2 \u00d7 10 -4 with 32K linear warm-up steps and linearly decaying to 0 at step 400K. All models are pre-trained with eight NVIDIA V100 GPUs with half-precision. We tie the weights of input and output embeddings of the decoder during pre-training and fine-tuning [29,46]."
                    ],
                    "subsections": []
                },
                {
                    "title": "Fine-tuning",
                    "paragraphs": [
                        "We use BPE with vocabulary size 1,000 to tokenize the text in ASR, spoken NER, or ST tasks. Similar to prior work [5,26], we use learning rate 5 \u00d7 10 -5 for fine-tuning and use tri-stage learning rate scheduler with 2,000 steps for linear warm-up and the last 50% of the updates for exponential decay. 2 The number of fine-tuning steps depends on the datasets. When fine-tuned on LibriSpeech 10h (or 100h) data, we use a batch size with at most 50 seconds per GPU of audio for 20K (or 80K) and updates on eight GPUs with half-precision. For spoken NER and low-resource ST, we use the same hyper-parameters as the LibriSpeech 10h setup. For high resource ST, we increase the number of updates to 320K because it has more data. We use beam size 10 for beam search decoding of Seq2Seq models and no length penalty for ASR and SNER tasks. For ST, we tune length penalty \u2208 {0.5, 1.0, 1.5} on the development sets because some languages may be more (or less) compact. For CTC baselines, we follow the hyperparamters provided by Hsu et al. [26]."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "5.1 Automatic Speech Recognition (ASR)"
            ],
            "subsections": [
                {
                    "title": "Small Model Experiment",
                    "paragraphs": [
                        "We conduct initial small-scale experiments using tiny models with embedding size 256, four attention heads, and feed-forward embedding size 1,024 in each Transformer block. Each model has 6 or 12 Transformer blocks. We use a compact wave feature extractor (WFE-C-c128l0) [62] to speed up the model. This extractor has been shown to perform similar to the wave feature extractor used in wav2vec 2.0 and HuBERT, but faster. All models are pre-trained on LibriSpeech with a semi-supervised setup using 960h unlabelled recordings for pre-training and 10h labelled data for fine-tuning. To be comparable, we use the 9th layer of the official second iteration HuBERT-base model to extract both hidden units and pseudo subwords for pre-training HuBERT baselines and our Wav2Seq models. All HuBERT models in this subsubsection are third-iteration of HuBERT models.",
                        "Table 1 shows word error rate (WER) of HuBERT and Wav2Seq with different fine-tuning setups. In the first two rows, as is already known, HuBERT performs strongly when fine-tuning with CTC objective which does not require a Transformer decoder. However, it is challenging to train a HuBERT encoder with a randomly initialized decoder with only 10h labelled data (rows 4 and 5). In contrast, our Wav2Seq can easily adapt to an ASR task with only limited supervision (row 6). In row 7, we show that it is more important to have a deeper encoder (8 layers) and a shallower decoder (4 layers), which allows closing the gap between CTC fine-tuning and Seq2Seq models. In row 8, we use Wav2Seq as a second stage pre-training -the encoder of Wav2Seq is initialized with a pre-trained HuBERT model. As we can see, with only a relatively small amount of additional pre-training cost, Wav2Seq is able to significantly improve HuBERT's Seq2Seq performance. This demonstrates that HuBERT and Wav2Seq pre-training are complimentary.",
                        "Transducer Models We conduct similar experiments on a Transducer [20]  Transducers require O(mnV ) space complexity (m, n, and V are input sequence length, output sequence length, and output vocabulary size), we set C = 25 and V = 1000 to fit the model into a single GPU memory, which may hurt its performance compared to the Seq2Seq counterparts. We study the impact of C and V in Subsec. A.1. We also reduce the number of decoder (a.k.a., label encoder in a transducer) layers because of memory issues. The pre-training learning rate is set to 5 \u00d7 10 -4 for Transducers."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Speech-to-text Transformer Models",
            "paragraphs": [
                "We show that Wav2Seq generalizes to other model architectures using the S2T Transformer model implemented by Wang et al. [57], which is a transformer-based Seq2Seq ASR model [39] that takes log-mel filterbank features as inputs and is widely used as a baseline under the fully supervised setup. In this experiment, we closely follow the default hyperparameters in the codebase, 3 except that we reduce the number of training iterations (300K originally) because the official example uses the complete 960h labelled data, while we use a semi-supervised setup with only 10h or 100h labelled data. Table 2 shows the WERs of an S2T Transformer-small trained from scratch and a Wav2Seq pre-trained counterpart. With merely 10h labelled data, it is impossible to train the S2T Transformer from scratch (row 1). In contrast, with Wav2Seq pre-training, the model can learn meaningful patterns from limited labels (row 2). With 100h labelled data, a model Wav2Seq pre-trained with 960h unlabelled data can significantly improve the WER of the model (32.8% to 26.8% on LibriSpeech test-other) while requiring merely one third of the fine-tuning time on the 100h ASR data (rows 3 and 4)."
            ],
            "subsections": [
                {
                    "title": "Standard Model Size Model Experiments",
                    "paragraphs": [
                        "We use the official HuBERT-base and HuBERT-large pre-trained on LibriSpeech and LibriLight as the initialization of Wav2Seq encoders following our observation that Wav2Seq works best as second stage pre-training. We further pre-train the models on the same corpus with relatively few   [53]. Wav2Seq achieves the best performance among all end-to-end methods without access to a language model. A pipeline model using an NLP pre-trained model (DeBERTa-large) remains the best among all the approaches. The numbers in the first four rows are provided by Shon et al. [53]. The last row is the state-of-the-art using external data and knowledge distillation from an NLP model, the test score is not available in their currently published paper [42].",
                        "updates (25K or 100K iterations compared to 400K interactions for HuBERT models). For the decoder, we use six Transformer blocks with the same width and number of heads as the encoder. Our Wav2Seq (from HuBERT-base) and Wav2Seq (from HuBERT-large) models take 14 and 49 hours to be fine-tuned for 25K updates on eight NVIDIA V100 GPUs, a relatively small compute budget (less than 10%) compared to training HuBERT models (usually 400K updates).",
                        "Table 3 shows WER for standard sized models. Even with a well trained HuBERT-base, fine-tuning with the Seq2Seq architecture using a randomly initialized decoder is inferior to the simple CTC objective (rows 2 and 3). Using Wav2Seq closes the gap or even makes Seq2Seq models outperform CTC counterparts (row 4). However, we observe that pre-training longer does not help for Wav2Seq (from HuBERT-base) (row 5). The reason might be that the encoder has already been pre-trained. This shows that the second-stage pre-training can be stopped early without hurting performance. We also compare models with 100h labelled data to confirm our observation."
                    ],
                    "subsections": []
                },
                {
                    "title": "Spoken Named Entity Recognition (SNER)",
                    "paragraphs": [
                        "Spoken named entity recognition has recentely received significant attention [18,53,65]. It is often addressed in one of two ways: end-to-end (E2E) or pipeline (i.e., a combination of an ASR model and an NLP NER model). Shon et al. [53] show that the pipeline approach is the state-of-the art, while more recent work [42] shows that with additional external data the order can be reverted.",
                        "End-to-end models are usually trained with a CTC objective [18,53,65]. In our experiment, we explore the potential of applying Seq2Seq to SNER. Following Shon et al. [53], we add special tokens around named entities in the transcription and train the model in an ASR manner, which results in a model that detects named entities as it transcribes the audio inputs.",
                        "Table 5: Test BLEU scores on CoVoST 2 low-resource X-to-En language pairs. When limited labels are provided, using Wav2Seq as a second stage pre-training consistently improves the performance of HuBERT-large and XLS-R (0.3B). Wav2Seq (from XLS-R (0.3B)) is on par with the XLS-R (0.3B) + mBART-ML50N1 which uses a large mBART decoder pre-trained on a 50-language text corpus and fine-tuned on 50 languages to English machine translation corpus with 4\u00d7 parameters. The scores in the second and third groups are from Babu et al. [4]. VP-100K is a wav2vec 2.0 large pre-trained on VoxPopuli 100K hour multilingual data [59], XLSR-53 [15] is a wav2vec 2.0 pre-trained on 53-language audios, and XMEF-En and XMEF-X are efficient fine-tuning method proposed by Li et al. [35]. Table 6: Test BLEU scores on CoVoST-2 X-to-En high-resource language pairs. We compare with the same baselines as Table 5."
                    ],
                    "subsections": []
                },
                {
                    "title": "Speech-to-text Translation (ST)",
                    "paragraphs": [
                        "Encoder-decoder (i.e., Seq2Seq) models are considered particularly suitable for speech-to-text translation tasks, where the input and output sequences are not monotonically aligned.",
                        "Our experiments extend beyond English. Besides the English Wav2Seq (from HuBERT-large), we also pre-train a Wav2Seq (from XLS-R (0.3B)) which uses a multi-lingual XLS-R encoder [4] pre-trained on 500K hours of audio in 128 languages. We use LibriLight for second stage pre-training and pre-train the model for only 25K updates. The length compression rate is the ratio between the decoder sequence length and the encoder sequence length (100% means no compression) computed on dev-other. We also show the compression rate of English tokens. The compression rate of pseudo subwords lies between English subwords and English tokens."
                    ],
                    "subsections": []
                },
                {
                    "title": "Ablation Study",
                    "paragraphs": [
                        "We conduct ablation study using the small scale model in Subsubsec. 5.1.1 to understand the gain of each component in the generation of pseudo subwords. Table 10 shows the results of pre-training with different type of target sequences. Deduplication plays a vital role and using BPE tokenization to convert characters into subwords further reduces WER. We hypothesize that the duration of these characters is less relevant to the semantic of speech and it is more important to capture the transition of different characters. We provide further ablation studies in Subsec. A.1."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion and Future Work",
            "paragraphs": [
                "We present Wav2Seq, a self-supervised learning framework for pre-training speech processing models. Unlike existing methods, Wav2Seq pre-trains both encoder and decoder parameters, thereby allowing the two main components of common encoder-decoder architectures to benefit from pre-training. Critically, Wav2Seq requires raw audio data only. Instead of using aligned text, we create a pseudo ASR task in which models transcribe audio inputs into pseudo subword tokens. We show that Wav2Seq closes the performance gap between encoder-decoder models and CTC models under low-resource conditions in ASR. On the SLUE-VoxPopuli spoken NER task, Wav2Seq achieves the best performance among all E2E models. On speech-to-text translation tasks, Wav2Seq consistently improves the performance of HuBERT and XLS-R and rivals mBART decoder initialization, which requires additional language data, while being more flexible and having fewer parameters.",
                "We demonstrate the potential of encoder-decoder models, which are applicable to more diverse tasks than CTC models. Understanding the impact of additional data (e.g., via knowledge distillation or self-training) or combining with external models (e.g., LMs trained on large text corpora) is an important direction for future studies. Another interesting direction is to apply pseudo subwords to generative spoken language models [32]. Last but not least, it is possible to add CTC loss to the encoder during fine-tuning and do joint CTC/attention decoding [25] to further improve the performance of Wav2Seq.",
                "Limitations We study ASR models that do not use language models (LMs). As seen in prior work [5,26], an external language model can boost the performance of CTC models. Due to the architecture difference between CTC and Seq2Seq, different LMs have to be used with CTC and Seq2Seq models which further complicates the experimental setup. The order of the performance may be swapped when different LMs are applied. Moreover, because CTC models are faster during inference, CTC models remain a valid choice for ASR if the performance difference is small."
            ],
            "subsections": []
        },
        {
            "title": "A.1 More Ablation Study",
            "paragraphs": [
                "Table 11 shows the ablation study on the number of clusters and vocabulary size. As we can see, number of clusters has a large impact on the compression rate of the target sequence length. Number of clusters has a larger impact on the length compression rate and the WER after fine-tuned on LibriSpeech. A moderate compression is preferable; too much or too less can hurt the performance. Finally, Table 12 shows the performance of using the optional average pooling. As we can see, having kernel size 2 is beneficial. The main goal for applying average pooling is to reduce prepossessing and pre-training cost: we store fewer HuBERT features and reduce the time spent on k-means and BPE tokenization for free. Table 12: Ablation study on average pooling kernel size K. We use C = 500, V = 30000 in this experiment."
            ],
            "subsections": []
        },
        {
            "title": "Model",
            "paragraphs": [
                "De Ca Ar Tr Avg.",
                "HuBERT-large 5.0 7.9 1.6 1.5 4.0 Wav2Seq (from HuBERT-large) 9.7 13.0 3.1 2.9 7.2 Table 8: Test BLEU scores on CoVoST-2 En-to-X language pairs. The complete 430h labelled audio data is used. Our Wav2Seq (from HuBERT-large) outperforms XLS-R (1B) [4] without using any multi-lingual data or using an mBART [37] decoder pretrained on text data. It also reaches a similar performance by self-training [60] which is computationally expensive since it has to label 60K hours of audios and fine-tuning on them in a second iteration. We also compare with VP-100K [59], XLSR-53 [15], and XMEF-JT [35].",
                "X-to-English Low-resource Experiments We experiment with 12 X-to-English language pairs where less than 10 hours of labelled data is available. We fine-tune both baselines and our models in a multi-task fine-tuning learning setup. Table 5 shows test BLEU scores on each language pair. Wav2Seq improves HuBERT and XLS-R (0.3B) models on most language pairs and achieves better average performance. Notably, our Wav2Seq (from XLS-R (0.3B) can match the performance of XLS-R (0.3B) with an mBART-ML50N1 decoder which has 4\u00d7 the number of parameters in the decoder, which is pre-trained on 50 language unlabelled texts and then fine-tuned on a 50-languageto-English machine translation corpus. Using Wav2Seq as a second-stage pre-training method makes the decoder size flexible and requires no additional text data.",
                "X-to-English High-resource Experiments Table 6 shows the test BLEU scores of the four highresource X-to-English language pairs. Our Wav2Seq (from XLS-R (0.3B)) outperforms the counterpart using an mBART-ML50N1 decoder (row 1 vs. 3). Since the input audio is always in English, we do not observe performance gain with a multi-lingual pre-trained encoder (row 1 vs. 2). XMEF models [35] and XLS-R (2B) [4] achieve better BLEU scores with a mBART-ML50N1 decoder and more parameters.",
                "English-to-X Low-resource Experiments Since English is a high resource language, all Englishto-X language pairs in CoVoST-2 have 430 hours of labelled audios. We subsample a 10 hour subset for de-en, ca-en, ar-en, tr-en language pairs. We choose these four language pairs because they are studied more often in prior works [4,60]. Table 7 shows the test BLEU scores of the models fine-tuned on each 10h training set. Wav2Seq (from HuBERT-large) outperforms HuBERT-large baseline.",
                "English-to-X High-resource Experiments Table 8 shows the test BLEU scores on four English-to-X pairs with the full 430h labelled data used for fine-tuning. Our Wav2Seq (from HuBERT-large) model outperforms most of the models with the same size encoder and match the performance of the models with 2\u00d7 or 3\u00d7 encoder size (W2V2 (0.72B) and XLS-R (1B)). It also matches the performance of a wav2vec 2.0 large using 60K hours of audios for self-training.",
                "Table 9 shows the performance gain of Wav2Seq with different amounts of labelled data. A pre-trained decoder has higher impact on low resource setup, but the gain diminishes with more supervision."
            ],
            "subsections": []
        }
    ]
}