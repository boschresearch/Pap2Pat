{
    "id": "https://semopenalex.org/work/W4226230162",
    "authors": [
        "Hamid Saber",
        "Jung Hyun Bae",
        "Homayoon Hatami"
    ],
    "title": "List Autoencoder: Towards Deep Learning Based Reliable Transmission Over  Noisy Channels",
    "date": "2021-12-18",
    "abstract": "In this paper, we present list autoencoder (listAE) to mimic list decoding used in classical coding theory. With listAE, the decoder network outputs a list of decoded message word candidates. To train the listAE, a genie is assumed to be available at the output of the decoder. A specific loss function is proposed to optimize the performance of a genie-aided (GA) list decoding. The listAE is a general framework and can be used with any AE architecture. We propose a specific architecture, referred to as incremental-redundancy AE (IR-AE), which decodes the received word on a sequence of component codes with non-increasing rates. Then, the listAE is trained and evaluated with both IR-AE and Turbo-AE. Finally, we employ cyclic redundancy check (CRC) codes to replace the genie at the decoder output and obtain a CRC aided (CA) list decoder. Our simulation results show that the IR-AE under CA list decoding demonstrates meaningful coding gain over Turbo-AE and polar code at low block error rates range.",
    "sections": [
        {
            "title": "I. INTRODUCTION",
            "paragraphs": [
                "Reliable transmission over noisy channels has been an active research area for the past decades. Channel coding is the main tool to achieve reliable transmission by finding higher dimensional representations of the input data. In his seminal work [1], Shannon proved the existence of capacity-achieving sequence of codes by random construction of an ensemble and investigating the conditions for the feasibility of reliable communication. Design of channel codes that approach or achieve the channel capacity has since then been an elusive goal. Among most landmark codes designed thus far are Turbo, LDPC and polar codes [2]- [4].",
                "Traditionally, an (N, K) channel code is constructed by designing an encoder that maps a set of 2 K binary message words of length K to a set of 2 K codewords of length N for transmission over the channel. Typically, mathematical analysis is used to tailor the encoder and decoder to one another. For instance, under the maximum a posteriori (MAP) decoder which minimizes the block error rate (BLER), the encoder is designed such that the pairwise distance properties of the code is optimized. MAP decoding is almost never used unless when the code is very short, or can be described via a trellis diagram with rather a small size, similar to convolutional codes. It is also possible to design the decoder first. Polar code design typically follows this approach, where the encoder design, suitably defined in [4], is carried out to optimize the performance under a successive cancellation (SC) decoder. Details aside, almost all classical code design approaches heavily rely on an information-theoretically welldefined channel model, which in most cases is additive white Gaussian noise (AWGN) channel, and employing mathematical analysis as an essential tool. More importantly, the code design progress has thus far been sporadic and heavily relying on the ingenuity of humans.",
                "There has been a growing interest in automating the design of encoder and decoders using deep learning framework. A deep-learning based framework allows for design of encoder and decoder for channels that cannot be described by a well-defined model or can be described but the model is too complex for code design. Although the ultimate goal of deep learning based design is envisioned to be for arbitrary channels, a first step towards this end can be designing codes which can compete with the state-of-the-art classical channel codes over the AWGN channel. The code design essentially can be applied to any channel provided that a sufficient number of transmissions over the channel are performed to construct a sufficiently large training set.",
                "Deep learning has been employed to design decoders for the classical encoders [5]- [9]. It has also been used to design both encoder and decoder based on autoencoders (AEs). AEs are powerful deep learning frameworks with a wide variety of applications which fall into two categories: under-complete and over-complete AEs. An under-complete AE is used to learn latent representation of the input data by transforming it to a smaller latent space. Under-complete AEs are used for numerous tasks including denoising, generative models and representation learning [10]- [14]. On the other hand, overcomplete AEs add redundancy to the input data to transform it to a higher dimension. One of the main application of overcomplete AEs is that the higher-dimensional representation can be transmitted over a noisy channel allowing the receiver to reliably decode the input data [15]- [21]. In particular, the authors in [21] used convolutional neural network (CNN) and recurrent neural network (RNN) to mimic the architecture of classical turbo encoder and decoder. The proposed Turbo-AE is reported to have competitive performance to the state-ofthe-art classical codes while being trainable for an arbitrary channel model. This paper is a further attempt to improve the design of AEs for reliable communication over noisy channels and is based on the concept of list decoding in the classical coding theory. Our contributions are:",
                "\u2022 We present list autoencoder (listAE) as a general deep learning framework applicable to any AE architecture. With listAE, the decoder network outputs a list of decoded message word candidates. The listAE mimics the list decoding in classical coding theory. \u2022 We provide a specific loss function which operates on the output list. The loss function aims to optimize the performance of a genie-aided (GA) decoder. We assume a genie is available at the decoder output and, whenever the transmitted message word is present in the list, it informs us which candidate it is. In other words, with the GA decoder, a block error event is counted if and only if the transmitted word is not present in the output list.",
                "During the testing phase, the functionality of the genie is emulated by using cyclic redundancy check (CRC) code. CRC is appended to the message word prior to encoding by the encoder network. At the decoder, CRC check is carried out for each output candidate to select a single candidate as the final output of the decoder. The concept of CRC-aided (CA) list decoding in widely used in classical coding theory. \u2022 listAE can be applied to any AE architecture. With the promising performance of Turbo-AE, it is natural to use its architecture in the listAE framework. While we train and evaluate the performance of listAE with this architecture, we also propose a more general architecture that decodes the received word on a sequence of component codes with non-increasing rates. The architecture, referred to as incremental redundancy AE (IR-AE), illustrates improvement over Turbo-AE architecture for smaller list sizes while having comparable performance at large list sizes."
            ],
            "subsections": [
                {
                    "title": "II. PROBLEM DEFINITION",
                    "paragraphs": [
                        "The problem of reliable transmission over a noisy channel can be defined as follows. As can be seen in Fig. 1, a message word of K bits is formed as u = [u 1 , . . . , u K ], where the u i take binary values from {0, 1}. The message word is encoded using an encoder neural network with an encoding function f \u03b8 (.) to obtain real-valued codeword x = [x 1 , . . . , x N ] = f \u03b8 (u) where the \u03b8 denotes the weights of the encoder neural network and N denotes the code length. A power normalization block is applied to x to give a codeword with zero mean and unit variance code symbols, i.e. E(x i ) = 0 and E(x 2 i ) = 1 for i = 1, . . . , N . The codeword x is transmitted over the channel.",
                        "The channel takes the codeword x as input and outputs a noisy version y = [y 1 , . . . , y N ], where the y i take real values. As mentioned before, having an information-theoretically defined channel model is not necessary, but if there is such a model, it is typically defined as a vector channel with transition probability density function (pdf) W N (y|x). A widely used channel among researchers for code design is additive white Gaussian noise (AWGN) channel for which the output y i = x i + w i where w i is Gaussian random variable with zero mean and variance \u03c3 2 . For AWGN channel",
                        ". The decoder network receives the channel output vector y and applies a decoding function g \u03c6 (.) to give the decoded message word u = [\u00fb 1 , . . . , \u00fbK ] = g \u03c6 (y) where the \u03c6 denotes the weights of the decoder neural network. The encoder and decoder networks together form an AE. The goal is to minimize the BLER or BER for different levels of impairment, e.g. SNR defined as 10log 10 1/\u03c3 2 for the AWGN channel."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "III. LISTAE",
            "paragraphs": [
                "Although designing new AE architectures can be a direction to improve the error correction performance, we choose to tackle the problem with a different approach. We posit that it may be too difficult for the decoder network to reliably decide which message word has been transmitted by only one guess. Therefore, we propose a framework which allows the decoder network to output a list of L candidates. Figure 2 shows a general listAE with a list size L. A conventional AE is a special case of listAE with a list size of L = 1. The concept of list decoding is well studied in the classical coding theory and, to a great deal, we have borrowed from that field. For example, successive cancellation list (SCL) of polar codes and its different variants have been well studied theoretically and also implemented for practical wireless communication systems [22]- [23].",
                "Since, in the testing phase, the decoder must output a single candidate \u00fb, there must be a selection process where a single candidate is chosen from the list. A GA decoder outputs u = u if u is equal to one of the rows of u (list) , otherwise it outputs a randomly chosen row of u (list) . In other words,",
                "where r is a random number chosen uniformly from 1 to L.",
                "During the training phase, the value of each element of vectors in the output list u (list) is made to take a real number between zero and one, for example by passing through a Sigmoid activation. In the testing phase the outputs are rounded to the nearest integer to give binary values. It is also possible to select a single candidate by replacing the genie with CRC as it will be demonstrated later. With GA decoding, the performance metric to optimize is BLER which is calculated between u and u given by (1). The challenge for defining a loss function which is tailored to the GA decoding of the listAE lies in how to mathematically model the genie operation. One may think of the genie operation as a processing block which takes the list of candidates as well as the transmitted message word and outputs a single candidate depending on the presence of the message word in the list. The condition for checking this presence involves rounding the candidate message words in the list to take binary values and then comparing them to the transmitted word. This operation a) introduces zero derivative in the back propagation, and b) additionally complicates it due to the comparisons. To tackle this problem, we propose a modified loss function that some how reflects how \"close\" the output list is to the message word without involving the precise genie operation. The loss function should take small values when the message word is close to any candidate in the list and is defined as follows.",
                "loss u (list) , u = min l\u2208{1,...,L} u",
                "where \u03c1 is the average BCE loss function which takes two vectors x and x of length K.",
                "It is noteworthy that the function min(a, b) is in general non-differentiable as the derivative does not exist at points where a = b. Similarly, the derivative of the loss function does not exist at points where an equality holds between the input arguments. Such points happen with zero probability, so they do not cause any issue to the backpropagation of the gradients during the training, as we will see later.",
                "With CA decoding and a Z bit CRC generated by a polynomial g(x) = g 0 +g 1 x+. . .+g Z x Z a word of K -Z bits is generated and is passed to the CRC calculator to generate Z CRC bits. The CRC bits are appended to the end of the message word to give the length-K vector u as the encoder input. At the decoder side, each candidate in the list is checked for passing CRC equations. Among the candidates which pass the CRC, one is randomly chosen as the final output of the decoder.",
                "To train listAE under CA decoding, we treat the CRC bits as information bits. In other words, the correlation between the bits of u is not taken into account to minimize the loss function. The reason is similar to those which leaded to employing the proposed loss function and avoiding the precise genie operation. Similarly, checking CRC involves binary Galois field operations which complicates the loss function and training. Therefore, we use the loss function given in (2) for training both GA and CA decoding.",
                "It is also noteworthy to mention that adding CRC to the message bits reduces the effective code rate by a factor of Z/K. To avoid rate reduction, one possible approach is to follow the SCL decoding of polar codes and assign a scalar metric \u03c1 l , l = 1, . . . , L, to each candidate in the list. Unfortunately we have not observed promising training results with this approach for listAE. However we think that this method is worth more investigation."
            ],
            "subsections": [
                {
                    "title": "B. An architecture for listAE: IR-AE",
                    "paragraphs": [
                        "In this section we present the IR-AE architecture. The encoder of IR-AE is essentially the same as Turbo-AE architecture and the decoder, too, relies on similar information exchange between the decoding blocks [21]. Roughly speaking a rate 1/n IR-AE uses n encoding blocks which are applied to interleaved length-K message words and give the length-N , N = nK, codeword x = [x 1 , . . . , x n ] after proper power normalization. Like Turbo-AE, IR-AE decoder consists of I iterations. At iteration i, a series of decoding blocks which are serially concatenated take a certain subset of {y 1 , . . . , y n } with applicable interleavers and a list matrix as input and output an updated list matrix. The same architecture is replicated in every iteration, but with independent learnable weights. If a decoding block takes a subset of {y 1 , . . . , y n } consisting of k vectors, i.e. {y i1 , . . . , y i k }, we say that the decoding block is a rate-1/k decoding block as one may associate it with an effective encoder which outputs the corresponding k vectors {x i1 , . . . , x i k }. An AE described as above is said to be an IR-AE if in an iteration each decoding block has a rate which is smaller or equal to the rate of the previous block. The heuristic motivation behind IR-AE architecture is to allow more powerful codes with smaller rates to attempt decoding the message word based on an improved list matrix given by previous weaker codes with higher rates. In this paper we mainly train and evaluate the performance of a rate-1/3 IR-AE. The detailed encoder and decoder architecture and training methodology is as follows.",
                        "1) Rate-1/3 IR-AE: Fig. 3 shows a rate-1/3 IR-AE. The encoder is identical to the rate-1/3 Turbo-AE encoder [21]. The output of the encoder is the length-N = 3K codeword with normalized power x = [x 1 , . . of size K \u00d7 L as an input and outputs a list matrix P i for iteration i + 1. When taking a K \u00d7 L matrix as input, the interleaver \u03c0 or deinterleaver \u03c0 -1 is applied on each column and generate a matrix of the same size. The list matrix P I , output by iteration I, is additionally passed through a Sigmoid function and gives the output list of message word candidates. Our observation shows that the intermediate list matrices can be thought as the log-likelihood ratios (LLRs) of the message bits; as we move from the output of the first iteration to the next iterations, the BER resulting from the list matrices decreases. The interleavers are employed to mimic their role on enhancing the distance properties of the code by introducing long term memory [21]. At the decoder side, deinterleavers are applied similar to Turbo-AE and turbo codes.",
                        "2) Power normalization: The output b = [b 1 , . . . , b 3 ] is given to a power normalization block giving the codeword x = h (b) to meet the power constraint requirements. Normalization can be done on code symbols, codewords or a batch of codewords [21]. In this paper we use the batch-wise normalization. With a batch size of B, and codewords of length N , there are B.N code symbols. Each codeword is normalized as x = b-\u00b5 \u03b3 where \u00b5 is the mean of the B.N code symbols and \u03b3 is the standard deviation of the B.N code symbols. This normalization method places the set of B.N code symbols in the batch on an B.N -dimensional sphere with radius \u221a B.N . In the testing phase, \u00b5 and \u03b3 can be pre-computed from a large batch and be directly used on a single message word.",
                        "3) Training methodology and hyper parameters: Fully connected neural network (FCNN), CNN and RNN are natural choices to employ in Fig. 3. Our experiments show that FCNN is more difficult to train and provides inferior performance to CNN. RNN models such as Long-Short Term Memory (LSTM) and Gate Recurrent Unit (GRU) can bring global dependency, which in turn may improve the Euclidean distance properties of the code. However, according to our experiments, CNN models were easier to train and had superior performance to FCNN and RNN. Therefore, in this paper we mainly present the result for CNN-based IR-AE.",
                        "Table I shows the details of the training and hyper parameters of our best IR-AE model. The model is trained for a maximum number of 500 epochs. At each epoch, we train the encoder T enc times while freezing the weights of decoder, and then train the decoder T dec times while freezing the weights of encoder. This specific scheduled training was proposed in [21] to avoid getting stuck in local minima. With a batch size of B, for each training a set of B randomly generated message words of length K = 100 are generated and encoded by the encoder network. A set of B noise vectors of length-N are generated and added to the codewords corresponding to the message words. Following the methodology in [21], a fixed SNR is used for training encoder while a range of SNR is used to train the decoder. For the latter, for each noise vector a SNR value is randomly picked from the range and is used to generate the noise vector. Sufficiently large batch sizes with small learning rates are needed for fine tuning the model and are implemented according to [20]. To decouple our investigation from interleaver design problem and for the sake of simplicity, in this paper a random interleaver is generated and fixed during the training and testing phase. There are existing works in the literature on interleaver desig of Turbo-AE which can be applicable to IR-AE. For instance, it is possible to design interleavers with uniform positional BER for Turbo-AE, which in turn may improve the overall performance. Interested readers are referred to [24] for more details."
                    ],
                    "subsections": []
                },
                {
                    "title": "IV. EXPERIMENT RESULTS",
                    "paragraphs": [
                        "In this section, we present the performance results for the listAE with both Turbo-AE and IR-AE architectures and compare the results with the classical codes. For IR-AE, the hyper parameters are given in Table I. The parameters for Turbo-AE are the same as the relevant blocks of the IR-AE, i.e. the first two decoding blocks. Figures 4 and5 show When we change the list size from 8 to 64, the converged value of loss drops almost one order of magnitude for Turbo-AE while the change is quite smaller for IR-AE. This would suggest an advantage of the latter over the former at smaller list sizes. Also, as expected, the test loss generally decreases from a smaller list size to a larger one. However, this trend does not hold at every epoch, which is probably because the optimizer needs to observe more data/epochs to train for a larger list size due to the increased model size. Fig. 6 demonstrates the BLER of the List Turbo-AE and List IR-AE under GA decoding for different list sizes. As shown, the performance for each architecture follows the trend given by the test loss trajectory. The trajectory also implies that for larger list sizes the IR-AE and Turbo-AE have similar performances, whereas for smaller list sizes IR-AE outperforms Turbo-AE. The improvement comes at the price of more than twice decoding network size as the Turbo-AE. For the remainder of the paper we focus on List IR-AE.  Next, we evaluate the performance of the List IR-AE under CA decoding and compare it to classical codes and Turbo- AE [21]. The code dimensions for the Turbo-AE and polar codes are (N = 300, K = 100). The polar codes are designed according to 3GPP NR reliability sequence and rate matching.",
                        "For IR-AE a length-8 CRC generated by polynomial g(x) = 1+x 2 +x 4 +x 6 +x 7 +x 8 is appended to the K = 92 message bits before encoding. To have a fair comparison due to slight rate reduction by CRC, we look at E b /\u03c3 2 instead of SNR for a code of rate R where E b /\u03c3 2 = SNR -10log 10 (R). The result of the comparison is shown in Fig. 7 only for large list sizes for the best performance. As can be seen, List IR-AE with a list size of 64, outperforms Turbo-AE and the polar code at BLERs smaller than 0.03. At high SNRs, the coding gain can be as large as 0.5 dB and 0.3 dB over Turbo-AE and polar code, respectively."
                    ],
                    "subsections": []
                },
                {
                    "title": "V. CONCLUSION",
                    "paragraphs": [
                        "In this paper, we presented listAE motivated by the goal of bringing the list decoding in classical coding theory into the area of deep learning based channel encoder and decoder design. To the best of our knowledge, this is the first work that introduces list decoding in the field of deep learning based channel code design. Our simulation results show performance gain over Turbo-AE and polar code which is promising and calls for future research. This work sets the stage to design AEs which can compete or outperform classical codes. For future directions, we note that the current choice of the loss function was somewhat heuristic and intended to optimize the performance under GA list decoding. Finding better loss functions to reflect the performance under CA list decoding is an interesting future direction."
                    ],
                    "subsections": []
                }
            ]
        }
    ]
}