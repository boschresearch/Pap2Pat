{
    "id": "https://semopenalex.org/work/W4381826952",
    "authors": [
        "Shen Yu",
        "Chaithanya Kumar Mummadi",
        "Aniruddha Saha",
        "Wan-Yi Lin",
        "Arash Norouzzadeh"
    ],
    "title": "Revisiting Image Classifier Training for Improved Certified Robust  Defense against Adversarial Patches",
    "date": "2023-06-21",
    "abstract": "Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser arXiv:2108.09135 [cs.CV], the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout arXiv:1708.04552v2 [cs.CV] augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1% to 62.3% against a 3% square patch applied anywhere on the image.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Deep learning based image classifiers are vulnerable to adversarial patches applied to the input [1,7], where image pixels bounded within a confined connected region, usually square or circular, can be adversarially crafted to induce misclassification. Patch attacks are formulated in this way to mimic printing and placing an adversarial object in the scene. It is easier to realize a patch attack in the physical world compared to full image adversarial perturbation, as the latter might need a compromise in the compute system. Therefore, adversarial patch attacks pose a significant threat to real-world computer vision systems. * Equal contribution. Work done during internship at BCAI, Pittsburgh.",
                "Defenses against patch attacks fall into two categories: empirical and certified defense. Empirical defenses [25,9,22] often utilize robust training which incorporates adversarial inputs generated from specific attacks to improve robustness. But they are susceptible to attacks unseen during training. On the other hand, certified defenses guarantee correct predictions against any adaptive white-box attacker under a given threat model. Hence, the certified robust accuracy for is the guaranteed lower bound of the model performance. Prior works [13,15,21,32] acknowledge this strong robustness property of certified defense and propose different certification procedures to improve the certified robustness. Note that, we consider a single adversarial square patch applied on an image in our threat model in this work.",
                "PatchCleanser [32] is the state-of-the-art certified defense method against patch attacks. Similar to other certified methods, it assumes the defense has a conservative estimate of the patch size (e.g. 3% pixels) and thereby designs a mask set -masks with certain size placed at reserved spatial locations on an image. Fig 1(a) shows a mask set M 3 \u00d7 3 of 9 masks, each with size 100 \u00d7 100, that are applied on 224 \u00d7 224 image. This mask set is designed to ensure that an adversarial patch of an estimated size (e.g. 3% pixels -39 \u00d7 39 patch) will be covered and therefore neutralized by at least one of the masks. The method proposes two round pixel masking defense and certifies an image if predictions of all its double-masked images agree with the ground truth. An image is certified if all the two-masked 9 2 = 81 combinations get correctly classified. More details about this method can be found in Sec 3. 3.",
                "PatchCleanser shows that a classifier trained on twomasked (or double-masked) images using random Cutout [5] augmentation improves certification accuracy. At training time, a two-masked image is obtained by applying random Cutout twice on a clean image (see Fig 4). During certification, we observe that some of the two-masked images are hard to classify as the masks completely occlude the object of interest beyond recognition as shown in  We revisit image classifier training strategy to particularly target the classifier's failure modes during Patch-Cleanser certification. The masks of misclassified images during certification would serve as possible worstcase masks i.e. masks that induce image misclassification. Instead of Random Cutout augmentation, we propose to choose worst-case masked images with high classification loss for training. The worst-case masks are chosen from the mask set that are used during certification. Finding such worst-case masks from mask set M 3 \u00d7 3 and M 6 \u00d7 6 exhaustively requires 9 \u00d7 9 = 81 and 36 \u00d7 36 = 1296 twomask forward passes respectively, which is prohibitively expensive to do on-the-fly during training. To this end, we propose a simple but effective greedy masking strategy called Greedy Cutout to approximate the worst-case masks with 42% and 96% less computation overhead than the grid search on M 3 \u00d7 3 and M 6 \u00d7 6 respectively. We propose to train the image classifiers on the Greedy Cutout images and demonstrate that such masking strategy improves certified robustness across different datasets as shown in Fig 1(c).",
                "We summarize our contributions as follows: We observe that training the classifier with standard Cutout augmentation is not entirely effective for all two-masked images. Therefore, we take a closer look at the invariance of image classifiers to pixel masking with an objective to improve PatchCleanser certification. We propose a simple Greedy Cutout strategy to train the classifier with worstcase masked images. We compare with various other masking strategies like Random Cutout [5], Cutout guided by certification masks, Gutout (Cutout guided by Grad-CAM) [3], Cutout with exhaustive search (refer Sec. 4) and show that our training strategy improves certified robustness over other baselines across different datasets and architectures."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Certified defenses against patch attack: Certified defenses against adversarial patch for image classifiers provide guaranteed robust accuracy lower bound. Because patch attack has maximum strength within the patch region, certified defenses either ensembles from local regions [13][26][15] or use models with small receptive field and masking [21][31] to reduce global influence of the patches. Current state-of-the-art PatchCleanser [32] combines masking and ensembling by aggregating predictions of specifically-designed masked copies and shows significant improvement over previous methods. Our work follows the certification and inference procedure of Patch-Cleanser [32]. However, we show that worst-case mask augmentation can improve invariance to pixel masking and consequently the certified robust accuracy of PatchCleanser.",
                "Augmentation for adversarial robustness: Data augmentations help in training smooth and generalizable models, and are used to train adversarially-robust classifiers.",
                "Adversarial training [18], [24] which augments input with gradient given a model has been established as one of the most powerful approach for training robust models. Studies have also shown that adversarial training improves generalizability of models [33,19]. Gradient-free augmentation has shown improvements in model robustness: [8] utilizes unconditional generative models to generate an additional training dataset, and [14] proposed padding before cropping (PadCrop) for augmentation to reduce robust overfitting. Cutout [5], randomly masks out square regions of input images during training to improve clean accuracy. We propose a variation to random Cutout augmentation scheme in our paper to improve certified robustness. [30] proposed an adversarial training framework using rectangular adversarial patches to improve robustness to patch-based evasion attacks. Our Greedy Cutout augmentation instead aims to improve certified robustness."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Patch attack formulation",
                    "paragraphs": [
                        "This paper focuses on adversarially-robust image classification. We use X \u2282 [0, 1] W \u00d7H\u00d7C to denote an image, where each image has width W , height H, number of channels C, and pixel values in [0, 1]. We use Y to denote the label set, and F : X \u2192 Y denotes the image classification model. Note that we do not make any assumptions about its architecture, and thus the method we present is compatible with any popular architecture.",
                        "We consider patch attack, denoted by A, that can arbitrarily manipulate the pixels within a square region covering up to 3% of image area. We use r \u2208 {0, 1} W \u00d7H to denote the aforementioned square region, where pixels within the region is set to 0 while others are set to 1. Since the region can be placed in any location of an image, we can denote all such square regions covering less than 3% of an image as R. Formally, for an image x with true label y, the patch attack A can generate a set of images under attack, i.e., A R",
                        "where \u2299 represents pixel-wise multiplication. Patch attack aims to find an image x \u2208 A R (x) such that F(x) \u0338 = y."
                    ],
                    "subsections": []
                },
                {
                    "title": "Defense objectives",
                    "paragraphs": [
                        "The goal of certifiable defense is to build a robust model F such that for image label pair (x, y), we have \u2200x \u2208 A R (x), F(x) = y, then such x is defined as a certified image and the fraction of certified images in test data is defined as certified robust accuracy. The process to determine whether an image-label pair (x, y) can be certified follows [32]. Certified robust accuracy provides a lower bound on model's classification accuracy under patch attack.",
                        "On the other hand, we want to avoid deteriorated performance over clean data free of patch attacks. Thus, the other metric we wish to maintain is the fraction of image label pair (x, y) satisfying F(x) = y over test data, which we call clean accuracy. Computing clean accuracy requires inference process sweeping over test data."
                    ],
                    "subsections": []
                },
                {
                    "title": "PatchCleanser",
                    "paragraphs": [
                        "In this paper, we follow the inference and certification procedures of PatchCleanser [32]. PatchCleanser achieves certified defense via double-masking strategy. A mask m \u2208 {0, 1} W \u00d7H is defined as a binary image with the same dimension as images in X where pixels within the mask take value 0, and the rest take value 1, and thus a masked image x \u2299 m will exhibit an artificially occluded region within the mask while other pixels remains unaffected. PatchCleanser crafts a well-designed set of masks M and provides a provable certification procure based on predicted classes over the set of masked images {x \u2299 m | m \u2208 M}.",
                        "PatchCleanser designs the mask set M satisfying Rcovering property, i.e., for any adversarial patch, there exists at least one mask m \u2208 M would completely cover the adversarial patch. Formally,",
                        "Therefore, improving the certified robust accuracy reduces to maintain high classification accuracy for F on masked images x \u2299 m 1 \u2299 m 2 , m 1 , m 2 \u2208 M \u00d7 M. To this end, PatchCleanser employs Cutout data augmentation [5] to finetune model F.",
                        "For inference, PatchCleanser first obtains all one-mask predictions F(x \u2299 m), \u2200m \u2208 M. If the image x has no adversarial patch, then all one-mask predictions should be the same as F(x), in this case it outputs the agreed prediction as the predicted class for x. If x contains an adversarial patch, since one of the one-masked copy does not have visible adversarial patch, then at least one one-mask prediction should be different from the rest. For each disagreed (minority) prediction with the corresponding single-masked copy, this copy is further masked with the second-round mask set and all two-masked copies are classified with F. If the single-masked copy has fully masked the adversarial patch, then all its two-masked copies also fully masked the adversarial patch, then the predictions of two-masked copies should be F(x). If none or more than one disagreed case have unanimous two-masked predictions, then none of the classes should be trusted, and the majority predicted class of the single-masked copies is the predicted class of x. Example masked copies can be found in Figure 1(a).",
                        "PatchCleanser shows that if an image-label pair (x, y) is certified, the above inference procedure will always output the correct label y."
                    ],
                    "subsections": []
                },
                {
                    "title": "Greedy Cutout",
                    "paragraphs": [
                        "Recall that to raise the certified robust accuracy, the classifier F should be fine-tuned to double-masked images",
                        "so it is less sensitive to presence of m \u2208 M. For this purpose, we resort to augmented training data with (x \u2032 , y) where x \u2032 is obtain from applying masks or combination of masks on x. Patch-Cleanser uses Random Cutout augmentation [5], i.e., applying two masks of size 128 \u00d7 128 at random locations to 224 \u00d7 224 training images. In this work, we develop a systematic approach to find x \u2032 that are more empirically effective. The main strategy is to use several inference rounds to identify worst-case masks that lead to highest losses, then we apply these worst-case masks to x to obtain augmented data point (x \u2032 , y) for training.",
                        "Image and mask preparations: For datasets of highresolution images such as ImageNet and ImageNette, we resize and crop them to 224 \u00d7 224 before feeding them into model F.",
                        "For datasets with images with lowresolution such as CIFAR-10, we resize them to 224\u00d7224 via bicubic interpolation for a better classification performance. In this work, we generate sets of masks following PatchCleanser [32]. Concretely, we use two mask sets M 3\u00d73 and M 6\u00d76 (see the leftmost part of Figure 2 for an illustration, where light gray blocks represent image area and deep gray smaller blocks represent masks) that both satisfy R-covering property: M 3\u00d73 is a set of 3 \u00d7 3 masks with 9 masks in total, and each mask is of size 100 \u00d7 100; M 6\u00d76 is a set of 6 \u00d7 6 masks with 36 masks, and every mask is in shape of 69 \u00d7 69. Note that, due to the careful choice of M 3\u00d73 and M 6\u00d76 , each mask in M 3\u00d73 can be fully covered by 4 masks in M 6\u00d76 . Empirically, M 6\u00d76 leads to higher certified robust accuracy since two smaller masks have a smaller effect on the prediction on F than the larger ones. However, M 6\u00d76 has \u2248 8 times larger size than M 3\u00d73 that requires more computation in both inference and certification. Striking a delicate trade-off between computational cost and accuracy is the core of our method.",
                        "Greedy Cutout: We present a baseline version of Greedy Cutout (See Figure 2 for an illustration). We first define the prediction loss of F on image label pair (x, y). Suppose within the model of F, for each label y \u2032 \u2208 Y, the confidence level for F to predict y \u2032 is p y \u2032 , then we use the cross entropy loss \u2113(F(x, y)) = -y\u2208Y c y \u2032 log p y \u2032 , where c y \u2032 = 1 for y \u2032 = y otherwise c y \u2032 = 0. For mask set M k\u00d7k (k = 3 or k = 6), we could identify the worst-case mask combination m 1 , m 2 \u2208 M k\u00d7k \u00d7 M k\u00d7k that incurs the largest loss \u2113(F(x \u2299 m 1 \u2299 m 2 , y) with grid search, but inferencing over all k 2 2 + k 2 (45 when k = 3, and 666 when k = 6) unique combinations would be computationally prohibitive, thus motivating the Greedy Cutout strategy. By Greedy Cutout, we find the worst-case mask in each individual masking round. In the first round of, for each m 1 \u2208 M k\u00d7k , we compute the loss \u2113(F(x\u2299m 1 ), y), and denote the mask incurring the highest loss as m * 1 . Then, in the second round, for each m 2 \u2208 M k\u00d7k , m 2 \u0338 = m 1 , we compute the loss \u2113(F(x\u2299m * 1 \u2299m 2 ), y), and find the m * 2 with the highest loss. Then, we empirically use",
                        "as the worst-case mask combination. Although M 2 may not be the exact worst-case mask, (x \u2299 M 2 , y) still provides a guidance for fine-tuning. More importantly, with Greedy Cutout, we significantly reduce the inference burden down to 2k 2 -1(17, k = 3; 71, k = 6) compared to grid search. However, for k = 6, this computation cost is still high but this mask set has better accuracy over M 3\u00d73 .",
                        "We then further develop Multi-size Greedy Cutout (See Fig 3) that benefits from the high accuracy of M 6\u00d76 while keeping computation cost comparable with Greedy Cutout with M 3\u00d73 .",
                        "Multi-size Greedy Cutout: Round 1 -Given clean image x, we first apply each mask in M 3\u00d73 and inference on each image in {x\u2299m | m \u2208 M 3\u00d73 }. We then find the mask that incurs highest loss \u2113(F(x \u2299 m), y), and denote this mask as M 1 . Then, we find the collection of 4 masks in M 6\u00d76 that fully covers M 1 . We apply the previous 4 masks from M 6\u00d76 on clean image x and inference each masked image, then we store the mask incurs highest loss among the four, denote this mask as M 11 .",
                        "Multi-size Greedy Cutout: Round 2 -We apply each m \u2208 M 3\u00d73 on x \u2299 M 11 , and find the m * 3 that incurs largest highest loss among the 9 masks. Then we output our first mask M 2 = M 1 \u2299 m * 3 . For m * 3 , we identify the 4 masks from M 6\u00d76 that fully covers m * 3 , and apply the 4 masks on x \u2299 M 11 to find m * 6 that incurs highest loss. We output the second mask M 22 = M 11 \u2299 m * 6 . From the previous two rounds, we identify two worstcase masks M 2 and M 22 . We only use in total 9 + 4 = 13 inferences in each round so only 26 inferences in total.",
                        "In our experiments, we show that this strategy uses similar amount of computation with baseline Greedy Cutout for k = 3, but achieves comparable performance of baseline Greedy Cutout for k = 6. We provide the pseudocode in the Algorithm 1 in the appendix, where two procedures ROUND-1 and ROUND-2 echo the round 1 and round 2 described above. Running Algorithm 1 with input (x, F, M 3\u00d73 , M 6\u00d76 ) will lead to desired augmented data points (x \u2032 1 , y) and (x \u2032 2 , y) that can be used for fine-tuning the vanilla model F."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "In this section, we describe our experimental setup, masking strategies and present the results. We use five  CIFAR-100 [12], SVHN [23] and image classifiers from three different architecture families: ResNet [10], ViT [6], ConvNeXt [17]. In particular, ResNetV2-50x1, ViT-B16-224 and Con-vNeXt tiny in22ft1k pretrained models from timm library [28]. We finetune these pretrained models on different datasets using the proposed masking stratgies for 10 epochs with SGD optimizer at learning rate 0.01 for ResNet and 0.001 for ViT, ConvNeXt and reduce the learning rate by a factor of 10 after 5 epochs. Images from all the datasets are resized to 224 \u00d7 224. We use batch size 128 for ImageNet and 64 for other datasets.",
                "Adversarial patches: As in prior work [32,2,13,20, 31], we report certified defense accuracy for square patches covering 1%, 2% and 3% of the input image for Ima-geNet, ImagNette and 0.4%, 2.4% for CIFAR-10, CIFAR-100, and SVHN. We allow patch perturbations to be unbounded (within image range) and consider a single square patch placed at any random location in the image. "
            ],
            "subsections": [
                {
                    "title": "Improving invariance to pixel masking",
                    "paragraphs": [
                        "The core idea of PatchCleanser [32] is to mask image pixels to occlude and neutralize the adversarial patch. We visually observe that two masks that completely occlude the object make images harder to certify (see Fig. 1(b)). However, masked images with partial occlusions and preserved semantic cues stand a chance to get classified correctly. Hence, a crucial property that governs the success of certification is the classifier's prediction invariance to pixel masking. Below we explain different train time masking strategies that we investigate towards improving pixel masking invariance.",
                        "Random Cutout in PatchCleanser (PC-Cutout) Random Cutout [5] to apply 2 masks of size 128\u00d7128 at random  4). Here, masks may be located partially beyond the image boundary. It is shown that the such Cutout augmentation improves certification over vanilla classifiers. We hypothesize that such Cutout strategy might not be fully effective to induce mask invariance and propose further improvement.",
                        "Random Cutout with certification masks Following random Cutout [5], we investigate whether training the classifier with Cutout masks from certification mask sets improve robustness. We apply two masks on the image that are randomly sampled from the either of the mask sets M 3 \u00d7 3 or M 6 \u00d7 6 during training (see Fig. 4). We refer rand 3 \u00d7 3 as the masking strategy that trains on mask set M 3 \u00d7 3 , rand 6 \u00d7 6 when using M 6 \u00d7 6 . We further use masked im-ages sampled from both the mask sets jointly during training, that doubles the batch size and we refer it as rand. The reason for our specific choice of mask sets will become clear when discussing our Greedy Cutout below. Gutout: Cutout guided by Grad-CAM Instead of random Cutout, we examine the effectiveness of classifier training on saliency guided Cutout. We use image explanation heatmaps from literature to guide this process. Specifically, we use Grad-CAM [27] to find the most salient region of the image and selectively mask out the region using certification mask sets M 3 \u00d7 3 or M 6 \u00d7 6 in a two-round fashion (see Fig. 4 for illustration). We refer the masking strategies as gutout 3 \u00d7 3 , gutout 6 \u00d7 6 and gutout when training on the mask sets M 3 \u00d7 3 , M 6 \u00d7 6 , and both the mask sets. Such augmentation strategy was proposed in [3] and we take inspiration from their implementation.",
                        "Cutout guided by exhaustive search Besides occluding the image region, these masks introduce artifacts on the image with long edges. The position of such artifacts interweaved with the image content influence classifier predictions. Therefore, we hypothesize that the two-masked images obtained from the saliency guided Cutout may not serve as better training examples for the classifier to improve pixel masking invariance. To this end, we sweep through the entire search area by evaluating classifier performance on all possible two-masked image combinations from mask sets M 3 \u00d7 3 or M 6 \u00d7 6 and find the worst-case masks that result in highest classification loss. We train classifiers on the worst-case masked images and refer this masking strategy as grid 3 \u00d7 3 and grid 6 \u00d7 6 with with masks set M 3 \u00d7 3 or M 6 \u00d7 6 respectively. We believe that this approach is close to an upper bound on the best masking strategy. The downside is that it requires classifier evaluation on large number of all possible two-masked images, 45 unique two-masked images among 9 2 = 81 in M 3 \u00d7 3 and 666 among 36 2 = 1296 in M 6 \u00d7 6 , thus makes this strategy computationally expensive.",
                        "Greedy Cutout (ours) As mentioned above, the compute requirement of an exhaustive search for the worstcase masked images might be prohibitive to train the classifiers. On the other hand, light weight masking strategy like random Cutout or a heuristic like Gutout might not be fully effective for our objective. Our proposed masking strategy Greedy Cutout (Sec. 3.4) approximates the worstcase masked images with much less computation overhead, bringing the best of both worlds. We propose to train the classifiers with the worst-case masked images obtained from this masking strategy. We refer the masking strategy of our baseline version of Greedy Cutout as greedy 3 \u00d7 3 and greedy 6 \u00d7 6 with masks set M 3 \u00d7 3 or M 6 \u00d7 6 respectively. Note that greedy 6 \u00d7 6 requires 71 classifier evaluations to find the worst-case masks, which is still high in computation. We address this issue with our other variant of greedy masking called Multi-size Greedy Cutout. As mentioned in Sec. 3.4, this variant decomposes the larger mask 100\u00d7100 mask from M 3 \u00d7 3 neatly into four 69 \u00d7 69 masks belonging to M 6 \u00d7 6 . Henceforth, this variant approximates the worst-case masks from M 6 \u00d7 6 in just 26 classifier evaluations which is 2.7\u00d7 faster than the former approach. We refer the masking strategy from Multi-size Greedy Cutout as greedy. The neat decomposition of masks from M 3 \u00d7 3 to M 6 \u00d7 6 allow us to use these two mask sets in our approach and therefore we adapt the same masks sets in the above masking strategies for fair comparison."
                    ],
                    "subsections": []
                },
                {
                    "title": "Results",
                    "paragraphs": [
                        "We report our main results in Table 1 and compare the performance of our method with various prior works.",
                        "Significant improvements in certified robust accuracy: PatchCleanser [32] is the state-of-the-art method for certification against adversarial patches. It has been shown that it improves certification performance compared to previous methods [2,34,13,31,20] by a large margin. Table 1 shows that our proposed Multi-size Greedy Cutout masking strategy improves robust accuracy over the Patch-Cleanser Random Cutout baseline across all three classifiers (ResNet, ViT, ConvNeXt) and datasets (ImageNette, ImageNet, CIFAR-10), thus setting new state-of-the-art certified robustness results. The performance gap is noticeably increasing with increased certification pixels. Notably, certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 52.5% to 57.7% against a 3% square patch applied anywhere on the image (mask set size of 3 2 = 9). We also see large improvements for ImageNette and CIFAR-10 datasets. In Table 2, we see that certified robust accuracy on CIFAR-100 with a ViT-B16-224 model increases from 69.1% to 74.5% against a 2.4% square patch applied anywhere on the image (mask set size of 6 2 = 36).",
                        "Performance of different masking strategies:   We observe that Greedy Cutout performs better than Random Cutout with certificaiton masks and also Gutout. Moreover, it is comparable with the expensive Grid search. For example, Certified robust accuracy for Multi-size Greedy Cutout on ImageNet with a ResNetV2-50x1 model is 51.2% against a 3% square patch applied anywhere on the image (mask set size of 3 2 = 9). This is better than Random Cutout at 45.1% and Gutout at 46.9%. Grid search is slightly higher at 51.6%. Results for ViT model are provided in Table A1 in the appendix.",
                        "Tradeoff between defense performance and compute time (Multi-size Greedy Cutout): We also compare the number of extra forward passes needed during training for each method. We see that the Greedy Cutout requires lesser compute than Grid seach, e.g. 71 unique forward passes compared to 666 for a 6 \u00d7 6 mask set, but achieves comparable robust accuracy (Table 3). Also, our Multi-size Greedy Cutout is an efficient strategy to reduce forward passes from 71 to only 26 but still achieve competitive certification performance with a mask set of 6 \u00d7 6.",
                        "We also plot the robust accuracy against certification mask set size in Fig 5 . We observe that robust accuracy increases when mask set size increases along with number of forward passes. We can benefit from our Multi-size Greedy Cutout when mask set size increases during inference."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "PatchCleanser [32] certification requires all the double masked image combinations to be correctly classified for an image to be certified against adversarial patches. An image classifier should be invariant to pixel masking to leverage such a certification process. PatchCleanser trains models with Random Cutout augmentation. In this paper, we show that training the classifier with cutout applied at worstcase regions, i.e., regions which produce the highest classification loss, improves certification accuracy over Patch-Cleanser by a large margin. This indicates that our training strategy improves classifier invariance to pixel masking over prior methods. We investigate other masking strategies like Grad-CAM guided masking and exhaustive grid search to find the worst-case mask regions. To reduce the computational burden of exhaustive grid search, we introduce a two round greedy masking strategy (Greedy Cutout) to find the worst-case regions. Greedy Cutout trained models along with PatchCleanser certification sets the state-of-the-art certified robustness against adversarial patches across different models and datasets."
            ],
            "subsections": []
        },
        {
            "title": "Revisiting Image Classifier Training",
            "paragraphs": [
                "for Improved Certified Robust Defense against Adversarial Patches "
            ],
            "subsections": []
        },
        {
            "title": "Appendix",
            "paragraphs": [
                "Algorithm 1 Multi-size Greedy Cutout Input: Image label pair (x, y), vanilla classifier F, mask sets M 3\u00d73 , M 6\u00d76 Output: Augmented data (x \u2032 1 , y), (x \u2032 2 , y)",
                "loss \u2190 -1."
            ],
            "subsections": []
        },
        {
            "title": "3:",
            "paragraphs": [
                "for each m \u2208 M 3\u00d73 do 4:",
                "if \u2113(F(x \u2299 m, y) > loss then M \u2190 {4 mask from M 6\u00d76 that covers M 1 }."
            ],
            "subsections": []
        },
        {
            "title": "9:",
            "paragraphs": [
                "loss \u2190 -1 end for 25:",
                "loss \u2190 -1",
                "28:  (5) SVHN: Street View House Numbers (SVHN) [23] is a digit classification benchmark dataset that contains 600,000 32\u00d732 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates.",
                "Models: We use image classifiers from three different architecture families.",
                "(1) ResNet: ResNets [10] are deep neural networks which use skip connections. This approach makes it possible to train the network on thousands of layers without affecting performance. We use the ResNetV2-50x1 model from the timm [29] library.",
                "(2) Vision Transformers (ViT): Convolutional Nets are designed based on inductive biases like translation invariance and a locally restricted receptive field. Unlike them, transformers are based on a self-attention mechanism that learns the relationships between elements of a sequence. We use ViT-B16-224 model.",
                "(3) ConvNeXt: ConvNeXt [17] is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers. The design starts from a standard ResNet (e.g. ResNet50) and gradually \"modernizes\" the architecture to the construction of a hierarchical vision Transformer (e.g. Swin-T [16]). We use the ConvNeXt tiny in22ft1k model from timm. It is trained on ImageNet-22k and fine-tuned on ImageNet-1k."
            ],
            "subsections": []
        }
    ]
}