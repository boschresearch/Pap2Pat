# Introduction

Droplet-scale dynamics for LMJ (Sukhotskiy et al. 2017;Bikas, Stavropoulos, and Chryssolouris 2016) can be modeled by coupled incompressible and immiscible multi-phase fluid flow, (convective and conductive) heat transfer, and solidification equations (Korneev et al. 2020), which can be spatially discretized using a finite volume (FV) approach and solved by time integration using computational fluid dynamics (CFD) platforms such as OpenFOAM (Jasak et al. 2007). Such simulations, in conjunction with experimental calibration of the material properties, can provide an accurate prediction of the droplet-scale dynamics. However, the com-putations can slow down due to constraints on the temporal step that guarantee stability during a numerical simulation, e.g., the Courant-Friedrichs-Lewy (CFL) condition. Part-scale build simulation requires calling the droplet-scale solver numerous times in a sequential loop with a moving domain of interest, where the final conditions of each droplet coalescence simulation serve as initial conditions to the next one. These conditions include values for phase, velocity, pressure, and temperature. In the context of LMJ, computing the coalescence of a single droplet, with a diameter of a few hundred microns, may take an FV solver up to an hour on a 96-core cluster 1 , while build simulation for 3D printed parts consisting of thousands to millions of droplets becomes prohibitively expensive, if not impractical.

Previously, (Korneev et al. 2020) constructed a ROM of the droplet-scale physics of the LMJ process based on a k-nearest neighbors (kNN) search within a set of data generated offline by a coupled multiphysics solver implemented in OpenFOAM. This algorithm can estimate the shape of solidified droplets on an arbitrary substrate at a speed of ∼ 33 droplets per second on the same 96-core cluster, a significant improvement compared to the high-fidelity OpenFOAM solver. Applying the ROM recurrently along a sampled toolpath, (Korneev et al. 2020) estimated the shape of a part consisting of ∼50,000 droplets, a result that would be impractical to achieve using OpenFOAM. Although using this ROM in place of OpenFOAM yielded orders of magnitude in speed up, unfortunately, the kNN search extrapolated poorly for out-of-training data, requiring a large data set to cover all possible substrate geometries, thereby offsetting the gains from the achieved speedup.

Here we present an improved ROM to enable partscale build simulations for LMJ using operator learning (OL) to approximate the droplet-scale physics. Rather than approximating the solution to the governing system of PDEs for a particular instance of initial/boundary conditions (ICs/BCs), as is done, for example, in physics-informed NNs (PINNs) (Raissi, Perdikaris, and Karniadakis 2019), OL allows one to learn the operator that maps the initial condition of a single droplet deposition in the moving subdomain to the final condition at the end of the deposition. The same trained operator can be used to predict this initialto-final condition mapping across numerous instances of the problem with the same PDEs and BCs, but different ICs. While a similar approach was already considered by the authors of (Korneev et al. 2020) using a fullyconnected feed-forward NN, the quadratic scaling of the number of network weights with the number of degrees of freedom (in this case, spatial grid size) required a prohibitively large network size for accurate predictions, making failures common after only a few sequentially deposited droplets. Instead, here we implement the recently developed Fourier neural operator (FNO) (Li et al. 2020(Li et al. , 2021)), a deep NN which learns a kernel integral operator related to the PDE's Green's function (or a generalization thereof, for nonlinear PDEs). This approach was found to yield a much smaller test error for the same amount of training data (Li et al. 2020). Moreover, FNO uses the convolution theorem to learn this operator in the Fourier domain, enabling speedup through the use of the Fast Fourier Transform (FFT) algorithm.

Below, we briefly review the moving subdomain approach used in (Korneev et al. 2020) in conjunction with a droplet-scale simulator of droplet-substrate coalescence, using either FV-based CFD (in OpenFOAM) or a kNN-based ROM (in Cython) to obtain a part-scale as-manufactured shape predictor. We then show how replacing kNN with FNO enables faster part-scale simulation at comparable accuracy with significantly fewer training data points.

# Reduced-Order Modeling for LMJ

The high-fidelity LMJ model can be decomposed into a series of single-droplet coalescence events applied along the toolpath (Fig. 1). The ICs for every coalescence event consist of a hot liquid droplet of spherical shape (pictured in red) captured by a phase field, its initial velocity, and a substrate of arbitrary shape. The substrate, on average, is composed of solid material. After hitting the substrate, the droplet solidifies and coalesces with the substrate surface; previous droplets that have coalesced with the substrate become part of the ICs for the next droplet. Figure 2 shows a time sequence of the coalescence for two consecutive droplets.

For the LMJ process, the droplet temperature is slightly above the solidification temperature. This low temperature difference minimizes residual stresses and eliminates warping of the final geometry. The absence of warping simplifies the physics of the LMJ process to the incompressible flow and heat transfer equations (Korneev et al. 2020).

High-fidelity numerical solutions of the droplet physics can be obtained using a finite volume (FV), volume of fluids (VoF) scheme in OpenFOAM. However, these simulations can become prohibitively expensive at the part scale, where thousands or even millions of droplets need to be deposited. This prompted (Korneev  Notice how the evolving temperature profile of the two simultaneously cooling droplets affects coalescence. Red corresponds to the hotter temperature.

Fig. 6. Comparison of the simulation and experimental results of a single droplet solidification on a substrate. In the simulation results, the blue represents liquid and gray represents solid. In the experimental images, the part below the dashed line includes the reflection of the droplet, which should be neglected.

high-fidelity multiphysics solver becomes prohibitively expensive when simulating the coalescence of thousands of droplets to estimate the as-manufactured shape of realistic parts. To circumvent this problem, we use machine learning (lazy learning) to construct a reduced order model of the full physics, such that the reduced order model can rapidly approximate the results of Eqs.

(2), (6), and (8) at a fraction of the computational cost. The reduced order model may therefore be considered a surrogate model of the solution to the multiphase flow equation.

## Generating the training data

Generating the training data (offline) to derive the reduced order model requires explicitly simulating the coalescence physics using a numerical solver. Droplet coalescence is simulated in OpenFoam [24] along a set of pre-defined tool-paths. Note that the size of the computational domain grows with every newly deposited droplet, and that the performance of the numerical solver is adversely affected when the multiphase flow has to be solved over a large domain. Observing that the physics of interest occurs in a small neighborhood around the location of droplet deposition, we use a sliding subdomain to efficiently model the build process. A subdomain of size Nx ⇥ Ny ⇥ Nz = 81 ⇥ 81 ⇥ 81 voxels or 1.35 mm ⇥ 1.35 mm ⇥ 1.35 mm in physical dimensions is chosen to simulate the droplet coalescence. Each such subdomain includes solidified substrate geometry that depends on the previously coalesced droplets. We assume the initial droplet is deposited on a flat substrate, and store the evolving substrate geometry due to droplet coalescence as a voxel model. This voxel model is accessed at each subdomain simulation to get a representation of the local substrate geometry on which the next droplet solidifies.

Given the substrate geometry within a subdomain, we describe the subdomain in Cartesian coordinates assuming the deposition along the z direction. To initiate the simulation, a spherical liquid droplet is placed in the middle of the x-y plane at {x0, y0} = {(Nx 1)/2, (Ny 1)/2}, where {x0, y0} (= {40, 40} in our case) are the planar coordinates of the droplet center in a direction perpendicular to the deposition direction. Proximity to the substrate determines the positioning of the liquid droplet in the z direction. Numerical simulation is then performed on the subdomain (see Section 3), and the updated shape of the solidified substrate is written into the voxel model storing the evolving substrate geometry. After each OpenFoam simulation within a subdomain, the subdomain window is moved to the next point on the (rasterized) tool-path and the simulation is repeated until the part build is finished.

The multiphysics solver inputs/outputs the spatial distribution of the solid, liquid, and gas phases, and the spatial distribution of the temperature, pressure, and flow velocity. In this paper, we only consider the spatial distribution of the solid phase as a controlling variable for the shape of the solidified droplet, but it should be noted that the training approach is not limited to a single field. Each element of the training set is a pair S = {{↵ i input , ↵ i output }, i = 1, . . . , Nset }, where • ↵ i input represents the solid phase before the simulation (i.e. the solidified substrate before the new droplet is placed), and ↵ i output represents the shape of the solidified droplet (without the substrate) obtained after simulation in the subdomain. The sum ↵ i output + ↵ i input represents the union of the input substrate and the solidified droplet, and will serve as ↵ i+1 input for the i + 1th simulation. • Nset is the total number of training data points. et al. 2020) to construct a kNN search algorithm that could predict the droplet coalescence at a fraction of the computational cost of the OpenFOAM solver. First, a set of 9, 000 samples was generated with the Open-FOAM solver, where the input and output included solid and liquid phase variables-from which the gas phase can be obtained, since, by definition, they must add up to unity-before and after the simulation, i.e., when the liquid droplet is slightly above the substrate and when it hits and merges with it after solidification, respectively (Fig. 1). When presented with a new input, the training set was searched for its kNNs and the predicted output was computed via averaging of the outputs corresponding to these neighbors (Korneev et al. 2020).

While an accelerated version of the kNN algorithm in (Korneev et al. 2020) could predict a single droplet deposition in about 0.03s (i.e., a 20,000x speedup compared to OpenFOAM) on the same 96-core cluster, this was still longer than the actual deposition time on the machine (0.01s for a 100Hz deposition frequency). Moreover, the method was not designed to generalize beyond the training set. To rectify these shortcomings, here we present an OL based approach to map initial to final conditions in the moving subdomain. We use an updated data set, obtained from OpenFOAM simulations, with an improved multiphysics model involving experimentally calibrated parameters.

# Operator Learning for LMJ

The underlying idea of OL for scientific computing is to approximate maps M † , between infinite-dimensional function spaces, representing solution operators of initial/boundary-value problems. More concretely, we aim to construct a parametric map:

for a finite-dimensional parameter space Λ by choosing an "optimal" value λ † ∈ Λ such that M λ † represents the best approximation to M † in some sense (e.g., minimizing a least-squares error). While the PDE itself is typically defined locally, its solution operator has non-local effects that can be described by integral operators. This inspired the authors of (Li et al. 2020) to approximate the (possibly generalized) Green's function of a problem's governing PDE by a graph kernel network. In (Li et al. 2021), the same authors then interpreted this kernel as a convolution operator through the architecture visualized in Fig. 3 and briefly reviewed in the Appendix. This approach enables a finite-dimensional parametrization of the input/output functions via a truncated Fourier basis.   Neural-FEM. The second approach directly parameterizes the solution function as a neural network [E and Yu, 2018, Raissi et al., 2019, Bar and Sochen, 2019, Smith et al., 2020]. This approach is designed to model one specific instance of the PDE, not the solution operator. It is mesh-independent and accurate, but for any given new instance of the functional parameter/coe cient, it requires training a new neural network. The approach closely resembles classical methods such as finite elements, replacing the linear span of a finite set of local basis functions with the space of neural networks. The Neural-FEM approach su↵ers from the same computational issue as classical methods: the optimization problem needs to be solved for every new instance. Furthermore, the approach is limited to a setting in which the underlying PDE is known.

# ARC internal use only

Neural Operators. Recently, a new line of work proposed learning mesh-free, infinite-dimensional operators with neural networks [Lu et al., 2019, Bhattacharya et al., 2020, Nelsen and Stuart, 2020, Li et al., 2020b, Li et al., 2020a]. The neural operator remedies the mesh-dependent nature of the finite-dimensional operator methods discussed above by producing a single set of network parameters that may be used with di↵erent discretizations. It has the ability to transfer solutions between meshes. Furthermore, the neural operator needs to be trained only once. Obtaining a solution for a new instance of the parameter requires only a forward pass of the network, alleviating the major computational issues incurred in Neural-FEM methods. Lastly, the neural operator requires no knowledge of the underlying PDE, only data. Thus far, neural operators have not yielded e cient numerical algorithms that can parallel the success of convolutional or recurrent neural networks in the finite-dimensional setting due to the cost of evaluating integral operators. Through the fast Fourier transform, our work alleviates this issue.

Fourier Transform. The Fourier transform is frequently used in spectral methods for solving di↵erential equations, since di↵erentiation is equivalent to multiplication in the Fourier domain. Fourier transforms have also played an important role in the development of deep learning. In theory, they appear in the proof of the universal approximation theorem [Hornik et al., 1989] and, empirically, they have been used to speed up convolutional neural networks [Mathieu et al., 2013]. Neural network architectures involving the Fourier transform or the use of sinusoidal activation functions have also been proposed and studied [Bengio et al., 2007, Mingo et al., 2004, Sitzmann et al., 2020]. Recently, some spectral methods for PDEs have been extended to neural networks [Fan et al., 2019a, Fan et al., 2019b]. We build on these works by proposing a neural operator architecture defined directly in Fourier space with quasi-linear time complexity  Neural-FEM. The second approach directly parameterizes the solution function as a neural network [E and Yu, 2018, Raissi et al., 2019, Bar and Sochen, 2019, Smith et al., 2020]. This approach is designed to model one specific instance of the PDE, not the solution operator. It is mesh-independent and accurate, but for any given new instance of the functional parameter/coe cient, it requires training a new neural network. The approach closely resembles classical methods such as finite elements, replacing the linear span of a finite set of local basis functions with the space of neural networks. The Neural-FEM approach su↵ers from the same computational issue as classical methods: the optimization problem needs to be solved for every new instance. Furthermore, the approach is limited to a setting in which the underlying PDE is known.

Neural Operators. Recently, a new line of work proposed learning mesh-free, infinite-dimensional operators with neural networks [Lu et al., 2019, Bhattacharya et al., 2020, Nelsen and Stuart, 2020, Li et al., 2020b, Li et al., 2020a]. The neural operator remedies the mesh-dependent nature of the finite-dimensional operator methods discussed above by producing a single set of network parameters that may be used with di↵erent discretizations. It has the ability to transfer solutions between meshes. Furthermore, the neural operator needs to be trained only once. Obtaining a solution for a new instance of the parameter requires only a forward pass of the network, alleviating the major computational issues incurred in Neural-FEM methods. Lastly, the neural operator requires no knowledge of the underlying PDE, only data. Thus far, neural operators have not yielded e cient numerical algorithms that can parallel the success of convolutional or recurrent neural networks in the finite-dimensional setting due to the cost of evaluating integral operators. Through the fast Fourier transform, our work alleviates this issue.

Fourier Transform. The Fourier transform is frequently used in spectral methods for solving di↵erential equations, since di↵erentiation is equivalent to multiplication in the Fourier domain. Fourier transforms have also played an important role in the development of deep learning. In theory, they appear in the proof of the universal approximation theorem [Hornik et al., 1989] and, empirically, they have been used to speed up convolutional neural networks [Mathieu et al., 2013]. Neural network architectures involving the Fourier transform or the use of sinusoidal activation functions have also been proposed and studied [Bengio et al., 2007, Mingo et al., 2004, Sitzmann et al., 2020]. Recently, some spectral methods for PDEs have been extended to neural networks [Fan et al., 2019a, Fan et al., 2019b]. We build on these works by proposing a neural operator architecture defined directly in Fourier space with quasi-linear time complexity 2 Identifying a(x) ∈ R and b(x) ∈ R for x ∈ Ω ⊂ R 3 , where Ω is the moving subdomain, as the initial and final conditions, respectively, specified through the combined solid, liquid, and brass2 phase fractions at t = 0 and t = 0.0025s for a 400 Hz deposition frequency, we replace kNN with FNO to sequentially deposit droplets along the toolpath as before.

We train the FNO surrogate using 770 input/output pairs generated by simulations of 4 pyramid parts (620 data points) and 1 hollow cylinder part (150 data points), where the latter is deemed useful by numerical experimentation to handle part geometries with thin features. We test the resulting model using 324 input/output pairs generated by simulations of a cube part (i.e., different from the training set). We repeat this process for different sets of hyperparameters-namely, Fourier layer width and number of retained Fourier modes-until a satisfactory combination is produced.

Training of and inference with the FNO surrogate was done using PyTorch code made available on the public domain under the MIT License (Li, Cao, and Griffiths 2021) by (Li et al. 2021). To take advantage of GPUaccelerated FFT, training and prediction were done on an NVIDIA RTX 3090 GPU.

# Results

Figure 4a shows the distribution of errors on the cubes test data set for an optimized set of hyperparametersnamely, Fourier layer width and number of retained Fourier modes. The distribution of errors is skewed toward smaller values than the average of 16.7% with a mode slightly above 10%.

Following this test set validation, we use the trained FNO model in conjunction with the moving subdomain method for inference of single lines of droplets sequentially deposited with spacings of a few hundred microns. Counterparts computed by the CFD solver in Open-FOAM serve as the "ground truth." Figure 4b visualizes the FNO prediction and corresponding OpenFOAM result for droplet spacings S norm equal to 62.72% (1), 89.61% (2) and 116.49% (3) of the droplet diameter D. For each of these cases, the left isosurface is predicted by FNO and colored according to the distance (normalized with respect to D) between each vertex on this surface and the vertex on the OpenFOAM isosurface (right, in gray) closest to that point. The largest of these distances corresponds to the so-called Hausdorff distance d H , which is visualized in the left part of Fig. 4b for all considered droplet spacings as d H,norm = d H /D (in %). Although d H,norm can reach values up to 30%, from the distance heat maps on the right we can see that the majority of the relative errors is less than 15%.

LMJ-generated parts are printed by layering many droplet lines such as those visualized in Fig. 4b on top of each other. Hence, the first step in assessing FNO's ability to predict such parts is to focus on only a few layers of stacked droplet lines, as shown in Fig. 5 for a normalized droplet spacing S norm of 89.61%. In dark gray, we show the prediction of FNO trained on the mixed training set consisting of both pyramid and hollow cylinder parts detailed in the previous section. Compared to the prediction (in blue) of FNO trained on 1,460 data points from only pyramid parts, we note a clear qualitative improvement in the prediction accuracy. This could be explained by the fact that inclusion of the hollow cylinder data in the training set improves FNO's learning of thin-wall scenarios, and allows it to outperform its counterpart trained on a larger, but less diversified, set of pure pyramid data.

Figure 6 shows FNO's inference of a gear-shaped part generated by 16,000 droplets with S norm = 89.61%. A more detailed view of the upper section reveals that FNO is capable of predicting repeated layers of droplet lines, including those along part edges, although some imperfections can be seen along both the inner and outer walls. Prediction of such a gear shape using kNN accelerated via height maps required 36,000 inputoutput data pairs (Korneev et al. 2020)   For three of these cases, we visualize the isosurfaces for the FNO prediction and its OpenFOAM ground truth counterpart, with the former color-coded by the distance between each vertex on the FNO isosurface and its closest neighbor on the OpenFOAM isosurface (i.e., representing an error "heat map"). almost two orders of magnitude. Moreover, inference of a single droplet deposition took 0.03s with kNN, while FNO performs this task in ∼3ms, which is one order of magnitude smaller.

# Conclusions

We implemented a surrogate model for liquid metal jetting (LMJ) based on deep learning of solution operators of the partial differential equations (PDEs) governing the droplet deposition process. Specifically, we employed the recently developed Fourier neural operator (FNO) based on approximating a kernel integral operator by a neural network (NN), and utilizing the convolution theorem to parametrize this NN in Fourier space and take advantage of Fast Fourier Transform (FFT), implemented on a GPU. We found that the FNO surrogate, trained on high-fidelity simulation data gener- ated with multiphysics computational fluid dynamics (CFD), is capable of predicting the geometric features for single and stacked droplet lines, showing promising results for part-scale simulations via a moving subdomain approach.

Our analysis yielded the following major conclusions:

1. FNO shows signs of sufficient out-of-training predictive capability for LMJ. Diversifying the training set with various geometric features (e.g., both infill and thin-wall artifacts) can improve the predictive capability of FNO for build simulation of complex parts, while reducing the amount of data required for training.

2. FNO can accurately predict lines of sequentially deposited droplets for droplet spacings either smaller or bigger than the droplet diameter. 3. FNO is qualitatively capable of predicting thin-wall features generated by stacked lines of droplets and the resulting simple part shapes. Future activities may include adding physics-based regularization into the FNO training loss to ensure compatibility with relevant conservation laws, and to check whether this can further reduce the amount of training data needed to achieve a given prediction error. We also plan to compare with other OL approaches such as DeepONet (Lu, Jin, and Pang 2021) to investigate the impact of the NN architecture on generalizability.

While this study addresses prediction of geometric features pertinent to dimensional accuracy and surface quality of as-printed parts, the extension of the predictions to more complex material properties such as residual stresses, elongation, and tensile/compressive strength remains to be investigated. Such predictions will inevitably require including more physical quantities (e.g., temperature fields) in the input/output sets, necessitating further changes in the NN architecture to incorporate multiple inputs and outputs.

# Appendix : Fourier Neural Operator (FNO) architecture

Here we briefly overview the architecture of FNO. More details can be found in (Li et al. 2021). As illustrated in Fig. 3, the mapping from input a(x) to output b(x) consists of the following steps: 1. Lift the input a(x) to a higher-dimensional space through a fully-connected NN representing the local (pointwise) transformation v 0 = P (a). 2. Apply iteratively v t+1 (x) = σ (W v t (x) + (K(a; φ)v t )(x)) , (2) for x ∈ Ω ⊂ R d . Here v t (t = 0, . . . , T -1) is a sequence of functions taking values in R dv , W : R dv → R dv is a linear transformation, and σ : R → R is a nonlinear activation function applied componentwise. 3. Project back the result v T into the original space through a fully-connected NN representing the local transformation b = Q(v T ). In Eq. ( 2), K is a kernel integral operator mapping given by: (K(a; φ)v t )(x) := D κ φ (x, y, a(x), a(y))v t (y)dy, (3) where x, y ∈ Ω. Both W and the parameters φ in the kernel κ φ : R 2(d+da) → R dv×dv are learned from data.

To improve the efficiency of their algorithm, (Li et al. 2021) assumed K to be a convolution operator which, through the convolution theorem, enabled parametrization of κ φ directly in the Fourier domain. When the domain Ω is discretized uniformly, this can be done via FFT, accelerated via GPU parallel computing.

# Acknowledgments

The authors are grateful to Zongyi Li (Caltech) for generously sharing the FNO code and helpful comments.

