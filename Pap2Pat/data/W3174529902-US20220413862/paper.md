# Introduction

The past ten years have witnessed a resurgence of Machine Learning, specifically in the form of Deep Learning. Deep Neural Networks (DNNs) such as Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) serve as the state-of-the-art foundation and core enabler of many applications that have only emerged within the last few years and yet have become extremely popular among all users of computing today [6,58]. Behind the success of deep learning are the increasingly large model sizes and complex model structures that require tremendous computation and memory resources [16]. There is a difficult trade-off between increasing complexity of DNNs (required for increasing accuracy) and deployment of these DNNs on resourceconstrained mobile devices (required for wider reach).

In recent years, there has been a significant emphasis on optimizing the execution of large DNNs. Operator fusion (or kernel/layer fusion) has been a common approach towards improving efficiency of DNN execution [1,12]. The basic idea of such fusion is the same as the traditional loop fusion done by optimizing compilers [31,32,44], and they lead to the following benefits: (i) eliminating unnecessary materialization of intermediate results, (ii) reducing unnecessary scans of the input; and (iii) enabling other optimization opportunities. Traditional end-to-end frameworks like TensorFlow Lite [1], TVM [12], MNN [29], and Pytorch-Mobile [48] all have operator fusion optimizations, which are broadly based on recognizing certain fusion patterns. These transformations have generally been based on a representation called computational graph [12], which views the application as a set of operations on tensors, and representation of dependencies in the form of consumption of tensor(s) output by an operation by another operation.

In this paper, we observe that the fusion patterns considered in the past work [1,12] are too restricted to cover the diversity of operators and layer connections that are emerging. For example, ONNX (Open Neural Network Exchange) [47] lists 167 distinct operators, and creating fusion patterns based on their combinations is unlikely to be a feasible approach. At the same time, traditional compiler loop transformations (including fusion [32,44]) work on a low-level view of the computation, i.e., loop (indices) and dependence between array elements. More recent work on loop fusion has been based on polyhedral analysis [10], with several different resulting algorithms [2,3,9]. Polyhedral analysis, while providing an excellent foundation to rigorously reason the legality of, and explore the space of, loop transformations, can be an "overkill" to capture the relatively simple data structures (tensors) and operations (without loop-carried dependencies) in DNNs. Moreover, polyhedral analysis is normally limited to affine-loop analysis and transformations (although latest efforts [61,71,72] do extend it to certain non-affine loop optimizations), and cannot capture certain operation (combinations) in DNNs. An example will be a combination of Gather, which copies input to output indirectly using an index array followed by Flatten, which changes the dimensionality of a tensor. Finally, the operator view in computational graphs can enable us to exploit properties of these computations, which may be lost when a lower-level view of the computation is considered.

This paper presents DNNFusion, a rigorous and extensive loop fusion framework that can exploit the operator view of computations in DNNs, and yet can enable a set of advanced transformations. The core idea is to classify operators into different types, and develop rules for different combinations of the types, as opposed to looking for patterns with specific combination of operations. Particularly, we first classify the existing operations in a DNN into several groups based on the mapping between their input and output , such as One-to-One, One-to-Many, and others. We also enhance the computational graph representation into the Extended Computational Graph (ECG) representation, where the type Table 1. An empirical study to motivate this work: The relation of overall computation, layer count, and execution efficiency of multiple DNNs. Results are collected on Qualcomm Adreno 650 GPU with an optimized baseline framework with fixed-pattern operator fusion that outperforms all state-of-the-art DNN execution frameworks (called OurB+ and will be introduced later).

# Model

#Total layer IR size #FLOPS Speed (FLOPs/S) VGG-16 [62] 51 161M 31.0B 320G YOLO-V4 [7] 398 329M 34.6B 135G DistilBERT [60] 457 540M 35.3B 78G MobileBERT [65] 2,387 744M 17.6B 44G GPT-2 [55] 2,533 1,389M 69.1B 62G

(and other properties) of the operation are explicitly noted.

Then, we design a mapping type analysis to infer the profitability of fusing operations of different combinations of these types of operators, binning the combination into three groups: likely profitable (and legal), likely not profitable, and ones where profitability may need to be determined through profile information.

Next, on the ECG representation, we apply a series of graph rewriting rules that we have developed. These rules exploit the mathematical properties of the operations and have a similar flavor to the classical optimization called strength reduction [14]. Unlike traditional compiler work, however, we apply these rules on operations on tensors (and not scalars) and our set of rules go well beyond the traditional ones. The rest of our framework comprises algorithms for determining fusion of specific operations (based on certain heuristics) and generating optimized fused code. Almost each fusion generates a new operator (and its implementation) that is not present in the original library; however, once a new operator is generated, its implementation can be reused when the same pattern is detected in the same or a different model. Overall, we show that an operator view of the DNN can enable rigorous optimizations, beyond what will be possible with a lower-level view of the computation or the existing (simplistic) work on applying a small set of fusion patterns on the operator view.

In summary, this paper makes the following contributions:   [29], TVM [12], TensorFlow-Lite [1], and Pytorch-Mobile [48], DNNFusion achieves up to 8.8× more loop fusions, 9.3× speedup with our proposed advanced operator fusion. Particularly, DNNFusion for the first time allows many latest DNN models that are not supported by any existing end-to-end frameworks to run on mobile devices efficiently, even in real-time. Moreover, DNNFusion improves cache performance and device utilization -thus, enabling execution on devices with more restricted resources -and reduces performance tuning time during compilation.

# Blessing and Curse of Deep Layers

This section presents a study that motivates our work, by demonstrating that it is challenging to execute deep(er) neural networks efficiently, particularly on resource-constraint mobile devices, due to the high memory and computation requirements.

As we stated earlier, there has been a trend towards deeper DNNs. With increasing amount of computation, there has also been a trend towards reducing the computation by reducing the weight size. Consider the well-known Natural Language Processing (NLP) model, BERT [19] as an example. TFLite takes 985ms to inference BERT on the latest CPU of Snapdragon 865. In recent efforts [55,65] (MobileBERT, GPT-2), machine learning researchers have addressed this issue by reducing the weight size on each layer and thus training thinner and deeper models to balance the computation workload and model accuracy.

However, we have observed that the depth of the model is the critical impediment to efficient execution. Our experimental study has correlated execution efficiency with the total amount of computation and the number of layers (Table 1). Particularly, we can see that although DistilBERT [60] and VGG-16 [62] have a similar number of computations (while having 457 and 51 layers, respectively), DistilBERT's execution performance (78 GFLOPs/S) is much worse than VGG's (320 GFLOPs/S). This is mainly because of two reasons.

First, models with more layers usually generate more intermediate results, thus increasing the memory/cache pressure. Second, deep models usually have an insufficient amount of computations in each layer, thus degrading the processor's utilization, particularly for GPUs. Operator fusion can be an effective technique to reduce memory requirements and improve efficiency, and is the focus of our study.

# Classification of DNN Operators and Fusion Opportunity Analysis

This section establishes the basis for our approach, by classifying DNN operators and their combinations.

## DNN Operators Classification

This work carefully studied all operators supported by a popular (and general) DNN ecosystem ONNX (Open Neural Network Exchange) [47], and finds that the mapping relation between (each) input and output of each operator is critical to determine both the profitability and correct implementation of fusion optimization. Moreover, it is possible for us to classify all operators into five high-level abstract types based on the relationship between input elements and output elements. These five types are One-to-One, One-to-Many, Many-to-Many (which includes Many-to-One, but we do not consider it separately here), Reorganize, and Shuffle. This classification serves as the foundation of our proposed fusion framework. Table 2 shows more details of this operator classification and gives one or two representative examples for each mapping type. If an operator has only one input or multiple inputs with the same mapping type to the output, the mapping type of this operator is decided by its any input/output pair. If multiple input/output pairs with varied mapping types exist, this operator's mapping type is decided by the more complex mapping type 1 .

Assuming each input element can be denoted as 𝑥 [𝑑 1 , . . . , 𝑑 𝑛 ], where 𝑥 means the operand of an operator and 𝑑 1 , . . . , 𝑑 𝑛 denotes the index for an element of an operand, the mapping types between one input and one output are classified as follows.

Table 3. Mapping type analysis. The first column and the first row (both without color) show the mapping types of first and second operators, respectively, before fusion, and the colored cells show the mapping type of the operator after fusion. Green implies that these fusion combinations can be fused directly (i.e., they are profitable). Red implies that these fusions are unprofitable. Yellow implies that further profiling is required to determine profitability.

# Mapping type combo

One-to-One One-to-Many Many-to-Many Reorganize Shuffle One-to-One One-to-One One-to-Many Many-to-Many Reorganize Shuffle

One-to-Many One-to-Many One-to-Many One-to-Many One-to-Many Many-to-Many Many-to-Many Many-to-Many Many-to-Many Many-to-Many Reorganize Reorganize One-to-Many Many-to-Many Reorganize Reorganize Shuffle Shuffle One-to-Many Many-to-Many Reorganize Shuffle

Mapping relation: One-to-one < (Reorganize, Shuffle) < (One-to-many, many-to-one)

# First op

Second op

• One-to-One: There is a set of functions 𝐹, 𝑓 

## Fusion Opportunity Analysis

Based on the mapping type of each operator, this work proposes a new fusion analysis. The basic idea is that given two fusion candidate operators with a certain combination of mapping types, it is possible to: 1) infer the mapping type of the resulting fused operation; and 2) simplify the profitability evaluation and correct implementation of this fusion. Table 3 shows the details of this analysis. The first column and the first row (without any color) show the mapping types of the first and the second operator to be fused and the colored cells show the mapping type of the resulting operator. It further classifies the fusion of this combination of mapping types into three groups (shown as green, yellow, and red, respectively). Green implies that these fusions are legal and profitable and no further analysis is required. Red implies that these fusions are known to be either illegal or clearly not profitable. Yellow implies that these fusions are legal; however, further profiling is required to determine profitability. This analysis eliminates the need for anytime runtime analysis or autotuning for red and green cases. For remaining (yellow) cases, we can further accelerate compilation using a profiling database that stores the execution results of various fusion combinations collected offline.

These five mapping types have a range of what we call transformation impedance (which we informally define as a metric to qualitatively express the difficulty to fuse), i.e., when they are fused with another type, they have different capability of deciding the fused mapping type. One-to-One has the lowest transformation impedance among all five types, whereas Reorganize and Shuffle's transformation impedance is in the middle, i.e., they can transform One-to-One to their types while they cannot transform others. One-to-Many and Many-to-Many have the strongest transformation impedance, i.e., the resulted mapping type is decided by them solely when they are fused with other operators. Moreover, One-to-Many and Many-to-Many have the same capability, and Reorganize and Shuffle have the same as well.

We elaborate on the following representative combinations to provide intuition behind the Table 3.

• One-to-One with others. When a One-to-One operator (𝑂𝑝 1 with the input I and the output O) is fused with an operator of any type (𝑂𝑝 2 ), i.e., 𝑂𝑝 2 takes O as the input, the memory access to each element of O can be mapped to the access to each element of I, as long as this mapping function is known. Unlike general programs where the dependencies can be more complex, the use of tensors and a limited set of operators limits the type of mappings, and DNN operators carry this mapping information. Our analysis leverages this high-level operator information to ensure the correctness of these fusions. Moreover, this fusion usually requires limited number of registers and does not incur extra overhead like data copying or redundant computations, so they are profitable. Take a case that fuses Add and GEMM in either order. Each element in the output of Add can be replaced by two elements in the two inputs of Add, ensuring correct and profitable fusion, irrespective of the order of these operations. • Reorder or Shuffle with others. Both types are variants of One-to-One with a special mapping function between the input and the output. Above reasons for the correctness analysis are also applied here; however, when fusing with One-to-Many or Many-to-Many types operators, profitability needs to be validated with further profiling because of the possibility of introduced data copying, change in data access order, or redundant computations.

As an example, consider Expand and Transpose operators -Expand copies the input tensor with a continuous memory access pattern, whereas, Transpose transposes the input tensor to the output tensor according to the permutation in operator properties. Thus, the resulting fused operation may not have continuous memory accesses. • One-to-Many with Many-to-Many. Take the case that Expand followed by Conv -as Conv reads the feature map input tensor with continuous access, while a One-to-Many operator can distribute the continuous input tensor elements.

As it is very desirable for the (compute-intensive) Many-to-Many operators to read the input tensors in a continuous way, we consider this fusion unprofitable. • Many-to-Many with Many-to-Many. When a Many-to-One mapping operator is followed by a Many-to-One operator, e.g. Conv followed by another Conv, attempting a combined execution will be too complicated and will likely negatively impact register and cache usage. Thus, we consider them unprofitable. • Many-to-Many with One-to-Many. When a Many-to-One mapping operator is followed by a One-to-Many operator, e.g. Conv followed by Expand or Resize, a combined execution may or may not have a desirable data access pattern.

When Conv is combined with Expand, as Expand operator only expands a single dimension of the input, so it will not adversely affect the computation pattern of Conv. On the other hand, if Conv is combined with a Resize that will copy the input tensor along different dimensions, it can negatively impact the computation of Conv. Thus, we consider such cases to be requiring further profiling. Extended Computational Graph. Based on the analysis above and as a background for the methods we will present next, we introduce Extended Computational Graph (ECG) as our intermediate representation (IR). As the name suggests, this represents builds on top of the (traditional) Computational Graph [12], which captures the data-flow and basic operator information like the operator type and parameters. ECG contains more fusion-related information, including mapping_type indicating the mapping type of each operator, IR_removable denoting if an intermediate result can be removed completely (which is true only if all its successors can be fused and which is calculated during fusion), and mathematical properties of the operations like whether the associative, commutative, and/or distributed properties hold.

# DNNFusion's Design 4.1 Overview of DNNFusion

Figure 1 shows an overview of DNNFusion. It takes the computational graph generated from compiler-based DNN execution frameworks (e.g., TVM [12], and MNN [29]) as the input, and adds key information to create the Extended Computational Graph (ECG). Based on this ECG, the main compiler 

## Mathematical-Property-Based Graph Rewriting

DNNFusion first employs a mathematical-property based graph rewriting pass to optimize the Extended Computational Graph (ECG). With this pass, DNNFusion is able to 1) remove unnecessary operations, 2) eliminate redundant intermediate data copies, and 3) replace costly (combination of) operators with more efficient ones. This graph rewriting carried out here is in the spirit of the classical compiler optimization of strength reduction [14]; however, here it is performed on complicated operators on matrices or tensors rather than on scalar expressions. Moreover, the rules we present are more complex and involved, and are based on operations that are common in DNNs. More importantly, compared to existing efforts on computational graph substitution (e.g., TASO [28]), our graph rewriting is designed to work in conjunction with operator fusion and identifies a set of operators and rules for that specific purpose. Our evaluation results (Section 5) show that with graph rewriting, there are 18% fewer fused layers left after fusion on GPT-2. We also do an experimental comparison against TASO later in this paper.

Figure 2 shows specific examples of leveraged mathematical properties (distributive, communicative, and associative). Table 4 shows a more complete set of rules. This table also shows the computation size (in #FLOPS) before and after the rewriting. Our rules mainly focus on operators in the One-to-One mapping type (e.g., element-wise multiplication, addition, reciprocal, square root, and others) and several reduction operators that are in Many-to-Many (e.g., ReduceSum and ReduceProd) -this is because these operators usually follow our defined mathematical properties. DNNFusion uses 

Computation based rewriting   

† First use commutative property to swap 𝐵 and 𝐴𝑏𝑠 (𝐶), then apply associative property. ‡ Even though this pattern has no #FLOPS gains, it can enable further optimization, e.g the case of †. ¶ #FLOPS is calculated by assuming the reduction of ReduceSum/ReduceProd is along with the inner-most dimension.

#FLOPs (rather than temporary output size or memory footprint) as the metric to drive graph rewriting mainly because of two reasons: first, in most of the applications scenarios of these rules, the temporary output size keeps the same before and after graph rewriting, and second, the size of the temporary output in a majority of other cases becomes a non-issue because fusion is applied after rewriting. For a small number of remaining cases, i.e., where temporary output size changes and the fusion is not applied, more sophisticated methods will be considered in the future. We now elaborate on some of the rules presented in Table 4, which were also depicted in Figure 2.

• Associative: By leveraging the associative property, the graph rewriting pass can identify an optimized order of operators execution, and hence replace the expensive combination of operators with a new cheaper one. Figure 2 (a) shows an example, in which a combination of two Recip operators and two Mul operators is replaced by a combination of a Recip, a Square, and a Mul. The latter is more efficient as it eliminates a Mul operator and the intermediate result size is significantly reduced, leading to reduced register pressure after subsequent fusion. • Distributive: Following the same ideas as above, applying distributive property also enables optimization opportunities. As shown in Figure 2 (b), the combination of two Mul operators and an Add can be replaced by an Add followed by a Mul, thus eliminating an unnecessary operator. • Commutative: The property guaranties the legality of swapping the position of two operators, which usually results in computation reduction. As shown in Figure 2 (c), BitShift 2 and ReduceSum 3 satisfy communicative property, thus ReduceSum can be scheduled to execute before BitShift, reducing the number of elements on which BitShift is applied. DNNFusion employs pattern matching [36,37] to recognize rewriting candidates. However, associative and commutative matching is NP-complete [5]. Therefore, DNNFusion first partitions the entire Extended Computational Graph into many sub-graphs by considering operators with neither of associative, communicative, or distributive properties as partitioning points within the original graph. Within each sub-graph, DNNFusion can explore all possible patterns and pattern combinations because these sub-graphs have limited number of operators. More specifically, all matching rules within a partition are considered and the rule leading to the largest reduction in #FLOPS is applied. This process is repeated till there are no additional matching rules within the partition. DNNFusion chooses this greedy scheme to keep the optimization overheads low.

## Light-Weight Profile-Driven Fusion Plan Exploration

4.3.1 Overall Idea. Optimal fusion plan generation requires a large search space [8,22] and has been shown to be NP-complete [15,32]. To keep the process at manageable costs, DNNFusion explores fusion plans by employing a new light-weight (greedy) approach based on our proposed Extended Computational Graph (ECG) IR and our classification of operations into mapping types. The high-level ideas are as follows. First, DNNFusion selects the starting operators (called fusion seed operators) from our ECG to restrict the search space. This is based on a key insight that operators of One-to-One mapping type have 2 Calculate the bit shifted value of elements of a given tensor element-wisely. 3 Calculate the reduced sum of elements of an input tensor along an axis. the potential to yield more benefits because they a) potentially result in fusion of more layers, including both with their predecessors and successors because of what we refer to as lower transformation impedance, and b) have lower memory requirements and need for fewer registers among all mapping types. Second, starting with these seed operators, DNNFusion explores fusion opportunities along the seed operator's successors and predecessors, respectively. Third, DNNFusion creates fusion plans based on an approach that combines machine-independent mapping type analysis and a profiling result database. The mapping type analysis follows Table 3 to check the operators' mapping type combination (in ECG) to decide if these operators should be fused. Such mapping eliminates unnecessary profile data lookup for most cases.

# Plan generation

## Fusion Plan Generation Algorithm.

List 1 shows our detailed fusion plan exploration algorithm. Its goal is to generate candidate fusion blocks that are further optimized by subsequent intra-block optimizations (Section 4.4) before fusion code generation. Figure 3 illustrates its basic idea with a simplified example. This algorithm consists of three main steps:

Step I: Fusion seed operator(s) selection. DNNFusion selects the One-to-One operator with the minimum intermediate result as the fusion seed operator (as shown in Listing 1 lines 1 to 5 

# Listing 1. Fusion plan generation

Step II: Propagated exploration along seed's successors.

Each operator may have one or multiple immediate predecessors and successors. DNNFusion first processes the seed operator's successors one by one (Listing 1 Lines 7 to 24). At any stage in this recursive exploration, if a node cannot be fused with any of its immediate successors, fusion is not considered any further. Broadly, this step proceeds as follows.

First, mapping type analysis (Listing 1 Step 2.1) categorizes the potential fusion result into three types based on Table 3: 1) fuse_break indicates this is a Red case, and fusion should be aborted; 2) fuse_through indicates that this is a Green case, and should be proceeded without any further analysis;

Code generation Step III: Propagated exploration along seed's predecessors. After processing along the seed's successor direction, DNNFusion processes along the seed's predecessors direction with the same algorithm as Step II (In fact, Step III and

Step II can be swapped). However, one difference is that if an operator has multiple immediate predecessors, there is an option of fusing with some, but not all, of these immediate predecessors. In the example in Figure 3, the first attempt of fusing current candidate fusion block with GEMM fails because both of them are of many-to-one mapping type. in Table 3. The basic idea is that as long as the operators are of the same type, the same rules lead to efficient code. While fusing more than two operators, these rules are invoked each time two operators are fused. Finally, the subsequent code optimizations (e.g, vectorization, unrolling, tiling, and memory/register optimizations, and auto-tuning of these optimizations) are handled by our existing framework called PatDNN [46], thus not a major contribution of this paper. Note that almost each fusion generates a new operator (and its codes) that is not present in the original operator library; however, once the new operator (and its code) is generated, it can be used for both the current model and future models. Figure 4 shows an example of the code generation. To elaborate, DNNFusion first generates a data-flow tree (DFT) from the Extended Computational Graph (ECG). This DFT represents the final output (Out), all intermediate results (IRS), and all inputs (A, B, C, and D) with the edges reversed as compared to the ECG (i.e., the parent node depends on the children nodes). During the fused code generation, DNN-Fusion traverses this DFT to recognize the input/output data dependence (and fuses corresponding ECG operations), recursively. The right-hand side of Figure 4, shows an example of this DFT traversal (the fused code generation based on the pre-defined code generation rules is omitted in this Figure for readability and is introduced in the next First, DNNFusion recognizes that 𝑂𝑢𝑡 depends on 𝐼𝑅𝑆 3 + 𝐼𝑅𝑆 5 ; next, it recognizes that 𝐼𝑅𝑆 3 depends on reciprocal of 𝐼𝑅𝑆 2 , and so on, until reaching the input of 𝐴, 𝐵, 𝐶, 𝐷. It is worth noting DNNFusion can also find redundant computations in DFT with a common sub-tree identification and eliminate them during code generation. In our example, both Mul operators use 𝐼𝑅𝑆 1 , resulting in a common sub-tree in DFT, so the recognition in two red of Figure 4 is only taken once.

During this DFT traversal, DNNFusion employs the predefined code generation rules to generate the code for each pair of operators to be fused. For the example shown in Figure 4, DNNFusion first fuses Add with its left input branch Recip. Both Add and Recip belong to One-to-One mapping, and hence the fused operator is also One-to-One. DNNFusion keeps fusing Mul (One-to-One) with this newly fused operator, and the result is still One-to-One. Next, this newly generated operator is fused with GEMM (Many-to-One), generating a new Many-to-One operator. Similar steps are taken along the right input branch of Add until all operators are fused into a single new Many-to-One operator. DNNFusion relies on the DFT traversal introduced in the prior paragraph to figure out the input/output data dependence, and employs the operator mapping type to handle the index mapping relationship and generate proper nested loop structures.

To explain this further, here is an example with more complicated mapping types: GEMM (Many-to-Many) + Div (One-to-One) + Transpose (Shuffle). First, DNNFusion fuses Transpose and Div, a case of ("Shuffle + One-to-One") by After:

012341-35($+,-ℎ+/,("))

Before: 4.4.2 Other Fusion-related Optimizations. DNNFusion also includes several advanced optimizations enabled by our fusion analysis and fused code generation. They broadly can be characterized into two groups, intra-fusion-block optimizations that are performed on Extended Computational Graph (ECG) immediately before the code generation and inter-fusion-block optimizations on the generated fused code.

Intra-block Optimizations: Operators in Shuffle and Reorganize mapping types usually involve intensive data movement. We observed many of these time/memory consuming data operations can be eliminated. In particular, consider the case when the transformed data is used by only one subsequent operator because the data locality improvement brought this data transformation cannot be compensated by the overhead of intermediate results generation and storage.

Figure 5 shows such examples -particularly, in these, data transpose and data slicing operations bring more overheads than the benefit. Thus, in such cases, DNNFusion replaces them with operations that have a changed data index. These optimizations are performed after graph rewriting and result in an ECG that should have a more efficient implementation.

Inter-block Optimization: Different operators prefer different data formats. Without the proposed graph rewriting optimizations and operator fusion, normally such choices are made at the level of each individual operator -however, this can result in redundant or unnecessary transformations.

In contrast, DNNFusion considers the data format choice at a global level, thus avoiding redundant or unnecessary transformations. Currently, DNNFusion employs a heuristic approach to optimize the data format, which is as follows. For a specific fusion block, it identifies one dominant operator whose performance is impacted the most by the choice of the layout (e.g., CONV, GEMM, and Softmax are most likely to such operators). The optimal layout for this operation is then used for the entire fusion block. This heuristic approach works based on a key observation that most other non-dominant operators can employ any layout/format without their performance being significantly affected. A potential future work will be to consider more sophisticated cost models, including balancing the cost of reformatting the data with reductions in execution because of the optimized layout.

# Evaluation

DNNFusion is implemented on top of an existing end-to-end DNN execution framework called PatDNN [46] that supports both dense and sparse DNN execution. It has been shown in our previous work that PatDNN [46] performs slightly better than TVM, MNN, and TFLITE even without our proposed operator fusion. For readability, we also call this optimized framework DNNFusion. Our evaluation has four objectives: 1) demonstrate that the proposed fusion framework (together with graph rewriting) is effective by showing how DNN-Fusion outperforms other state-of-the-art frameworks, and no-fusion fixed-pattern fusion implementations on various DNN models; 2) validating DNNFusion's generality by showing its efficient execution on both CPU and GPU on a wide spectrum of DNNs (for 5 types of tasks, with varied sizes, and layer counts ranging from relatively shallow to extremely deep); 3) analyzing the impact of different compiler optimizations on both execution time and compilation time; and 4) demonstrating the effective portability of DNNFusion by evaluating it on three different mobile phones. More specifically, DNNFusion (also called DNNF for short) is compared against four popular state-of-the-art end-to-end DNN execution frameworks: MNN [29], TVM [12], TensorFlow-Lite (TFLite) [1], and Pytorch-Mobile (Pytorch) [48]. Because certain extremely deep neural networks are not supported by any of these existing frameworks (or just supported by their mobile CPU implementation), we also set a baseline by turning off DNNFusion's all fusion related optimizations (called OurB, i.e., our baseline version without fusion) and implement a version that optimizes OurB with fixed-pattern fusion (using operator fusion described in TVM [12]) (called OurB+), and compare DNNFusion against them.

## Evaluation Setup

Models and datasets. DNNFusion is evaluated on 15 mainstream DNN models. Table 5 characterizes them with a comparison of their targeted task, number of parameters, total number of layers, and number of floating point operations (FLOPS). Particularly, we have 1) two image classification 2D CNNs (EfficientNet-B0 [66] and VGG-16 [62]), 2) two object detection two-dimensional (2D) CNNs (MobileNetV1-SSD [42] and YOLO-V4 [7]), 3) two action recognition threedimensional (3D) CNNs (C3D [69] and S3D [73]), 4) one image segmentation 2D CNN (U-Net [59]) and two image segmentation R-CNNs (Mask R-CNN [26] and FasterRCNN [57]), and 5) six natural language processing (NLP) models (Tiny-BERT [30], DistilBERT [60], ALBERT [39], BERT BASE , Mo-bileBERT [65], and GPT-2 [55]).

Because the choice of datasets has a negligible impact on the final inference latency or relative execution speeds (and also because of space limitations), we report results from one dataset for each model. EfficientNet-B0 and VGG-16 are trained on ImageNet dataset [18]; MobileNetV1-SSD and YOLO-V4 are trained on MS COCO [41]; C3D and S3D are trained on UCF-101 [63]; U-Net, Faster R-CNN, and Mask R-CNN are trained on PASCAL VOC 2007 [23]; TinyBERT, DistilBERT, ALBERT, BERT base , MobileBERT, and GPT-2 are trained on BooksCorpus [19] and English Wikipedia [19]. Because the model accuracy is identical among all frameworks, and also because of space limitations, we only focus on execution times and do not report accuracy. Evaluation environment. The evaluations are carried out on a Samsung Galaxy S20 cell phone that has Snapdragon 865 processor [54], which comprises an octa-cores Kryo 585 CPU and Qualcomm Adreno 650 GPU yielding high performance with good power efficiency. For demonstrating portability, we further use a Samsung Galaxy S10 with a Snapdragon 855 [53] (Qualcomm Kryo 485 Octa-core CPU and a Qualcomm Adreno 640 GPU), and an Honor Magic 2 with a Kirin 980 [27] (ARM Octa-core CPU and a Mali-G76 GPU). All executions used 8 threads on mobile CPUs, and similarly all pipelines on mobile GPUs. 16-bit and 32-bit floating points are used for all GPU runs and CPU runs, respectively. All experiments were run 100 times but as the variance was very small, we only report averages.

## Overall Mobile Inference Evaluation

Our comparison includes both fusion rate 4 and execution latency. Fusion rate. Table 5 shows detailed layer counts (including computation-intensive (CIL), memory-intensive (MIL), and all layers), and intermediate result sizes for models before fusion and after fusion with different frameworks. Note that DNNFusion is the only end-to-end framework that can support all of the target models on both mobile CPU and mobile GPU. In Table 5, "-" implies that this framework does not support this model. Certain extremely deep neural networks (e.g., Faster R-CNN and Masker R-CNN) are not supported by any other frameworks on mobile devices because these frameworks either lack the support of multiple key operators and/or limited optimization supported in them lead to a large model execution footprint. For transformer-based models, only TFLite can support execution on mobile CPU (without GPU support).

Table 5 shows that compared with the other frameworks, DNNFusion results in better fusion rates, with 1.3× to 2.9×, 1.3× to 8.1×, 1.3× to 8.8×, and 1.3× to 2.8× over MNN, TVM, TFLite, and Pytorch, respectively. Particularly, compared with original models, DNNFusion yields more benefits for R-CNN and Transformer-based models (3.9× to 10.0× fusion rate) than 2D/3D CNNs (1.7× to 3.6× fusion rate). This is  To further validate DNNFusion's performance, Figure 6 compares it with a state-of-the-art computational graph substitution approach mentioned earlier, TASO [28]. We use TASO to optimize all eleven models (computational graphs) supported by TFLite among those listed in Table 6, including EfficientNet-B0 (ENT-B0), VGG-16 (VGG), MobileNetV1-SSD (MNT), YOLO-V4, U-Net, TinyBERT (TBERT), DistilBERT (DBERT), ALBERT (ABERT), BERT 𝐵𝑎𝑠𝑒 (BERT), MobileBERT (MBT), and GPT-2. Then, for our experiments, these models are executed under TFLite on mobile CPU (not GPU because TFLite lacks GPU support for many of these models). Compared with TASO, DNNFusion yields 1.4× to 2.6× speedup on mobile CPU. The graph rewriting in DNNFusion is designed to work in conjunction with operator fusion and identifies a set of operators and rules for that specific purpose, thus enabling more fusion opportunities. TASO does not emphasize the relationship between graph rewriting and fusion, resulting in less efficient execution as compared to DNNFusion.

## Understanding Fusion Optimizations

This section studies the effect of our key optimizations. Optimization breakdown. Figure 7 shows the impact of our proposed optimizations on latency with four models        5, the profiling database consists of around 22K profiling entries with each one including operators' information (e.g., operator types, shape, and their combinations) and the latency achieved.

## Portability

Figure 10 shows the execution latency on additional cell phones (Samsung Galaxy S10 and Honor Magic 2) to demonstrate effective portability. Only YOLO-V4 and GPT-2 are reported due to limited space. Other models show similar trends. In particular, DNNFusion shows a more stable performance on older generations of mobile devices. This is because our fusion significantly reduces the overall number of layers and intermediate result size, and older cell phones with more restricted resources are more sensitive to these.

Operator fusion in end-to-end mobile DNN frameworks.

Operator fusion is an important optimization in many stateof-the-art end-to-end mobile DNN execution frameworks that are based on computational graphs, such as MNN [29], TVM [12], TensorFlow-Lite [1], and Pytorch [48]. However, they all employ fixed-pattern fusion that is too restricted to cover diverse operators and layer connections in deep models like BERT  Operator fusion on other ML frameworks. There are certain recent frameworks that rely on polyhedral analysis to optimize DNN computations and support operator fusion. R-Stream•TF [51] shows a proof-of-concept adaptation of the R-Stream polyhedral compiler to TensorFlow. Tensor Comprehensions [70] is an end-to-end compilation framework built on a domain-specific polyhedral analysis. These frameworks do not support mobile execution (i.e. ARM architecture), and thus we cannot perform a direct comparison between DNNFusion and them. As we have stated earlier, DNNFusion maintains an operator view but builds a higher-level abstraction on them. In the future, we can combine DNNFusion's high-level abstraction to existing domain-specific polyhedral analysis. Similarly, another promising direction will be to integrate DNNFusion into other compilation-based DNN frameworks [25,45] or other popular general tensor/matrix/linear algebra computation frameworks, such as MLIR [40], Tiramisu [4], TACO [33,34], Halide [56], and LGen [38,64]. There also exist several other frameworks to optimize chine learning with operator fusion or fusion-based ideas. Closely related to DNNFusion-Rammer [43] relies on fixpattern operator fusion to further reduce kernel launch overhead of their optimized scheduling, Cortex [24] proposes a set of optimizations based on kernel fusion for dynamic recursive models, TensorFlow XLA [67] offers a more general fusion method than fix-pattern operator fusion by supporting reduce operations and element-wise operations, and TensorFlow Grapper [68] provides an arithmetic optimizer that performs rewrites to achieve both fusion and arithmetic expression simplification (e.g., 𝑎 × 𝑏 + 𝑎 × 𝑐 = 𝑎 × (𝑏 + 𝑐)).

Comparing with these frameworks, DNNFusion works by classifying the operators and their combinations into several mapping categories, thus resulting in a more aggressive fusion plan and more performance gains. Elgamal [22] and Boehm [8] presently optimize general machine learning algorithms (e.g., SVM and Kmeans) with operator fusion. These efforts have both different targets and techniques compared to DNNFusion. Polyhedral-based and other loop fusion methods. Polyhedral analysis [10,11,20,35,49,74] is a prominent approach that offers a general and rigorous foundation for loop transformation and optimization.

Many existing efforts [2,3,9] rely on a general polyhedral analysis to achieve optimized loop fusion. Pouchet et al. [50] have demonstrated that polyhedral analysis can decompose the loop optimization problem into sub-problems that have much lower complexity, enabling optimal selection. The problem arising because of a large number of operators in our target applications (models) is quite different, and thus there does not seem to be a direct application of Pouchet et al.'s approach in our context. There have also been other loop fusion efforts targeting general programs [17,31,32,44]. In contrast to these general efforts, DNNFusion is more domain-specific, leveraging the knowledge of DNN computations with a higher-level abstraction to explore more aggressive loop fusion opportunities.

# Conclusions and Future Work

This paper has presented a new loop fusion framework called DNNFusion. The key advantages of DNNFusion include: 1) a new high-level abstraction comprising mapping type of operators and their combinations and the Extended Computational Graph, and analyses on these abstractions, 2) a novel mathematical-property-based graph rewriting, and 3) an integrated fusion plan generation. DNNFusion is extensively evaluated on 15 diverse DNN models on multiple mobile devices, and evaluation results show that it outperforms four state-of-the-art DNN execution frameworks by up to 8.8× speedup, and for the first time allows many cutting-edge DNN models not supported by prior end-to-end frameworks to execute on mobile devices efficiently (even in real-time). In addition, DNNFusion improves both cache performance and device utilization, enabling execution on devices with more restricted resources. It also reduces performance tuning time during compilation.

Our future work will enhance DNNFusion by combining it with the latest model pruning advances [21,46]. Though model pruning is effective, with fusion the dense versions are outperforming these efforts by having fewer layers. Thus, there is an opportunity to combine the two set of approaches to achieve an even better performance.

National Science Foundation (NSF) under CCF-1629392, CCF-2007793, CCF-1919117, and OAC-2034850. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Thomas F. and Kate Miller Jeffress Memorial Trust, or NSF.

# Acknowledgments

The authors would like to thank the anonymous reviewers for their valuable and thorough comments. The authors are especially grateful to the shepherd Tatiana Shpeisman for her innumerable helpful suggestions and comments that help improve this paper substantially. This work is supported in part by Jeffress Trust Awards in Interdisciplinary Research, and

