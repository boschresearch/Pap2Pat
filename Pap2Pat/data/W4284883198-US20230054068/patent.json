{
    "id": "US20230054068",
    "authors": [
        "Haopeng Zheng",
        "Semih Yavuz",
        "Wojciech Kryscinski",
        "Kazuma Hashimoto",
        "Yingbo Zhou"
    ],
    "title": "SYSTEMS AND METHODS FOR ABSTRACTIVE DOCUMENT SUMMARIZATION WITH ENTITY COVERAGE CONTROL",
    "date": "2022-01-31 00:00:00",
    "abstract": "Embodiments described herein provide document summarization systems and methods that utilize fine-tuning of pre-trained abstractive summarization models to produce summaries that more faithfully track the content of the documents. Such abstractive summarization models may be pre-trained using a corpus consisting of pairs of articles and associated summaries. For each article-summary pair, a pseudo label or control code is generated and represents a faithfulness of the summary with respect to the article. The pre-trained model is then fine-tuned based on the article-summary pairs and the corresponding control codes. The resulting fine-tuned models then provide improved faithfulness in document summarization tasks.",
    "sections": [
        {
            "title": "DESCRIPTION",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "TECHNICAL FIELD",
                    "paragraphs": [
                        "The present disclosure relates generally to machine learning (ML) systems, and more specifically to intermediate pre-training for document summarization tasks."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "relate to machine learning systems for document summarization"
                    ],
                    "num_characters": 159,
                    "outline_medium": [
                        "relate to machine learning systems for document summarization"
                    ],
                    "outline_short": [
                        "relate to machine learning systems for document summarization"
                    ]
                },
                {
                    "title": "BACKGROUND",
                    "paragraphs": [
                        "Document summarization is a machine learning (ML) task that aims to generate a compact summary that preserves the most salient content of a document. Previous document summarization techniques have struggled to produce faithful summaries that only contain contents that can be derived from the document rather than hallucinated or fabricated information. Therefore, there is a need for pre-training techniques that improve faithfulness in document summarization.",
                        "In the figures and appendices, elements having the same designations have the same or similar functions."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "describe limitations of previous document summarization techniques"
                    ],
                    "num_characters": 568,
                    "outline_medium": [
                        "introduce document summarization and its challenges"
                    ],
                    "outline_short": [
                        "introduce document summarization and its challenges"
                    ]
                },
                {
                    "title": "DETAILED DESCRIPTION",
                    "paragraphs": [
                        "As used herein, the term \u201cor\u201d shall convey both disjunctive and conjunctive meanings. For example, the phrase \u201cA or B\u201d may be interpreted to include element A alone, element B alone, or and the combination of elements A and B.",
                        "As used herein, the term \u201cnetwork\u201d may comprise any hardware or software-based framework that includes any artificial intelligence network or system, neural network or system and/or any training or learning models implemented thereon or therewith.",
                        "As used herein, the term \u201cmodule\u201d may comprise hardware or software-based framework that performs one or more functions. In some embodiments, the module may be implemented on one or more neural networks.",
                        "Machine learning (ML) methods have been applied to document summarization tasks such as abstractive summarization, which extracts words and/or phrases from the document to formulate a summary of the document. Given a document, such methods aim to infer a brief summary or abstract that captures most of the meaning of the document. However, such methods may be prone to \u201challucinating\u201d facts that do not faithfully represent the contents of the document to be summarized. For example, addition of information that is not actually contained within the document itself may be abstracted as part of the abstractive summary. Such additional information may include entities that are not mentioned within the document itself, and thus mis-represents the content of the document.",
                        "For example, Table 1 shows an example of an unfaithful summary of a portion of an article that exhibits hallucination. The original Article discusses a teenage science competition streamed on the YouTube website. However, the Summary generated from the Article includes the website Gumtree, which does not appear in the original Article. This is an example of model hallucination, whereby information that is not actually contained within the Article nonetheless appears in the Summary. The systems and methods presented herein may greatly reduce the occurrence of such model hallucination.",
                        "In view of the need for more accurate document summarization systems, embodiments described herein provide document summarization systems and methods that utilize fine-tuning of pre-trained abstractive summarization models to produce summaries that more faithfully track the content of the documents. Such abstractive summarization models may be pre-trained using a corpus consisting of pairs of articles and associated summaries. For each article-summary pair, a pseudo label or control code is generated and represents a faithfulness of the summary with respect to the article. The pre-trained model is then fine-tuned based on the article-summary pairs and the corresponding control codes. The resulting fine-tuned models then provide improved faithfulness in document summarization tasks."
                    ],
                    "subsections": [
                        {
                            "title": "Systems for Abstractive Summarization",
                            "paragraphs": [
                                "FIG. 1 is a simplified structural diagram of a system 100 for abstractive summarization of a document, according to some embodiments. As shown in FIG. 1, the system may comprise a pre-processing module 110, an entity coverage precision module 120, a pseudo labeling module 130, and a summarization module 140.",
                                "The pre-processing module 110 may be configured to receive a training dataset. The training dataset may comprise a plurality of n articles d={d1, d2, . . . , dn} and a plurality of n summaries s={s1, s2, . . . , sn}. The plurality of summaries may be written by one or more human summarizers of the plurality of articles, by one or more ML summarization models, or any combination thereof. Each summary of the plurality of summaries may correspond to an article of the plurality of articles. In some cases, a single unique article may appear in the training dataset more than once and may correspond to multiple summaries. For instance, a single unique article may correspond to multiple summaries, each summary written by a different human summarizer or a different ML summarization model. The pre-processing module 110 may be configured to generate a plurality of n article-summary pairs D={(d1, s1), (d2, s2), . . . , (dn, sn)}. The pre-processing module 110 may generate the plurality of article-summary pairs by pairing each article of the plurality of articles with at least one associated summary of the plurality of summaries. The pre-processing module 110 may then pass the plurality of article-summary pairs to the entity coverage precision module 120.",
                                "In one embodiment, during the inference phase, a sequence to sequence model generates summary hypothesis hi for a given document di by p\u03b8(hi|di). Ideally, the generated summary hi shall be faithful, which means all the information in hi should be entailed by the source document di.",
                                "The entity coverage precision module 120 may be configured to compute, for an article-summary pair of the plurality of article-summary pairs, an entity coverage precision metric precen, which track the degree of entity-level hallucination to maintain faithfulness of the generated summary. The entity coverage precision metric may be based on a number of entity mentions in the article and/or the summary. The entities may comprise, for example people, places, or things in the article and/or the summary. As such, the entity mentions may comprise, for example, mentions of people, places, or things in the article and/or the summary. For instance, the entity coverage precision metric may be calculated by identifying the number (s) of entities that appear in the summary and the number (s\u2229d) of entities that appear in both the summary and the article. The entity coverage precision metric may then be calculated as the ratio of (s\u2229d) and (s), such that precen=(d\u2229s)/(s). The entity coverage precision module 120 may then pass the entity coverage precision metric to the pseudo labeling module 130.",
                                "The pseudo labeling module 130 may be configured to determine a pseudo label for an article summary pair of the plurality of article-summary pairs. The pseudo label may indicate a faithfulness level of the summary to the article. The pseudo label may be based on the entity coverage precision metric computed by the entity coverage precision module. For instance, the pseudo label may be based on an entity coverage rate or an entity coverage ratio, which is computed as the number of entities mentioned by both the summary and the article divided by the number of entities mentioned by the summary (such as the ratio precen=(d\u2229s)/(s) described herein) between the summary and the article. For each article-summary pair, the pseudo label may be generated by a binning procedure. That is, a plurality of entity coverage precision metrics (such as precen described herein) may be computed for each article-summary pair of the plurality of article-summary pairs. The resulting plurality of entity coverage precision metrics may then be binned, resulting in a plurality of binned pseudo labels. For each article-summary pair, an entity coverage precision metric may be determined and a binned pseudo label of the plurality of binned pseudo labels may be assigned to the article and the summary based on the entity coverage precision metric. For instance, an entity coverage precision metric precen(di, si) may be calculated for each article-summary pair in D={(d1, s1), (d2, s2), . . . , (dn, sn)} to generate a plurality of entity coverage precision metrics P={precen(d1, s1), precen(d2, s2), . . . , precen(dn, sn)}. The set P may then be binned into k discrete bins, each of which represents a range of entity coverage precision metrics. The boundaries of the bins may be established using a variety of techniques. For instance, the boundaries of the bins may be chosen such that each bin covers an equal range of entity coverage precision metrics. As an example, when k=2, the bins may be chosen to coverage the ranges [0, 0.5], (0.5, 1], when k=3, the bins may be chosen to coverage the ranges [0, 0.33], (0.33, 0.66], (0.66, 1], and so forth. Alternatively, the boundaries of the bins may be chosen such that each bin contains roughly the same number of article-summary pairs. Each bin may then be assigned a pseudo label from the set ={L1, L2, . . . , Lk}. Each article-summary pair of the plurality of article-summary pairs may then be assigned a pseudo label from the set  based on its associated entity coverage precision metric precen(di, si). The article may then be prepended with the determined pseudo label.",
                                "The summarization module 140 may be configured to receive the article-summary pair and the prepended pseudo label. The summarization module 140 may be configured to use a summarization model to generate an output summary O conditioned on both the article and the prepended pseudo label. The summarization module may be configured to update the summarization model based on a training objective that compares the output summary and the summary from the training sample of the article-summary pair, e.g., a cross-entropy loss between the output summary and the summary from the training pair. The summarization model may comprise an encoder-decoder model. The summarization model may comprise a sequence-to-sequence (seq2seq) model. The summarization model may be based at least in part on a Bidirectional and Auto-Regressive Transformer (BART) abstractive summarization model (disclosed in M. Lewis et al, BART: denoising sequence-to-sequence pretraining for natural language generation, arXiv: 1910.13461 (2019), which is herein incorporated by reference in its entirety for all purposes).",
                                "FIG. 2 shows a diagram illustrating aspects of faithfulness (entity coverage) control in an abstractive summarization system, according to one or more embodiments described herein. To allow the model to learn different entity-level faithfulness patterns, a faithfulness control code 215 may be adopted. Specifically, a control code Ci is generated for each training document 201 and reference summary 202 pair (di, si). The training document 201 is then input to the transformer encoder 210 and transformer decoder 212, which generates an output summary conditioned on both the source document di and its control code Ci, which is represented as by p\u03b8(hi|di, Ci).",
                                "The entity coverage precision precen is then computed for each document 201 and reference summary 202 in the pair (di, si) in the training dataset D. Then, the precision metric is quantized in to k discrete bins, each representing a range of entity faithfulness. These bin boundaries are selected to ensure that each bin contains roughly the same number of training examples to avoid data imbalance. Then each bin is represented by a special token control code Ci and the special tokens {C1, C2, . . . , Ck} to the input vocabulary of the summarization model.",
                                "During training, the pseudo label (control code) Ci is prepended to the input document 201 as control code. The model of transformer encoder 210 and decoder 212 is now conditioned on both the source document 201 and the control code 215 to learn different faithful level generation patterns from the control codes. During inference, the high faithfulness control code Ck is prepended to all documents in the test set and generate faithful summaries by p\u03b8(hi|di, Ck).",
                                "As shown in FIG. 2, a training sample of an article 201 and a summary 202 thereof are used to determine the pseudo label (control code) 215. The control code 215 may be generated in any manner described herein with respect to FIG. 1 (for instance, based on a computed entity coverage precision metric described herein with respect to FIG. 1). The control code 215 may be any pseudo label described herein with respect to FIG. 1. The control code 215, the article 201, and the summary 202 may be passed as input to a summarization model, e.g., a transformer encoder 210. The input may be generated by prepending the article with the determined pseudo label, as described herein with respect to FIG. 1. As shown in FIG. 2, the summarization model may be built based on a transformer comprising a transformer encoder 210 and a transformer decoder 212. However, any summarization model described herein with respect to FIG. 1 may take the place of the transformer encoder-transformer decoder pair depicted in FIG. 2. The transformer decoder 210 may then output an output summary. The output summary may be an output summary conditioned on both the article and the prepended pseudo label, as described herein with respect to FIG. 1. The transformer encoder-transformer decoder may then be updated based on a training objective that compares the output summary to the input summary, as described herein with respect to FIG. 1, e.g., the cross-entropy between summary 202 as the target and the output summary from transformer decoder 212.",
                                "The training sample of the article 201 and the summary 202 may belong to a training dataset. The training dataset described herein may comprise a plurality of articles and a plurality of summaries that are each associated with a domain-specific database. For instance, the plurality of articles and the plurality of summaries may be obtained from a database such as the Xsum, Pubmed, Samsum, or any other domain-specific database. Such domain-specific databases may utilize article summaries that are written by human experts, such as expert annotators or the authors of the articles themselves.",
                                "FIG. 3 is an example block diagram illustrating an intermediate pre-training pipeline for zero-shot summarization, according to one or more embodiments described herein. In FIG. 3, a controllable generalized intermediate pre-training framework for zero-shot summarization is provided. Various target datasets 301a-c from different domains may be incorporated into a Wikipedia corpus 305, which is used to generate pseudo document and summary pairs as intermediate training data. Then a single sequence to sequence model may be trained on the intermediate training data 307, using a similar framework shown in FIGS. 1-2. Zero-shot summarization may then be performed on the target datasets 301a-c.",
                                "For example, target-specific intermediate data may be generated from Wikipedia articles. Let T (n, m, a) denote a downstream target dataset of average document length n sentences, average summary length m sentences, and abstractiveness level a. Here abstraciveness level is defined as the upper bound extractive ROUGE1 performance of the target dataset 301a-c. For each available Wikipedia article in a Wikipedia dump 305, the first m sentences of the encyclopedia article may be used to generate a summary. The next n sentences may be used as the corresponding article to the generated summary. Given an abstractiveness level a, a training instance I(n, m, a) may be constructed from the encyclopedia article. This procedure may be repeated for different values of m, n, and a. Thus, a training set using l different values for m, n, and a may allow for the construction of a training set ={I(n1, m1, a1), I(n2, m2, a2), . . . , I(nl, ml, al)}. Each member I(ni, mi, ai) of the set  may be associated with a pseudo label Ei representing the target-specific generation pattern and also add all these special tokens E={E1, E2, . . . , El} to the input vocabulary of the model.",
                                "In the training phase, each corresponding target pseudo label Ei may be prepended to a corresponding training instance I(ni, mi, ai) to generate the training set. In this way, a summary is generated conditioned on both the source document 201 and the target control code, i.e., the target label Ei. Such training sets may generalize well across different domains of knowledge.",
                                "FIG. 4 is a simplified logic flow diagram illustrating a method 300 for abstractive summarization of a document, according to some embodiments. One or more of the processes of method 300 may be implemented, at least in part, in the form of executable code stored on non-transitory, tangible, machine-readable media that when run by one or more processors may cause the one or more processors to perform one or more of the processes. In some embodiments, method 300 corresponds to the operation of the pre-processing module 110, the entity coverage precision module 120, the pseudo labeling module 130, and/or the summarization module 140 described herein with respect to FIG. 1.",
                                "At operation 310, the method 300 may comprise receiving a training dataset comprising a plurality of articles and a plurality of summaries corresponding to the plurality of articles. The training dataset may comprise any training dataset described herein with respect to FIG. 1. The plurality of articles may comprise any plurality of articles described herein with respect to FIG. 1. The plurality of summaries may comprise any plurality of summaries described herein with respect to FIG. 1.",
                                "At operation 320, the method 300 may comprise generating a plurality of article-summary pairs by pairing each article with at least one associated summary. The plurality of article-summary pairs may be any plurality of article-summary pairs described herein with respect to FIG. 1. The plurality of article-summary pairs may be generated in any manner described herein with respect to FIG. 1.",
                                "At operation 330, the method 300 may comprise computing, for an article-summary pair, an entity coverage precision metric based on a number of entity mentions in a corresponding summary or a corresponding article. The entity coverage precision metric may be any entity coverage precision metric described herein with respect to FIG. 1. The entity coverage precision metric may be computed using any manner described herein with respect to FIG. 1.",
                                "At operation 340, the method 300 may comprise determining a pseudo label indicating a faithfulness level of the corresponding article and the corresponding summary based on the computed entity coverage precision metric. The pseudo label may comprise any pseudo label described herein with respect to FIG. 1. The pseudo label may be determined in any manner described herein with respect to FIG. 1.",
                                "At operation 350, the method 300 may comprise prepending the article with the determined pseudo label as a training input to a summarization model. The article may be prepended with the pseudo label in any manner described herein with respect to FIG. 1.",
                                "At operation 360, the method 300 may comprise generating, by the summarization model, an output summary conditioned on both the article and the prepended pseudo label. The summarization model may comprise any summarization model described herein with respect to FIG. 1. The output summary may be generated in any manner described herein with respect to FIG. 1.",
                                "At operation 370, the method may comprise updating the summarization model based on a training objective comparing the output summary and the corresponding summary. The summarization model may be updated in any manner described herein with respect to FIG. 1.",
                                "**Computer Systems**",
                                "FIG. 5 is a simplified diagram of a computing device 400 for abstractive summarization of a document, according to some embodiments. As shown in FIG. 4, computing device 400 includes a processor 410 coupled to memory 420. Operation of computing device 400 is controlled by processor 410. Although computing device 400 is shown with only one processor 410, it is understood that processor 410 may be representative of one or more central processing units, multi-core processors, microprocessors, microcontrollers, digital signal processors, field programmable gate arrays (FPGAs), application specific integrated circuits (ASICs), graphics processing units (GPUs) and/or the like in computing device 400. Computing device 400 may be implemented as a stand-alone subsystem, as a board added to a computing device, and/or as a virtual machine.",
                                "Memory 420 may be used to store software executed by computing device 400 and/or one or more data structures used during operation of computing device 400. Memory 420 may include one or more types of machine readable media. Some common forms of machine readable media may include floppy disk, flexible disk, hard disk, magnetic tape, any other magnetic medium, CD-ROM, any other optical medium, punch cards, paper tape, any other physical medium with patterns of holes, RAM, PROM, EPROM, FLASH-EPROM, any other memory chip or cartridge, and/or any other medium from which a processor or computer is adapted to read.",
                                "Processor 410 and/or memory 420 may be arranged in any suitable physical arrangement. In some embodiments, processor 410 and/or memory 420 may be implemented on a same board, in a same package (e.g., system-in-package), on a same chip (e.g., system-on-chip), and/or the like. In some embodiments, processor 410 and/or memory 420 may include distributed, virtualized, and/or containerized computing resources. Consistent with such embodiments, processor 410 and/or memory 420 may be located in one or more data centers and/or cloud computing facilities.",
                                "In some examples, memory 420 may include non-transitory, tangible, machine readable media that includes executable code that when run by one or more processors (e.g., processor 410) may cause the one or more processors to perform the methods described in further detail herein (such as method 300 described herein with respect to FIG. 3). For example, as shown, memory 420 includes instructions for pre-processing module 110, that may be used to implement and/or emulate the systems and models, and/or to implement any of the methods described further herein. In some examples, the pre-processing module 110, may receive an input 440, e.g., such as a training dataset comprising a plurality of articles and a plurality of summaries corresponding to the plurality of articles, via a data interface 415. The data interface 415 may be any of a user interface that receives a training dataset from a user, or a communication interface that may receive or retrieve a training dataset from a database. The pre-processing module 110 may then generate a plurality of article-summary pairs by pairing each article with at least one associated summary.",
                                "The memory 420 may further include instructions for entity coverage precision module 120, that may be used to implement and/or emulate the systems and models, and/or to implement any of the method described herein. In some examples, the entity coverage precision module 120 may compute, for an article-summary pair, an entity coverage precision metric based on a number of entity mentions in a corresponding summary or a corresponding article.",
                                "The memory 420 may further include instructions for pseudo labeling module 130, that may be used to implement and/or emulate the systems and models, and/or to implement any of the method described herein. In some examples, the pseudo labeling module 130 may determine a pseudo label indicating a faithfulness level of the corresponding article and the corresponding summary based on the computed entity coverage precision metric. The pseudo labeling module may prepend the article with the determined pseudo label as a training input to a summarization model.",
                                "The memory 420 may further include instructions for summarization module 140, that may be used to implement and/or emulate the systems and models, and/or to implement any of the method described herein. In some examples, the summarization module 140 may generate, by the summarization model, an output summary conditioned on both the article and the prepended pseudo label.",
                                "The memory may further include instructions to update the summarization model based on a training objective comparing the output summary and the corresponding summary.",
                                "Some examples of computing devices, such as computing device 400 may include non-transitory, tangible, machine readable media that include executable code that when run by one or more processors (e.g., processor 410) may cause the one or more processors to perform the processes of method 300. Some common forms of machine readable media that may include the processes of method 300 are, for example, floppy disk, flexible disk, hard disk, magnetic tape, any other magnetic medium, CD-ROM, any other optical medium, punch cards, paper tape, any other physical medium with patterns of holes, RAM, PROM, EPROM, FLASH-EPROM, any other memory chip or cartridge, and/or any other medium from which a processor or computer is adapted to read."
                            ],
                            "subsections": [],
                            "outline_long": [
                                "introduce system for abstractive summarization",
                                "describe pre-processing module",
                                "describe entity coverage precision module",
                                "describe pseudo labeling module",
                                "describe summarization module",
                                "describe sequence-to-sequence model",
                                "describe BART abstractive summarization model",
                                "describe faithfulness control code",
                                "describe entity coverage precision metric",
                                "describe binning procedure for pseudo labels",
                                "describe training dataset",
                                "describe article-summary pairs",
                                "describe entity mentions",
                                "describe precision metric calculation",
                                "describe pseudo label generation",
                                "describe control code generation",
                                "describe summarization model training",
                                "describe cross-entropy loss function",
                                "describe transformer encoder",
                                "describe transformer decoder",
                                "describe output summary generation",
                                "describe training objective",
                                "describe domain-specific databases",
                                "describe Wikipedia corpus",
                                "describe intermediate pre-training pipeline",
                                "describe target datasets",
                                "describe abstractiveness level",
                                "describe training instance construction",
                                "describe pseudo label generation",
                                "describe training set construction",
                                "describe summarization model training",
                                "describe zero-shot summarization",
                                "describe target-specific intermediate data generation",
                                "describe Wikipedia article processing",
                                "describe summary generation",
                                "describe article generation",
                                "describe training instance construction",
                                "describe pseudo label generation",
                                "describe training set construction",
                                "describe summarization model training",
                                "describe zero-shot summarization",
                                "describe generalizability across domains"
                            ],
                            "num_characters": 21506,
                            "outline_medium": [
                                "introduce system for abstractive summarization",
                                "describe pre-processing module",
                                "generate article-summary pairs",
                                "compute entity coverage precision metric",
                                "determine pseudo label for faithfulness",
                                "prepend pseudo label to article",
                                "generate output summary conditioned on article and pseudo label",
                                "update summarization model based on training objective",
                                "describe entity coverage precision module",
                                "compute entity coverage precision metric",
                                "describe pseudo labeling module",
                                "determine pseudo label for faithfulness",
                                "describe summarization module",
                                "generate output summary conditioned on article and pseudo label",
                                "describe faithfulness control code",
                                "generate control code for each training document",
                                "compute entity coverage precision metric",
                                "quantize precision metric into discrete bins",
                                "represent each bin with special token control code",
                                "prepend control code to input document",
                                "generate faithful summaries during inference"
                            ],
                            "outline_short": [
                                "introduce system for abstractive summarization",
                                "describe pre-processing module",
                                "describe entity coverage precision module",
                                "describe pseudo labeling module",
                                "describe summarization module",
                                "describe sequence-to-sequence model",
                                "describe faithfulness control code",
                                "describe training process",
                                "describe inference process",
                                "describe updating summarization model"
                            ]
                        }
                    ],
                    "outline_long": [
                        "define terms used in the disclosure",
                        "describe machine learning methods for document summarization",
                        "introduce entity coverage precision metric for faithfulness",
                        "describe pseudo labeling module for generating control codes",
                        "describe summarization module for generating output summaries"
                    ],
                    "num_characters": 2841,
                    "outline_medium": [
                        "define abstractive summarization and its limitations",
                        "introduce entity coverage precision metric for faithfulness"
                    ],
                    "outline_short": [
                        "describe abstractive summarization model with entity coverage precision metric"
                    ]
                },
                {
                    "title": "EXAMPLES",
                    "paragraphs": [],
                    "subsections": [
                        {
                            "title": "Example 1: Experimental Methods and Results",
                            "paragraphs": [
                                "Experiments implementing the systems and methods described herein were performed using a variety of domain-specific databases and encyclopedias. For domain-specific experiments, summarization datasets from the news, scientific paper, and dialog domains were utilized. The news dataset comprised the Xsum dataset, which contained 226,711 British Broadcasting Corporation (BBC) articles paired with their one-sentence summaries. All summaries were written by the author journalists writing the articles. The scientific paper dataset comprised the Pubmed dataset, which contained 93, 204 medical scientific papers from PubMed OpenAccess repositories. The introduction section of each paper was used as source article and the abstract section as the corresponding summary. The dialog dataset comprised the Samsum dataset, which contained 16,369 messenger-like conversations between two or more interlocutors pairs with summaries written by language experts.",
                                "Results from the systems and methods described herein were compared with the following methods: original BART-large (described in M. Lewis et al, BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, in Proc. 58th Ann. Meeting of the Ass'n for Comput'l Linguistics, 7871-7880 (2020), which is herein incorporated by reference in its entirety for all purposes), BART outputs with post-processing correction (as disclosed in S. Chen et al, Improving faithfulness in abstractive summarization with contrast candidate generation and selection, arXiv: 2104.09061 (2021), which is herein incorporated by reference in its entirety for all purposes), BART with entity-based data filtering (disclosed in F. Nan et al, Entity-level factual consistency of abstractive text summarization, in Proc. 16th Conf. of the Euro. Chapter of the Ass'n for Comput'l Linguistics, 2727-2733 (2021), which is herein incorporated by reference in its entirety for all purposes), and zero-shot Wikipedia intermediate fine-tuning WikiTransfer (disclosed in A. Fabbri et al, Improving zero and few-shot abstractive summarization with intermediate fine-tuning and data augmentation, in Proc. 2021 Conf. of the N. Amer. Chapter of the Ass'n for Comput'l Linguistics, 704-717 (2021), which is herein incorporated by reference in its entirety for all purposes.",
                                "The generated summaries were compared based on quality and faithfulness. For summary quality, the Rouge (disclosed in C. Y. Lin, Rouge: a package for auomatic evaluation of summaries, in Text summarization branches out, 74-81 (2004), which is herein incorporated by reference in its entirety for all purposes) and BERTSCORE (disclosed in T. Zhang et al, Bertscore: evaluating text generation with bert, arXiv: 1904.09675 (2019), which is herein incorporated by reference in its entirety for all purposes) metrics were used to measure the fluency and salience of output summary. For summary faithfulness, the Entity Coverage Precision (disclosed in F. Nan et al, Entity-level factual consistency of abstractive text summarization, in Proc. 16th Conf. of the Euro. Chapter of the Ass'n for Comput'l Linguistics, 2727-2733 (2021), which is herein incorporated by reference in its entirety for all purposes) and FEQA (disclosed in E. Durmus et al, FEQA: a question answering evaluation framework for faithfulness assessment in abstractive summarization, in Proc. 58th Ann. Meeting of the Ass'n for Comput'l Linguistics, 5055-5070 (2020), which is herein incorporated by reference in its entirety for all purposes) metrics were used. FEQA is an automatic question answering (QA) based metric for faithfulness by generating questions from summary and extract answers from the corresponding document by QA models. Expert annotators were also asked to perform human evaluation in both summary faithfulness and quality.",
                                "Huggingface libraries (disclosed in T. Wolf et al, Transformers: state-of-the-art natural language processing, in Proc. 2020 Conf. on Empirical Methods in Natural Language Processing, 38-45 (2020), which is herein incorporated by reference in its entirety for all purposes) were used for all experiment implementations. The backbone abstractive summarization model was BART-large, a pre-trained denoising autoencoder language model with 336 million parameters based on the sequence-to-sequence transformer (disclosed in A. Vaswani et al, Attention is all you need, in Advances in neural info processing systems, 5998-6008 (2017), which is herein incorporated by reference in its entirety for all purposes). For fair comparison, BART-large was fine-tuned on each dataset on 8 Tesla A100 GPU pods with same learning rate 5e-5 with weight decay using the Adam optimizer (disclosed in D. P. Kingma and J. Ba, Adam: a method for stochastic optimization, arXiv: 1412.6980 (2014), which is herein incorporated by reference in its entirety for all purposes). For entity recognition, a neural Named Entity Recognition (NER) system from the Stanza NLP toolkit (disclosed in P. Qi et al, Stanza: a python natural language processing toolkit for many human languages, in Proc. 58th Ann. Meeting of the Ass'n for Comput'l Linguistics, 101-180 (2020), which is herein incorporated by reference in its entirety for all purposes) was used and trained on the OntoNotes corpus (disclosed in P. Weischedel et al, OntoNotes release 4.0, LDC2011T03 (2011), which is herein incorporated by reference in its entirety for all purposes) except for the Pubmed dataset. Since Pubmed is a medical scientific article collection, biomedical, scientific and clinical text Named Entity Recognition toolkit scispaCy (disclosed in M. Neumann et al, SciscpaCy: fast and robust models for biomedical natural language processing, in Proc. 18th BioNLP Workshop and Shared Task, 319-327 (2019), which is herein incorporated by reference in its entirety for all purposes) was used instead.",
                                "Table 2 shows an example of an article and a summary generated using the systems and methods disclosed herein.",
                                "Table 3 shows the performance of the systems and methods described herein on three downstream datasets in different domains. Compared to the output summaries of BART without entity control, the systems and methods described herein increased the entity coverage precision (second column) of generated summaries with roughly the same summary quality (Rouge score and BertScore). The Rouge scores and BertScore dropped a little bit compared to BART on Xsum dataset, but increased on Pubmed and Samsum. This may be due to the low faithfulness level of the reference summaries in the Xsum dataset.",
                                "The systems and methods described herein were also compared to state-of-the-art baseline methods in increasing entity level faithfulness on the Xsum dataset, as shown in Table 4. There was a trade-off between entity coverage precision and the quality of the generated summary. When the model learned to copy more from the original document, the entity coverage precision tended to increase, but the quality of the output summary dropped at the same time. Compared to F. Nan et al, 2021, where only faithful training examples are kept, the systems and methods described herein didn't need to sacrifice any training data and could maintain the original distribution of the training set. The Question Answering (QA) based metric had a similar trend to the entity level faithfulness metric entity coverage precision, which verifies the effectiveness of increasing entity faithfulness in summary generation.",
                                "The mechanism by which the controllable Wikipedia intermediate pre-training systems and methods described herein help zero-shot summarization was also studied. Table 5 shows the zero-shot performance results of the systems and methods presented herein model on the Xsum and Pubmed datasets. Without any fine-tuning, BART tended to directly copy from the original source document so it achieved a very high entity coverage precision (92:61), but a rather low summary quality since the model was not trained on the dataset. In contrast, with the intermediate pre-training described herein, BART learned the characteristic of the downstream dataset and achieved a large improvement in Rouge score. Compared to the baseline model Wikitransfer, the systems and methods presented herein achieved improvements in both the entity coverage precision and summary quality. The systems and method described herein were also generalized across datasets, allowing for a single model for different downstream tasks instead of training separate models like in Wikitransfer.",
                                "Table 6 shows the human evaluation results on 50 randomly sampled subset of articles from the Xsum dataset following the setting of prior works (S. Chen et al, 2021). Four expert annotators assiged each summary output into three faithfulness categories (faithful summary (FF), intrinsic hallucination (IN), extrinsic hallucination (EX)), and three summary quality categories (low, medium, and high). Following the approach Chen et al., 2021, additional annotations from two other experts were used to calculate the inter-annotator agreement.",
                                "To verify if there was a need to control the number of entities during summary generation, the distribution of number of entities in the generated summaries by the systems and method described herein and by BART-large are shown in FIG. 6. Panel (a) shows the distribution for the systems and methods described herein, while panel (b) shows the distribution for BART-large. The two distributions were very similar and had almost the same mean number of entities. As a result, the systems and methods described herein likely didn't under-generate nor over-generate entities from the source document. Thus, there is likely no need to separately control the entity compression rate.",
                                "The method by which the control codes helped to improve the model performance was investigated. Pseudo faithfulness labels (low, medium, high) were generated and prepended for each training examples during training phase and generate with high control code during inference. In this way, the model was implicitly taught to learn the generation style from faithful examples. As shown in Table 7, the systems and methods described herein still generated reasonable summaries even inferred with low and medium control codes. There was also a trade-off between entity coverage precision and the quality of the generated summary during inference, such that summaries inferred with low control codes had even higher ROUGE scores. This may be due to the unfaithful reference summaries of the XSUM dataset.",
                                "Table 7 shows qualitative examples where the systems and methods described herein were trained on the Xsum dataset. Example 1 shows how entity control methods get rid of hallucination terms from BART output. Example 2 shows the outputs of the systems and methods described herein with different control codes during inference. Example 3 shows the zero-shot setting of BART and intermediate pre-training models described herein. While BART simply copied some random sentences in the zero shot setting, the systems and methods described herein model generated high quality summarizes instead.",
                                "This description and the accompanying drawings that illustrate inventive aspects, embodiments, implementations, or applications should not be taken as limiting. Various mechanical, compositional, structural, electrical, and operational changes may be made without departing from the spirit and scope of this description and the claims. In some instances, well-known circuits, structures, or techniques have not been shown or described in detail in order not to obscure the embodiments of this disclosure. Like numbers in two or more figures represent the same or similar elements.",
                                "In this description, specific details are set forth describing some embodiments consistent with the present disclosure. Numerous specific details are set forth in order to provide a thorough understanding of the embodiments. It will be apparent, however, to one skilled in the art that some embodiments may be practiced without some or all of these specific details. The specific embodiments disclosed herein are meant to be illustrative but not limiting. One skilled in the art may realize other elements that, although not specifically described here, are within the scope and the spirit of this disclosure. In addition, to avoid unnecessary repetition, one or more features shown and described in association with one embodiment may be incorporated into other embodiments unless specifically described otherwise or if the one or more features would make an embodiment non-functional.",
                                "Although illustrative embodiments have been shown and described, a wide range of modification, change and substitution is contemplated in the foregoing disclosure and in some instances, some features of the embodiments may be employed without a corresponding use of other features. One of ordinary skill in the art would recognize many variations, alternatives, and modifications. Thus, the scope of the invention should be limited only by the following claims, and it is appropriate that the claims be construed broadly and in a manner consistent with the scope of the embodiments disclosed herein."
                            ],
                            "subsections": [],
                            "outline_long": [
                                "introduce experimental setup",
                                "describe news dataset",
                                "describe scientific paper dataset",
                                "describe dialog dataset",
                                "compare with baseline methods",
                                "describe evaluation metrics",
                                "introduce Rouge metric",
                                "introduce BERTSCORE metric",
                                "introduce Entity Coverage Precision metric",
                                "introduce FEQA metric",
                                "describe human evaluation",
                                "introduce Huggingface libraries",
                                "describe BART-large model",
                                "describe fine-tuning process",
                                "introduce entity recognition",
                                "describe Stanza NLP toolkit",
                                "describe OntoNotes corpus",
                                "describe scispaCy toolkit",
                                "show example article and summary",
                                "show performance on downstream datasets",
                                "compare with state-of-the-art methods",
                                "study controllable Wikipedia intermediate pre-training",
                                "show zero-shot performance results",
                                "show human evaluation results",
                                "analyze entity distribution",
                                "investigate control codes"
                            ],
                            "num_characters": 13260,
                            "outline_medium": [
                                "introduce experimental setup",
                                "describe datasets used",
                                "explain evaluation metrics",
                                "compare with baseline methods",
                                "describe implementation details",
                                "present results on summary quality",
                                "present results on summary faithfulness",
                                "show example of generated summary",
                                "compare with state-of-the-art methods",
                                "study effect of controllable Wikipedia intermediate pre-training",
                                "present human evaluation results",
                                "analyze distribution of entities in generated summaries",
                                "investigate effect of control codes on model performance"
                            ],
                            "outline_short": [
                                "describe experimental setup",
                                "introduce datasets and evaluation metrics",
                                "present results of systems and methods",
                                "compare with baseline methods",
                                "analyze entity control and faithfulness",
                                "discuss human evaluation and qualitative examples"
                            ]
                        }
                    ],
                    "outline_long": [],
                    "num_characters": 0,
                    "outline_medium": [],
                    "outline_short": []
                }
            ],
            "outline_long": [],
            "num_characters": 0,
            "outline_medium": [],
            "outline_short": []
        }
    ],
    "claims": [
        "1. A method for abstractive summarization of a document, the method comprising:\nreceiving, via a data interface, a training dataset comprising a plurality of articles and a plurality of summaries corresponding to the plurality of articles;\ngenerating a plurality of article-summary pairs by pairing each article with at least one associated summary;\ncomputing, for an article-summary pair, an entity coverage precision metric based on a number of entity mentions in a corresponding summary or a corresponding article;\ndetermining a pseudo label indicating a faithfulness level of the corresponding article and the corresponding summary based on the computed entity coverage precision metric;\nprepending the article with the determined pseudo label as a training input to a summarization model;\ngenerating, by the summarization model, an output summary conditioned on both the article and the prepended pseudo label; and\nupdating the summarization model based on a training objective comparing the output summary and the corresponding summary.",
        "2. The method of claim 1, wherein the summarization model is based at least in part on a Bidirectional and Auto-Regressive Transformer (BART) summarization model.",
        "3. The method of claim 1, wherein the plurality of articles comprise a first article and a second article, each associated with a domain-specific database.",
        "4. The method of claim 1, wherein the plurality of articles comprises an encyclopedia article.",
        "5. The method of claim 4, further comprising:\ngenerating the output summary conditioned on the encyclopedia article and the prepended pseudo label.",
        "6. The method of claim 1, wherein the entity coverage precision metric between the summary and the article is computed as a first number of entities mentioned by both the summary and the article divided by a second number of entities mentioned by the summary.",
        "7. The method of claim 1, wherein the determining the pseudo label indicating the faithfulness level of the corresponding article and the corresponding summary based on the at least one computed entity coverage precision metric for the training dataset comprises:\ncomputing a plurality of entity coverage precision metrics corresponding to the plurality of article-summary pairs;\nbinning the plurality of entity coverage precision metrics to determine a plurality of binned pseudo labels;\ncomputing an entity coverage precision metric for the corresponding article and the corresponding summary; and\nassigning a binned pseudo label of the plurality of binned pseudo labels to the corresponding article and the corresponding summary based on the entity coverage precision metric for the corresponding article and the corresponding summary.",
        "8. A system for abstractive summarization of a document, the system comprising:\na non-transitory memory; and\none or more processor coupled to the non-transitory memory and configured to read instructions from the non-transitory memory to cause the system to perform operations comprising:\nreceiving, via a data interface, a training dataset comprising a plurality of articles and a plurality of summaries corresponding to the plurality of articles;\ngenerating a plurality of article-summary pairs by pairing each article with at least one associated summary;\ncomputing, for an article-summary pair, an entity coverage precision metric based on a number of entity mentions in a corresponding summary or a corresponding article;\ndetermining a pseudo label indicating a faithfulness level of the corresponding article and the corresponding summary based on the computed entity coverage precision metric;\nprepending the article with the determined pseudo label as a training input to a summarization model;\ngenerating, by the summarization model, an output summary conditioned on both the article and the prepended pseudo label; and\nupdating the summarization model based on a training objective comparing the output summary and the corresponding summary.",
        "9. The system of claim 8, the plurality of articles comprise a first article and a second article, each associated with a domain-specific database.",
        "10. The system of claim 8, wherein the training dataset comprises a plurality of articles and a plurality of summaries each associated with a domain-specific database.",
        "11. The system of claim 8, wherein the training dataset comprises a plurality of articles and a plurality of summaries each associated with an encyclopedia article.",
        "12. The system of claim 11, wherein the operations further comprise generating the output summary conditioned on the encyclopedia article and the prepended pseudo label.",
        "13. The system of claim 8, wherein the entity coverage precision metric between the summary and the article is computed as a first number of entities mentioned by both the summary and the article divided by a second number of entities mentioned by the summary.",
        "14. The system of claim 8, wherein the determining the pseudo label indicating the faithfulness level of the corresponding article and the corresponding summary based on the at least one computed entity coverage precision metric for the training dataset comprises:\ncomputing a plurality of entity coverage precision metrics corresponding to the plurality of article-summary pairs;\nbinning the plurality of entity coverage precision metrics to determine a plurality of binned pseudo labels;\ncomputing an entity coverage precision metric for the corresponding article and the corresponding summary; and\nassigning a binned pseudo label of the plurality of binned pseudo labels to the corresponding article and the corresponding summary based on the entity coverage precision metric for the corresponding article and the corresponding summary.",
        "15. A non-transitory, machine-readable medium having stored thereon machine-readable instructions executable to cause a system to perform operations comprising:\nreceiving, via a data interface, a training dataset comprising a plurality of articles and a plurality of summaries associated with the plurality of articles;\ngenerating a plurality of article-summary pairs by pairing each article with at least one associated summary;\ncomputing, for an article-summary pair, an entity coverage precision metric based on a number of entity mentions in a corresponding summary or a corresponding article;\ndetermining a pseudo label indicating a faithfulness level of the corresponding article and the corresponding summary based on the computed entity coverage precision metric;\nprepending the article with the determined pseudo label as a training input to a summarization model;\ngenerating, by the summarization model, an output summary conditioned on both the article and the prepended pseudo label; and\nupdating the summarization model based on a training objective comparing the output summary and the corresponding summary.",
        "16. The non-transitory, machine-readable medium of claim 15, wherein the training dataset comprises a plurality of articles and a plurality of summaries each associated with a domain-specific database.",
        "17. The non-transitory, machine-readable medium of claim 15, wherein the training dataset comprises a plurality of articles and a plurality of summaries each associated with an encyclopedia article.",
        "18. The non-transitory, machine-readable medium of claim 18, wherein the operations further comprise generating the plurality of summaries associated with the encyclopedia article.",
        "19. The non-transitory, machine-readable medium of claim 15, wherein the entity coverage precision metric between the summary and the article is computed as a first number of entities mentioned by both the summary and the article divided by a second number of entities mentioned by the summary.",
        "20. The non-transitory, machine-readable medium of claim 15, wherein the determining the pseudo label indicating the faithfulness level of the corresponding article and the corresponding summary based on the at least one computed entity coverage precision metric for the training dataset comprises:\ncomputing a plurality of entity coverage precision metrics corresponding to the plurality of article-summary pairs;\nbinning the plurality of entity coverage precision metrics to determine a plurality of binned pseudo labels;\ncomputing an entity coverage precision metric for the corresponding article and the corresponding summary; and\nassigning a binned pseudo label of the plurality of binned pseudo labels to the corresponding article and the corresponding summary based on the entity coverage precision metric for the corresponding article and the corresponding summary."
    ]
}