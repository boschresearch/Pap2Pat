# I. INTRODUCTION

Automatic speech recognition (ASR) system converts speech to text and it forms the back-end in many state-ofthe-art virtual voice assistants like Apple's Siri, Amazon's Alexa, Samsung's Bixby, etc. These voice assistants are trained to recognize the uniform speech of users with no speech disorders. The performance of ASR systems degrades in presence of incomplete, distorted, or broken speech. This limits technology accessibility to users with speech disorders. The three most common speech, language disorders are aphasia, apraxia, and dysarthria. Aphasia is a disturbance of the comprehension and formulation of language caused by dysfunction in specific brain regions. The major causes are a stroke or head trauma [1], [2]. Apraxia is a speech disorder caused due to the impairment of motor planning of speech [3]. Dysarthria is also a speech disorder caused due to neurological damage to the motor component of the motor-speech system and it is closely related to Apraxia [4]. People recovering from these speech disorders produce distorted and incomplete speech. The work described by authors in [5], [6] demonstrate that electrophysiological monitoring of neural signals like electroencephalography (EEG) and electrocorticography (ECoG) carry important information about speech articulation and speech perception. They demonstrated the results using neural signals recorded from subjects with no speech disorders. In [7] authors demonstrated that EEG features can be used to enhance the performance of isolated speech recognition systems trained to decode speech of users with no speech disorders. In their work, they demonstrated results on an English vocabulary consisting of four words and five vowels. EEG is a noninvasive way of measuring the electrical activity of the human brain. The EEG sensors are placed on the scalp of the subject to obtain EEG recordings. The EEG signals offer a very high temporal resolution. The non-invasive nature of EEG signals makes it safe and easy to deploy eventhough EEG signals offer poor spatial resolution and signalto-noise ratio compared to invasive ECoG neural activity recording techniques. The high temporal resolution property of EEG signals also allows capturing the human speechrelated neural activities as normal human speech occurs at a high rate of 150 words per minute. In [8] authors explored speech recognition using aphasia speech and reported a very high word error rate (WER) during test time. For a reduced vocabulary, they reported a WER as high as 97.5 %. In [9] authors demonstrated aphasia speech recognition by training their acoustic models on a large scale aphasia speech dataset named AphasiaBank but they reported a high phoneme error rate (PER) in the range of 75% to 89% for severe cases of aphasia. A high PER indicates an even higher WER. In a very recent work described in [10] authors explored the possibility of using ASR systems as a feedback tool while providing speech therapy to aphasia patients. Their results demonstrated an increase in the effectiveness of the speech therapy when coupled with ASR technology. References [11], [12], [13] investigated speech recognition for apraxia and dysarthria speech and reported low accuracy on a wordlevel vocabulary. In [14] authors carried out an EEG study to analyze the EEG delta wavebands to understand the brain damage on patients recovering from aphasia. In related studies described in references [15], [16] authors investigated EEG activity in the left-hemisphere of the brain of subjects recovering from aphasia and an EEG sleep study to understand the brain activity of the aphasia patients. These studies demonstrated that EEG signals carried useful information about brain function recovery in aphasia patients. In this paper, we propose an algorithm to train a deep learning-based speech recognizer using acoustic features along with acoustic representations derived from EEG features to significantly improve the test time decoding performance of aphasia + apraxia + dysarthria isolated speech recognizer. We were able to achieve a performance improvement of more than 50% during test time for the task of isolated speech recognition arXiv:2103.00383v2 [cs.SD] 18 Jul 2021 and a slight improvement in performance for the more challenging task of continuous speech recognition using our proposed algorithm. The results presented in this paper demonstrate how non-invasive neural signals can be utilized to improve the performance of speech recognizers used to decode aphasia, apraxia, and dysarthria speech. Designing a speech recognizer that can decode aphasia, apraxia, and dysarthria speech with high accuracy has the potentiality to lead to a design of a speech prosthetic and a better speech therapy tool for stroke survivors.

Our main contributions and major highlights of our proposed algorithm are listed below:

• We developed a deep learning-based algorithm to improve the performance of speech recognition for aphasia, apraxia, and dysarthria speech by utilizing EEG features.

• We collected large-scale aphasia, apraxia and dysarthria Speech-EEG data set that will be released to the public to help further advance this research. • Our proposed algorithm can be used with any type of speech recognition model, for example in this work we demonstrate the application of the algorithm on isolated as well as continuous speech recognition models.

## II. PROPOSED DEEP LEARNING ALGORITHM TO IMPROVE SPEECH RECOGNITION

Figure 1 describes the architecture of our proposed deep learning training strategy to improve the ASR performance of aphasia, apraxia, and dysarthria speech by utilizing EEG features. As seen from the figure, we make use of an EEG to acoustic feature mapping, regression model to generate additional features that are provided to the ASR model to improve its training. We first train the regression model described on the right-hand side of the figure to predict acoustic features or Mel frequency cepstral coefficients (MFCC) [17] of dimension 13 from EEG features. The regression model consists of a single layer of gated recurrent unit (GRU) [18] with 128 hidden units connected to a time distributed dense layer consisting of 13 hidden units with a linear activation function. The regression model was trained for 70 epochs with mean square error (MSE) as the loss function and with adam [19]   Our isolated speech recognition model consists of a single layer of GRU with 512 hidden units connected to a dropout regularization [20] with a drop-out rate of 0.2. The dropout regularization is followed by a dense layer consisting of 57 hidden units and a linear activation function. The dense layer contained 57 hidden units since our vocabulary contained 57 unique English sentences. The last time-step output of the GRU layer is passed to dropout regularization and dense layer. Finally, the dense layer output or logits are passed through a softmax activation function to obtain the label prediction probabilities. Each label token corresponds to a complete English sentence text. The labels were one-hot vector encoded and the model was trained for 10 epochs with batch size set to 50. We used early stopping during training to prevent over-fitting. We used categorical cross-entropy as the loss function and adam was used as the optimizer. The model architecture is described in Figure 2. Our continuous speech recognition model consists of a GRU layer with 512 hidden units acting as an encoder and the decoder consists of a combination of a dense layer with linear activation function and softmax activation function. The output of the encoder is passed to the decoder at every time-step. The model was trained for 100 epochs with batch size set to 50 to optimize connectionist temporal classification (CTC) loss function [21], [22]. We used adam as the optimizer. For this work, a character-based CTC model was used. The model was predicting a character at every time-step. We used an external 4-gram language model along with a CTC beam search decoder during inference time [23].  I. Each subject was asked to perform two different tasks while they were receiving speech therapy at Austin Speech Labs. The first task involved subjects reading out loud English sentences shown to them on a computer screen and their simultaneous EEG, electromyography (EMG), and speech signals were recorded. The second task involved subjects listening to the recorded audio of English sentences and they were then asked to speak out loud what they listened to and their simultaneous EEG, EMG, and speech signals were recorded. We collected a total of 8854 data samples from the 9 subjects for both the tasks combined. The vocabulary consisted of 57 unique daily used common English sentences. We used brain products wet EEG sensors for this data collection. We used 29 EEG sensors in total. The placement of 29 sensors was based on the standard 10-20 EEG sensor placement guidelines. Figure 4 shows a subject wearing our EEG cap during the experiment. We used the brain product's Actchamp amplifier as the EEG amplifier. We further used two EMG sensors to keep track of EMG artifacts generated during articulation. The EMG sensor placement location is shown in Figure 5. The speech signals were recorded using a mono-channel microphone. We used 70% of the data as the training set, 10% as the validation set, and the remaining 20% as the test set. The data set split was done randomly using the scikit-learn traintest split python function. There was no overlap between training, validation, and test set data points.   

## IV. EEG AND SPEECH FEATURE EXTRACTION DETAILS

The recorded EEG signals were sampled at a sampling frequency of 1000Hz and a fourth-order IIR bandpass filter with cut-off frequencies 0.1Hz and 70Hz was applied. A notch filter with a cut off frequency of 60 Hz was used to remove the power line noise. We then used the linear regression technique to remove EMG artifacts from EEG signals.

where α is the regression coefficient computed by Ordinary Least Squares method. We then extracted five features per EEG channel. The five features extracted were root mean square, zero-crossing rate, moving window average, kurtosis, and power spectral entropy [7], [24]. This EEG feature set was first introduced by authors in [7] where they demonstrated that these features carry neural information about speech perception and production. The EEG features were extracted at a sampling frequency of 100 Hz per channel. The speech signal was recorded at a sampling frequency of 16KHz. We extracted Mel frequency cepstral coefficients (MFCC) [17] of dimension 13 as features for speech signal. The MFCC features were also extracted at the same sampling frequency 100Hz as that of EEG feature extraction. 

## V. EEG FEATURE DIMENSION REDUCTION ALGORITHM DETAILS

Since the local structure of our EEG feature space was not linear, we used non-linear dimension reduction technique to perform dimension on EEG features. We plotted cumulative explained variance vs the number of components as shown in Figure 6 to identify the optimal EEG feature space dimension. We used kernel principal component analysis (KPCA) [25] with a polynomial kernel of degree 3 to reduce our EEG feature space of dimension 145 (five features per each of the 29 channels) to a final dimension of 10. Before applying KPCA, the EEG features were normalized by removing the mean and scaling to unit variance.

## VI. RESULTS AND DISCUSSION

We used percentage accuracy, F1-score, precision, and recall [26] as performance metrics to evaluate the performance of the isolated speech recognition model. The higher the accuracy, F1-score, precision, and recall values the better the performance of the model. For computing F1-score, precision and recall we added a small value e-07 called epsilon to the denominator of F1-score, precision and recall expressions to prevent a divide by zero error. We used word error rate (WER) as the performance metric to evaluate the performance of the continuous speech recognition model. The lower the WER value, the better the speech recognition system performance. For obtaining baseline results, the speech recognition models were trained and tested using only acoustic or MFCC features. Table II shows II demonstrate that choice of EEG frequency range had less effect on decoding performance for the isolated speech recognition task. The work carried out by authors in [6] demonstrated that both high and low-frequency neural signals carry important information about speech production. Table III shows test times results for isolated speech recognition task with and without EMG artifact removal and obtained results demonstrate that even though removing EMG artifacts improved the test-time performance of the model, the improvement was not that significant. Table IV shows the test time results for isolated speech recognition task with and without EEG dimension reduction. The results demonstrate that EEG dimension reduction using KPCA had resulted in significant performance improvement of the model during test time. Table V shows the test time results for isolated speech recognition task when we used only temporal lobe EEG sensor features, frontal lobe EEG sensor features, and concatenation of temporal and frontal lobe EEG sensor features. The temporal and frontal lobe contains brain regions responsible for speech perception and production [27], [28]. EEG features from frontal lobe sensors Fp1, Fz, F3, F7, FT9, FC5, FT10 , FC6 , FC2 , F4 , F8 and Fp2 were extracted and then reduced to a dimension of 10 using KPCA. Similarly, EEG features were extracted from temporal lobe sensors T7, TP9, TP10, and T8 and then reduced to a dimension of 10 using KPCA. The results shown in Table V demonstrate that it is possible to achieve comparable decoding performance for isolated speech recognition task using EEG sensors from just frontal and temporal lobe regions instead of using all the EEG sensors. Table VI shows the test time results for isolated speech recognition task when we used only the first half-length of the input EEG and MFCC features instead of the complete length of EEG or MFCC features for decoding text. The motivation here was to see if the model can decode text if we provide incomplete input as most of the aphasia or apraxia speech involves a lot of pauses in between. As seen from the Table VI results we observed that when half the length of the input signal is used, the baseline results improved significantly but adding acoustic representation in EEG to MFCC features still outperformed the baseline for all the test-time performance metrics. We believe the baseline results improved when shorter sequences were used as input signal due to the fact that GRU can process shorter sequences more efficiently than longer input sequences [18], [29]. The overall results from Tables II,III,V and VI show that adding acoustic representation in EEG features with MFCC features significantly outperform the baseline for all the test-time performance metrics for the task of isolated speech recognition using aphasia, apraxia, and dysarthria speech. Figure 7 shows the training and validation loss convergence for the regression model and Figure 8 shows the training and validation accuracy of the isolated speech recognition model. The training, validation loss values were comparable as well as the training and validation accuracy values, indicating the models didn't overfit. Figure 9 shows the confusion matrix obtained during test time for the isolated speech recognition task when tested using MFCC+ Highfrequency EEG representation. Each token in the confusion matrix represents a complete English sentence from the test set. Table VII shows test time results for isolated speech recognition task when acoustic features were concatenated with acoustic representation in EMG features of dimension 10 compared to acoustic representations from EEG features of dimension 10. We extracted the same set of 5 features that we extracted for EEG for each EMG channel. The results show that the acoustic representations present in EMG is not significant compared to acoustic representation features present in EEG signals for boosting the performance of the speech recognizer. Table VIII shows the test time average WER obtained for the continuous speech recognition task. The obtained results demonstrate that adding acoustic representation in High-frequency EEG features to MFCC outperformed the baseline for a test set vocabulary consisting of 1771 English sentences. We obtained a p value [30] of 0.0000213, demonstrating high statistical significance for our result. We further computed the test time WER's with 95 % confidence level value and observed that for the baseline, the WER range was between 48.58% and 51.1% where as using our proposed method, the WER range for the same confidence level value was between 44.25% and 47.13%.

Therefore a thorough statistical analysis of our test time continuous speech recognition results demonstrate that our proposed method outperformed the baseline result. 

## VII. CONCLUSION, LIMITATION AND FUTURE WORK

In this paper, we proposed a deep learning based algorithm to improve the performance of isolated and continuous speech recognition systems for aphasia, apraxia, and dysarthria speech by utilizing non-invasive neural EEG signals recorded synchronously with the speech. Our proposed algorithm outperformed the baseline results for the task of isolated speech recognition during test time by more than 50% and at the same time outperforming the baseline results for the more challenging task of continuous speech recognition by a small margin. To the best of our knowledge, this is the first work that demonstrates how to utilize noninvasive neural signals to improve the decoding performance of speech recognition systems for aphasia, apraxia, and dysarthria speech. One major limitation of the proposed algorithm is the latency that might be observed when this system is deployed in real-time as the first step is to obtain the acoustic representations in EEG using the trained regression model before it is concatenated with the acoustic features to decode text. All the results presented in this paper are based on the offline analysis. The latency will be a function of the input sequence length, model size, and computational      resources (GPU memory and RAM). Our future work will focus on validating these results on larger data set as we make progress in our data collection efforts. Future work will also focus on performing more experiments for the task of continuous speech recognition and developing tools to improve the performance of our proposed algorithm. Our aphasia, apraxia, and dysarthria speech-EEG data set will be released to the public to help further advance this interesting    

