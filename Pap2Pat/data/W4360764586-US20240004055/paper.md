# I. INTRODUCTION

Radar sensors are gaining momentum in the modern semiconductor industry. Various modulation types, independence of light conditions, low-cost and privacy-friendly features lead the radar technology to be successfully employable in applications such as people detection and object tracking [1]. To keep up the pace of advancements, attention directs to applications like multi-person tracking, which is critical in several areas such as automotive safety, medical services, or logistics [2], [3]. Multi-person tracking attempts to estimate the position of each target in a scene. For estimating the track positions, the Unscented Kalman Filter (UKF) is often used in radar tasks [4]. This method has the scope of a bayesian estimation of the target position incorporating a nonlinear dynamic movement model. The nonlinear transition is approximated by the unscented transform, described in [5]. By design, the UKF relies on hyperparameters. These tunable hyperparameters describe the dynamics of the sensor system and real-world scenarios. Particularly for radar data, occlusions, non-human disturbances, and limited resolution are significant challenges for robust tracking. Thus, the choice of optimal hyperparameters varies with the environment, and often their initial choice is suboptimal. Given the environment's variety of settings and noise, finding the best hyperparameters for any possible scenario is infeasible. Recent work focused on estimating the underlying dynamics of a UKF with specific neural networks [6] or using Reinforcement Learning (RL) to provide the best hyperparameters for a given scene, as shown in [7], [8]. Although this approach is promising, it often lacks robustness, and the data distribution on inference time might be different from the one used for training. Furthermore, the UKF model might fail to track in overcrowded scenarios, given the capability of its underlying model. In real-life applications such as automotive radar sensing, robustness also has to be critical, and then Deep Reinforcement Learning (DRL) is employed. However, neural networks are known to fail when the test data distribution is far off the training data distribution [9]. As a consequence, meta-learning is often used in literature to close the gap between training and test data distributions [10] and aims to adapt quickly to novel tasks. Although model-based meta-learning algorithms have obtained excellent results, they are limited to the network design stage [11]. For this reason, research has recently focused on model-agnostic meta-learning, which enables learning tasks independently of the machine learning model. This is granted through task-specific optimization and is widely applicable in domains such as few-shot classification and RL. In the case of Meta-RL, context variables are a promising way to incorporate task-specific information, as shown in [12]. In this method, the context variable is learned by a neural network from taskspecific data. Although the method is performant, it requires computing a context variable using multiple data samples from each additional task. Storing this data during inference is inefficient. Hence, our proposed method uses domain information to formulate a context variable. Our approach does not need to store data: it computes the context variable from easyto-obtain input distribution statistics, which we refer to as context prior. Furthermore, as shown in the experiments, our method leads to an improved domain generalization compared to other state-of-the-art approaches. Nevertheless, in several real-world applications (e.g., radar-tracking), improved domain generalization cannot address the limitations inherent to the task itself. For example, tracking operations in small, crowded scenarios with obstacles becomes increasingly tricky. In order to assess the reliability of our Meta-RL method, we develop an uncertainty mechanism via bootstrapped networks. The uncertainty mechanism is combined with the context prior that encodes information about the task difficulty. In this approach, scenes where tracking is prone to failure, are classified as OOD, thus assessing the particular reliability of the tracker in the current scenario. As a summary, the contributions of the paper are the followings:

1) Meta-RL for domain generalization without additional memory footprint using context priors 2) Enhanced OOD detection with context priors that encode task difficulty The remainder of the paper proceeds as follows: in Section II, we introduce the radar-tracking problem and how to tackle it with RL. Afterward, we explain the specific signal processing in Section III. In the same section, we show how the input data distribution is used to compute an informative context variable. At the end of this section, using the context variable, we propose a Meta-RL algorithm for environment generalization and detection of OOD scenarios. Finally, we evaluate our method on a multi-target radar tracking dataset against related Meta-RL methods in Section IV. Our proposed approach outperforms comparable Meta-RL approaches in terms of peak performance by 16% on the test scenarios and the baseline of fixed parameters by 35%. In the same way, it detects OOD scenarios with an F1-score of 72%. Thus, our approach is more robust to environmental changes and reliably detects OOD scenarios. In Section V, we summarize our results and give an outlook on future work in Section.

# II. BACKGROUND AND MOTIVATION

In this section, we review the background and related work. In Section II-A, we first outline the principle of radar tracking. Afterward, we explain how RL can be used to optimize radar tracking. Additionally, we extend this concept by introducing the fundamentals of Meta-RL and Uncertainty-based RL.

## A. Radar Tracking

Frequency Modulated Continuous Wave (FMCW) radars can estimate the range, Angle of Arrival (AoA), and velocity of targets. In the case of radar tracking, we use the range and AoA to determine the target position. The typical radar tracking pipeline can be divided into signal processing, detection, clustering, and tracking, as shown in [13]. A high level description is given in Figure 1. The signal processing stage elaborates the sensor data from each radar antenna to estimate the reflected signal's range and angle. The resulting image is a so-called Range-Angle Image (RAI). Afterward, the RAI is convolved with a window that determines the signal threshold based on the surrounding noise. Usually, a Constant False Alarm Rate (CFAR) algorithm [14] or a variation thereof defines the threshold. A clustering algorithm groups nearby detected signals, and the respective cluster means are input to the tracking stage. In this part of the pipeline, the track management determines whether to assign the measurement to a track, open a new track, discard the measurement or delete non-active tracks. Before updating the track, the measurement has to be filtered by the tracking filter based on the last position and an underlying movement model. The UKF is a commonly used tracking filter [15]. The presented tracking pipeline heavily relies on hyperparameters. Namely, the tracking performance depends on the gating threshold for assigning tracks and the covariance matrix of the measurement and state transition models. Typically, those hyperparameters are determined by an expert user with recorded data and ground truth positions evaluating the Normalized Estimation Error Squared (NEES). However, this approach is unlikely to perform well once the radar is deployed in a different environment. Thus, recent work proposed to use RL to tackle the combinatorial problem of finding the best set of parameters for any scenario [16].

## B. Reinforcement Learning

In RL the problem is formalized as a Markov Decision Process (MDP) by (S, A, R, p, γ), where S is the state space as radar sensor input, A is the action space defined as hyperparameters, R is the reward as tracking performance shown in [16], p π is the unknown transition probability between states following policy π and γ is the discount factor. Let τ = (s t , a t , r t , s t+1 ) define the transition from state s at time step t to the next state s t+1 following action a t with reward r t . In traditional RL the goal is to maximize the sum of expected rewards

This can be achieved by value iteration methods [17]. There, we define a Q-Value Q(s t , a t ) for each state and action pair that estimates the expected reward. Afterward, for each state we select the action which maximizes the Q-Value. With the advancements of neural networks in the context of universal function approximators [18], methods like DQN [19] use neural networks for the Q-value approximation. However, value iteration methods become infeasible when the action space is continuous. Thus, the policy gradient theorem [20] provides a way to update the policy for continuous actions. The combination of Q-Learning and policy gradients forms the basis of actor-critic methods. In recent years, policy gradientbased algorithms like Proximal Policy Optimization (PPO) [21] or actor-critic methods like Soft Actor-Critic (SAC) [22] have shown great success. The particular choice of the algorithm depends on the task at hand. On-policy methods update the policy only with transitions from following the same policy, which is data inefficient. In contrast, off-policy methods store transitions in a replay buffer and update the policy from these. The current policy and the policy that collected the transitions can be unrelated, which is the core concept of off-policy methods. Since the policy update is possible with transitions from any policy, the collected data is reusable. Due to the data efficiency, off-policy methods are widely used in applications where data is scarce [23]. Despite advancements in RL, transferring RL agents to realworld problems remains challenging [24]. To address these limitations, [25] has shown how meta-learning can be used to transfer an RL agent to a real-world problem.

## C. Meta Reinforcement Learning

State-of-the-art Machine Learning (ML) models usually require many data [26]. However, humans can transfer experiences to have a "educated guess" about new tasks based on their knowledge from related tasks, e.g., recognizing an animal from the zoo in the TV. Meta-learning has been introduced to support the training of ML models to imitate this human capability. To this end, each task is associated with a respective dataset D. The meta-learner aims to increase the performance on unseen test tasks, which are different from the tasks it has been trained on. This procedure can be adapted to many machine learning fields, such as supervised learning [27] and RL [28]. In Meta-RL, the task difference can be induced by different reward functions or environments, e.g., several Maze puzzle environments [29]. Successful algorithms as Pearl [12], Reptile [30] or MAML [31] are designed to minimize the adaptation steps in new environments. As an example, adaptation is not feasible in a radar tracking application during inference due to the lack of rewards. Thus, in applications without reward signal during inference, domain generalization is key. Inspired by [32], where the target objective of MAML is adapted to improve domain generalization, we adapt the design of Pearl to an efficient context variable computation. In Pearl, these context variables are computed by a neural network from stored transitions of the test task. As a consequence, there is a need for context variables that can be efficiently computed from the input data and do not require reward information during inference. Although, the discussed methods increase the generalization, they do not guarantee the robustness in case the inference distribution differs from the training distribution.

## D. Uncertainty-based Reinforcement Learning

In order to increase the robustness, highly uncertain predictions need to be classified as OOD in safety-critical applications, as shown in [33]. At the same time, the radar tracker is often limited by the resolution of the radar settings. Thus, there is a limit on the amount of people who can be tracked simultaneously in the scene. In addition, moving disturbances like curtains can be detected as false targets. Consequently, the algorithm needs to detect those scenarios and notify the user that tracking is impossible. Thus, we have a bayesian setting, where we estimate the posterior probability p(θ|x) on the network parameters θ given the data X. The posterior is proportional to a prior believe on the distributional function p(θ) and the likelihood p(x|θ). Further, the posterior probability determines the uncertainty on the prediction. RL literature proposed several ways to estimate the uncertainty on the neural network parameters: dropout [34] or bootstrap DQN [35] and bootstrap DQN with random prior functions [36]. Dropout applies a random Bernoulli mask on the neural network weights to prevent co-adaptation. In [34], they argue that the dropout rate distribution approximates the posterior distribution. However, dropout is insufficient as a posterior estimation since the dropout rate is not dependent on the data. Hence, this method can not differentiate between data points it has seen once or multiple times, as pointed out in [36]. A more prominent way is the bootstrap method with random prior functions. The bootstrap DQN defines a neural network with K heads that estimate K Q-Values Q i=1,...,K (s t , a t ). The update is computed for every single head with the Temporal Difference (TD) error, shown in Equation 2, with the reward r t , discount factor γ, state s, action a and time step t.

For each state and action, the variation in the head prediction estimates the uncertainty about the given scene. In order to enhance this variation, a binary mask selects the active heads during training, such that the heads observe different data. In addition, randomized prior functions dictate the network behavior wherever is no training data. The random prior function can be estimated by a non-trainable, random initialized neural network that processes the input data [36].

Since radar tracking is a continuous action space problem, bootstrap DQN is not applicable. However, DQN is a criticonly method, referring to [37]. Hence, we can adapt the bootstrap mechanism with priors to actor-critic methods without loss of generality.

# III. APPROACH

In this section we present an approach for robust radar tracking with meta-RL in combination with OOD detection. In Section III-A, we introduce the signal processing of the radar-based tracking chain and RAI generation. Afterward, we describe context priors in Section III-B and how they encode the complexity of the ongoing RL task. Furthermore, as described in Section III-C, the information is used for meta-RL, which bears the generalization on tasks of different complexity. Finally, the approach is completed by Section III-D, where, using uncertainty, the approach can detect OOD tasks where the tracking is likely to fail.

## A. Radar Signal Processing

The received data at each time step is a three-dimensional array of shape (N C , N S , N rx ), dependent on the number of chirps N C , the number of samples N S per chirp, and the number of receiving antennas N rx . Let the axis along the chirps denotes the slow time, and the axis along the number of samples the fast time. We subtract the mean in the fast time to prevent any transmit/receiving antenna leakage. Additionally, we subtract the mean in the slow-time as a high-pass filter to remove completely static target information, e.g., furniture. As the last step, we transform the array to the frequency domain with a Fast-Fourier Transformation (FFT). The range corresponds to frequency along the fast time. Let B be the bandwidth of the transmit signal, T c the active chirp time, f s the sampling frequency, and c 0 the speed of light. The maximum range is defined in Equation 3.

The AoA can be estimated by the phase difference between the minimum two receiving antennas using Digital Beamforming (DBF), which depends on the antenna gain at a given AoA.

Here we use the Capon beamformer described [38]. This paper uses a FMCW radar with one transmit and three receiving antennas in a triangular alignment. Hence two antennas are used for the azimuth AoA estimation. By utilizing a bandwidth of 1 GHz, chirp time of 399 µs, and the sampling frequency of 2 MHz, the maximum detection range is approximately 5m. As example, we show an RAI with two targets in Figure 2. In our method, the RAI is used as input to the meta-RL algorithm.

-60 0 60

angle in degrees  

## B. Random Priors and Context Variables

In Section II we introduced random priors for uncertainty estimation and context variables for meta-learning. Both are random vectors computed from the input data and serve as additional input to the neural network. However, random priors are computed by a non-trainable and random initialized neural network. In contrast, context variables in [12] are Gaussian random variables computed from task-specific transitions following the context-conditioned policy. In that way, the context variable infers information about the task difficulty from the reward. For radar tracking, the reward is given by the tracking performance, which requires ground truth positions. This information is not available during inference time. However, the average intensity and the variance in the RAI are increasing with the number of people in the current scene, as shown in Figure 3a and Figure 3b. Since the RAI distribution is proportional to the intensity of the tracking targets, we can use the RAI distribution to encode the task difficulty. To this end we use the mean µ rai and standard deviation σ rai of the radar input to define a Gaussian context prior N (µ rai , σ rai ).

## C. Meta-RL with Context Prior

In this Meta-RL problem, we define different rooms as tasks since the goal is to optimize the tracking parameters independent of the room. In addition, we divide the different rooms into training and test tasks. The test tasks are not observed during training. Moreover, we report the evaluation reward as the average reward over the test tasks, while the Meta-RL algorithm is trained on the train tasks. A detailed description of the training and testing procedure is given in Figure 4. The reward is given in Equation 4, where N is the  predicted number of targets, N is the true number of targets, M = min( N , N ), μk is the predicted mean, Σk the predicted covariance and p k ∼ N (μ, Σ), as described in [16].

The first term ρ describes the relative error between the predicted and actual number of tracks. The second term evaluates the likelihood of missing the ground truth positions incorporating the variance of the UKF. For stability reasons, p(•) is clipped to 1.

As baseline RL algorithm we use SAC, a state-of-the-art offpolicy actor-critic method from [22]. By design of the context prior, gradient calculation can be omitted and we can use the loss functions for the from [22], given in Equation 5and the critic loss for the individual bootstrap heads in Equation 6.

Furthermore, the critic uses the bootstrap mechanism. To this end, we define a Base Network that computes an embedding vector x from the state action pairs. During training, we compute a binary mask in every epoch, determining whether the parameters for specific heads are updated. For every active head, we sample a context vector from the context prior distribution and add it to x. Afterward, every head predicts a Q-Value Q i (s, a). In detail, we describe the bootstrap critic algorithm in Algorithm 1. We apply the exact prior mechanism for the actor network that predicts the actions from the current state. A detailed description of the used networks is given in Figure 5. For meta-learning, we update the parameters with the accumulated losses from every training task, as shown in [12]. (s t , a t , r t , s t+1 ) from B for head in H i m do

11:

x = BaseN etwork(s t , a t )

12:

13:

14:

end for 16: for task in T i do 6:

rollout π(θ|s) and store transitions in B i 7:

end for In literature, OOD approaches aim to detect whether an environment has not been seen yet. The challenge in our setup is, for new scenarios, to generalize well and detect lowperformance simultaneously. We apply the bootstrap mechanism for the critic for OOD, as shown in [39]. The critic aims to predict the future reward, and the variation along multiple predictions is a measure of uncertainty. As we see from Figure 3, a huge variance in energy peaks between two detected targets in a radar image, is an example of increased task complexity. Thus, our proposed context prior emphasizes higher variation for a more difficult task. The training procedure of the bootstrap critic is shown in Algorithm 1. In order to detect OOD samples during inference, we define a threshold c as given in Equation 7,

where µ head is the prediction mean of all heads, σ head is the standard deviation of the predictions and α is a hyperparameter that determines the size of the OOD detection interval.

# IV. RESULTS AND DISCUSSION

In this section, we present the detailed implementation settings for the experiments, starting with the used hardware and software tools, followed by the dataset specifications. The results for Meta-RL are presented in Section IV-B and the results for OOD detection are shown in Section IV-C.

## A. Implementation Settings and Dataset

In the implementation, we used Tensorflow ™ -GPU v2.9.0 with CUDA ® Toolkit v11.2.0 and cuDNN v8.1.0. As a processing unit, we used the Nvidia ® Tesla ® P40 GPU, Intel ® Core i9-9900K CPU, and DIMM 16GB DDR4-3200 module of RAM. As a radar sensor for data recordings, we used three of Infineon Technologies XENSIV ™ 60 GHz BGT60TR13C FMCW radar and three cameras for labeling purposes. The recorded dataset includes 200.000 radar frames from five different rooms, including human activities from zero to five people, divided into recordings of 350 frames. The radar data has been recorded with a frame rate of 10 Hz. We use Yolo-v5 1 , an object detection framework from Ultralytics ® , with the three cameras to gather ground truth positions for the recordings. For meta-training, we take recordings from three rooms and 1 https://github.com/ultralytics/yolov5 use the other two rooms for the evaluation. The number of frames per environment is evenly split for the number of targets.

## B. Domain Generalization

The current policy obtains the Meta-RL results in two unknown test environments. We report the average evaluation reward over the test environments after each iteration. The optimal reward is 0, and a higher reward indicates better tracking performance. The reward formulation is given in Equation 4and leverages the distance between predictions and ground truth and the number of false targets. As a baseline, we use the performance of the tracker using fixed hyperparameters determined by an expert user. In addition, we compare our method against MAML with the formulation from [32] and Reptile. In Figure 6, we show that our proposed method improves 35% over the baseline and 16% over the comparable Meta-RL methods. In addition, our proposed method explores more efficiently and is improving while MAML and Reptile saturate.

The effect of adaptive parameter selection, where the reward focuses on the distance and false targets, is depicted in Figure 7. There we see that our approach can estimate the correct number of tracks and is more accurate with a Root Mean Squared Error (RMSE) of 1.45 compared to the baseline (RMSE of 1.94) in estimating the target's angular position (x-position). To further gain insights on meta-learning, we scale the reward during training to observe the impact on the environment generalization. Since the reward signal is used to improve tracking, and the context prior is used for environment generalization, weighting the reward higher than the context prior should decrease the generalization capabilities. We report the peak performance of the Meta-RL approach in the test environments with different reward scaling factors during training n Table I. There we show that scaling the reward by two results in the highest average evaluation reward. Further we see that scaling the reward far off the context prior leads to worse evaluation performance, which supports our hypothesis. 

### IUDPHV [SRVLWLRQ JURXQGWUXWK DGDSWLYHFRQWH[WSULRU EDVHOLQH

WKSHUFHQWLOH WKSHUFHQWLOH Fig. 7: Comparison of our adaptive approach with the baseline for estimating the angular direction (x-position in Cartesian coordinates). The adaptive approach predicts the correct number of tracks while resulting more accurate in estimating the position with an RMSE of 1.45 incorporating the predicted variance of the UKF prediction.

## C. Uncertainty-based OOD Detection

With our defined setup, the expected limit of the tracking system is up to three targets. Hence, we aim to predict scenes with more than three targets as OOD. In Figure 8 we show the distributions of the critic predictions. Both mean and standard deviation are proportional to the number of people in the scene. Since we have fewer OOD scenarios (four and five targets) than training scenarios (zero to three targets), we report the F1-Score, which considers data imbalance. With the threshold defined in Equation 7, α determines the size of the detection interval. Thus, choosing α very small focuses on accurately predicting all the OOD scenarios and vice versa, leveraging precision and recall. With α = 0.17 we reach a maximum F1-Score of 72%.   The OOD scenarios are highlighted in red.

# V. CONCLUSION

This paper presents an approach that utilizes context priors for an uncertainty-based Meta-RL. This is used for domain generalization and OOD detection. In order to assess the performance of our contribution, we benchmark it on a radartracking dataset towards related Meta-RL algorithms. In a set of multi-target radar-tracking scenarios, the proposed method outperforms related Meta-RL approaches in peak performance by 16% and the baseline by 35% while detecting OOD data with an F1-Score of 72%. This shows that our method is more robust to environmental changes and well-addresses the OOD tracking scenarios. In future work, we want to expand the Meta-RL framework to different radar positions, radar devices, and internal settings (e.g., bandwidth, sampling frequency).

