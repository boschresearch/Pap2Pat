{
    "id": "https://semopenalex.org/work/W4311642127",
    "authors": [
        "Amit K. Roy-Chowdhury",
        "Kuan-Chuan Peng",
        "Abhishek Aich"
    ],
    "title": "Cross-Domain Video Anomaly Detection without Target Domain Adaptation",
    "date": "2022-12-13",
    "abstract": "Most cross-domain unsupervised Video Anomaly Detection (VAD) works assume that at least few task-relevant target domain training data are available for adaptation from the source to the target domain. However, this requires laborious model-tuning by the end-user who may prefer to have a system that works ``out-of-the-box. To address such practical scenarios, we identify a novel target domain (inference-time) VAD task where no target domain training data are available. To this end, we propose a new `Zero-shot Cross-domain Video Anomaly Detection (zxvad)' framework that includes a future-frame prediction generative model setup. Different from prior future-frame prediction models, our model uses a novel Normalcy Classifier module to learn the features of normal event videos by learning how such features are different ``relatively to features in pseudo-abnormal examples. A novel Untrained Convolutional Neural Network based Anomaly Synthesis module crafts these pseudo-abnormal examples by adding foreign objects in normal video frames with no extra training cost. With our novel relative normalcy feature learning strategy, zxvad generalizes and learns to distinguish between normal and abnormal frames in a new target domain without adaptation during inference. Through evaluations on common datasets, we show that zxvad outperforms the state-of-the-art (SOTA), regardless of whether task-relevant (i.e., VAD) source training data are available or not. Lastly, zxvad also beats the SOTA methods in inference-time efficiency metrics including the model size, total parameters, GPU energy consumption, and GMACs.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Unsupervised Video Anomaly Detection (VAD) methods  have been widely used in security and surveillance applications [34][35][36][37] over the supervised or weakly-supervised VAD methods [38][39][40][41][42][43][44][45][46]. This is mainly because unsupervised VAD methods do not need training videos containing abnormal events which are rare and laborious to annotate [35,36]. Hence, with only normal events in training videos, the unsupervised VAD methods mark the activities unexplained by the trained model as anomalies during testing. Recently, unsupervised VAD works  [1,2]. We relax this assumption of having such access to training data from the target domain and tackle a more stringent, yet practical, case using our proposed zero-shot xVAD or zxVAD framework. under cross-domain settings have been introduced [1,2,47,48]. Given the video data containing only normal events from a source domain, the goal is to perform VAD in a different target domain. However, these cross-domain VAD (xVAD) works [1,2,47,48] are methods which need access to either the source and target domain VAD training data [1,2] or strong supervision from pre-trained object detectors (e.g., YOLOv3 [49] in [48]). Collecting such data in the target domain and adapting or tuning the model may not be feasible by the end-user who may want a system that works \"out-of-the-box\" [50,51]. Moreover, granting access to such video data may be time-consuming to third-party corporations due to intellectual property and security concerns [52,53]. This renders the current xVAD works ineffective as they assume access to at least some target domain training data. Problem Statement. Based on the aforesaid issues, we formally identify the following new unsupervised xVAD problem of detecting anomalies in the target domain with strictly no access to target domain training data and no prior knowledge of its anomaly types. More specifically, our goal is to detect anomalies in the target domain's testing set, without having any training data on the target side. Fig. 1 contrasts this problem setup with prior xVAD problem definitions. Proposed framework. We tackle this new problem using a novel xVAD framework, namely 'Zero-shot Cross-domain Video Anomaly Detection' (zxVAD). The term zero-shot implies no training videos available from the target domain for adaptation to perform anomaly detection. zxVAD has a generator [54][55][56] in a future-frame prediction setup [3] similar to xVAD approaches [1,2]. However different from these methods, zxVAD's generator training is assisted by a novel Normalcy Classifier (NC) module and an Untrained Convolutional Neural Network (CNN) Anomaly Synthesis (O) module. Prior unsupervised xVAD works learn features from only videos with normal events. This leads to overfitting to the source domain distribution and the poor generalizing ability for target domain VAD [2]. In contrast, zxVAD's generator uses NC and O modules to learn features of normal activities in input videos, by focusing on how such features are relatively different from features of abnormal frames. This \"relative\" learning strategy enhances the generator's ability in identifying anomalies in target domain without any adaptation at test time.",
                "Normalcy, by definition, is always contextually dependent [43,57] (e.g., running in playgrounds vs highways). Hence, to generalize across new target domains (without its training data) where we have no prior knowledge about anomaly types, we propose to learn normal event features that consider the contextual or relative difference between \"normal\" and \"abnormal\" patterns. More concretely, rather than learning only the normalcy features (i.e. features of normal video frames), our model learns the relative normalcy features (i.e. difference between features of normal and abnormal video frames) using our proposed NC module. These pseudo-abnormal frames are created through our proposed O module which is capable of localizing objects from both Task-Relevant or VAD data and Task-Irrelevant (TI) or non-VAD data (i.e., data irrelevant to the VAD task). The O module crafts pseudo-abnormal frames by localizing objects in input TI or VAD video frames and pasting them (with random location and size) on normal VAD video frames. Furthermore, a major advantage of introducing TI data to our problem setup is that they can be treated as video distributions for learning patterns of normal activities and also assist in creating diverse anomalies. Hence, along with the strategy of learning the relative normalcy difference, zxVAD aims to mitigate the generalizing issue via learning this relative normalcy with respect to abnormal frames having different kinds of foreign objects (either from VAD or TI frames). This allows zxVAD to avoid being limited to specific anomaly types in the source domain, making it fundamentally different from supervised specific anomaly learning.",
                "Our NC module is designed to distinguish between a pseudo-abnormal and the predicted normal future-frame through novel loss functions. The highlighting attribute of these functions is to consider different properties of normal and abnormal frames through our NC's logit predictions and derived attention maps. Our O module is uniquely capable of using VAD or TI data with an untrained randomly initialized CNN to create anomalies at no extra training cost. To sum up, we make the following key contributions: "
            ],
            "subsections": []
        },
        {
            "title": "Related Works",
            "paragraphs": [
                "Unsupervised VAD works. Early unsupervised VAD works formulated the anomaly detection using handcrafted features to characterize the normal event or regular pattern distribution [4][5][6][7][8][9][10][11][12]. However, these methods were outperformed by the CNN approaches [3,[13][14][15][16][17][18][19][20][21][22][23][24][25] (both categorized as C 1 in Tab. 1). Some of these CNN based unsupervised VAD works use generators [54] to model the normal frame distributions [3,18,[20][21][22]61], and further introduce memory modeling networks to record various normal event patterns in videos [1,2,18,21,61]. Another category of works (C 0 in Tab. 1) [57][58][59][60][61][62][63] proposed computationally heavy approaches that used strong priors like object extraction (using pre-trained object-detectors [57,60]) for VAD, in order to focus only on specific objects to detect anomalies. Compared to aforesaid VAD works in C 0 and C 1 , zxVAD (a) is designed to tackle unsupervised cross-domain VAD problem, (b) is a future-frame prediction method with a memory module, and (c) needs no strong prior knowledge from object extraction. Finally, few works [48,57,63,[65][66][67] have shown different VAD strategies where pseudo-anomalies are used. For example, [57,67] uses a generator to create fake anomaly data. [65,66] ",
                "G(\u2022) is adversarially trained against D(\u2022) in the Least-Square GAN [54,75] setup where D(\u2022) aims to distinguish between v T +1 and the ground truth frame v T +1 . Similar to [2,18], we introduce a memory module M. In zxVAD, G(\u2022) is further regularized using N (\u2022) with our proposed four novel objectives (explained in Sec. 3.2) which uses pseudo-anomaly examples generated using an untrained CNN based strategy. Following prior works [1,76], we optimize G(\u2022) with the mean square error loss",
                ", where SSIM represents the structural similarity index measure [77] between v T +1 and v T +1 , and Gradient loss L GD [3,76]. To optimize M and encourage modeling normal videos using sparse but most relevant memory slots, we follow [18] and apply a hard-shrinkage on M's memory addressing vectors w i using continuous ReLU activation function with a shrinkage factor \u03bb set as 0.0005. Next, we normalize each element w i \u2190 wi / w 1\u2200i and get z = wM. We also apply a sparsity regularizer on w by minimizing its entropy as L MEM = N i=1 -w i log w i [18]. We combine these losses as",
                "where the reconstruction loss is",
                "We set the loss weight \u03b1 MEM =0.0025 following [18]. Totally, the weights \u03b8 G , \u03b8 D and \u03b8 N are updated during training, while \u03b8 R is randomly initialized before training and remains fixed.",
                "Better than prior works which do not consider the relative difference between normal and abnormal events, zxVAD introduces a novel strategy to regularize this backbone generator by learning normal features with respect to pseudo-abnormal features. As our normalcy classifier module utilizes pseudoanomalies to learn the relative normalcy features, we first present our pseudo-anomaly creation strategy."
            ],
            "subsections": [
                {
                    "title": "Pseudo-Anomaly Synthesis via Untrained CNN",
                    "paragraphs": [
                        "Prior works [57] have focused on creating anomalies using pre-trained object detectors (i.e., YOLOv3 [49] in [57]) that result in issues like additional training overheads. Different from such methods, we present a training-free strategy to extract objects from video frames. These objects can be obtained on both VAD and TI video frames (i.e., v t and u t ). For brevity, we refer to the input frame as x. Given an input frame x \u2208 R C\u00d7H\u00d7W , we denote the output of a CNN R(\u2022) (before the classification layer) as tensor G \u2208 R d\u00d7h\u00d7w . For example, if R(\u2022) is ResNet152 [78], G is the output of 'conv5 x' with size 2048\u00d78\u00d78 if input size is 3\u00d7256\u00d7256. We employ SCDA [79] to perform channel-wise summation on G to obtain an attention map A \u2208 R h\u00d7w . We then obtain a binary mask M from A as follows. We set M (i,j) = 1 if A (i,j) > \u03c2, or 0 otherwise. Here, (i,j) represents position in h \u00d7 w locations. We empirically set \u03c2 =0.1. M (i,j) =1 indicates the foreground objects. Finally, M is resized from h\u00d7w to H \u00d7W . As noted in [80], the idea behind this surprising property that randomly initialized CNN can localize objects is: because the background in the input frame x is relatively texture-less in comparison to the foreground objects in the scene, these background regions have higher chances to be deactivated by nonlinear activation functions like ReLU [81]. The object is finally localized as M x =M x. To create pseudo-abnormal frame \u1e7d, we combine M x and one of the input frames to G(\u2022), i.e., v t \u2208 {v 1 ,v 2 ,\u2022\u2022\u2022,v T } by pasting M x on v t at random location r z with random size r x \u00d7r y . We discuss the method to choose the location r z and size r x \u00d7 r y in Supplementary Material. Note that most of the video frames used for creating pseudoanomalies happen to contain at least one foreground object for the untrained CNN to extract. Even if there are no such objects, our untrained CNN will still focus on some patches (on the input frame) and treat them as anomaly on normal event VAD frame."
                    ],
                    "subsections": []
                },
                {
                    "title": "Learning Normality w.r.t. Abnormality",
                    "paragraphs": [
                        "Our Normalcy Classifier Module is a classifier N (\u2022) that is optimized by the following four loss functions. These loss functions are complementary to each other as follows: normalcy loss and attention affirmation loss focus on the difference between normal and abnormal frames, whereas relative normalcy loss and relative attention affirmation loss focus on how relatively different are normal frames from abnormal frames (and viceversa). For clarity, we drop the subscript of the predicted frame v T +1 and mark it as v. The data distribution of normal and pseudo-abnormal frames are denoted as \u03c1 and \u03ba, respectively. Normalcy loss L N . Given the predicted future-frame v and pseudo-abnormal frame \u1e7d, L N optimizes N (\u2022) to increase the probability that v is 'normal' (label set as 1) and \u1e7d is 'abnormal' (label set as 0), using following loss function.",
                        "Relative normalcy loss L RN .Abnormal events can be viewed as deviation with respect to normal events. We argue that the key missing attribute of ( 2) is that the probability of normal data being normal (N ( v)) should increase as the probability of abnormal data being normal (N (\u1e7d)) decreases and vice-versa. Rather than just maximizing P[ v is normal], we also ask N (\u2022) to maximize P[ v is more normal than \u1e7d] (P[\u2022] denotes probability operator). We define this novel relative normalcy loss below:",
                        "Attention affirmation loss L AA . The decision of N (\u2022) on the normal frame v and abnormal frame \u1e7d should be based on the following information: (1) N (\u2022) should consider the whole scene in v to classify it as 'normal,' and (2) N (\u2022) should consider the foreign object (introduced by our module O in \u1e7d) to classify it as 'abnormal.' Our strategy in Sec. 3.1 allows us to obtain the exact location of foreign objects in \u1e7d. Hence, we leverage this knowledge and create ground-truth masks of \u1e7d. We first initialize a tensor M with zeroes. Next, we update this tensor by pasting M after resizing to r x \u00d7 r y at location r z (obtained from O in Sec. 3.1). We show examples of M in Fig. 3. We extract feature maps from the last convolutional layer of N (\u2022) and apply SCDA [79] to obtain attention maps A( v) and A(\u1e7d) for normal and abnormal frames, respectively. A(\u2022) denotes the operation to extract attention maps from N (\u2022). We enforce this constraint via the attention affirmation loss L AA as (1 is a tensor of the same size as A( v) filled with ones):",
                        "Relative attention affirmation loss L RAA . Similar to the concept of L RN , we argue that L AA does not consider the relative difference of attention maps from normal frames with respect to attention maps from abnormal frames. Hence, we propose a relative attention affirmation loss L RAA that aims to learn this difference. We create two attention map pairs: (Pair-1) A( v) and A(g( v)), and (Pair-2) A( v) and A(\u1e7d). The function g(\u2022) denotes a series of transformations (Color Jitter, Random Affine, and Random Perspective) applied to v using the package Kornia [82] (related parameters are provided in Supplementary Material). The relative difference between the attention on 'augmented normal' frame should be smaller than that of the 'pseudo-abnormal' frame with respect to the 'normal' frame.",
                        "We enforce this difference with a margin m that simultaneously enhances the intra-class compactness between normal and augmented-normal frames and inter-class discrepancy between normal and pseudo-abnormal frames. We design L RAA using the ArcFace loss [83] enforcing this margin as follows.",
                        "log e s(cos(\u03c9y i +m)) e s(cos(\u03c9y i +m)) + 1 j=0,j =y i e scos(\u03c9 j ) , (5) where label y i is set as 1 for normal frame v and augmented frame g( v), and 0 for pseudo-abnormal frame \u1e7d. We transform A(x) with \u03c8 yi = W yi vec(A(x)) cos(\u03c9 yi ) (with \u03c9 yi \u2208 [0,\u03c0] as the angle between W yi and vec(A(x))). Here, vec(\u2022) is a vectorizing operation. W yi and vec(A(x)) are normalized to 1 which leads to \u03c8 yi =cos(\u03c9 yi ). With ArcFace loss, W yi behaves as a centre for each class (i.e. normal and abnormal) [83] which creates a distance margin penalty of m. We set scaling factor s = 64 and margin m = 28.6 degrees following [84]. L RAA can be implemented as any triplet metric learning loss [84]. However, we choose the ArcFace loss as it has been shown to perform well in recent non-VAD works [85][86][87].",
                        "Final learning objectives. To summarize, zxVAD is trained end-to-end with G(\u2022) learning loss L G , D(\u2022) learning loss L D , and N (\u2022) learning loss L N as follows:",
                        "We set \u03b1 D =0.05 following [3]. The rest of loss weights \u03b1 N = 0.5,\u03b1 n =1,\u03b1 rn =0.01,\u03b1 aa =1, and \u03b1 raa =1 are set empirically. To make the further discussion concise, we show the statistics and acronyms of the VAD and TI datasets in Tab. 2."
                    ],
                    "subsections": []
                },
                {
                    "title": "Introduction to Task-Irrelevant (TI) Datasets",
                    "paragraphs": [
                        "In this section, we discuss the utilities of task-irrelevant or non-VAD videos for unsupervised VAD. Task-relevant or VAD datasets provided by VAD research community are known to be limited in scale as shown in Tab. 2 and [1,43,91]. (e.g. Ave [9], Ped1, Ped2 [10] datasets have <100 training videos). Further, it is difficult to collect different kinds of scenarios of normal activities with such limited scale. Hence, we propose to introduce the utility of Task-Irrelevant (TI) datasets to the task of VAD.",
                        "We define a dataset as 'Task-Irrelevant' which is freely available from different other video downstream or non-VAD tasks (e.g., video classification, action recognition, etc.). Examples of such datasets are UCF101 [89] and HMDB [88] (see Tab. 2). Such datasets were originally introduced for non-VAD works, specifically curated for large-scale deep learning-based tasks. For example, Jester was originally introduced for video classification of 25 hand gesture classes [90]. To show the performance using diverse types of datasets in our zxVAD task, we choose Jester, UCF101, and HMDB to be our TI datasets. Please see Supplementary Material for dataset examples. Next, we discuss how the task-relevancy of these datasets is measured with respect to the VAD task, followed by two simple strategies to use these datasets in the proposed problem scenario. Note that zxVAD needs nothing from the TI-VAD relevancy measure to operate. The purpose is to only validate TI data's irrelevancy to the VAD task. We highlight the difference in amount of training data between VAD and TI datasets. : the train/test disjoint camera (dc) split is provided by [1]. As stated in [1], UCFC dataset does not contain ground truth frame-level labels and hence is not considered for evaluation. "
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Reference Values",
            "paragraphs": [
                "S(\"husband\",\"wife\")=0.829 high relevancy S(\"fox\",\"paper\")=0.102 low relevancy S(\"bag\",\"dime\")=0.209 example w.r.t. max score (0 : 'lower', 1 : 'higher' semantic similarity) Figure 4: Relevancy measure between VAD and TI labels. Using the relevancy score matrix between the TI and VAD labels, we find that TI datasets have low semantic similarity with the VAD datasets. The maximum score occurs between HMDB and Ave.",
                "Measuring relevancy of non-VAD datasets. Following [92,93], we use word2vec [94] (pre-trained on Google News dataset [94]) to measure the task-relevancy of our introduced TI datasets: Jester, UCF101, and HMDB. We first compute an embedding vector of the input labels (in case the label contains more than one word, we average the embedding). Next, we compute the mean absolute cosine similarity S \u2208 [0,1] of the embedding for all possible pairs of labels between the TI datasets and abnormal classes of VAD datasets. This is denoted as S = 1 / P Q P p=1 Q q=1 cos-sim \u03c0 p ,\u03c0 q , where P and Q are the total number of labels in the TI and VAD dataset, \u03c0 p and \u03c0 q are the word2vec representation of the p-th and q-th label of the TI and VAD dataset, respectively. cos-sim(\u2022) denotes the cosine similarity operation on input vectors. A value of S closer to 0 indicates a higher degree of irrelevancy (or lower degree of relevance). In Fig. 4, we show the mean cosine similarity S for all the TI (i.e., HMDB, UCF101, Jester) and VAD (i.e., SHT, Ped1/Ped2, UCFC, Ave) datasets used in this paper. Following reference values: S(\"object\", \"scene\") = 0.829, S(\"bag\",\"dime\") = 0.209, and S(\"fox\",\"paper\") = 0.109, we find that the maximum semantic similarity S =0.207 occurs between Ave and HMDB indicating that all the TI datasets are quite irrelevant to the task of VAD problem. Methods to use TI datasets. We provide two methods to use TI datasets. Firstly, unsupervised VAD methods learn features from normal events during training. These events are particularly marked by continuous activities without any sudden disruption from alien objects. Such kinds of videos are readily available in other video downstream tasks like action recognition, where a sample video only contains frames from a continuous activity. In cases where there is no VAD training data available in the source domain (worst-case scenario), we show later in Sec. 4 that training zxVAD solely with TI datasets reaches the SOTA results across 3 different target domain datasets. We hypothesize that TI datasets represent the recording of normal activities as in VAD training data with normal videos. Hence, learning from such TI data helps in modeling features similar to normal videos. Secondly, we recommend using TI frames to create anomalies containing diverse types of objects (see Fig. 3). Using our proposed method to create pseudo-anomaly frames using TI data (details in Sec. 3.1), our generator learns features from normal frames relative to abnormal frames. Such pseudo-anomalies contain diverse foreign entities extracted from TI video frames allowing our generator to learn relative normalcy difference in a broad manner."
            ],
            "subsections": []
        },
        {
            "title": "Experiments and Results",
            "paragraphs": [
                "Implementation details. We implement our framework in PyTorch [95]. The generator G(\u2022) is an U-Net [96] adapted from [3] with a memory module at its bottleneck similar to [20]. The discriminator D(\u2022) and normalcy classifier N (\u2022) are Patch-GAN discriminators [97]. We provide more details of our implementation in Supplementary Material.",
                "Evaluation details. We evaluate zxVAD under three training scenarios with respect to types of available source data: (1) Both VAD and TI data are available: G(\u2022) takes VAD videos and O takes TI frames as input, (2) Only one of the VAD or TI data are available: Both G(\u2022) and O take VAD or TI videos as input. We did not observe any performance gain empirically when G(\u2022) takes both VAD and TI videos as input, so we drop this case as it adds a computational burden. We compare zxVAD with [1,2] using the area under ROC curve (AUC), model storage, total parameters, GPU energy consumption, inference time FPS, and GMACs.",
                "Baselines. Since the problem of 'cross-domain VAD without target domain training data adaptation' is identified by us, we cannot find other methods which are designed for such a setup. The latest and closest baselines we found are rGAN [1] and MPN [2], which are designed for the xVAD task without needing strong priors from VAD frame object extraction. Since both methods report their performance under the proposed problem setup, we use them as our baselines. Even though we outperform strong prior based xVAD methods [47,48,57] without any such computationally expensive operation under our problem setup, we do not consider them as part of our baselines for fair comparisons with respect to [1,2]. In Tab. Ablation study. We show the ablation study of our proposed loss functions in zxVAD in Fig. 5(a) on the SHT dc dataset. Fig. 5(a) shows that each of our proposed loss functions contributes to the AUC, and jointly training with them all achieves the best AUC. In Fig. 5(b), we analyze different combinations of an autoencoder and generative adversarial network with (AE-M,GAN-M) and without our memory module (AE, GAN) as our zxVAD backbone. In Fig. 5(c), we analyze the impact of different mixing strategies (MixUp [99], CutMix [100] within our module O and compare with recent SOTA strategy called Patch [66] that proposes a pseudo-anomaly method. In Fig. 5(d), we analyze impact of changing R(\u2022) with (ResNet50, ResNet152 [78], DenseNet161 [101], AlexNet [102], MnasNet [103]) in zxVAD. Fig. 5 shows that regardless of the backbone choice, the pseudo-anomaly strategy, and the architecture of R(\u2022), zxVAD still outperforms the SOTA baselines in most settings, which supports that zxVAD is flexible w.r.t. these factors. Same-dataset experiments. We compare zxVAD with [1,2] on the SHT dc , SHT and Ped2 datasets. Tab. 3 shows that zxVAD outperforms both baselines in AUC in such experiment. For example, zxVAD shows better generalization ability across different camera angles than the baselines in the SHT dc dataset with the least efficiency metrics like model parameters and GMACs. We also find that using extra TI data (HMDB and UCF101) can improve the AUC further compared to baselines (results in Supplementary Material).",
                "Cross-dataset experiments. We compare zxVAD with [1,2] under the cross-dataset setting. In the top two sections of Tab. 4, we train zxVAD with either the SHT or UCFC dataset with optional TI data and test it on the Ped1, Ped2, and Ave datasets. Tab. 4 shows that zxVAD outperforms both baselines in AUC under most settings, regardless of whether the extra TI data are used, which supports that zxVAD has better generalization ability across different datasets (with different types of anomalies under different scenes) than the baselines. For example, when our model is trained on the SHT dataset [16], it outperforms existing xVAD methods in the proposed problem setup in detecting anomalies like \"chasing\" and \"brawling\" in SHT's test set as well as anomalies like \"bicycles\" and \" cars\" in Ped1/Ped2's test set without performing any kind of adaptation on Ped1/Ped2's training set. This shows that our method is not specific to anomalies in the source domain, but generalizes well to target domain scenes during inference without adaptation. Tab. 5 shows that even without using any source domain VAD training data at all, zxVAD still outperforms [1,2] in most settings by training with only TI data, which supports our proposed mechanism of using the TI data under the proposed problem setup. These encouraging results suggest that making use of TI data is a promising research direction for the zxVAD problem. Interestingly, when either G or O or both use TI data, it's not surprising to see slightly lower AUC than if both G and O use VAD relevant data, i.e. more relevant source data lead to less source-target domain gap, resulting in better AUC. This is confirmed by average AUC (Tab. 4 and 5) when source is only VAD: 84.26%, VAD w/ TI: 83.46%, and only TI: 82.30%. We also analyzed the impact of the amount videos needed when solely training with TI data with HMDB and UCF101 in zxVAD setup and found that even as little as \u223c1.25% of UCF101 or \u223c8% of HMDB is enough to outperform the SOTA (details in Supplementary Material).",
                "Following [1], we do not perform a cross-domain evaluation with Ped1/Ped2 as a source as the training dataset is too small to make reasonable conclusions. In Fig. 6, we show that zxVAD outperforms existing strong prior based unsupervised xVAD methods [47,48,57] that report cross-domain VAD testing performance when source domain data is SHT. This implies that zxVAD provides a computationally efficient and reduced supervision approach with no need for object extraction from videos (using YOLOv3 [49] in [48,57] and CenterNet [104] in [47]) both in source and target domain, under the proposed problem setup. Compared to [48] (in Fig. 6) and [66] (in Fig. 5(c)), our untrained CNN based abnormal example generation strategy results in superior VAD for the proposed problem setup.",
                "Our \"relative normalcy\" learning approach optimizes the VAD model to learn features that differentiate normal events from (pseudo)-abnormal events, rather than focusing on learning only patterns of normal events as in prior xVAD works. Results in Tab. 5 (when zxVAD uses only TI data) validate this claim as zxVAD still outperforms SOTA on target VAD by learning such differentiating features from TI videos. [105] is a few-shot VAD method that puts together three off-the-shelf pre-trained models (YOLOv4 [106], AlphaPose [107], Flownet2 [108]) to perform xVAD. Even with such costlier storage, high training overhead, and strong priors from different distributions, zxVAD easily beats [105] by 11.76% (Ave), 13.85% (Ped2) with source as SHT, and 10.12% (Ave), 29.12% (Ped2) with source as UCFC with extremely less parameters and no initial priors. Finally, we provide qualitative evaluation under the cross-domain setting with anomaly curves of two testing videos of Ped1 and Ped2 when trained with SHT in Fig. 7, where zxVAD provides better cross-domain detection ability than MPN [2]. We also visualize difference maps in Fig. 8 (absolute error between ground truth and the predicted frame) that indicate the presence of anomalies by zxVAD in three datasets under cross-domain setting after training with     SHT. We show more such qualitative results of zxVAD in Supplementary Material. In addition to the above, zxVAD achieves such results with much better inference-time efficiency than the baselines. Tab. 3 shows that zxVAD outperforms [1,2] in model size, total parameters, GPU energy consumption (computed by pyJoules [109] following [110,111]), and GMACs by 34.3%, 31.3%, 36.1%, and 21.76%, respectively."
            ],
            "subsections": []
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We  domain VAD training data are available or not. Our results demonstrates the potential of task-irrelevant data as a promising direction for addressing the xVAD problem. As part of our future work, we will extend our method to enhance it's ability in directly localizing the anomaly in the videos. Another important aspect of VAD models to explore is sensitivity to input manipulations [112][113][114][115] in order to test their robustness. Additional Implementation Details. We implement zx-VAD in PyTorch [95]. We resize the input frames to 256 \u00d7 256 and normalize them to the range of [-1,1]. The generator, the discriminator, and the normalcy classifier are trained with the learning rates of 0.0002, 0.00002, and 0.00002, respectively with the Adam [116] optimizer (\u03b21 = 0.5,\u03b22 = 0.999), following [3]."
            ],
            "subsections": []
        },
        {
            "title": "A. Additional Details for zxVAD",
            "paragraphs": [
                "The generator takes 4 frames as input and outputs one frame. We drop the last sigmoid layer of N (\u2022) as suggested in [117]. We extracted the frames of all TI datasets at 30 frames/sec. ",
                "Here, b2 >b1 and b4 >b3. We then resize Mx and M to size (b2-b1)\u00d7(b4-b3). Finally, only the pixels corresponding to regions where M (i,j) =1 are replaced in v to create anomaly frame \u1e7d. To handle boundary conditions where 0\u2264bx,bw \u2264W and 0\u2264bx,bw \u2264H, we clip the values to be in the range of [0,W ] and [0,H], respectively. Here, \u03b2 \u223cUnif 0,1 .",
                "Evaluation criteria. For anomaly scores, we follow [2,3] and compute Peak Signal to Noise Ratio (PSNR) [76] scores per frame and normalize PSNR of all frames in each testing video to the range [0, 1] in order to compare with ground-truth binary labels. Note that we observed such normalization practice (adopted from [3]) impacts anomaly scores."
            ],
            "subsections": []
        },
        {
            "title": "B. Additional Results on zxVAD",
            "paragraphs": [
                "Impact of the amount of TI Data. We analyzed the impact of the amount of TI data on our zxVAD framework in extreme settings.",
                "Particularly, we evaluated zxVAD when the amount of videos of TI datasets (HMDB and UCF101) is close to the number of training videos available in the VAD datasets. With 0.5%, 1%, 2%, 4%, and 8% of HMDB data, we observed an average cross-domain AUC performance of 74.99% on Ped1, 93.82% on Ped2, and 79.49% on Ave. A similar observation was made on UCF101 (0.0625%, 0.125%, 0.315%, 0.63%, and 1.25% of data resulted in average cross-domain AUC performance of 74.61% on Ped1, 94.17% on Ped2, and 79.46% on Ave). This demonstrates that almost SOTA cross-domain performance on the current VAD datasets is achievable even with an extremely low amount of TI data.",
                "Relevancy among VAD data. We followed [92,93] for the relevancy analysis between the TI to target domain (Ave, Ped1/2) VAD data. We observed higher relevancy scores among SHT (to Ave: 0.241, to Ped1/2: 0.250) and UCFC (to Ave: 0.201, to Ped1/2: 0.167) compared to average TI (to Ave: 0.186, to Ped1/2: 0.138). This confirms: TI data is indeed less relevant to VAD data.",
                "More results on the impact of randomly initialized networks for Pseudo-Anomaly Synthesis. We analyzed the impact of the randomly initialized network R(\u2022) on our untrained CNN based pseudo-anomaly synthesis module. In Fig. 1, it can be observed that our zxVAD method outperforms the state-of-the-art (SOTA) xVAD works on the Ped1 and Ped2 datasets in the zero-shot settings when the source is SHT irrespective of kind of randomly initialized network R(\u2022) employed to extract objects from all our TI datasets. Ablation analysis. zxVAD is not too sensitive to the loss ratios and Table (on right) validates this point. For our backbone GAN, we use exact same ratios as suggested in [3]. For the proposed normalcy classifier, we do not use ratios for our losses LAA and LRAA (i.e. set as 1). Finally, the effect of ratios \u03b1n on LN and \u03b1rn on LRN is shown. All cases show better AUC than SOTA MPN [2]."
            ],
            "subsections": []
        },
        {
            "title": "C. Examples from Datasets",
            "paragraphs": [
                "We provide some video examples of the VAD datasets (SHT, UCFC, Ped1, Ped2, and Ave in Fig. 2  More results on same-dataset testing. We beat our baselines in the same-dataset testing in all VAD and TI combination scenarios as shown in Tab. 2. We also compare with more state-of-the-art unsupervised VAD methods under the same-dataset setting in Tab. 3.    "
            ],
            "subsections": []
        },
        {
            "title": "D. More Qualitative Results",
            "paragraphs": [
                "We show additional examples of pseudo-abnormal frames created using our pseudo-anomaly module in Fig. 3 and difference maps from three different datasets indicating anomalies in Fig. 4.  "
            ],
            "subsections": []
        }
    ]
}