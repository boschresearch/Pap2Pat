{
    "id": "https://semopenalex.org/work/W4385565212",
    "authors": [
        "Angelo Fabbri",
        "Wenhao Liu",
        "Jesse Vig",
        "Prafulla Kumar Choubey",
        "Chien-Sheng Wu",
        "Nazneen Fatema Rajani"
    ],
    "title": "CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization",
    "date": "2023-01-01",
    "abstract": "Hallucination is a known issue for neural abstractive summarization models. Recent work suggests that the degree of hallucination may depend on factual errors in the training data. In this work, we propose a new method called Contrastive Parameter Ensembling (CaPE) to use training data more effectively, utilizing variations in noise in training samples to reduce hallucination. Starting with a base model fine-tuned on an entire dataset, we additionally train expert and anti-expert models on clean and noisy subsets of the data, respectively. We then adjust the parameters of the base model by adding (subtracting) the parameters of the expert (anti-expert), advancing the recent work on additive parameter ensembling approaches. Trained on a much smaller data subset, expert and anti-expert models only fractionally (<14%) increases the total training time. Further, CaPE uses parameter ensembling and does not increase the inference time. Experimental results show that CaPE improves performance across different automatic factual metrics and human evaluation, with a maximum improvement of 16.69% and 15.38% on summary-level dependency-arc entailment accuracy for the XSUM and CNN/DM datasets. The CaPE model performs comparably to the base model on metrics of informativeness such as ROUGE.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Neural abstractive summarization systems have been shown to generate plausible summaries with high lexical overlap with the references. However, human analyses (Fabbri et al., 2021a;Pagnoni et al., 2021;Tejaswin et al., 2021) and automatic evaluations (Falke et al., 2019;Kryscinski et al., 2020;Maynez et al., 2020;Durmus et al., 2020) show that state-of-the-art models trained on widely used XSUM (Narayan et al., 2018) and CNN/DM (Hermann et al., 2015) datasets tend to hallucinate information with high frequency. The degree of a \u2020 work was done at Salesforce AI Research.  model's hallucinations further correlate with the quality of training data (Aralikatte et al., 2021;Pagnoni et al., 2021). For instance, models trained on the XSum data tend to generate a higher proportion of factual errors as compared to models trained on the CNN/DM dataset.",
                "Given the association between training data quality and hallucinations in resulting models, the easiest method to reduce hallucinations is to remove noisy samples from the training data (Nan et al., 2021). However, data filtering reduces the size of training data and consequently the diversity in target summary since the removed noisy samples might also include useful task-specific knowledge. This impacts other aspects of generated summaries such as information recall or fluency. In Table 1, we show ROUGE (R-1/2/L) and named entity recall (E-R ref ) scores of a BART model (Lewis et al., 2020) trained on the entity precision-filtered XSUM data (24.6% of the original data). The new model drops 8-18% in ROUGE and 20% drop in entity recall.",
                "In this work, we design a simple yet effective strategy to utilize both clean and noisy training samples. We use the observation that \"the level of hallucination in summarization model correlates with the level of noise in training data\". Specifically, a model trained by maximizing the likelihood of a reference summary given its source document learns the hallucinations in training data. Therefore, we use an automatic factual metric to select clean data samples without any factual errors and fine-tune a base summarization model, which is trained on all data, to obtain an expert. Similarly, we select noisy data samples that contain abundant factual errors and fine-tune the base summarization model to get an anti-expert. The difference in factual qualities of data used to train our expert and anti-expert makes the anti-expert hallucinate more than the expert.",
                "Next, we adjust base model's parameters by combining it with expert and anti-expert. Typically, when we have many models, a straightforward approach to combine them would be to take a weighted average of their output (Jacobs et al., 1991;Liu et al., 2021a). However, this requires running each model separately and increases computational cost linearly in the number of models, further slowing the auto-regressive generation of summaries. Alternatively, Madotto et al. (2020) proposed attention over parameters that jointly optimizes multiple models and directly combines all their parameters through learned attention coefficients. Furthermore, Wortsman et al. (2021) recently shows that average of a pre-trained CLIP (Radford et al., 2021) model with its another version that is further fine-tuned on a new data distribution performs better than both models on their complementary distributions. Motivated by these findings and the fact that the anti-expert possesses undesirable behavior, we propose Contrastive Parameter ensembling (CaPE), a generalization of parameter averaging, which adds the expert's and subtracts the anti-expert's parameters (equivalent to adding the difference between expert's and antiexpert's parameters) from the base model.",
                "We evaluate our CaPE model on two benchmark abstractive summarization datasets, XSUM and CNN/DM. We train an expert and an anti-expert corresponding to each of the dependency-arc entail-ment (Goyal andDurrett, 2020, 2021) and entity overlap (Nan et al., 2021) metrics. Then, we combine each expert and anti-expert pair to obtain four variants of CaPE and evaluate them using the metrics used for data selection as well as a different entailment metric, MNLI (Williams et al., 2018), and two question answering-based metrics, QuestEval (Scialom et al., 2021) and QAFactEval (Fabbri et al., 2021b), for factual consistency. We find that all variants of our CaPE consistently outperform the state-of-the-art models on all factual metrics, with marginal variations in ROUGE scores and information recall."
            ],
            "subsections": []
        },
        {
            "title": "Contrastive Parameter Ensembling",
            "paragraphs": [
                "In this work, we propose Contrastive Parameter Ensembling (CaPE) for reducing hallucinations in text summarization systems. This method refines a base summarization model by training two additional models: an expert model, which is trained on the subset of data with the highest factual consistency, and an anti-expert model, trained on the subset of data with the lowest factual consistency. An ensemble model is then constructed through a simple linear combination of the parameters of the three models, an approach inspired by recent work on weight (a.k.a. parameter)-space ensembling (Izmailov et al., 2018;Frankle et al., 2019;Neyshabur et al., 2020;Wortsman et al., 2021)."
            ],
            "subsections": [
                {
                    "title": "Measuring Hallucinations for Selecting",
                    "paragraphs": [
                        "Training Data",
                        "To select data for training the expert and antiexpert, we assume the availability of automated metrics for measuring hallucinations in reference summaries. There are several automatic metrics to evaluate factual consistency such as entity overlap (Nan et al., 2021), entailment score (Kryscinski et al., 2020;Goyal and Durrett, 2020;Maynez et al., 2020), and QA-based metrics (Durmus et al., 2020;Scialom et al., 2021). These methods vary greatly in computational cost and agreement with human judgements for factuality. We use the two of the faster metrics that are based on entity overlap and entailment metrics, and have shown good correlation with human-based evaluations, described below.",
                        "Entity Overlap is the simplest method measuring token-level overlap of the named entities, between the summary and source document (Nan et al., 2021). We use entity token overlap precision (E-P src ), the percentage of named-entities tokens in the summary that are also present in the source. This metric can be used as a proxy to measure simpler cases of hallucinations, such as outof-article entity errors (Pagnoni et al., 2021), also known as extrinsic hallucinations (Maynez et al., 2020). A human study by Pagnoni et al. (2021) finds this to be the most frequent form of error in models trained on XSUM data. However, it fails to capture intricate cases of hallucinations such as semantic frame errors (e.g., when an entity is present in the source but is attributed to the wrong predicate).",
                        "DAE (Dependency Arc Entailment) measures fine-grained entailment by breaking the summary into smaller claims defined by dependency arcs, covering errors such as incorrect predicates or their arguments, coreference errors, discourse link errors, in contrast to the simpler token-level entity overlap. Dependency arcs define grammatical structures in a sentence and often describe semantic connections between words, such as predicateargument relations (Mel'\u010duk, 1988). Pagnoni et al. (2021) finds that DAE correlates with the human judgment of factuality, and has the highest correlation with complex discourse errors, such as entity coreference. Therefore, we use DAE errors (the number of dependency arcs in summary that are not entailed by the source document) to identify cases of more intricate hallucinations for selecting training data."
                    ],
                    "subsections": []
                },
                {
                    "title": "Expert and Anti-expert based Model's Parameters Adjustment",
                    "paragraphs": [
                        "Using the entity overlap or DAE error metrics, we select samples for training expert and anti-expert models that are then used to adjust the base model parameters. The data selection strategy, SELECT-CLEAN (SELECTNOISY), and the generic process for building CaPE are described below and further illustrated in Algorithm 1."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "SELECTCLEAN (SELECTNOISY):",
            "paragraphs": [
                "For the entity overlap metric, we select clean (noisy) samples with entity precision above (below) a predefined threshold E-Psrc clean ( E-Psrc noisy ). For DAE error metric, we select clean (noisy) samples with the number of DAE errors below (above) a predefined threshold Fine-tuning Expert (Anti-expert) We train a base summarization model using all training data, and then fine-tune this model on the clean dataset to obtain the expert and on the noisy dataset to obtain the anti-expert. By training on the full data followed by fine-tuning on clean (noisy) subset, we want our expert (anti-expert) model to retain other aspects such as ROUGE and information recall of the base model, and only differ in the factual qualities. As noted in Table 1, this is in contrast to training a BART model on just clean (or noisy) samples that severely deteriorates ROUGE and information recall (analyzed further in \u00a7 4.3).",
                "Finally, for a mixing coefficient \u03b1, we obtain our Contrastive Parameter Ensembled model (\u03b8 CaP E ) from base (\u03b8 B ), expert (\u03b8 E ) and anti-expert (\u03b8 \u0112 ) parameters following:",
                "The mixing coefficient (\u03b1) balances factual quality with other aspects of summarization such as ROUGE and information recall.",
                "Initializing the expert (anti-expert) from the base or BART model is critical; prior work (Izmailov et al., 2018;Frankle et al., 2019;Neyshabur et al., 2020) has shown that parameter-averaging works well when all constituent models share the same optimization trajectory. On the other hand, averaging parameters of disjointly trained deep neural models, starting from different initializations, may not work better than a model with randomly assigned parameters. Since both methods of fine-tuning and training have a common initialization, the resulting CaPE model exhibits performance comparable to the base model or expert."
            ],
            "subsections": [
                {
                    "title": "CaPE: A generalization of WiSE-FT",
                    "paragraphs": [
                        "Contrastive Paremeter Ensembling generalizes the recently proposed WiSE-FT (Eq. 1) model (Wortsman et al., 2021), which only performs a weighted sum of a base model and a single fine-tuned model, for ensuring distributional robustness on image classification.",
                        "where the anti-expert is a null (base) model. We believe Eq. 1 a sub-optimal solution for our objective of minimizing factual errors. Being trained on the noisiest subset of the training data, the anti-expert model hallucinates with higher frequency than the base and expert models, removing parameters responsible for hallucinations more than the other two. We empirically find that our proposed contrastive ensembling outperforms the models that just use one of the expert or anti-expert in \u00a7 4.4.",
                        "3 Results"
                    ],
                    "subsections": []
                },
                {
                    "title": "Experimental Setup",
                    "paragraphs": [
                        "We evaluate CaPE on the XSUM (Narayan et al., 2018) and CNN/DM (Hermann et al., 2015) datasets. The XSUM data is highly abstractive and noisy. On the other hand, CNN/DM is more extractive and contains fewer factual errors (Tejaswin et al., 2021). These data variations allow us to evaluate CaPE under different data quality settings.",
                        "Besides the standard ROUGE-1/2/L (R1/R2/RL) scores, we use a diverse set of metrics for evaluating factual consistency and summary quality.",
                        "\u2022 D arc measures the percentage of dependency arcs in summary that are entailed by the source article.",
                        "\u2022 D sum measures the percentage of summaries that do not have any dependency arc error.",
                        "\u2022 E-P src measures the percentage of entities in summary that are present in the source article.",
                        "\u2022 E-R ref measures the percentage of entities in reference that are also present in the generated summary.",
                        "\u2022 BS-P (R) represents the BERTScore (Zhang et al., 2019) precision (recall) w.r.t. the source article.",
                        "\u2022 QEval represents a QA-based factual consistency metric (Scialom et al., 2021).",
                        "\u2022 MNLI measures the entailment score based on the RoBERTa large (Liu et al., 2019) model trained on MNLI dataset (Williams et al., 2018).",
                        "The score of a summary sentence is the maximum entailment score over all input sentences, and the final score is averaged across summary sentences as in Laban et al. (2022).",
                        "\u2022 QAFactEval is another QA-based factual consistency metric that improves question filtering and answer overlap components (Fabbri et al., 2021b)."
                    ],
                    "subsections": []
                },
                {
                    "title": "Models",
                    "paragraphs": [
                        "We use the BART-based summarization (BART sum ) models released with Huggingface's transformers library (Wolf et  We train an expert (anti-expert) for each of the DAE error (Exp DAE (Anti DAE )) and entity token overlap precision with source (Exp E-P (Anti E-P )) metrics. We evaluate four variants of CaPE. CaPE P P uses Exp E-P and Anti E-P , CaPE DP uses Exp DAE and Anti E-P , and likewise. Depending on the value of \u03b1, CaPE may reduce ROUGE or information recall while improving the factual consistency. Therefore, for each variant of CaPE, we select the \u03b1 such that it does not under-perform the base model by more than 1% on ROUGE 1 (R1) and entity recall (E-R ref ) metrics on the validation set.1 ",
                        "Baselines: We compare CaPE with two summarization baselines, BART sum (a.k.a. base) and an ensemble of BART-based summarization models, and three post-processing (PP) based models for improving factual consistency. Similar to CaPE, the ensemble model uses the average of a base summarization and two other summarization models obtained by fine-tuning the base model on two randomly sampled subsets of the training data. For post-processing based models, we implement a variation of the autoregressive fact correction model from Dong et al. (2020); we train a BARTlarge model to produce the reference summary conditioned on the concatenation of the source and reference summary with all entity slots masked.   We call this model PP and train a variation of it on the subset of data with an entity precision of 100 (PP-clean). We also apply the model from Chen et al. ( 2021), called PP-CC, that generates candidate summaries by enumerating all ways to replace entities in the summary with entities of similar type in the input and training BART with an additional classification layer to re-rank these summaries."
                    ],
                    "subsections": []
                },
                {
                    "title": "Automatic Evaluation",
                    "paragraphs": [
                        "Table 2 summarizes the results on the XSUM and CNN/DM datasets. First, we find that ensembling multiple summarization models improves ROUGE scores, BERTScore recall and entity recall, but not necessarily factual consistency metrics. On the other hand, all variants of CaPE outperform the base as well as ensemble across all factual consistency metrics on both the XSUM and CNN/DM datasets. Given the controllability achieved by \u03b1, we ensure that all variants of CaPE preserve ROUGE scores and information recall within a predefined threshold of maximum 1% drop from the base model. We also find that CaPE models im-prove BERTScore precision (BS-P) with respect to the source article on both XSUM and CNN/DM. This is interesting given recent work on benchmarking different evaluation metrics that suggests that BERTScore precision with respect to the source document correlates with the human judgment of factuality (Pagnoni et al., 2021). CaPE models also outperform the postprocessing based approaches PP and PP-CC on XSUM and all three PP, PP-clean and PP-CC approaches on CNN/DM dataset with significant margin. However, PP-clean performs similar to CaPEs on factual consistency metrics on XSUM and even obtains a higher E-P src score of 72.98. At the same time, PP-clean lowers the performance on ROUGE and information recall, reducing E-R ref performance by \u223c15%. Fortunately, we can set the mixing coefficient \u03b1 in CaPE to a higher value, achieving higher factual consistency at the cost of reduced ROUGE and information recall. To confirm this, we also report the performance of CaPE DP * on XSUM data which uses Exp DAE and Anti E-P mixed with \u03b1 value of 1.0 (underlined results in Table 2). We find that CaPE DP * obtains much higher score than PP-Clean model on all factual consistency metrics, while competently retaining the information recall of the base model (E-R ref reduced by 3.5% compared to \u223c15% drop for PP-clean).",
                        "Finally, in Table 3, we compare CaPE DP (the variant of CaPE with the best trade-off, discussed in \u00a74.2), base and PP-clean models using two additional metrics, QAFactEval and MNLI. As noted by Fabbri et al. (2021b), prior studies comparing factual metrics draw inconsistent conclusions, with a few observing QA-based metrics as superior to entailment metrics (Durmus et al., 2020;Scialom et al., 2021) and others reporting the opposite (Maynez et al., 2020). To the best of our knowledge, QAFactEval performs the best on the SummaC benchmark (Laban et al., 2022), used for comparing factual consistency metrics. On both metrics, we find that CaPE DP outperforms both base and PP-clean models, improving the QAFactEval score by 4.8% and 1.14% over base model on XSUM and CNN/DM, respectively."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Transferability of Experts (Anti-experts):",
            "paragraphs": [
                "We observe that CaPE models also improve performance on the metrics that were not used for training the expert or anti-expert. For instance, CaPE P P outperforms base model on the D arc /D sum metrics, and CaPE DD outperforms base model on the E-P src metrics on both XSUM and CNN/DM. All variants of CaPE also outperform base model on QEval, QAFactEval and MNLI, which were also not used during the development of experts (antiexperts). Secondly, we find that the experts and anti-experts are interchangeable, an expert trained on data selected using one metric can be used in conjunction with an anti-expert based on another metric. As evident, both CaPE DP and CaPE P D outperform base model, with CaPE DP achieving best trade-offs among other variants of CaPE on the XSUM data, discussed further in \u00a74.2.",
                "Computational Efficiency: We also report the approximate training (TT) and inference (IT) time for different models relative to the base model in Table 2. We exclude the time required for data processing (e.g. data selection for CaPE and PP-Clean during training, or entity recognition for all post-processing based models both during training and inference). We find that CaPE models only marginally increase the training time (\u226414%) required for fine-tuning expert (anti-expert) on a smaller selected subset of training data. Further, CaPE models do not increase the inference time. In comparison, post-processing methods use separate models for correcting summaries generated by the base model, increasing the memory required to store the additional model as well as both the training and inference time."
            ],
            "subsections": [
                {
                    "title": "Human Evaluation",
                    "paragraphs": [
                        "Following Cao and Wang (2021), we also perform pairwise comparison of summaries, where human annotators rate each CaPE DP generated summary against the base model generated summary for factual consistency. We rate 100 random articles from each of the XSUM and CNN/DM datasets. The inter-annotator agreement is 0.8385 (Krippendorff, 2011)  In Table 4, we compare the performance of individual expert and anti-expert models on DAE-and entity-based metrics. Our key findings include: An expert reduces hallucinations in generated summaries. We find that all experts, except the entity-based expert (Exp E-P ) on CNN/DM, are able to achieve improved performance on the metric used for selecting the training data subset. The unchanged performance of Exp E-P on CNN/DM is unsurprising given the base model is consistent against out-of-article entity error on CNN/DM dataset (E-P src of 98.44) and has very small room for improvement. This aligns with findings from human evaluation that the base model has very few extrinsic entity errors (Pagnoni et al., 2021). On   the noisy XSUM data, we observe that the improvement for experts are not limited to the metrics used for data selection. For instance, Exp DAE improves entity precision (E-P src ) by \u223c6% and Exp E-P improves D arc and D sum by \u223c3-4%.",
                        "An anti-expert increases hallucinations in generated summaries. All anti-experts reduce performance on factual consistency metrics for both the XSUM and CNN datasets, with the maximum drop seen on summary-level D sum metric, indicating that a greater proportion of anti-expert generated summaries are hallucinated. At the same time, they generate well-formed summaries, as indicated by their maintained ROUGE scores. This is the desirable behavior for an anti-expert that should generate hallucinated but well-formed summaries."
                    ],
                    "subsections": []
                },
                {
                    "title": "Effects of Mixing Coefficient \u03b1",
                    "paragraphs": [
                        "We combine expert and anti-expert pair with the base model using different mixing coefficients (\u03b1) and plot their performance on the XSUM and CNN/DM datasets in Figure 2 and 3. We choose to vary \u03b1 from 0.0 to 1.0. We compare models on the D arc /D summ , E-P src /RT, ROUGE 1 metrics. In addition, we compare the average summary length to capture artifacts introduced by data selection. We observe:",
                        "Inter-mixing the expert and anti-expert based on different metrics provides the best performance trade-offs. CaPE DD , which uses the DAE-based expert and anti-expert, improves D arc /D summ accuracy at the fastest rate on both datasets. Likewise, CaPE P P improves entity precision, E-P src , at the fastest rate. CaPE DP and CaPE P D models that inter-mix the expert and antiexpert based on different metrics provide the best bargain on all factual consistency metrics, evenly improving all D arc /D sum and E-P src scores. On the ROUGE score, we do not find any uniform pattern between the two datasets. On XSUM, all CaPE variants exhibit similar behavior while on CNN/DM, CaPEs using the entity precision-based anti-expert (CaPE P P/DP ) retain ROUGE better than their alternatives. Similarly, CaPE P P/DP retain entity recall better than their alternatives for all values of \u03b1s on both datasets. Overall, CaPE DP provides the best balance for all performance measures on both datasets.",
                        "Average summary length of data subset used for training expert (anti-expert) influences the length of CaPE-generated summaries. On XSUM data with shorter summaries, CaPE models tend to reduce the length of summary with increasing \u03b1. Contrarily, on CNN/DM data with more extractive and longer summaries, models increase the average length of the summary with the increase in \u03b1. From our initial analysis, as shown in Table 5, this association can be explained by the average size of summaries in the data subset used for training expert (anti-expert). Specifically, CaPE DD/DP models see a maximum increase in the summary length on the CNN/DM  "
                    ],
                    "subsections": []
                },
                {
                    "title": "(Anti-)Expert Initialization: A Base Summarization Model outperforms BART",
                    "paragraphs": [
                        "In Figure 4, we compare the performance of CaPE P P models using expert (anti-expert) obtained by fine-tuning base summarization and training BART model. First, we find that both models improve performance on all factual consistency metrics. On the E-P src metric, which was also used to select the training samples, both models obtain comparable improvement. Contrarily, the base model already yields higher ROUGE score and fine-tuning it for 1 epoch is sufficient to reduce hallucinations, making fine-tuning a more efficient approach for building experts (antiexperts)."
                    ],
                    "subsections": []
                },
                {
                    "title": "CaPE outperforms Simple Parameter Ensembling (WiSE-FT)",
                    "paragraphs": [
                        "In Figure 5, we compare the CaPE P P model with the expert (anti-expert) only model that replaces the anti-expert (expert) with the base model in \u03b8 CaP E . Accordingly, the expert only model is equivalent to the WiSE-FT formulation (\u03b8 W iseF T ). While both the expert only and anti-expert only improve performance on factual consistency metrics, we observe that CaPE P P improves performance at a faster rate than the former two models. On ROUGE-1 and E-R ref scores, the CaPE P P performance lies in be- tween the expert only and anti-expert only models.",
                        "The performance variations for the three models indicate that the contrastive ensembling combines the gains from expert and anti-expert, helping us to effectively use both clean and noisy data."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Abstractive text summarization metrics such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019) evaluate lexical and semantic overlap respectively but fail to sufficiently evaluate factuality and faithfulness (Tejaswin et al., 2021). This has led to a line of research dedicated to evaluating factual consistency and hallucination in text summarization using new metrics such as entailment and question answering-based evaluation (Falke et al., 2019;Kryscinski et al., 2020;Maynez et al., 2020;Zhou et al., 2021;Eyal et al., 2019;Scialom et al., 2019;Wang et al., 2020;Durmus et al., 2020;Scialom et al., 2021). Research focused on comparing these factual consistency evaluation metrics (Gabriel et al., 2021;Fabbri et al., 2021a;Pagnoni et al., 2021;Goyal and Durrett, 2021;Tejaswin et al., 2021), however, often have contradicting observations. For instance, Durmus et al. (2020) found that entailment-based automated metrics have lower correlation with factual consistency while Pagnoni et al. (2021) concluded that the entailment-based FactCC exhibits the highest correlations with human judgments of factual consistency. Given the variations in findings from different human analyses of popular factual consistency evaluation metrics, we select a few metrics from each of the entailment, entity overlap, and QA-based evaluations, as well as use ROUGE and BERTScore metrics for evaluating CaPE.",
                "Along with the growing body of work on analysis and evaluation of factual consistency, there has been some recent work on developing methods to enforce factual consistency in pre-trained language models. These include sampling techniques such as constrained decoding (Mao et al., 2020) and neurologic decoding (Lu et al., 2020). Another strategy is to control generation either by using language models to guide a base language model as in GeDi (Krause et al., 2020) and DExperts (Liu et al., 2021a) or via a hallucination knob (Filippova, 2020). Although these methods claim to be generic, they have not been successfully applied to constrain summary generation on the source document.",
                "Comparatively, there are fewer papers that propose methods for factual consistency in text summarization. Most of these focus on posthoc correction such as SpanFact (Dong et al., 2020), contrast entity generation and selection (Chen et al., 2021), loss truncation (Kang and Hashimoto, 2020;Goyal and Durrett, 2021), and encoding SRL structure (Cao et al., 2020). Aralikatte et al. (2021) uses focus attention and sampling to improve the diversity and faithfulness of summaries while Liu et al. (2021b) uses data augmentation with a contrastive loss for factual consistency of abstractive summarization applied to customer feedback.",
                "Finally, works focusing on data noise include revising hallucinated summaries in training data (Adams et al., 2022), dropping hallucinated samples (e.g. Nan et al. (2021) and Narayan et al. (2021) for summarization, Matsumaru et al. (2020) for headline generation), or defining curriculum based on the factual quality of training samples (Kano et al., 2021)."
            ],
            "subsections": []
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We present Contrastive Parameter Ensembling (CaPE) to reduce content hallucinations in abstractive summarization models. We first select clean (noisy) training samples to fine-tune an expert (antiexpert) model. Then, we use the difference between the parameters of expert and anti-expert models to adjust the parameters of a base summarization model. We evaluate CaPE on the XSUM and CNN/DM datasets using a diverse set of factual metrics, finding that CaPE effectively reduces hallucinations without a significant drop in ROUGE and information recall."
            ],
            "subsections": []
        },
        {
            "title": "A Experimental Details",
            "paragraphs": [
                "Training Experts (Anti-experts): We use Huggingface Transformers library (Wolf et al., 2020) (PyTorch (Paszke et al., 2017)) to implement our experts (anti-experts). We initialize experts with the pre-trained summarization models (bart-largexsum, bart-large-cnn) and fine-tune them for 1 epoch with batch size of 64 using default training hyperparameters (optimizer: Adam, learning rate: 5e-5, \u03b2 1 : 0.9, \u03b2 2 : 0.999, : 1e-8). The experts (anti-experts) initialized with BART are trained for 5 epochs.",
                "Inference: We adopt the standard hyperparameters for all models during the inference, e.g. beam size of 6 (4), the minimum and maximum sequence length of 11 (56) and 62 (142), etc. for the XSUM (CNN-DM) model."
            ],
            "subsections": []
        }
    ]
}