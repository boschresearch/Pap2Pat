# Introduction

Atrial fibrillation (AF) affects about 2% to 3% of the population in Europe and North America as of 2014 [15]. One of its treatments is to perform catheter ablation to destroy the atypical tissues. During catheter ablation, intracardiac echocardiography (ICE) is often used to guide the intervention. Compared with other imaging modalities such as transoesophageal echocardiography, ICE provides better patient tolerance, requiring no general anesthesia [2]. Moreover, modern ICE devices are equipped with an embedded position sensor that measures the precise 3D location of the ICE transducer. Such spatial geometry information associated with the ICE image is key to this study. Some gross morphological and architectural features of the left atrium (LA) are important to AF interventions and recognizing these features relies on a clear view of LA's surrounding structures (see Fig. 1 (a)) and their junctions with the LA [10]. However, due to the limitations of 2D ICE and the difficulty in manual manipulation of the ICE transducer, these 3D anatomical structures may not be sufficiently observed in certain views. This introduces difficulties to electrophysiologists as well as echocardiography image analysis algorithms that attempt automatic multi-component contouring or segmentation.

Existing approaches to 2D echocardiogram segmentation only focus on single cardiac chamber such as left ventricle (LV) [7,11,13] or LA [1]. They are designed to distinguish between the blood tissues and the endocardial structures which is relatively easy due to the significant difference in appearance. When it comes to multiple cardiac components (chambers and their surrounding structures), where the boundaries cannot be clearly recognized, these methods may fail. To the best of our knowledge, this paper is the first to handle the multi-component echocardiogram segmentation from 2D ICE images.

Recently, deep convolutional neural networks (CNNs) have achieved unprecedented success in medical image analysis, including segmentation [14]. However, our baseline method of training a CNN to directly generate segmentation masks from 2D ICE images does not demonstrate satisfactory performance, especially for the less-observed pulmonary veins . Such a baseline solely relies on the brute force of big data to cover all possible variations, which is difficult to achieve. To go beyond brute force, we further integrate knowledge to boost contouring performance. 1 Such knowledge stems from two sources: (i) 3D geometry information provided by a position sensor embedded inside an ICE catheter, and (ii) 3D image appearance information exemplified by cross-modality computed tomography (CT) volumes that contain the same anatomical structures.

# Method

The proposed method consists of three parts. Using the 3D geometry knowledge, we first form a 3D sparse volume based on the 2D ICE images. Then, to tap into 1 The outcome of this research has been patented [4]. the 3D image appearance knowledge, we design a multi-task 3D network with an adversarial formulation. The network performs cross-modality volume completion and sparse volume segmentation simultaneously for collaborative structural understanding and consistency. Finally, taking as inputs both the original 2D ICE image and the 2D mask projected from the generated 3D mask, we design a network to refine the 2D segmentation results. We form a 3D sparse ICE volume from a set of 2D ICE images with each including part of the heart in its field of view and its 3D position from a magnetic localization system. As shown in Fig. 1, we use the location information to map all ICE images (left) to 3D space (middle), thus forming a sparse ICE volume (right). The generated sparse ICE volume keeps the spatial relationships among individual ICE views. A segmentation method based on the sparse volume can take this advantage for better anatomical understanding and consistency.

## 3D Sparse Volume Segmentation and Completion

The architecture of the proposed 3D segmentation and completion network (3D-SCNet) is illustrated in Fig. 2(a). The network consists of a generator G 3d and two discriminators D c 3d and D s 3d . Taking the sparse ICE volume x as input, G 3d performs 3D segmentation and completion simultaneously, and outputs a segmentation map G s 3d (x) as well as a dense volume G c 3d (x). During training, the ground truth of G c 3d (x) is a CT volume instead of a dense ICE volume as we lack the training data of the latter. The ICE images and the CT volumes are from completely different patients. This inherently indicates a challenging cross-modality volume completion problem with unpaired data. We target this problem through adversarial learning and mesh pairing (See Sec. 3). The two discriminators judge the realness of the outputs from the generator. When trained adversarially together with a generator, they make sure the generator's outputs are more perceptually realistic. Following conditional GAN [5], we also allow the discriminators to take x as the input to further improve adversarial training.

Adversarial loss The segmentation task s and completion task c are trained jointly in a multi-task learning (MTL) fashion [3,6]. The adversarial loss for a task t ∈ {s, c} can be written as

where p denote the data distributions. For a real data y t , i.e., the ground truth segmentation map or CT volume, D t 3d is trained to predict a "real" label. For the generated data G t 3d (x), D t 3d learns to give a "fake" label. On the other hand, the generator G 3d is trained to deceive D t 3d by making G t 3d (x) as "real" as possible.

Reconstruction loss Adversarial loss alone, however, does not give a strong structural regularization to the training [8]. Hence, we use reconstruction loss to measure the pixel-level error between the generator outputs and the ground truths. For the segmentation task, we first convert the score map to a multichannel map with each channel denoting the binary segmentation map of a target anatomy and then apply an L2 loss L s rec between G s 3d (x) and y s . For the completion task, the L1 loss L c rec between G c 3d (x) and y c is measured. We use L1 loss against L2 loss for this task due to the observation that outputs from L2 losses are usually overly smoothed. The total loss of the sparse volume segmentation and completion network is given by

where λ t rec and λ t adv balance the importance of the reconstruction loss and reconstruction loss, respectively.

Architecture details We use a 3D UNet-like network [9] as the generator. There are 8 consecutive downsampling blocks followed by 8 consecutive upsampling blocks in the network. We use skip connections to shuttle feature maps between two symmetric blocks. Each downsampling block contains a 3D convolutional layer, a batch normalization layer and a leaky ReLU layer. Similarly, each upsampling layer contains a 3D deconvolutional layer, a batch normalization layer and a ReLU layer. The convolutional and deconvolutional layers have the same parameter settings: 4×4×4 kernel size, 2×2×2 stride size and 1×1×1 padding size. Finally, a tanh function is attached at the end of the generator to bound the network outputs. The two discriminators D s 3d and D c 3d have identical network architecture with each of them having 3 downsampling blocks followed by a 3D convolutional layer and a sigmoid layer. The downsampling blocks for the discriminators are the same as the ones used in the generator. The final 3D convolutional layer (3 × 3 × 3 kernel size, 1 × 1 × 1 stride size and 1 × 1 × 1 padding size) and sigmoid layer are used for realness classification.

## 2D Contour Refinement

As shown in Fig. 2(b), the 2D refinement network (2D-RefineNet) has a similar structure to the 3D-SCNet. Actually, G 2d and D r 2d have almost the same structure as their 3D counterparts except that the convolutional and deconvolutional layers are now in 2D. The inputs to the 2D-RefineNet is a 2D ICE image x i together with its corresponding 2D segmentation map m i , where m i is obtained by projecting G s 3d (x) onto x i . The training of the 2D-RefineNet is also performed in an adversarial fashion and conditional GAN is used to allow D r 2d observing the generator inputs. We compute the adversarial loss L r adv the same way as Eq. ( 1) and use the L2 distance between the refinement network output G 2d (x i , m i ) and the ground truth 2D segmentation map y r as the reconstruction loss L r rec . The total loss is

where λ r rec and λ r adv are the corresponding balancing coefficients.

# Experiments

Dataset and preprocessing The left atrial ICE images used in this study are collected using a clinical system with each image associated with a homogeneous matrix that projects the ICE image to a common coordinate system. We perform both 2D and 3D annotations on the ICE images for the cardiac components of interest, i.e., LA, LAA, LIPV, LSPV, RIPV and RSPV. For the 2D annotations, contours of all the plausible components in the current view are annotated. For the 3D annotations, ICE images, from the same patient and at the same cardiac phase2 , are first projected to 3D, and 3D mesh models of the target components are then manually annotated. 3D segmentation masks are generated using these mesh models. In total, the whole database has 150 patients. For each patient, there are 20-80 gated frames for use. We have 3D annotations for all 150 patients. For 2D annotations, we annotated 100 patients, resulting in a total of 11,782 annotated ICE images. By anatomical components, we have in 2D 4669 LA, As we do not have dense ICE volumes available for training, we use CT volumes instead as the ground truth for the completion task. Each CT volume is associated with a LA mesh model. To pair with a sparse ICE volume, we pick the CT volume whose LA mesh model is closest to that of the targeting sparse ICE volume (after Procrustes analysis [12]). In total, 414 CT volumes are available, which gives enough anatomical variability for the mesh pairing. All the data used for 3D training are augmented with random perturbations in scale, rotation and translation to increase the generalizability of the model.

# Training and evaluation

We train the 3D-SCNet and 2D-RefineNet using Adam optimization with lr = 0.005, β 1 = 0.5, β 2 = 0.999. The 3D-SCNet is trained for about 25 epochs with λ s adv = 0.2, λ c adv = 1, λ s rec = 1000, λ c rec = 100. The 2D-RefineNet is also trained for about 25 epochs with λ r adv = 1, λ r rec = 1000. All λs are chosen empirically and we train the models using 5-fold crossvalidation. The segmentation results are evaluated using the Dice metric and average symmetric surface distance (ASSD).

# Results

The outputs from the 3D network model are shown in Fig. 3. We can observe that the model not only gives satisfying segmentation outputs, Fig. 3(d), but also gives a good estimation about the CT volume, Fig. 3(b). Especially, we note that the estimated completion outputs do not give structurally exact results as the "ground truth" but instead try to match the content from the sparse volume. Since the "ground truth" CT volume is paired based on mesh models, this difference is expected. It demonstrates that the completion outputs are based on the sparse volume and the system only tries to complete the missing region such that it looks like a "real" CT volume. We also quantitatively evaluate the performance of the 3D sparse volume segmentation and obtain the following Dice scores: LA (89.5%), LAA (50.0%), LIPV (52.9%), LSPV (43.4%), RIPV (62.43%), RSPV (57.6%) and overall (86.1%). This shows that using the limited information from sparse volumes our model still can achieve a satisfactory 3D segmentation performance. As we will show in later experiments, the segmentation accuracy, actually, is even higher in the region where 2D ICE images are presented. We also notice that it is vital to use the 3D appearance information -the training fails to converge in our experiment of learning the 3D network without using the 3D appearance information from CT.

Fig. 4 shows the 2D ICE contouring results using different models: the "2D only" model that is trained directly with the 2D ICE images, the "3D only" model by projecting the predicted 3D segmentation results onto the corresponding ICE image, and the "2D + 3D" model by refining the outputs from 3D-SCNet using 2D-RefineNet. We observe from the first row that the "3D only" outputs give better estimation about the PVs (red and orange) than the "2D only" outputs. This is because the PVs in the current 2D ICE view are not clearly presented which is challenging for the "2D only" model. While for the "3D only" model, it makes use of the information from other views and hence predicts better the PV locations. Finally, we see that the outputs from the "2D + 3D" model combines the knowledge from both the 2D and 3D models and generally gives superior outputs than these two models. Similar results can also be found in the second row where we see the "2D + 3D" model not only predicts the location of the PVs (purple and brown) better by making use of the 3D information but also refines the output according to the 2D view.

The quantitative results of these models are given in Table 1. The "3D only" model in general has better performance in PVs and worse performance in LA and LAA than the "2D only" model. This is because LA and LAA usually have a clear view in 2D ICE images, unlike the PVs. The "2D + 3D" model combines the advantages of the "2D only" and "3D only" model and in general yields the best performance. The IRR scores from human experts are relatively lower, especially for the LSPV and RSPV. This is expected as these two structures are difficult to view with ICE. The IRR scores are generally lower than those from our models, which demonstrates the benefit of using an automatic segmentation model -better consistency.

# Conclusions and Future Work

We present a knowledge fusion + deep learning approach to ICE contouring of multiple LA components. It uses 3D geometry and cross-modality appearance knowledge for better anatomical understanding and structural consistency. Then, it refines the contours in 2D by exploiting the detailed 2D appearance information. We show that the proposed model indeed benefits from the integrated knowledge and gives superior performance to the models trained individually. In the future, we will investigate the use of temporal information for better modeling and the clinical utility of the generated dense 3D cross-modality views.

