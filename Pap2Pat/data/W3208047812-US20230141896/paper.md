# Introduction

Early diagnosis is vital for the treatment of various vision degradation diseases [1], such as glaucoma, Diabetic Retinopathy (DR), and age-related macular degeneration. Many eye diseases can be revealed by the morphology of Optic Disc (OD) and Optic Cup (OC) [2]. For instance, glaucoma is usually characterized by a large Cup to Disc Ratio (CDR), the ratio of the vertical diameter of the cup to the vertical diameter of the disc. Currently, determining CDR is mainly performed by pathology specialists. However, it is extremely expensive to accurately calculate CDR by human experts. Furthermore, manual delineation of these lesions also introduces subjectivity, intra-and inter-variability [3]. Therefore, it is essential to automate the process of calculating CDR. OD and OC segmentation are commonly adopted to automatically calculate the CDR. Nevertheless, OD segmentation is challenging because pathological lesions usually occur on OD boundaries, which affect the accurate identification of the OD region. Accurate OC segmentation is more challenging due to the region overlap between the cup and the blood vessels [4].  Recently, deep learning-based methods [5,6,7,8] have been proposed to overcome these challenges and some of them, e.g., M-Net [5], have demonstrated impressive results. Although these methods tend to perform well when being applied to well-annotated datasets, the segmentation performance of a trained network may degrade severely on datasets with different distributions, particularly for retinal fundus images captured with different imaging devices (e.g., different cameras, as illustrated in Fig. 1). The variance among the diverse data domains limits deep learning's deployment in reality and impedes us from building a robust application for retinal fundus image parsing. To recover the degraded performance, annotating the fundus images captured from every new domain and then re-training or fine-tuning a model is an easy way but extremely expensive and even impractical for the medical areas that require expertise.

To tackle this challenge, recent studies [9,10,11,12,13,14,15] have demonstrated the effectiveness of using deep learning for unsupervised domain adaptation to enhance the performance of applying models on unlabeled target domain data. Existing works have mainly focused on minimizing the distance between the source and target domains to align the latent feature distributions from different domains [12]. Several primary approaches can guide the alignment process, which includes image-to-image translation of the input images [16], adversarial training for the intermediate representations in the layers of the model (encoder or decoder) [17], and applying adversarial learning to the output of the model [11]. However, adversarial discriminative learning usually suffers from the instability of its training. Numerous methods have been studied to tackle this challenge. Self-ensembling [18] is one of them recently applied to visual domain adaptation [19]. In particular, gradient descent is used to train a student network, and the exponential moving average of the weights of the student is transferred to a teacher network after applying each training sample.

The mean square difference between the outputs of the student and the teacher is used as an unsupervised loss to train the student network. The paradigm of student-teacher has been a widely used strategy for unsupervised training of a deep neural network, feature extraction [20], and knowledge distillation [21].

This unsupervised training strategy allows the student network to capture more information about the data during training and achieve a better prediction. Furthermore, most of the existing methods have not considered providing multi-scale information of the data to the deep neural networks for having a better understanding of the difference between target and source domain features. In addition, most deep neural networks employ pooling layers to reduce model parameters and extract important features. However, pooling layers lead to a significant loss of the information in the original input data. In addition, the optic cup and disc in fundus image have high variance in brightness, color, shape, and orientation, which makes single-scale and single-level adversarial adaptation insufficient. Capturing only information from the output space neglects the intuition that low-level features are similar across various domains, leading to poor domain adaptability and missed opportunities for incorporating hierarchical features in segmentation predictions. To overcome these problems, in this study, we propose a multi-scale input training strategy to integrate different scales of features into different levels of the network layers. On one hand, multi-scale inputs performing at different levels of network layers can reduce the information loss due to the pooling layers in the network; On the other hand, it can provide rich information for the network to easily distinguish the difference between the source and target domain features. These multi-scale inputs are integrated into an encoder-decoder structure to form multiple sub-networks with multi-outputs. In this way, we can have multiple discriminators between the source and target domain input and leverage ensembling internally on the multiple distinctions at different levels of the network layers to have a better final segmentation prediction in target domain data. This proposed multi-domain adaptor approach can overcome the limitation of the current methods while comprehensively applying domain adaptation at hierarchical multi-scale in both feature and output space.

In this paper, we propose a novel unsupervised domain adaptation framework called Collaborative Adversarial Domain Adaptation (CADA) to further the state-of-the-art in overcoming the underlining domain shift problem. In particular, we take advantage of self-ensembling to stabilize the adversarial discriminative learning of the latent representations from domain shifting to prevent the network from getting stuck in degenerate solutions. Most importantly, we apply the unsupervised loss by adversarial learning not only to the output space but also to the input space and the intermediate representations of the network. Thus, from a complementary perspective, adversarial learning can consistently provide various model space and time-dependent weights to self-ensembling to accelerate the learning of the domain-invariant features and further enhance the stabilization of adversarial learning, forming a benign collaborative circulation and unified framework. The significant contributions of this paper are as follows:

• We propose CADA, a novel unsupervised domain adaptation framework, which exploits collaborative adversarial learning and weights self-ensembling for feature adaptation to tackle domain shift in a mutually beneficial and complementary manner at different network layers, resulting in a robust and accurate model.

• We propose a multi-scale input training strategy to overcome the information loss when applying pooling layers in the network and offer an opportunity to integrate various scales of low-level and high-level features for improved network learning.

• We optimized feature adaptation by applying adversarial discriminative learning in two phases of the network, i.e., intermediate representation space and output space. More specifically, we apply adversarial learning at multiple layers of the network to learn the invariant features in both encoder and decoder phases simultaneously.

• We reduce the uncertainty of multiple discriminators collaborative learning for domain shift via the EMA to ensemble model weights dynamically during training.

• We evaluate the effectiveness of our CADA on the challenging task of unsupervised joint segmentation of the retinal OD and OC. Our CADA can overcome performance degradation to domain shift and outperform one of the state-of-the-art domain adaptation methods with a significant performance gain on various datasets. This work is a substantial extension of our conference paper "CFEA: Collaborative Feature Ensembling Adaptation for Domain Adaptation in Unsupervised Optic Disc and Cup Segmentation" [22] published in Medical Image Computing and Computer Assisted Intervention (MICCAI) 2019. In this extension, we substantially expanded our framework's reliability and scalability of overcoming domain shift issue in fundus images. In particular, we demonstrate the significant new contributions as below:

• We propose a novel multi-scale input layer to enhance the feature interaction between the encoder and the decoder where CFEA only uses a single scale. An input on each scale provides original image information to an encoder layer, which is followed by a decoder layer at the same network "pyramid" level. The rich original pixel-wise feature can infuse the interaction between encoder and decoder at the different feature-learning levels in the network. This infusion triggered by the multi-scale input can further guide the model learning and promote performance by reducing the significant information loss due to the pooling layers applied in the network and reducing the high variance of the optical cup and disc images in brightness, color, shape, and orientation.

• Instead of a single domain adaptor (e.g., a discriminator network) at the end of the network in CFEA, we propose to apply multi-domain adaptors at hierarchical multi-scales in both feature and output space, which encourages the network to learn the domain-invariant features consistently.

More importantly, they can collaboratively distinguish robust latent features in the scenarios that the optical cup and disc images have high variance in brightness, color, shape, and orientation, thus leading to a reliable and scalable domain adaptation framework.

• Comprehensive ablation studies are performed to investigate the effectiveness of the proposed framework. The ablation study investigates the importance of the encoder adversarial discriminative adaptation, the power of weights self-ensembling adaptation, the scalability of using multiple domain adaptors, and the choice of various combinations of the weights of loss functions.

• Evaluation on multiple public datasets is performed to show generalizability and stability of the proposed CADA framework compared to state-ofthe-art methods.

# Related Work

## Optic Disc and Optic Cup Features

The features of the Optic Disc (OD) and Optic Cup (OC) are critical for the diagnosis of eye diseases [2]. For example, ophthalmic pathologies (e.g., glaucoma) can be indicated by the variations of the shape, color, or depth of OD.

The Cup to Disc Ratio (CDR), the ratio of the vertical diameter of the cup to the vertical diameter of the disc, is considered a valuable feature for diagnosing eye diseases [23], such as glaucoma, because higher CDR is highly associated with detectable visual field damage [24]. Currently, determining CDR is mainly performed by pathology specialists. However, it is expensive to calculate CDR by human experts accurately. Furthermore, the variance of determining the CDR among professionals is usually significant, which can be caused by both the diversity of retinal fundus images and different experiences of the professionals [25]. Therefore, it is essential to automate the process of calculating CDR.

On one hand, this automated process can reduce the cost of diagnosis. On the other hand, it can stabilize diagnostic accuracy and improve the efficiency of retinopathy screening procedures.

## OD and OC Image Segmentation

Image segmentation is a long-term research topic in the field of computer vision and image analysis. It is the basis for feature recognition and quantitative feature understanding [26]. In medical imaging, image segmentation is particularly important since it can help to locate related lesions/tumors and provide quantitatively analytical results of shapes/morphologies for clinicians. For example, image segmentation can automatically detect the OD and OC regions and calculate the CDR simultaneously, e.g., [4]. The task of OD segmentation is to detect the region between retinal and the rim. The presence of pathological lesions on the OD boundaries become problematic for OD detection. More to the point, OC detection is hindered by the region overlap between the cup and the blood vessels, as well as the color intensity changing between the cup and rim. It is critical to erase these challenges for reducing incorrect OD and OC segmentation that may cause a false diagnosis.

Recently, many deep learning-based studies [2,4,5] have been proposed to overcome these challenges. In general, there are several steps to achieve a decent result. Firstly, a pre-trained disc center localization method [27] is used to detect the OD and OC. The localization mainly acts as an attention mechanism so that the network can focus on essential regions and meanwhile, the polar transformation amplifies the relevant features to enable a more accessible learning process. Secondly, the localized areas are transformed (e.g., cropped, re-sized, and image coordinate system consistency) into the segmentation model training stage. Lastly, these transformed image regions are fed into an encoderdecoder convolutional network to predict the actual OD and OC regions for an arbitrary fundus image. The encoder is utilized to extract rich image features; the decoder part is used to produce accurate segmentation results based on the encoded features. These combined techniques can reduce the negative effect on model performance caused by the variance in retinal images. However, the variation is only constrained within one image domain, in which the training and testing images usually have similar distributions, such as background color and intensity. In practice, the testing images can be acquired from different types of cameras and have varying background or image intensity (as illustrated in Fig. 1). The performance of a model trained on the dataset collected from one domain is severely degraded in another domain. This issue is referred to as "domain shift" [28]. It is critical to overcome this issue for a generalized and robust model in medical practice.

## Unsupervised Domain Adaptation

Saenk et al. [29] originally introduced the unsupervised domain adaptation problem in tackling the performance degradation caused by the domain shift.

In particular, unsupervised domain adaptation aims to tackle domain shift via adapting the training process of a model in an unsupervised manner. The model is adapted to improve the performance on the target domain. More importantly, leveraging unsupervised learning can reduce the tremendous and expensive data labeling work for the target domain. Therefore, unsupervised domain adaptation is a promising direction to solve domain shift problems, especially, in the medical field whose data is multi-modal and requires expensive and expertise data labeling.

Recently, many deep learning-based domain adaptation methods [4,17,30] have been proposed and achieved several encouraging results. Many of these methods tackle the domain shift issue by extracting invariant features across the source and target domains. A critical approach for reducing the domain discrepancy is adversarial learning [9], which has become a fundamental method  to obtain invariant information across multiple domains. In particular, it leverages the gradient discrepancy between learning the labeled and unlabeled data to minimize performance degradation. The implementation can either be imageto-image translation [31,32,33] in a convolutional neural network (CNN) inputend or multiple adversarial learning [11,34] applied at the output-end of a CNN.

Noticeably, the image-to-image translation usually introduces artifacts, which may be not a proper approach in the medical field. Therefore, in this work, we focus on gradient-based adversarial learning.

In addition, previous adversarial learning based approaches [35] are mainly applied domain adaptation at the output space of a deep neural network. However, accurate image segmentation requires the model to capture both lowand high-level image representations; thus, it would be ideal for applying domain adaptation at various feature spaces with multi-scale representations. In this study, we comprehensively investigated whether employing multiple domain adaptors at a different level of layers of the network can benefit the representation learning across domains compared to the approaches only focusing on adaptation at the output space. We showed the design difference between previous methods and our framework and illustrated the major novelty in Fig. 2.

Furthermore, although adversarial learning can align the latent feature dis-tribution of the source and target domain and have achieved encouraging results [9], the results of multiple adversarial learning-based methods are easily suffering from sub-optimal performance due to the difficulty of stabilizing the training process of multiple adversarial modules. Thus, in this work, we propose to leverage the Exponential Moving Average (EMA) [36] 

A r e n C f n x X l 3 P q a j J W e 2 s w / + w P n 8 A Q 5 1 l i 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " J 9

A r e n C f n x X l 3 P q a j J W e 2 s w / + w P n 8 A Q 5 1 l i 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " J 9  

x H S 8 5 s Z x / 8 g f P 5 A w z q l i 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " K G 1 V Y 2 X p A W k q M + 8 q i q f H z K k T S q 0 = "

x H S 8 5 s Z x / 8 g f P 5 A w z q l i 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " K G 1 V Y 2 X p A W k q M + 8 q i q f H z K k T S q 0 = "

x H S 8 5 s Z x / 8 g f P 5 A w z q l i 0 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " K G 1 V Y 2 X p A W k q M + 8 q i q f H z K k T S q 0 = " parameters [11,37]. The labeled samples enable the network to learn accurate segmentation predictions while the unlabeled samples bring unsupervised learning and further present a type of perturbation to regularize the model training [38]. Furthermore, TTN conducts the weight self-ensembling [19] part by replicating the average weights of the TSN instead of predictions. TTN solely takes unlabeled target images as input and then the mean square difference between TSN and TTN is computed for the same target sample. Different data augmentations (e.g., adding Gaussian noise and random intensity or brightness scaling) are applied to TSN and TTN to avoid the loss vanishing issue.

In this work, the U-Net [39] with encoder-decoder structure is employed as the backbone of each network, since U-Net is one of the most successful segmentation frameworks in medical image segmentation. With the adaptability and flexibility of our framework, we expect that the results can generalize to other backbone networks and medical image analysis tasks.

## Multi-scale Input Sub-networks

We allow multi-scale inputs to provide rich original pixel-wise features that can infuse the interaction between encoder and decoder at the different featurelearning levels of the network. Each level of the network is considered as a sub-network that primarily processes one scale of the original input for segmentation prediction. Fig. 4 shows the paradigm of the multi-scale input networks.

To simplify the re-scaling procedure, we apply a 2 × 2, 4 × 4, and 8 × 8 size of pooling followed by a 3 × 3 convolutional layer to reduce the scale of the origi-   segmentation prediction, especially those from the fundus optic cup and disc.

There are 5 adaptors and each of them is a combination of supervised segmentation learning, the mean square difference between the prediction of the teacher and student network, and adversarial learning through deep convolutional networks as discriminators. We show the paradigm of the multi-scale input in the combination of multiple domain adaptors applied at different hierarchical layers and the details of network design in Fig. 4. Notably, we include the multi-scale inputs to both the student and teacher networks of our CADA framework.

## Multiple Adversarial Discriminative Learning

We apply five discriminators at the encoder and decoder of the networks, separately, to achieve adversarial discriminative learning. To simplify our method's explanation, the following section is using two types of discriminators for discussion: one discriminator is applied for encoder features and the other four discriminators are used for the outputs of the decoder. The adversarial loss functions are calculated between SN and TSN. Each of the loss calculations is performed by two steps in each training iteration: (1) train the target domain segmentation network to maximize the adversarial loss L adv , thereby fooling the domain discriminator D to maximize the probability of the source domain feature P s being classified as target features:

and 2) minimize the discriminator loss L d :

where P t is the target domain feature.

Note that discriminators can be added between all the intermediate decoder layers of SN and TSN. However, we only add the discriminators among the input (P sf and P tsf ) and output (P so and P tso ) of the decoders in this section for simplicity.

## Self-ensembling

In self-ensembling for domain adaptation, the training of the student model is iteratively improved by the task-specific loss upon an Exponential Moving Average (EMA) model (teacher) of the student model, which can be illustrated as:

where Φ t and Φ t denote the parameters of the student network and the teacher network, respectively. EMA transfers a smooth version of the weights of the student to the teacher network. Thus, the teacher network is more stable and robust than the student.

More specifically, at each iteration, a mini-batch of the labeled source domain and unlabeled target samples are drawn from the target domain T . Then, the EMA predictions and the base predictions are generated by the teacher model and the student model respectively with different augmentations applied to the target samples. Afterward, a Mean-Squared Error (MSE) loss between the EMA and target predictions is calculated. Finally, the MSE loss together with the task-specific loss on the labeled source domain data is minimized to update the parameters of the student network. Since the teacher model is an improved model at each iteration, the MSE loss helps the student model to learn from the unlabeled target domain images. Therefore, the student model and teacher model can work collaboratively to achieve robust and accurate predictions.

## Unsupervised Domain Adaptation

Unlike existing methods, our method appropriately integrates adversarial domain confusion and self-ensembling with an encoder-decoder architecture.

### Adversarial Feature Adaptation

Adversarial domain confusion is applied to both the encoded features and decoded predictions between Source domain Network (SN) and Target domain Student Network (TSN) to reduce the distribution differences. According to Eq. 1 and 2, this corresponds to the adversarial loss function L E adv for the encoder output of SN and TSN, and the adversarial loss function L D adv for the decoder output of SN and TSN:

where P sf ∈ R We×He×Ce is the encoder output and The discriminator loss L E d for the encoder feature and the discriminator loss L D d for the decoder feature are as follows:

where P tsf ∈ R We×He×Ce is the encoder output and P tso ∈ R W d ×H d ×C d is the decoder output of TSN.

### Collaborative Adaptation with Self-ensembling

Self-ensembling is also applied to both the encoded features and decoded predictions between TSN and Target domain Teacher Network (TTN). In this work, the MSE is used for self-ensembling. The MSE loss L E mse between encoder outputs of TSN and TTN, and the MSE loss L D mse between decoder outputs of TSN and TTN can be formulated as:

where p tsf i , p ttf i , p tso i , and p tto i denote the i th element of the flattened predictions (P tsf , P ttf , P tso , and P tto ) of the student encoder, student decoder, teacher encoder, teacher decoder, respectively. M and N are the number of elements in the encoder feature and decoder output, respectively.

### Patch-based Discriminator Learning in Multi-scale Output Space

Unlike the domain adaption for classification problems, we need to adapt both low-level and high-level features for pixel-wise image segmentation tasks.

To have a better segmentation on the image across the source domain, the invariant features from both low-level and high-level layers are considered in this study. Particularly, like the study [35], the geometry structure of the predicted segmentation masks in the output space is used for domain adaptation. However, the dimension of the feature space from different levels of the layers varies from 256, 128, 64, to 32. Thus, we perform a 2D convolutional layer with 1 × 1 kernels on each scale of features (see Fig. 4) to reduce the feature dimensions consistently to be the number of pixel classes in the segmentation mask, which is 3 classes, including background, optic disc, and optic cup. In other words, we convert the features in high-dimensional feature space to a low-dimensional output space. We achieve domain adaptation at each level of layers of the network by applying adversarial learning on the converted low-dimensional features in the output space. We apply a patch-based discriminator on each output of the segmentation network. In the adversarial learning, the segmentation network is to fool each discriminator by producing a m × n × 3 size of output having a similar distribution either from source or target domain, where m and n is the width and height of the output, respectively. A patch-based discriminator [32] is used to perform adversarial learning for capturing the local statistical similarity. Basically, this type of discriminator tries to classify whether each patch in a predicted mask image is following the distribution of that from the source or target domain. More particularly, each discriminator network is composed of five convolutional layers. They have 64, 128, 256, and 512 channels, respectively.

A kernel size 4×4 and a stride size 2×2 are implemented in each layer. A Leaky ReLU [40] layer with an alpha value of 0.2 is used after each convolutional layer.

Each discriminator produces a 16 × 16 size output.

### Total Objective Function

Finally, we use cross-entropy as the segmentation loss for labeled images from the source domain. Combing Eq. 4, 5, 6, 7, 8, and 9, the total loss is obtained, which can be formulated as below.

where λ E adv , λ D adv , λ E mse , and λ D mse balance the weights of the losses. The choice of each weight component λ was accomplished by cross-validation in our experiments. L seg (X s ) is the segmentation loss. Based on Eq. 10, we optimize the following min-max problem:

where f φ and f φ are the source domain network with trainable weights φ and target domain network with trainable weights φ.

# Experiments and Results

## Dataset

Extensive experiments are conducted on three public datasets, including REFUGE [41], Drishti-GS [42], and RIM-ONE-r3 [43], to validate the effectiveness of the proposed method. The dataset REFUGE includes 400 source do- 

## Data Preprocessing

Firstly, we detect the center of the optic disc by a pre-trained disc-aware ensemble network [5] and then center and crop the optic disc regions. The REFUGE source domain was preprocessed to a size of 600 × 600 while the   REFUGE target domains were pre-processed to a size of 500×500. The Drishti-GS and Rim-One datasets were preprocessed to a size of 700 × 700. This is due to the different sizes of images acquired by the various cameras.

During training, all images are resized to a small size of 400 × 400 to adapt the network's receptive field. In addition, we used this pre-trained model to understand the image quality of the datasets. For example, some images' optic discs are not detectable because the image contrast is significantly low. In this case, we removed two low-quality images from the dataset RIM-ONE-r3 to evaluate our method, and the other two datasets remained as the original.

## Implementation Details

The U-Net is used for both student and teacher networks. Four scales of layer blocks are used for network design. Each block includes two groups of the combination with one convolutional layer, one batch normalization layer, and one ReLU layer. Each layer block is followed by either one max-pooling layer or one up-sampling layer. The details can be found in Fig. 4. To train our network on the REFUGE dataset, we started from a randomly initialized U-Net and used both source and target domain images. However, for another two smaller datasets (e.g, RIM-ONE-r3 and Drishti-GS), to avoid overfitting, we first trained our network on REFUGE using the source domain images with their annotations and then applied our multi-scale domain adaptors to further train the network on the smaller datasets for domain adaptation.

We used the stochastic gradient descent (SGD) optimizer for both student and teacher networks and used the Adam [44] optimizer for all discriminators. We set the initial learning rate as 1e -4 for both student and teacher networks and adjusted the learning rate during training by Lr initial × (1 -Iter current /Iter max ) 0.9 , where Lr initial means the initial learning rate, Iter current means the current iteration number, and Iter max means the maximum iteration number. We also applied polynomial decay with a power of 0.9 in a total of 200 epochs for network training. Lastly, we used morphological operations (i.e., hole filling) to post-process the results. For the discriminators, we set the initial learning rate as 2.5e -5 and applied the same learning rate adjusting function. All experiments are processed on Python v3.6, and PyTorch v1.0.0 with NVIDIA TITAN Xp GPUs. The detailed implementations and code will be available at https://github.com/cswin/CADA.

## Evaluation Metrics

We followed the REFUGE challenge [41] and used a spatial overlap index, dice coefficients (DI), to evaluate the segmentation performance for both OD and OC. We also used the optic cup to disc ratio (CDR) to understand the overall model performance for clinical glaucoma screening convention. The DI  We compared with AdaptSegNet [11], which is one of the state-of-the-art domain adaptation methods for image segmentation. AdaptSegNet adopts adversarial learning with two discriminator networks at the last two output layers.

POSAL [35], the winner of the REFUGE challenge, applied a similar technique into their framework for optic disc and cup segmentation at the final layer. In this study, we compared their domain adaptation technique with ours, which applies multiple domain adaptors with adversarial learning at multi-scales in both feature and output spaces. Moreover, compared to POSAL, our approach applied domain adaptation ensembles inside our framework rather than outside by averaging the results of multiple trained sub-networks. From this perspective, the primary goal of this study is to investigate a novel domain adaptation This computational discrepancy is conjectured to be influenced by the computational burden of the residual network structure [46], the depth of the CNN's, as well as the coding framework differences (differences in parallel computing sup- port, memory utilization, etc.). Nevertheless, the inference time suggests that our CADA model is an invaluable tool to practitioners for diagnostic purposes, being substantially faster than manual practice. We also investigate how self-ensembling adaptation affects domain adaptation performance. For this, we retrain our framework after removing the teacher network. The performance comparison of the models with modifications shows in Fig. 7. As one can see, the average performance on the test dataset is much worse than using both adversarial domain confusion and self-ensembling adaptation. Especially, for predicting the CDR, in Fig. 7-c, we can see that without each layer of the decoder), we obtain the best result. Notably, CADA-2D is the method proposed in our previous work [22]. More importantly, this comparison result further indicates that collaborative feature learning between adversarial adaptation and dynamic weight ensembling can overcome domain shift.

### Evaluation of λ

We evaluate the various combinations of λ for balancing the segmentation, adversarial, and self-ensembling loss. Due to the tremendous combinations, it is impossible to study all of them. We follow the existing studies [47] and use cross-validation to investigate the most effective λ combinations. We find the following combination is the most effective one that can stabilize our framework training: λ seg = 1, λ E adv = 0.002, λ D adv = 0.018, λ E mse = 0.057, λ D mse = 0.79. We also show qualitative results in Fig. 8 to demonstrate the effectiveness of the proposed domain adaptation model. As one can see, these qualitative results are consistent with Fig. 7. This result can further support that collaboration between adversarial learning and dynamic weight ensembling is an effective strategy to overcome domain shift in fundus images.

# Discussions and Conclusion

In this work, we propose a novel method, CADA, for unsupervised domain Notably, the proposed framework also suggests a generalizable unsupervised learning approach. For example, we could replace the discriminator with the contrastive learning objective functions [20,48]. With which, the encoder can learn the rich representations rather than the invariant features. Then, we can fine-tune the encoder with limited labeled data for specific tasks, such as image classification and segmentation. Simultaneously transferring weights with EMA from both encoder and decoder during model training is a significant novelty compared to existing representation learning methods.

In terms of the running time, due to the multiple discriminator architecture, our framework needs relatively more computational during the training stage, compared to AdaptSegNet [11], to help the segmentation network to adapt to the target domain. However, in the testing stage, the computational costs are the same as a standard U-Net network, as the images only need to go through the TTN network. Experimental results demonstrate the superiority of our domain adaptation method over other methods either by a significant performance gain or computational efficiency. Our approach potentiates a general and extendable framework to other semi-supervised and unsupervised representation learning problems.

Lastly, although we have shown marked advantages of our method, the current study has some limitations that we hope to address in the future. First, it is hard to balance the contributions (e.g., learning losses) from multiple domain adaptors during training. To find the optimal weight for each domain adaptor, we currently have to apply a massive grid-search, which can be time-consuming.

Second, our framework solely investigates one type of encoder-decoder network architecture. It would be interesting to understand how our framework can improve other architectures for domain adaptation and performance across various tasks. The current work sheds light on the underlying superiority of applying multiple domain adaptors at hierarchical multi-scale feature and output space.

