{
    "id": "https://semopenalex.org/work/W4380433215",
    "authors": [
        "Eugene Brevdo",
        "Ryan Marcus",
        "H. K. Huang",
        "Deniz Alt\u0131nb\u00fcken",
        "Campbell Fraser",
        "Lyric Doshi",
        "Vincent Zhuang",
        "Gaurav Jain"
    ],
    "title": "Kepler: Robust Learning for Parametric Query Optimization",
    "date": "2023-05-26",
    "abstract": "Most existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training anML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.",
    "sections": [
        {
            "title": "INTRODUCTION",
            "paragraphs": [
                "Parametric query optimization (PQO) aims to optimize parameterized queries, i.e. queries that have identical SQL structure and only differ in the value of bound parameters. Such parameterized queries are ubiquitous in modern database usage and present a significant opportunity for improving query performance because they are executed repeatedly. However, PQO has primarily been studied from the perspective of reducing query planning time by avoiding re-optimization when possible [7,9,13,17,18,34]. Such approaches are implicitly constrained by the performance of the system's query optimizer, and therefore inherit all of the well-studied sub-optimalities of traditional query optimizers [22]. Thus, an ideal system for parameterized queries should not only seek to minimize planning time via PQO, but also optimize query execution performance via query optimization (QO).",
                "A variety of approaches have attempted to improve query optimization by applying machine learning [20,25,29,38,39]. Unfortunately, most learned query optimization techniques suffer from at least four drawbacks: (1) they require inference times higher than traditional methods [19,24], (2) they have inconsistent performance across dataset sizes and distributions [19,26,31], and (3) they often have unclear query performance improvements [19]. Worse yet, many of these learned systems lack (4) robustness: regressions in query performance are unacceptable in most production scenarios [12]. This poses an especially large challenge for learning-based approaches, since they typically cannot guarantee that all of their predictions result in improved execution time [35].",
                "We propose that restricting the query optimization problem to the parameterized query setting poses a more tractable learning problem and hence can be more robustly solved. To this end, we present Kepler (K-plan Evolution for Parametric Query Optimization: Learned, Empirical, Robust), an end-to-end learning-based approach for parameterized queries. Building on prior work in PQO [34], Kepler leverages a novel plan generation strategy, a training query execution phase, and a robust neural network model design. Combined, we show that these techniques provide significant improvements in both planning time and query execution performance, satisfying both the PQO and QO objectives. Best of all, Kepler's use of robust neural network techniques drastically reduces the frequency and magnitude of performance regressions. Figure 1 highlights how each of Kepler's components contribute to a 2.41x geometric mean speedup across the entire Stack benchmark [25].",
                "Kepler follows a decoupled plan generation and learning-based plan prediction architecture similar to the approach of [34] with three key differences. First, Kepler provides the key insight that designing better candidate plan generation algorithms can lead to substantially faster plans than the built-in optimizer's. We propose Row Count Evolution (RCE), a method that efficiently generates candidate plans by perturbing the optimizer's cardinality estimates. RCE only requires a simple interface to any standard cost-based optimizer, making it compatible with most database systems.",
                "Second, Kepler leverages actual query execution data to build a training dataset for best-plan prediction, avoiding the well-studied mismatch between cost models and execution latency [22]. While Kepler's collection of execution data may be costly if the parameterized query is run infrequently, we argue that the additional execution data in our setting is justified by (1) the scale of parameterized queries in production and (2) the query execution speedups afforded by RCE.",
                "Third, Kepler uses robust neural network prediction techniques to decrease tail latency and reduce query regressions (i.e. worse performance than the existing query optimizer). Specifically, Kepler uses Spectral-normalized Neural Gaussian Processes (SNGPs) [23] to accurately quantify how confident it is about a prediction, and falls back to the database's query optimizer when it is uncertain."
            ],
            "subsections": []
        },
        {
            "title": "Our contributions.",
            "paragraphs": [
                "\u2022 We identify a novel and practical formulation of query optimization for parameterized query templates in which speedups against a classical query optimizer can be robustly achieved. \u2022 We propose a novel candidate plan generation algorithm, Row Count Evolution (RCE), that produces significant speedup compared to classical query optimizers on real-world and synthetic datasets. \u2022 We demonstrate that incorporating robust ML techniques allows models to capture large portions of the speedups while greatly reducing the risk of regressions. \u2022 We demonstrate that our model inference costs are negligible via an end-to-end PostgreSQL integration for the query path. \u2022 We open-source both our system implementation for PostgreSQL 1 as well as our query execution datasets, which we believe is the first dataset tailored towards parameterized query optimization. The datasets collectively represent \u223c14.2 CPU years of query execution time. They serve as a benchmark for further work on best-plan prediction as well as simulating more efficient techniques for training data collection."
            ],
            "subsections": []
        },
        {
            "title": "RELATED WORK",
            "paragraphs": [
                "Parametric query optimization. PQO has been extensively studied in a variety of works [7,9,13,17,18,34]. The goal of the standard PQO formulation is to reduce the amount of times the query optimizer is invoked while minimizing the corresponding regression in query latency [7,13,34].",
                "Although Kepler also focuses on parametric queries, its primary objective is closer to that of standard query optimization, which seeks to improve query latencies. Kepler also simultaneously improves on the PQO objective by leveraging fast-inference ML models.",
                "Prior PQO approaches typically make simplifying assumptions such as heavily relying on the optimizer cost model or using base table selectivities as input features [13,34]. This may be feasible for some advanced commercial systems; however, this over-reliance on the existing optimizer is particularly dangerous given the well-studied deficiencies of optimizers such as PostgreSQL [22].",
                "Our approach follows a similar structure as [34], which also decouples the populateCache (candidate generation) and getPlan stages (ML-based prediction). However, since they focus on the standard PQO objective of attempting to match the existing optimizer, they require using a bandit algorithm to reduce their training data cost. By contrast, the primary objectives of Kepler are query performance and robustness, leading to a lower emphasis on training query efficiency.",
                "Several popular database systems have implemented PQO features, including Oracle Adaptive Cursor Sharing, Aurora Managed Plans, and SQL Server Parameter Sensitivity Plan optimization [1][2][3]. These features all heavily rely on their cost models (based on traditional statistics and heuristics), and do not utilize machine learning models.",
                "Query plan generation. Several prior works suggest methods for candidate generation, which we divide into four main categories.",
                "(1) Default optimizer plans. The simplest method combines the optimizer's selected plan for each query instance. This approach is frequently found in PQO algorithms since they seek to cache the optimizer's plans [34]. This strategy is also employed in [30] to estimate the empirical suboptimality of existing query optimizers. The quality of the resulting candidate plan set is predicated upon the optimizer's ability to either generate optimal plans for each query instance or a sufficient variety of good plans across the workload to benefit from plan sharing. However, we empirically observed that the optimizer fails to do so on real-world datasets. (Table 10b). ( 2) Cost-based plan pruning. populateCache algorithm [34] extends the default optimizer candidate generation method with cost-based \ud835\udc3e-set identification to prune the candidate set to size \ud835\udc3e. However, this pruning method may mistakenly prune good plans if the correlation between the cost estimates and actual execution times are poor. (3) Optimizer configuration parameters. Query optimizers typically expose a variety of configuration parameters that can be used to alter their query planning behavior. In particular, PostgreSQL has configuration parameters that allow one to disable entire classes of join and scan operators from being used in query plans. Bao selectively applies subsets of these parameters in order to generate new query plans [25]. Although simple, disabling operator types is a heavy-handed and indirect approach to generating new plans. (4) Exact cardinalities. Exact cardinality query optimization (ECQO) attempts to construct the optimal plan by computing the plan induced by the exact cardinality values of all possible sub-plans [10]. However, for sufficiently complex queries, evaluating these exponentiallymany sub-plans is prohibitively slow even with optimizations [32]. The selected plans are also not always the fastest, as observed by [30].",
                "In summary, these methods are all unsatisfactory for a variety of reasons: failure to generate faster plans (1,2,4), ineffectively exploring the plan space (3), or are computationally intractable (4).",
                "Machine learning for query optimization. A wide range of techniques apply ML on QO, most notably for predicting cardinality estimates (CE) [20,38,39]. Recent work show cardinality estimation may be brittle in practice, and that even small Q-errors can lead to noticeably worse plans [22,35]. In general, these work do not measure the actual end-to-end execution latency of selected plans after integrating their models into an optimizer [24].",
                "Several approaches have demonstrated improved query performance, but typically do not consider the issue of robustness. Neo [26] and Bao [25] leverage tree convolutional neural networks to adaptively optimize plans using reinforcement learning and contextual bandits respectively. These online algorithms offer no guarantees on stability or regression avoidance, and hence cannot reliably be deployed in production. Similarly, techniques applying deep reinforcement learning to QO have not demonstrated consistently better performance and suffer from robustness issues [21,28,37]. For example, Figure 9 in [37] indicates a significant amount of regressions both at train and test time."
            ],
            "subsections": []
        },
        {
            "title": "OVERVIEW",
            "paragraphs": [
                "In this section, we describe our problem setting (Section 3.1), give an overview of our approach (Section 3.2), and further discuss specific design choices that are made in Kepler (Section 3.3).",
                "109:5"
            ],
            "subsections": [
                {
                    "title": "Problem Setting",
                    "paragraphs": [
                        "As in prior work [34], we consider parameterized queries that are repeatedly invoked with different parameter bindings. Such queries are specified by a template \ud835\udc44 with \ud835\udc5a parameterized predicates2 \ud835\udc65 0 , . . . , \ud835\udc65 \ud835\udc5a-1 of varying data types. We let \ud835\udc5e denote a specific query instance, i.e. \ud835\udc44 with a fixed set of parameter binding values. A query plan \ud835\udc5d associated with a template \ud835\udc44 specifies how to execute any query instance \ud835\udc5e \u223c \ud835\udc44. A sub-plan query of \ud835\udc44 is \ud835\udc44 restricted to only a subset of its tables [14], and its output cardinality is referred to as its sub-plan cardinality. We assume a fixed database system with a built-in query optimizer, and denote the default plan \ud835\udc5d default (\ud835\udc5e) to be the plan selected by the query optimizer for \ud835\udc5e. Finally, a workload \ud835\udc4a \u2282 W consists of a set of query instances {\ud835\udc5e 0 , . . . , \ud835\udc5e \ud835\udc5b-1 } for a single template \ud835\udc44, where W denotes the space of all possible query instances."
                    ],
                    "subsections": []
                },
                {
                    "title": "Kepler Overview",
                    "paragraphs": [
                        "Our approach at a high level follows that of [34]: we consider a single, isolated query template \ud835\udc44, and decouple the problems of generating a set of possible plans and deciding which plan to use for each query instance. More formally, these problems can be described as:",
                        "(1) Candidate generation. Generate a candidate set of \ud835\udc58 plans {\ud835\udc5d 0 , . . . , \ud835\udc5d \ud835\udc58 -1 } for \ud835\udc44, out of the exponentially-large set of all possible plans P (corresponding to populateCache in [34]). (2) Best-plan prediction. Learn a mapping \ud835\udc40 : W \u2192 \ud835\udc43 that minimizes some objective, e.g. some measure of execution latency over the workload (corresponding to getPlan in [34]). Unlike [34], who attempt to match the performance of the built-in optimizer, our goal is to improve upon the built-in optimizer as much as possible. To achieve this, Kepler includes a sophisticated candidate generation algorithm, described in Section 4, that empirically generates better plans than the built-in optimizer. The afforded speedups allow Kepler to avoid relying on potentially-brittle online learning approaches (e.g. contextual bandits) during the training data collection phase.",
                        "Objective. We first define several key metrics and terms in our problem setting. For a given query instance, we denote the optimal plan over some plan set \ud835\udc43 as \ud835\udc5d \ud835\udc43 opt = min \ud835\udc5d \u2208\ud835\udc43 \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47 \ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d, \ud835\udc5e), where \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47 \ud835\udc56\ud835\udc5a\ud835\udc52 refers to the actual execution time. We define \ud835\udc5d * opt (\ud835\udc5e) as the optimal plan over all possible plans, i.e. when \ud835\udc43 = P. Typically, the optimal plan refers to \ud835\udc5d * opt for candidate generation and \ud835\udc5d \ud835\udc43 opt for modeling. We also refer to near-optimal plans as plans that have similar execution time to \ud835\udc5d \ud835\udc43 opt or \ud835\udc5d * opt . For some fixed candidate set \ud835\udc43, we define the (oracle) speedup ratio relative to the default plan as:",
                        "This quantity is the factor by which we can improve the total execution time of the workload if we had oracle access to the optimal plan in \ud835\udc43 for each query instance. We note that this ratio corresponds exactly with the definition of execution cost sub-optimality in [34]; the re-naming to speedup emphasizes the differences in our system objectives. Since we can union \ud835\udc43 with the set of all default plans over \ud835\udc4a , this speedup ratio is always lower bounded by 1.",
                        "Similarly, for some model \ud835\udc40 : W \u2192 \ud835\udc43, we define its model speedup as:  This quantity corresponds to how much faster the model is at executing a workload than the default optimizer. Although \ud835\udc46 model is by definition upper bounded by \ud835\udc46 opt , it is not necessarily lower bounded by 1, i.e. if the model selects plans worse than the default plan.",
                        "An auxiliary objective of Kepler is reducing workload tail latency. Several work have identified that database optimizers may perform significantly worse in the tail of the query latency distribution, which poses a significant obstacle for use cases that require a more uniform runtime [25].",
                        "Kepler architecture. Figure 2 shows the architecture of Kepler, consisting of a Kepler trainer and Kepler client. The trainer ingests query instances from the query logs produced by production DBMSs and aggregates them into query templates. For each query template \ud835\udc44 \ud835\udc56 , the Kepler trainer aims to find the near-optimal plans for all its query instances \ud835\udc5e \ud835\udc57 . It uses Row Count Evolution (RCE) to generate candidate plans \ud835\udc5d \ud835\udc58 and executes the queries with these plans to collect execution statistics. To minimize impact on production DBMSs, the trainer may optionally request a production DBMS to spawn ephemeral instances to execute these queries. The Kepler trainer trains an ML model to predict the best plan for \ud835\udc5e \ud835\udc57 based on these execution statistics and deploys the trained models into the production DBMSs.",
                        "A Kepler client maintains a mapping from a query template to an ML model. When a production DBMS receives a query instance \ud835\udc5e, the client first checks if an ML model is available for \ud835\udc5e. If available, it performs model inference to predict the best plan hints and provides the hints to the optimizer only if the associated confidence score is higher than a threshold. Otherwise, it falls back to the built-in optimizer to produce a plan.",
                        "Changing environments and workloads. In our current implementation, Kepler assumes a fixed system state, including database configuration, optimizer implementation, and data distribution. If any of these aspects changes relatively slowly or infrequently, Kepler can periodically collect new execution data and retrain purely on data from the new system state. We posit that in the majority of production parameterized query use cases, (1) the database is reconfigured infrequently, and (2) the data distribution drifts slowly, e.g. in scenarios in which a relatively small amount of similarly-distributed data is added each day. Additionally, Kepler is designed to be robust to dynamic workloads in which query parameter binding values change by detecting when inputs are out of its training distribution (see Section 7.4). Limitations. The target usage of Kepler is for parameterized queries that are executed frequently enough to justify the training data collection cost. As discussed in Section 8, the exact training data collection regime in this paper serves the dual purposes of definitively demonstrating the speedups available and enabling further research in efficiency. We anticipate a final production system will use an iteration of this research with leaner training data collection. The cost-benefit analysis of using Kepler is situation-dependent; ultimately the user must weigh the potential query performance gains against the cost. If ephemeral instances are used for training data collection, Kepler assumes they are representative of production query performance."
                    ],
                    "subsections": []
                },
                {
                    "title": "Kepler Design Choices",
                    "paragraphs": [
                        "In this section, we further discuss the specific design choices made to ensure that Kepler can be reliably deployed with minimal production overhead.",
                        "Using actual execution latencies. Since the objective of Kepler is to reduce actual end-to-end query latencies, it necessitates executing queries on a real database to provide ground-truth signal. To minimize the training collection time and avoid load on the production system, the DBMS may spawn ephemeral instances to speed up and isolate the training execution process.",
                        "Limiting reliance on cost models. By collecting actual execution latencies, Kepler eschews explicitly relying on optimizer cost estimates for determining the quality of a plan. Figure 3 shows the estimated vs. exact cardinalities of all joins on a sample of query instances from Stack [25]. In particular, 64% of points have estimated cardinality = 1, likely due to the independence assumption of the PostgreSQL optimizer.",
                        "Falling back to the built-in optimizer. Kepler avoids regressions by falling back to the existing query optimizer when it is not confident in identifying the optimal plan. Given the low overhead of model inference, the overall Kepler inference cost is nearly always lower than that of Opt-Always. For cases where planning time is a concern due to high fallback frequency, one can incorporate an additional model designed to predict a safe plan without re-invoking the optimizer.",
                        "Independence of query templates. Kepler handles query templates independently, i.e. each query template will generate its own candidate plans, collect its own training data, and train a model specific to that template. Though potentially more expensive than a procedure that generalizes over multiple query templates, this design has the advantages of 1) providing a more tractable ML problem, and 2) isolating each query from regressions caused by changes pertaining to other queries as models iterate over time and new query templates are on-boarded. Leveraging shared information between query templates while not increasing the risk of regressions is an interesting direction for future work."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "ROW COUNT EVOLUTION",
            "paragraphs": [
                "The goal of the candidate generation stage is to construct a set of plans \ud835\udc43 such that it contains a near-optimal plan for every query instance \ud835\udc5e in the workload distribution W. Additionally, \ud835\udc43 should be sufficiently small such that it is feasible to execute each plan for training dataset collection. Balancing these two competing objectives is the main challenge for any candidate generation algorithm.",
                "In this work, we only consider generating fully-specified plans, i.e. the join order and every join/scan method are defined. Alternatively, a candidate generation algorithm could specify a subset of the plan decisions and allow the query optimizer to determine the remainder.",
                "Workload candidate generation. Given an algorithm \ud835\udc34 for generating a candidate plan set over a single query instance \ud835\udc5e, we define the corresponding plan set over a workload \ud835\udc4a as the union of the per-instance plan sets \ud835\udc34(\ud835\udc4a ) := \ud835\udc5e \u2208\ud835\udc4a \ud835\udc34(\ud835\udc5e) (see lines 1-5, Algorithm 1). We also define plan sharing to describe the case where \ud835\udc5d \ud835\udc43 opt (\ud835\udc5e) is generated from some other query instance \ud835\udc5e \u2032 (i.e. \ud835\udc5d \ud835\udc43 opt (\ud835\udc5e) \u2209 \ud835\udc34(\ud835\udc5e), \ud835\udc5d \ud835\udc43 opt (\ud835\udc5e) \u2208 \ud835\udc34(\ud835\udc5e \u2032 )). Our approach. We propose Row Count Evolution (RCE) 3 , a computationally-efficient algorithm that generates new plans by randomly perturbing the optimizer's cardinality estimates. RCE is predicated on the idea that cardinality misestimates are the primary underlying reason for optimizer suboptimality. RCE exploits the fact that our candidate generation stage only needs to generate a set of plans that contains a (near-)optimal plan instead of directly identifying a single performant plan. Like Bao [25], RCE leverages the built-in query optimizer to generate candidate plans, but does so in a more fine-grained and efficient way.",
                "We instantiate the idea of applying random perturbations as an evolutionary-style algorithm, described in Algorithm 1. RCE maintains a sequence of generations of plans, with the initial generation consisting solely of the query optimizer's plan. To construct subsequent generations, RCE first uniformly samples parent plans from the previous generation. For each of these base plans, RCE perturbs the join cardinalities of only the sub-plans that appear in the parent plan by multiplicative factors sampled from an exponentially-spaced range (lines [21][22][23][24][25][26][27][28]. By repeating this process multiple times and feeding in the resulting perturbations into the query optimizer, RCE generates a set of children plans (lines [14][15][16][17][18]. Out of these, only unseen plans (i.e. those that did not appear in any prior generation) are kept for the next generation (lines 17-18). for (base plan \ud835\udc5d, row count map \ud835\udc5f ) \u2208 \ud835\udc35 \ud835\udc54 do 14:",
                "for \ud835\udc56 = 1, 2, . . . , \ud835\udc41 do 15: \ud835\udc52 \ud835\udc59 \u2190min(log \ud835\udc4f (\ud835\udc64), \ud835\udc5a)  Example. Figure 4 shows an example of RCE generating candidate plans for a query instance with two generations. The base plan joins the result of \ud835\udc34 \ud835\udc35 and \ud835\udc36 \ud835\udc37 with estimated |\ud835\udc34 \ud835\udc35| = 40, |\ud835\udc36 \ud835\udc37 | = 17. RCE first constructs a set of candidate row counts for each sub-plan by perturbing their cardinalities by multiplicative factors. These candidate row counts for \ud835\udc34 \ud835\udc35 and \ud835\udc36 \ud835\udc37 are [4,40,400] and [1,17,170], respectively, using a base of 10 and a range of 1. RCE then uniformly samples new join cardinalities from these sets; one sample of 400 and 17 influences the optimizer to produce a new plan Plan-1 in generation 1 1 . It repeats the same process N times to produce \ud835\udc36 1 plans in generation 1. Next, RCE samples \ud835\udc46 from a deduplicated set of plans from generation 1 2 and randomly perturbs the row counts on each sampled plan \ud835\udc41 times to generate \ud835\udc36 2 plans in generation 2 3 . RCE as exact cardinality matching. One interpretation of RCE is that it efficiently builds a covering set of exact-cardinality plans. The RCE-generated candidate set contains plans generated from a diverse range of perturbed sub-plan cardinalities. If there are sufficiently many perturbations, likely at least one will be reasonably close to the exact cardinalities for any particular query instance and their respective induced plans will also likely be similar.",
                "Multiplicative perturbations. Applying multiplicative perturbations is well-motivated by the standard metric of Q-error in cardinality estimation. RCE further uses an exponentially-spaced perturbation set in order to have a similar support as the optimizer's Q-error distribution.",
                "Perturbing only relevant sub-plans. Instead of perturbing all 2 \ud835\udc5b -1 sub-plans (for a query joining \ud835\udc5b tables), RCE only perturbs the cardinalities of the \ud835\udc5b -1 sub-plans that actually appear in the sampled query plans. This significantly increases the efficiency of RCE with only a small loss of generality: since the set of perturbations is inherited between generations, a misestimated sub-plan cardinality will only never be perturbed if its cardinality is significantly overestimated by the query optimizer. However, this is an unlikely scenario since query optimizers tend to underestimate sub-plan cardinalities due to the independence assumption.",
                "This re-optimization of only the sub-plan cardinalities that appear in the optimizer plan bears a strong resemblance to the re-optimization procedure in [36], which iteratively re-optimizes using sampling-based cardinality estimates. The key differences in our setting are (1) we do not have to return a single plan, and (2) we require a fast procedure since we repeat it for each query instance, motivating the use of perturbations over sampling. RCE as local search. RCE effectively explores the plan space via a random walk in the lowdimensional subspace of sub-plan cardinalities, initialized at the optimizer's cardinality estimates. This formulation implicitly leverages the fact that while these initial estimates are typically incorrect, they are still more informative than random estimates. RCE hyperparameters. Our implementation of RCE includes a variety of hyperparameters that allow one to flexibly trade off the number of generated plans against the potential total speedup (Table 1).",
                "\u2022 Width and depth of the perturbation tree. Increasing the number of generations \ud835\udc3a increases the number of plans, making it more likely a good plan is found. However, plans in later generations are perturbed further from the original plan, and may have a lower likelihood of being relevant. To ensure constant-time processing for each generation, we sample (up to) a fixed number \ud835\udc46 of base plans in each generation, and perturb each one \ud835\udc41 times. \u2022 Perturbation values. The exponent base \ud835\udc4f and range \ud835\udc5a limits the magnitude of a single perturbation. We also introduce a sub-plan perturbation limit that controls the number of times a specific sub-plan can be perturbed, effectively controlling the total perturbation range of any given sub-plan. \u2022 Direct limits on number of plans. We implement limits on the number of plans that can be generated from a single parameter and in total. Once the limit is reached, the evolutionary candidate generation process is terminated and only default plans are kept for remaining parameters. The total plans limit is a soft limit since the final evolutionary iteration may produce up to the single-parameter limit and the remaining parameters may contribute new default plans."
            ],
            "subsections": []
        },
        {
            "title": "TRAINING DATA COLLECTION",
            "paragraphs": [
                "After generating candidate plan set \ud835\udc43, we execute each plan over a training workload to generate a dataset of execution latencies for supervised best-plan prediction. The training workload may be provided by the user or captured in a DBMS query log [34]. Rather than executing all candidate plans for each query instance, we use adaptive timeouts and construct near-optimal plan covers to prune suboptimal plans.",
                "Execution mechanics. We force the optimizer to produce a candidate plan by providing all join/scan methods and the join order via hints. We parallelize the execution of query instances and their candidate plans in multiple databases. We simulate a warm buffer cache scenario by executing each plan multiple times and taking the minimum as the estimated latency [22]. This repeated execution strategy also reduces the potential noise in our execution time measurements; though we observed the amount of noise to be inconsequential in our experimental setup. We leave a full analysis of query execution time under different caching, concurrency, and resource availability settings to future work.",
                "Adaptive timeouts and plan execution reordering. We use a timeout policy to minimize wasted resources on executing sub-optimal candidate plans. The timeout policy adapts from [37] with two main modifications. First, we always execute the default plan first and adaptively reorder the remaining plans to maximize the impact of the timeout's progressive tightening on a per queryinstance basis. We execute plans in ascending order of their historical execution latencies across query instances as a simple heuristic for tightening the timeout as quickly as possible. Second, we do not apply the tightened timeout for the first iteration of each plan in order to ensure that each plan simulates a warm-cache scenario.",
                "Online plan cover pruning. We also use an online plan pruning technique to eliminate plans based on actual execution time. Specifically, we initially execute all plans for the first \ud835\udc41 query instances of a query template, then use a Set Cover formulation to prune down to a minimal plan cover set for the remaining query instances. The pruned set becomes our \ud835\udc58 candidate plans for the query template, i.e. our models only attempt to predict from those plans.",
                "We consider a plan to be near-optimal for a query instance \ud835\udc5e if its execution time is within a 1 + \ud835\udf16 factor of the fastest time for \ud835\udc5e we have seen so far. Each plan has an associated set of query instances for which it is near-optimal. The plan cover is the smallest set of plans such that each query instance has a near-optimal plan in the set. We construct the plan cover using the standard greedy approximation for Set Cover, which iteratively picks the plan that is near-optimal for the most remaining query instances. We additionally relax the problem to require that only 1 -\ud835\udeff of all query instances be covered, allowing us to trade off the plan cover size and the achievable speedup.",
                "Tail latency reordering. For many query templates, the distribution of default execution latencies is heavy-tailed. Parameters in the tail tend to be more sub-optimal, and therefore have an outsized impact on the total speedup. To ensure the plan cover computation includes these parameters, we evaluate these query instances first. This reordering produces a 7-8x reduction in total execution time and number of plans over the entire Stack dataset."
            ],
            "subsections": []
        },
        {
            "title": "ROBUST BEST-PLAN PREDICTION",
            "paragraphs": [
                "After collecting a full training dataset of actual execution latencies over our candidate plan set, we use supervised ML to predict the best plan for any query instance. Kepler trains one model for each query template with the objective to maximize workload speedup while minimizing regressions. Kepler also falls back to the optimizer's plan when its predicted confidence is low. Section 7 shows that the inference time of our model is negligible compared to the typical query planning time of a classical optimizer."
            ],
            "subsections": [
                {
                    "title": "Features",
                    "paragraphs": [
                        "Given a template \ud835\udc44 with \ud835\udc5a parameters, Kepler uses solely the \ud835\udc5a parameter values as input features. The supported types include numerics (float/int), strings, and dates/timestamps. We apply standard preprocessing techniques to each type: embeddings for strings/low-dimensional integer features, normalization to \ud835\udc41 (0, 1) for numerics, and numeric conversion for date/time features.",
                        "We do not convert the parameter values to their respective base table selectivities as in [34] for the following reasons. First, selectivity is inherently a lossy representation and may obscure information when two distinct values have the same selectivity. Second, selectivity is inferior when the optimizer's cardinality estimation is sub-optimal, see Figure 3.",
                        "String columns and vocabulary selection. For each string-valued column, we construct a fixed-size vocabulary in order to limit model size. String features are one-hot encoded via a lookup table, with buckets for out-of-vocabulary values. An embedding layer is then applied on this one-hot encoding, creating a learnable embedding for each value in the vocabulary.",
                        "We choose the vocabulary as the top-\ud835\udc58 values ordered by the total possible improvement of all query instances with that value. We define the max marginal improvement strategy as selecting the top-\ud835\udc58 column values \ud835\udc63 in column \ud835\udc56 under the following objective:",
                        "Our evaluation shows that this strategy is effective. For columns with significantly more distinct values, one may factorize embeddings over subcolumns [38]."
                    ],
                    "subsections": []
                },
                {
                    "title": "Training Objectives",
                    "paragraphs": [
                        "Kepler models maximize the model speedup defined in Equation 2 while minimizing the number of regressions. This objective is not directly differentiable, so we discuss various surrogate learning objectives.",
                        "Multi-label classification. We model best-plan prediction as a multi-label classification problem in which each near-optimal plan has a positive label (as opposed to just the optimal plan) [33]. The multi-label objective also provides a richer supervised signal, improving the quality of the learned intermediate representations. We use the single-label transformation of multi-label classification loss by training the near-optimal probability of each candidate plan with binary cross-entropy loss.",
                        "Although our models only predict plans in the plan cover, which may not necessarily contain every query instances' default plan, our definition of near-optimality does exploit the availability of default plan execution data during training. We define a plan to be near-optimal if its estimated latency improvement is within a 1 + \ud835\udf0f factor of the optimal improvement latency. Namely, we say a plan \ud835\udc5d is near-optimal if (\u2113 \ud835\udc51\u2113 \ud835\udc5d ) (1 + \ud835\udf0f) \u2265 (\u2113 \ud835\udc51\u2113 \ud835\udc5c ), where \ud835\udf0f > 0, \u2113 \ud835\udc51 = \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47 \ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d default , \ud835\udc5e), \u2113 \ud835\udc5d = \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47 \ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d, \ud835\udc5e), and \u2113 \ud835\udc5c = min \ud835\udc5d \u2208\ud835\udc43 \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47 \ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d, \ud835\udc5e). Computing near-optimality requires execution times for all query instances for all plans.",
                        "Prior work formulate best-plan prediction as a regression [6,27,34] and multi-class classification problem [34]. Both formulations are unsatisfactory for a variety of reasons. Regression across a significant range can be unstable, a problem that is exacerbated by our timeout procedure, which obscures the true latency of suboptimal plans. Regression attempts a more challenging problem with finer granularity than required, imposing unnecessary constraints and objectives on the training. We only need to predict the identity of the optimal plan rather than its execution time. Inversions and gross over-estimates of non-optimal plans are acceptable to us but will weigh heavily in regression loss. Meanwhile, classification objectives that predict a single optimal plan perform poorly in scenarios when multiple plans can be near-optimal and empirical execution latencies can be subject to noise. For example, consider a problem where plans \ud835\udc5d 1 , \ud835\udc5d 2 execution latencies' are both drawn from \ud835\udc36 + \ud835\udc41 (0, 1) for some large \ud835\udc36. Then a multi-class classifier will have equal predicted likelihood for \ud835\udc5d 1 and \ud835\udc5d 2 and thus have low confidence, when in actuality being confident in \ud835\udc5d 1 and/or \ud835\udc5d 2 is desirable.",
                        "Example-dependent loss. Different query instances may have disproportionate impact on the overall objective Equation 2. We leverage the standard sample-weighting approach example-dependent cross entropy [8,16] to prioritize those with the largest improvement delta. For plans worse than the default plan, we upweight them by a factor \ud835\udc36. For all near-optimal plans, we apply a soft weighting based on their empirical execution improvement, i.e. 1 + \ud835\udc37 log(\u2113 \ud835\udc51\u2113 \ud835\udc5d ), where \ud835\udc36 and \ud835\udc37 are both tunable hyperparameters."
                    ],
                    "subsections": []
                },
                {
                    "title": "Models",
                    "paragraphs": [
                        "We use simple feedforward neural networks as our base models. For inference efficiency, we consider a neural network with one output head per plan on top of a shared representation, which improves inference speed and model size over approaches that have separate models for each plan [34].",
                        "We train our neural network models with standard minibatch SGD. In a real-world setting, the model's hyperparameters can be tuned via simple search techniques or more sophisticated algorithms by partitioning the training data into a train and validation set.",
                        "Uncertainty. Kepler models incorporate calibrated predictions and uncertainty estimates to avoid predicting significantly suboptimal plans. Two state-of-the-art approaches for incorporating uncertainty into neural networks are ensembling and Spectral-normalized Neural Gaussian Processes (SNGPs) [23]. The former trains \ud835\udc40 distinct models simultaneously and estimates the uncertainty from their joint outputs. The latter applies spectral normalization to all layers, providing a bi-Lipschitz guarantee on all intermediate representations, and uses a Gaussian process output layer to efficiently estimate the uncertainty. Since ensembling increases the training and inference cost by a linear factor \ud835\udc40, Kepler uses the SNGP approach due to its lower overhead."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "EXPERIMENTS",
            "paragraphs": [
                "Our evaluation of Kepler seeks to demonstrate that it robustly achieves state-of-the-art execution latency speedups on parameterized query workloads. We summarize our main results as follows:",
                "\u2022 An end-to-end implementation of Kepler on PostgreSQL substantially outperforms the builtin optimizer and Bao. Both RCE and ML models play large roles in achieving this speedup. (Section 7.2) \u2022 RCE discovers significantly better plans than existing candidate generation baselines. We also observe that RCE plans are frequently superior to exact-cardinality plans. (Section 7.3)",
                "\u2022 Using SNGP models is crucial to capturing speedups generated by RCE while minimizing query regressions. (Section 7.4). \u2022 We release a dataset consisting of \u223c14.2 years of query executions as a benchmark for future research in modeling approaches (Section 7.5).",
                "Objectives. To evaluate our methods, we use both RCE speedup \ud835\udc46 opt (\ud835\udc45\ud835\udc36\ud835\udc38) (shortened as \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 ) and model speedup \ud835\udc46 model , defined in Equations 1 and 2 respectively. We note that \ud835\udc46 model = \ud835\udc5d \u2022 \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 , where 0 \u2264 \ud835\udc5d \u2264 1 corresponds to the proportion of the speedup the model captures. Since the model may predict worse plans than the built-in optimizer, we also measure the query regression frequency \ud835\udc43 reg , defined as the proportion of test query instances the model does at least 10% worse than the default optimizer on. The primary metrics for each of our components are:",
                "(1) End-to-end performance: \ud835\udc46 model (2) Candidate generation performance: \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 (3) Model performance: \ud835\udc5d, \ud835\udc43 reg Kepler aims to maximize \ud835\udc46 model by maximizing \ud835\udc5d and \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 , while minimizng \ud835\udc43 reg . We also report the 99th-percentile tail latency speedup, which may be relevant in applied scenarios. We define this as \ud835\udc43 method 99",
                "\ud835\udc5d 99 ( {\ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47 \ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d method ,\ud835\udc5e) \u2200 \ud835\udc5e \u2208\ud835\udc4a } ) , where \ud835\udc5d 99 (\ud835\udc36) denotes the 99th percentile highest value in a collection \ud835\udc36."
            ],
            "subsections": [
                {
                    "title": "Setup",
                    "paragraphs": [
                        "Datasets and query extraction. We use two synthetic benchmarks: TPC-H (uniform and skewed with Zipf factor = 1, 10 GB [4]), and Stack, a database consisting of real-world StackExchange data [25]. TPC-H consists of 22 parameterized queries. We use an augmented version of Stack with 87 parameterized queries: 42 from the original benchmark and 45 additional manually-written query templates.",
                        "All experiments were run using PostgreSQL 13.5 on Google Cloud Platform (GCP) n1-highmem-16 instances with 16 CPU cores, 108 GB of RAM, and 2 TB of SSD. Following [22], we set shared_buffers to 75 GB, effective_cache_size to 80 GB, and work_mem to 4 GB to ensure that the entire dataset fits in memory. For TPC-H, we use the indexes defined in BenchBase [11]. For Stack, we add indexes on all primary keys, foreign keys, and columns that appear in a predicate of any query.",
                        "Query instance generation. We follow the official TPC-H specification [5] to generate parameter values of each query template. For Stack, we synthetically generate parameter values so that every query instance returns nonempty results. This is accomplished by uniformly sampling rows from the result set of a derived query that selects column values for which parameterized predicates would produce at least one value. Range predicates are constructed by first sampling a single value in the manner, then sampling lower/upper bounds around this value.",
                        "Training query execution. For each query instance and plan hints, we execute the resulting plan three times to simulate a warm-cache scenario, and take the minimum latency as the ground truth. For slow queries, we executed each up to 8 times in parallel on the same machine, and observed negligible differences with the serial execution setting. We leave a full analysis of different execution scenarios to future work."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "RCE hyperparameters.",
            "paragraphs": [
                "Unless stated otherwise, we use the same values for all RCE hyperparameters in all of our experiments, demonstrating its efficacy even when untuned for specific benchmarks. We set the number of generations \ud835\udc3a to 3, the exponent base \ud835\udc4f to 10, the exponent range \ud835\udc5a to 2, the number of perturbations per plan \ud835\udc41 to 20, and the number of samples extracted    from each generation \ud835\udc46 to 20. For each query template, we run RCE on the first 50000 query instances for Stack, and all query instances for TPC-H.",
                "Model details. All of our experiments use a fixed base neural network with three layers of 64 hidden units each. We use Adam with learning rate 3e-4, ReLU activation functions, and 10dimensional string embeddings. For SNGP models, we additionally apply spectral normalization to all dense layers, and replace the output dense layer with a random Fourier feature Gaussian Process with 128 random features. For all models, we fall back to the default plan if the predicted confidence is less than 0.9. For all queries, we use a 80/20 train/test split, and report results (speedups, regressions) on the test workload. We did not attempt to tune our models or perform model selection, although it is straightforward to do so by reserving a validation set from the training dataset. End-to-end performance. We integrate Kepler into the PostgreSQL query optimizer to demonstrate that it delivers large speedups in a real deployment. Our implementation loads Tensorflow Lite models on the database server for fast CPU model inference and uses the pg_hint_plan extension to force specific plans via hints. Providing the query id as a comment with the SQL query text from any PostgresSQL client connection triggers Kepler query plan prediction."
            ],
            "subsections": [
                {
                    "title": "Kepler Improves",
                    "paragraphs": [
                        "We executed a sample of 1000 evaluation set query instances per query on the integrated Kepler PostgresSQL system.  Figure 5b confirms that the Kepler deployment achieves near-identical speedups to those expected based on the pre-collected execution dataset. This is because the use of lightweight ML models and planning hints incur low planning-time overhead. Figure 5c shows the distribution of the ratio of model inference times to PostgreSQL planning time for all queries in Stack. The model inference time is mostly under 5% of PostgreSQL planning time and at most 30%.",
                        "Our total speedup results over entire workloads are quite significant since our workloads -query instances sampled uniformly from the space of non-empty query instances -are not designed to adversarially challenge the optimizer. Next, we summarize the contributions from the two key components: (1) RCE to uncover the potential speedups and (2) the ML models to capture speedups. Finally, we compare the results to Bao as a baseline. RCE speedups. We illustrate the efficacy of RCE by showing that it achieves large speedups on both Stack and TPC-H. Figure 6 shows the per-template \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 and \ud835\udc43 \ud835\udc45\ud835\udc36\ud835\udc38 99 , with RCE achieving over 2x speedup on 32/87 queries and over 1.2x speedup on 78/87 queries.",
                        "Similarly, RCE improves 6/22 queries on TPC-H uniform (Figure 7a) and 9/22 queries on TPC-H skewed (Figure 7b). In particular, RCE finds larger speedups on TPC-H skewed due to the non-uniformity in its data distribution.",
                        "ML models predict fastest plans and avoid regressions. Next, we show that our ML models are able to robustly capture the speedup produced by RCE, i.e. they maximize \ud835\udc5d while minimizing \ud835\udc43 reg . In Figures 8a and8b, we plot what proportion of \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 and \ud835\udc43 \ud835\udc45\ud835\udc36\ud835\udc38 99 on Stack we respectively capture per query. These distributions show that our models can reliably predict near-optimal plans: our models capture over 80% of \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 on over 80% of Stack queries. In Figure 9, we plot the distribution Count (log scale)"
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Model improvement Model regression",
            "paragraphs": [
                "Fig. 9. Query improvements and regressions on all query instances in Stack. of the absolute magnitude of model improvements and regressions compared to the default plan, illustrating that the frequency and magnitude of the regressions are minimal compared to those of the improvements.",
                "Bao on parameterized queries. We evaluate Bao, one of the few prior approaches that demonstrates actual improvement in execution latency, on our parameterized version of Stack [25]. For illustrative purposes, we run Bao for 2000 query instances on six representative templates from the original Stack dataset [25]. As shown in Table 2, we observed that Kepler outperforms Bao on 5 out of 6 templates, and in particular is able to find far greater speedups on q15_0 and q16_0. As we later show in Figure 10a, this is because the candidate generation algorithm in Bao is severely suboptimal.",
                "Training data collection cost. The speedups achieved by Kepler come at a nontrivial training query execution cost -on average, we used 39 CPU days worth of query execution time per query template. Hence, Kepler is most applicable to workloads where query templates are executed at high frequency. We discuss directions to significantly reduce this training data collection cost at the end of Section 7.4 as well as Sections 7.5 and 8."
            ],
            "subsections": [
                {
                    "title": "Analyzing RCE",
                    "paragraphs": [
                        "Having observed the behavior of the overall system, we now discuss characteristics of the first major component of Kepler: RCE. We evaluate RCE's performance against candidate generation and plan pruning baselines before exploring the effects of index configuration and key hyperparameter choices. After discussing RCE's empirical performance against those of exact cardinality (EC) plans, we close the section by offering perspectives and empirical justifications for why RCE works well.",
                        "Comparison against baselines. We compare against two main candidate generation baselines:",
                        "\u2022 PG: the default candidate generation method (Section 2) using the PostgreSQL optimizer. q11_0 q12_6 q15_0 q16_1 q20_0 q21_1 q25_3 q33_0 q34_1 (b) Comparison between PG and RCE with alternative pruning algorithms. We report average and 99th-percentile speedup aggregated using geometric mean, as well as the percentage of queries having total speedup greater than 20%. PG + CBP corresponds to the method in [34].  \u2022 Bao: for each query instance, we try every Bao arm, i.e. all 48 valid combinations on disabling various join/scan methods in PostgreSQL [27].",
                        "Since [34] consider PG + cost-based pruning (CBP) as their candidate generation method, we simultaneously compare RCE against PG and our plan-cover pruning (PCP) algorithm against CBP in Table 10b. RCE finds significantly more speedups than PG: \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 is over 1.2 on 89% of Stack queries, as opposed to 49.4% for PG. Although CBP achieves little speedup loss when applied to PG, it incurs far greater loss when applied to RCE, indicating that optimizer cost estimates cannot reliably predict the quality of RCE plans. By contrast, PCP achieves almost no degradation in speedup since it uses actual execution data to evaluate plans.",
                        "The Bao candidate generation method produces far more plans than RCE, so for computational reasons we evaluated it on a diverse subset of queries designed to be representative of the entire Stack dataset. Figure 10a shows that RCE achieves similar or better speedup on all queries, and is notably able to find non-trivial speedup on each query. shown in Table 3. The number of plans and plan cover size steadily grows up to three generations, although most of the speedup is captured in plans found using two generations. Increasing \ud835\udc3a to four generations does not produce any marginal benefit.",
                        "Exponential row count perturbation range: \ud835\udc4f and \ud835\udc5a. To justify our choice of perturbation range hyperparameters \ud835\udc4f = 10 and \ud835\udc5a = 2, we analyzed the perturbation factor necessary to induce a change at a single level in the plan. For each of \ud835\udc5b -1 sub-plans in a query tree (for a query \ud835\udc44 with \ud835\udc5b tables), we use binary search to identify the factor by which the cardinality estimate for that sub-plan must be perturbed in order to induce a change in the optimizer's plan. On Stack, we observed that although some plans needed a 10 6 perturbation factor to induce a plan change, the vast majority of factors were less than 10 2 .",
                        "Robustness to index configuration. RCE finds better plans on databases with different index configurations. We ran RCE over the following index configurations for Stack: primary keys only (PK), foreign keys (FK), predicate columns, and database administrator defined additional indexes [25]. Table 4 shows RCE finds faster plans in all configurations. Similar to [22], we find that more indexes leads to a larger speedup.",
                        "Can RCE discover optimal plans? Although RCE demonstrably generates faster plans than a variety of baselines, we would also like to know how close are RCE-generated plans to the true optimal plans \ud835\udc5d * opt (\ud835\udc5e). Since it is infeasible to determine \ud835\udc5d * opt (\ud835\udc5e) in practice, we instead compare against the standard benchmark of exact cardinality plans [22,29].",
                        "We executed exact cardinality plans for a subset of the training workload and compared them against their respective best RCE plans in Table 5. We again used a subset of queries from the Stack dataset for computational reasons. Due to the large number of joins in some query templates, we additionally set the exact cardinality for a subset of tables to be a high constant if its corresponding query did not finish within 15 minutes. Notably, RCE plans are substantially faster than exact cardinality plans on 5 out of 6 queries. This illustrates that even accurate cardinality estimation methods can lead to suboptimal plans due to incorrect assumptions and other deficiencies in the cost model.  Query clusters facilitate RCE.. Query instances with similar parameter binding values often have similar query plan behavior, e.g. as visualized by plan diagrams [15]. We hypothesize that these groups of similar instances, or clusters, can dramatically increase the efficacy of RCE via plan sharing. Recall that in our candidate generation procedure, we execute RCE for each query instance and take the union over all resulting plan sets. Hence, each cluster only requires a single query instance's RCE process to reach the cluster-wide optimal plan. For a cluster of size \ud835\udc41 and probability P(\ud835\udc5e \ud835\udc56 ) of query instance \ud835\udc5e \ud835\udc56 discovering the optimal plan, the overall probability of discovering the plan over the cluster is 1 -\ud835\udc41 \ud835\udc56=1 (1 -P(\ud835\udc5e \ud835\udc56 )), which rapidly approaches 1 for sufficiently large \ud835\udc41 and a reasonable distribution of P(\ud835\udc5e \ud835\udc56 ).",
                        "To demonstrate the existence and impact of clusters, we define a cluster for each plan \ud835\udc5d as all query instances for which \ud835\udc5d is the fastest plan. Then, for each query instance \ud835\udc5e, we compare the execution latencies of the fastest plan from three plan sets: (1) RCE-all, containing all plans from all query instances, (2) RCE-cluster, containing all plans from query instances in the same cluster as \ud835\udc5e, and (3) RCE-instance, containing only the plans generated from the RCE process for \ud835\udc5e. As shown in Table 6, RCE-all and RCE-cluster have very similar execution times, while RCE-instance is often slower, indicating that intra-cluster plan sharing plays a large role in the efficacy of RCE."
                    ],
                    "subsections": []
                },
                {
                    "title": "ML Models",
                    "paragraphs": [
                        "We first motivate the use of ML models by demonstrating that Stack is highly parameter sensitivei.e. different query instances have different optimal plans. We then justify modeling design choices with an ablation of using SNGP in our models and evaluation of varying confidence thresholds highlight the importance of incorporating robustness as a primary design component in Kepler. We conclude with extensive analyses around feature space selection, embedding vocabularies, and training data size.   Parameter sensitivity. We investigate the parameter sensitivity of Stack and TPC-H queries based on our execution data. On Stack, all but four query templates had plan cover size greater than one (Figure 11a). In Figure 11b, we show the distribution of single-best plan suboptimality ratios, defined as the ratio of total latency of the oracle best plan against the total latency of the single best plan (i.e., the fixed plan with minimum total execution time). Ratios less than 1 indicate that using only a single plan incurs a loss in speedup, with lower values being more severely suboptimal. Thus, Figure 11b implies that multiple plans are necessary to capture the full speedup.",
                        "On the other hand, TPC-H is designed to not be parameter sensitive, which we confirmed by observing a plan cover of size 1 for all queries. Hence, our modelling results focus solely on Stack.",
                        "Loss functions/training objectives. Figure 12 compares various loss functions and models: SNGP with multilabel loss, SNGP with multiclass loss, and a vanilla NN with multilabel loss. Multilabel loss + SNGP achieves similar or better speedup to other methods, while having a lower regression frequency for all queries.",
                        "Model calibration and uncertainty. We further investigate the ability of our SNGP models in producing calibrated output probabilities. In Figure 13, we plot the test workload model speedup and regression frequency as a function of confidence threshold varying from 0 (no falling back) to 1 (always falling back). The captured speedup and regression frequency both decay smoothly as a function of confidence, which allows the user to specify their tolerance for regressions. This   figure also demonstrates the importance of the fallback mechanism: one can dramatically reduce the regression frequency while only sacrificing a small portion of the speedup.",
                        "SNGP out-of-distribution detection. Although Kepler is designed for relatively static workloads, it is robust to dynamic workloads by falling back to the default plan for out-of-distribution (OOD) inputs. We evaluate SNGP's OOD detection ability by holding out specific slices of the training distribution for an example query, q21_2 on Stack. We consider two variants: (1) holding out sites totaling up to 20% of the workload, and (2) holding out the last 20% of last_activity_date values on the question table. Figure 14 shows that in both scenarios, Kepler's fallback mechanism allows it accurately detect OOD inputs and drastically reduce \ud835\udc43 reg while still preserving some speedup.",
                        "Raw parameter values vs selectivity features. In Figure 15a, we ablate our feature choice using raw features and selectivity features. We compare their model speedups to \ud835\udc46 \ud835\udc45\ud835\udc36\ud835\udc38 on a subset of Stack queries. Selectivity features perform far worse due to the poor cardinality estimates in PostgreSQL.",
                        "Vocabulary. In Figure 15b, we ablate how we select the embedding vocabulary for string features. In particular, we evaluate our max marginal improvement method (described in Section 6) against choosing the most frequent values based on the PostgreSQL histogram. Our results confirm that the best strategy is choosing the vocabulary to the be the values with the most potential impact on the speedup.",
                        "How much training data is required? We evaluate the performance of our models when using less data by subsampling the training data size, as shown in Figure 16. As expected, model speedup improves with more training data while maintaining a regression frequency below 0.2%. The leftmost point uses only 5% of the data, or 200 training query instances, demonstrating that a large amount of speedup can be robustly captured with small amounts of training data. "
                    ],
                    "subsections": []
                },
                {
                    "title": "Dataset Contribution",
                    "paragraphs": [
                        "To train models and evaluate Kepler, we generated a query execution dataset that comprises \u223c14.2 years of CPU time across 200 million query executions of 131 query templates. By releasing this dataset online 4 , we envision that it can be used to facilitate future research in modeling and efficiency without having to execute any queries. For example, possible use cases include simulating active learning approaches that only selectively execute a subset of queries, or developing better models or loss functions. We believe that the techniques developed using this dataset -and not the specifically trained models -will be directly transferable to other parameterized query settings."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "CONCLUSION AND FUTURE WORK",
            "paragraphs": [
                "We introduced Kepler, a system that can robustly speed up parameterized queries using a learningbased approach. We extensively evaluated Kepler on PostgreSQL and demonstrated that (1) our novel candidate generation algorithm RCE can provide significant speedups in query execution latency, and (2) robust ML models can reliably predict faster plans while avoiding regressions. Interestingly, we observed that RCE-generated plans were often far better than exact cardinality plans, indicating that even a widely-used system as PostgreSQL has significant room for improvement. Evaluating Kepler on database platforms other than PostgreSQL is a natural next step; we believe that the empirical nature of Kepler allows it to discover performance gains regardless of the DBMS.",
                "There are a myriad of future directions for improving the efficiency and performance of Kepler. For example, prior work in cardinality estimation can benefit Kepler in several ways. Instead of perturbing uniformly, RCE can leverage generative cardinality distributions to sample higher likelihood perturbations, e.g. from NeuroCard [38]. Another possibility is to augment the model features with the query plan tree and selectivity estimates, allowing the model to determine when cardinality estimates are accurate, as well as leveraging shared structure between similar query templates. Our models are the first demonstration that speedups can be robustly captured; they can likely be substantially improved via additional modeling techniques and tuning. Similarly, while our end-to-end PostgreSQL integration is sufficient to demonstrate Kepler's performance gains on a real system, every aspect of this implementation can be further tuned.",
                "We utilized an expensive training data collection procedure in order to make more robust claims about our results and produce a complete dataset for further modeling and efficiency research. For practical purposes, our training procedure can likely be made much more efficient, e.g. via active learning. In conjunction with Figure 16, this implies that similar performance can be achieved with significantly less training cost."
            ],
            "subsections": []
        }
    ]
}