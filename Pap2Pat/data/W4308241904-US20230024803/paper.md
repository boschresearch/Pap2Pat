# Introduction

Segmenting and understanding a long video sequence is very important for many applications, such as surveillance, intelligent advertisement, etc. It is of high importance to automatic human assistance systems where video sequences tend to have a temporally ordered list of actions. For example, an instructional video of 'making coffee' tends to have a temporal order of actions such as 'add milk', 'take cup', 'stir', etc. Recently, several approaches have achieved significant success in segmenting activities in videos [6,9,19,21,38]. Despite their success, these approaches are limited by their need for time-consuming and costly-to-obtain frame-level annotation. To overcome these need for frame annotations, many have started to explore approaches to train with weaker supervision in the form of timestamp supervision [25][26][27], transcripts [2, 22,31,35], or even sets [10,23,30]. In timestamp supervision, only a single frame of each action segment is annotated. At the same Figure 1. In a semi-supervised setting, we have a combination of a considerable number of unannotated videos and a few annotated video sequences. The aim is to leverage additional information from the 'in-domain' unannotated video sequences to boost the performance. As annotating frame labels are costly, semisupervision can help in scaling the training data without additional cost.

time, the videos are annotated with an ordered list of actions devoid of the starting and ending time in transcript-level supervision. Finally, in set-level supervision, only the set of actions are used in training. These set level annotations lack any order information or indication of the repetition for each action within a video.

On the one hand, weak supervision, such as transcripts or sets, drastically reduces the annotation cost, the overall performance is far from satisfaction as these approaches often fail to correctly align actions in time. On the other hand, an annotator needs to go through each video in detail to generate timestamp supervision. This is a significant bottleneck towards scaling the training data substantially without incurring a high cost. In this paper, we propose a semi supervision approach for the action segmentation task that addresses the limitations of the current level of supervisions. Our semi-supervised data consist of a small amount of detailed frame annotated videos and a large corpus of unannotated videos as shown in Fig 1 . Though at a limited quan-tity, the use of frame labels provides more substantial supervision than the transcript or set annotation, as it helps to locate and better understand the action variations within a segment.

Due to the lack of substantial supervision in unannotated videos, leveraging the additional knowledge from these videos can be challenging. A simple approach to solve this problem would be to use the confident frames in the unannotated video based on a threshold as additional frame labels. This, however, raises a question in the form of defining the confidence as classification confidence grows over the training cycle. Further, any early-stage misidentification might propagate the error in later stages -resulting in poor overall performance. Moreover, any model can easily overfit the training data due to few annotations, resulting in false confidence on unannotated videos. Finally, longer video sequences tend to have temporal order across various action segments, which the above approach fails to consider.

In this work, we propose a different approach where we use predictions from multiple streams of segmentation models to compute temporally ordered frame labels on the unannotated data. Despite limited annotations, the proposed multiple streams generate more accurate and robust predictions by repeated refinement and accumulation. Further, our method also learns to predict an ordered list of actions with a sequence-to-sequence model for the unannotated videos. This predicted ordered list is later used as an alignment constraint to maintain the temporal order while estimating frame labels based on the frame predictions. Any error in the temporal order prediction can significantly impact frame label computing. We, therefore, use beam search to generate multiple ordered candidates to find the optimal matching. This ensures better chances of estimating the correct frame labels for the unannotated data.

Our contributions thus are two folds as follows:

• We introduce an approach to train a temporal action segmentation model from semi supervision. The approach computes frame labels for an unannotated video aligned to the predicted temporal order of actions.

• We propose multi-stream segmentation models that repeatedly refine and accumulate predictions to reduce any noise that may arise from overfitting on the limited training data.

# Related Work

Fully Supervised Action Segmentation: The goal of action recognition [3,11,33] has been to classify short trimmed videos. This is quite different from temporal action segmentation where one requires to capture long-range dependencies in order to classify each frame of the input video. Many approaches in past combined frame-wise classifiers with grammars [28,37] or with hidden Markov models (HMMs) [16,18,20], to capture this dependency. With the recent advancement of neural networks, many recent approaches utilized temporal convolutional networks to capture long-range dependencies for the temporal action segmentation task [19,21]. Despite accurate predictions, these approaches still suffer from an over-segmentation problem. In order to overcome this problem, recent state-of-the-art methods proposes a multi-stage architecture with dilated temporal convolutions [5,9,13,14,24,38]. Chen et al. [6] further enhances the performance by utilizing activity level domain adaptation to compute consistent action labels and reduce over-segmentation. Yet, these approaches rely on each from annotation resulting in high cost towards real world deployment. On the contrary, in this paper, we perform the temporal action segmentation task in a semisupervised setup.

Weakly Supervised Action Segmentation: To circumvent the need for expensive annotations of full-supervision, weakly supervised action segmentation has been gaining significant interest recently. Early approaches, in this context, used discriminative clustering based on distinctive action segments to detect and locate temporal action segments using movie scripts [2, 8]. Despite the apparent presence of temporal ordering in the scripts, these approaches failed to harness them in a meaningful manner. However, Bojanowski et al. [2] were the first to use an ordered list of actions as supervision, thus explicitly modeling the action order to detect and segment actions in a video. Due to the inherent ordering of the actions and the video frames, many researchers recently started addressing the task as an alignment problem between video frames and the transcripts using connectionist temporal classification [12], dynamic time warping [4] or energy-based learning [22]. On the other hand, approaches such as [7,17,18,29] iteratively generate and refine pseudo ground truth labels for the training. Few approaches, such as [31,34,35], use the temporal ordering constraints over-frame predictions to generate the target labels. Lastly, some approaches [10,23,30] tried tackling the action segmentation problem with a much weaker set level supervision without the temporal order. Eventhough these approaches have improved recently, their qualitative performance is far from that of fully supervised approaches due to errors in aligning these actions in time. Bridging the gap, recently [25] proposed an approach with timestamp supervision that focused on estimating the action transition in time. Even though we aim to reduce the annotation cost similar to these approaches, we also want to leverage additional information from unlabeled data. This will help in scaling the training data without any additional cost.

# Semi-Supervised Temporal Action Segmentation

The task of the temporal action segmentation task is to predict frame action labels for a given input video. Formally, for a given video of T frames X = [x 1 , . . . , x T ], the objective is to obtain frame action labels Y = [y 1 , . . . , y T ]. Here x t ∈ R D and y t ∈ C are the frame embedding and action label at time t respectively. C is the set of action classes. The frame labels Y can also be perceived as sequence of N action steps S = [s 1 , . . . , s N ]. Here, s n ∈ C is the n th action step. This ordered sequence of steps S = [s 1 , . . . , s N ] is also known as video transcript. Note the two representations Y and S for are related.

For the current semi-supervised formulation, we consider that only a tiny percentage of the training videos (X s ) have frame-level supervision (Y s ). We scale the training data with additional videos (X u ) with neither frame annotations nor action transcripts. Note, both supervised and unsupervised videos are assumed to be 'in-domain'. The focus is to leverage additional information from unannotated data and improve the overall performance paving the way towards scaling training data with less cost. We now present the proposed method in detail. Here, the objective of the method is to leverage additional information from unannotated training videos. This leveraging can be done by computing frame labels for those videos. Despite all these videos being 'in-domain', computing frame labels for an unannotated video is not trivial. Moreover, wrong labels might confuse the model and thus impacting the overall performance. We know that videos have a natural order of actions e.g. while making an omelet, pour oil always precede pour egg action. Thus, our approach focuses on predicting the action order/transcripts for the videos. We argue that the action transcripts should be aligned to their frame action predictions and thus can be used as constraints while computing frame labels for the unannotated videos. Furthermore, noisy or erroneous predictions can impact the label generation despite the transcript constraints. Thus, we also propose multi-stream distillation and collection to repeatedly refine the predictions in a single forward pass in an end-to-end manner.

# Proposed Method

We first present our temporal action segmentation model based on multi-stream distillation and collection in Section 4.1. It is then followed by the details of sequence-tosequence modeling used for the transcript generation model in Section 4.2. In Section 4.3, we describe our sequence matching module to compute the additional frame labels. Finally, we provide the details of all the loss functions in Section 4.4.

## Distill and Collect: Multi-Stream Predictions

As shown in Figure 2, we generate the additional frame labels and the action transcripts based on the frame action predictions. Thus, any noisy predictions can impact the overall performance. To reduce the noise, we propose a refinement strategy based on multiple streams of segmentation. The objective of the refinement strategy is to repeatedly distill the next segmentation stream and finally collect all the frame predictions across multiple streams.

### MSTCN++: Baseline Model

Our proposed approach uses multiple streams of MSTCN++ [9, 24] segmentation models.

We first briefly describe the standard MSTCN++ model, which is the current state-of-the-art architecture for action segmentation. The architecture uses pre-computed frame features X = [x 1 , . . . , x T ] to compute frame labels Ŷ = [ŷ 1 , ŷ2 , . . . , ŷt ]. It is performed by multiple stages of the temporal convolutions network. The initial stage is also known as prediction or generation stage, consisting of a dual dilated layer that combines two temporal convolutions with different dilation factors. It is, then, followed by multiple layers of a single-stage temporal convolution network as known as SS-TCN. A combination of frame-wise cross-entropy loss and smoothing loss [9, 24] is used to train the segmentation model.

In the current semi-supervised setting, where (X s , Y s ) and (X u , Ŷu ) corresponds to annotated and un-annotated video sequence, our frame-wise losses are defined as follows:

Here, L f s and L f u are the frame loss over the annotated and unannotated videos, respectively. ŷt,c and ŷt,ĉ are the predicted probability for true class c and estimated-label ĉ, respectively. Same as in [9, 24], ∆t,c = min(τ, | log ŷt,clog ŷt-1,c |) and τ = 4 and β = 0.15. α is a weight factor to control the impact of computed-labels as they might be erroneous.

### Distill -Learn from previous stream:

In 4.1.1, we have explained how a single vanilla stream MSTCN++ is trained using ground truth frame labels and estimated labels. However, the estimated frame labels for unannotated videos are computed based on the model predictions, as explained later in Section 4.3. Hence, computing robust and generalized frame predictions is imperative During training, we feed a sequence of pre-computed frame features into our proposed multi-stream MSTCN++ to predict frame-level action predictions. For the annotated video sequence, we use these predictions to train our 'Transcription Generation Module' and 'Multi-Stream MSTCN++' using L g s and L f s respectively. Here, the Transcription Generation Module' predicts the M possible candidate transcripts for an unannotated video sequence. We, then, compute the frame labels Ŷu for these videos by finding an optimal match in the Sequence Matching Module. Lastly, the optimally matched frame labels Ŷu is also used to further train both 'Transcription Generation Module' and 'Multi-Stream MSTCN++' using L g u and L f u . to reduce subsequent frame label computation errors. Similar to [36], we can use multiple streams to computer robust. However, it has some limitations: 1) It is very timeconsuming as each stream requires individual computation for label computation; 2) Estimated labels for each stream may vary, which will impact the convergence; 3) It fails to harness the internal structure of MSTCN++.

Instead, we propose distillation across multiple streams as shown in Figure 3. Note, the distilled knowledge is only passed to the first stage of the following MSTCN stream instead of the final stage of MSTCN. This enables the following MSTCN to build upon the distilled knowledge. We define our distillation loss (L d ) as follows:

Here, l and L denotes the index of a stream and the total number of streams, respectively. ŷl t,c is the predictions of l th stream at time t for a class label c. τ = 4.

Note, the 1 st stream is only trained with ground truth labels from the annotated videos. Although less generalized, the usage of clean labels at the first stream ensures no-incorrect seed predictions are passed onto the refinement streams. However, in the refinement streams, we additionally use the estimated labels to increase the generalization.

### Collect -Multiple Streams:

Even though using multi-stream distillation, as explained in 4.1.2, helps in refining predictions, ambiguity in predictions still exists. This is especially true at time instants where action transition takes place in the video. We further reduce the noise in predictions by collecting the predictions of all the streams. This collection step is defined as follows:

l and L denotes the index of a stream and the total number of streams, respectively. ŷl t,c is the predictions of l th stream at time t for a class label c.

Note, our usage of multiple streams increases the size of the overall model in comparison to a single MSTCN++. Post-training, this might be a bottleneck during inference of a test sample where memory is limited. In such cases, the predictions only from the final MSTCN++ stream can be used. Experiments at Section 5.4.6 showcases the inference performance tradeoff when using the final stream in comparison to the overall model.

## Transcript Generation Module:

In this section, we present the details of our transcript generation module that will be used in the computation of frame labels for the unannotated videos in Section 4.3. The objective of the transcript generation module is to translate the temporal frame probabilities of a video into its action transcript. To this end, we use sequence-to-sequence model with a bidirectional LSTM encoder to encode the input sequence. This is followed by an LSTM with MLP attention as decoder [1]. It is well known that such sequenceto-sequence model is very ineffective in handling long sequences. This is a critical problem as the input video sequence can be significantly longer for such models to handle. However, due to high similarity between consecutive frames, most temporal information is similar and redundant. Thus, we reduce the effective length of the input video sequence by dividing it into K non-overlapping temporal segments. We then temporally 'maxpool' the class probabilities within these segments to collapse them into K length temporal sequence as shown in Figure 2. In this way, we can significantly reduce the temporal length of the video sequence needed for the sequence-to-sequence model to generate action transcripts effectively.

Ideally, we expect the decoder of our transcript generation module to generate the exact action order for those videos. However, in reality, our transcripts may contain a few errors. We use beam-search decoding to generate M possible candidate transcripts for estimating the frame labels, to counteract such errors. Further, if we have additional knowledge about the unannotated video, such as the knowledge of the overall activity, we can add the domainheuristics during beam-search to generate more relevant transcripts. e.g. while preparing coffee, it is highly unlikely that the action crack egg is performed. Thus, our decoding can ensure the action crack egg is not present in the candidate transcripts.

We train our Transcript Generation Module with both annotated videos and unannotated videos. The video transcript S s is easily obtained from the frame labels Y s to train upon for annotated videos. However, the unannotated videos lack any such information. Instead, we use the best Ŝu among the M candidates after our Sequence Matching Module explained later in Section 4.3 to train our transcript generation module upon. We define our transcript predictions loss over both annotated and unannotated videos as follows:

Here, L g s and L g u are a cross-entropy loss over ground truth and estimated transcripts respectively. N is the length of the transcript excluding the EOS token. Note, ŝn,c and ŝn,ĉ are n th step transcript predictions for annotated step c and estimated step ĉ respectively. α is a weight factor to control the impact of estimated transcripts as they might be erroneous.

## Sequence Matching Module:

Given the possible set of transcripts S u and the frame probabilities ŷt , our aim is to compute the frame-labels ( Ŷu ) for the unannotated videos. This will enable us to utilize the additional labels from the unannotated videos to train the model better. To this end, we use Dynamic Time Warping [32] to find an optimal alignment between a transcript and the frame predictions. We select the alignment corresponding to the least alignment scores over M candidate transcripts. Note, |S u | = M . The overall sequence matching module is defined as follows:

Ŷu , Ŝu = arg min

(8) is the equation for the DTW where C S is the cost matrix between frame probabilities and a candidate transcript S. Y * S is a optimal alignment of the frame predictions with a candidate transcript S. Ŷu and Ŝu are computed frame labels and transcripts based on DTW matching over M candidate transcripts. ŷt,sn is the action probability at time t and action (s) at transcript step n.

## Loss Functions:

As shown in ( 1) and (2), MSTCN++ combines crossentropy loss and smoothing loss over both ground-truth and estimated labels. The distillation loss in (4) enables the model to refine the predictions across multiple streams repeatedly. Finally, the transcript prediction loss from ( 6) and (7) helps in predicting the action order that is used to generate the estimated labels on the unannotated data. Thus, the overall loss is defined as:

Here, L s and L u are the overall losses for annotated videos and unannoated videos respectively. Note, we set α = 0.3 in ( 1) and ( 7) and β = 0.15 for our experiments.

# Dataset and Experiments

## Datasets and Metrics

Datasets: We evaluate our approach on two datasets: Breakfast [15], and Hollywood Extended [2].

The Breakfast dataset consist of 1712 videos of breakfast preparing activities with roughly 3.6M frames. Here, the frames are annotated with 48 action classes. The dataset has four splits. We follow the leave-one-split-out strategy for evaluation as explained in [15]. However, to mimic annotated and un-annotated data for the training, we additionally considered leave-one-split annotated, thus effectively reducing the amount of annotation roughly by 1/3. Note, at any moment, one split is used for testing, another one split as annotated data, and the rest two are unlabeled data. Our final result is an average of all possible split combinations.

The Hollywood extended dataset contains 937 video sequences taken from Hollywood movies. The videos contain 16 different action classes. We divided that data into training and testing sets of 843 and 94 videos, respectively. This corresponds to seed 5 of the original setting [2]. We, further, divide the training set into three equal splits of 281 videos. To mimic annotated and unannotated data, we assumed only one training split is annotated, and the rest two are unannotated, thus reducing the amount of available annotation by 1/3. The final result over the test set is the average of all possible combinations of annotated and unannotated training splits.

Metrics: We use the standard metrics for temporal action segmentation such as frame accuracy (MoF), segmental edit distance (Edit), and segmental F1 scores at overlapping thresholds 10%, 25% and 50% on the Breakfast dataset. For Hollywood extended, we use the standard non-background frame accuracy (MoF-BG) and intersection over Detection (IoD) as explained in [16]. Additionally, we also report the segmental edit distance (Edit) and segmental F1 scores at overlapping thresholds 10%, 25%, and 50% for the Hollywood dataset.

Baselines: Our baseline is a standard MSTCN++ [24] when trained only with the annotated data, using the above experimental framework of only one split being annotated. Whereas, MSTCN++ [24] trained on original training data is considered as full-supervised

## Implementation Details:

As explained used MSTCN++ [24] as our base model. We train our model for 12000 steps with Adam optimizer. To minimize the impact of initialization, we compute the frame-wise cross-entropy loss and segment-wise cross-entropy loss on the unannotated data only after the first 2000 steps. We set the learning rate to 0.0005, and the batch size to 3. We use the I3D features per frame for both Breakfast and Hollywood Extended as input to our model, respectively.

## Comparison with the Baselines:

In this section, we compare the proposed approach for semi-supervised action segmentation against a simple baseline (as explained before) that fails to utilize the additional annotated data. The results on the two datasets are shown in Table 1. Despite training on a few annotated data, our approach significantly outperforms these baselines in all the evaluation metrics. It achieves nearly 0.9 times MoF and 0.87 times MoF-BG on the full supervised Breakfast and Hollywood extended dataset, respectively.

## Ablation Studies:

### Impact of Domain Knowledge:

The crux of the proposed algorithm lies in predicting the correct transcripts for the unannotated videos. Any error in the transcripts impacts the sequence matching module and the subsequent estimation of frame labels. Thus, we compare our results (No-Heuristics) against the scenario where we have been provided with ground-truth transcripts for the unannotated videos. We denote it as mixed supervision.

Table 2. Performance impact of using correct transcripts to estimate labels on the additional videos. No-Heuristics, Domain-Heuristics and Mixed-Supervision corresponds to different level of transcript correctness. The table showcases the result over all the splits of the Breakfast dataset. F1 = {0.1,0.25,0.5} Edit MoF No-Heuristics 61.5 55.2 41.7 59.9 60.1 Domain-Heuristics 63.9 57.6 43.7 61.8 62.9 Mixed-Supervision 64.2 58.1 44.4 62.9 63.5

Note, we do not need our sequence generation module in such a scenario. Additionally, we experimented with the case where our action predictions are constrained by the domain-heuristics of the videos (as explained in 4.2). Table 2 showcases the result on the Breakfast dataset with increasing order of transcript correctness. Despite having no prior knowledge about the video content, our approach can still achieve comparable results to mixed supervision.

### Impact of amount of Labelled Data:

The amount of labelled data impacts the performance of the temporal segmentation. Our approach improves its performance with the increase in labelled data as seen by results on Split 3 (test set) as shown in Table 3. 100% corresponds to the result with full supervision. We also display that our approach achieves nearly 0.77× frame wise accuracy with  

### Impact of Multiple-Stream (L):

The proposed approach accumulates the prediction of multiple streams to have a consistent prediction. This is shown by the quantitative improvement in Table 4. In this experiment, we utilize the Breakfast dataset with domain heuristics. The results improve from a frame accuracy of 57.9% to 62.9% when the number of streams increases from 2 to 4. We also experimented with full-supervised models and usage of multiple streams. Our experiments improve the frame accuracy from 67.6% to 70.1% when the number of streams is increased from 1 to 4. Note, there is no change in each stream of the MSTCN++ model during full-supervision apart from the use of proposed distillation (4) and collection (5).  Our proposed approach uses a combination of distillation and collection of predictions to reduce noise and increase robustness. Though, in theory, both contribute towards better prediction, the question remains with their overall impact. In Table 5, we disentangle each of the processes individually. In the experiment, Only Distil uses only distillation without the collection step (5). In this case, the prediction of the final stream is used to compute the labels and during inferencing, both. Only Collect uses multiple stream without the distillation loss (4). Though distillation improves the result compared to the baseline, the experiment showcases that the collection of predictions over multiple streams contributes more towards the overall approach.

### Impact of M candidates:

Beam-search decoding enables our approach to generate multiple transcript candidates. Table 6 showcases the impact of various candidates on the overall performance. Using a value of M = 5 increases the result slightly compared to just greedy decoding (M = 1) for a transcript. Note, the computation cost of the sequence matching module increases with an increase in value of M . Thus, we set M = 5 for all our experiments, which enables us to rectify any greedy decoding errors and retain tractable computation costs. Table 6. Performance impact due to M candidates. The experiment was performed on all the splits of the Breakfast dataset. F1 = {0.1,0.25,0.5} Edit MoF M = 1 61.2 54.9 40.8 58.9 59.9 M = 5 61.5 55.2 41.7 59.9 60.1

### Performance tradeoff: Using Final Stream vs

Overall Model: As stated earlier, we collect the predictions of all the streams to compute the final prediction. This might be a bottleneck during inference with limited memory. A simple tradeoff uses only the final stream instead of the whole trained model and ignores the collection step. Table 7 showcases the performance tradeoff in such cases. Despite the drop in performance when using only the final stream, it is still far better than the Baseline model, where additional training data was ignored.

# Ablation-Studies: Impact of α

In Eq. (2) and Eq. ( 7), we use a weight factor α to control the errors in the estimated transcripts from impacting the overall model. If the estimated transcripts were accurate and α = 1, this proposed semi-supervised approach would be equivalent to full-supervision. Table 8 showcases the impact of the α on the overall performance. Increasing the value of α impacts the performance negatively as the model is forced to learn the errors with higher confidence. On the other hand, setting α to a lower value enables the approach to balance the impact of the transcripts error towards the overall frame label prediction. Thus, we set our value to α = 0.3 for all our experiments. In this section, we compare our approach with semisupervised annotation against the recent state-of-the-art approaches. To the best of our knowledge, semi supervision has not been studied extensively. Results for the Breakfast, and Hollywood extended datasets are shown in Table 9 and Table 10, respectively. Our approach is comparable to timestamp supervision and full-supervision despite using roughly 1/3 rd of training data on Breakfast dataset.

This demonstrates the capability of our approach to scaling quickly when additional 'in-domain' data is available. We also compare our approach with the weakly supervised setup where the action order in the form of transcripts is only provided. Due to the availability of detailed frame annotations for a few videos, the model performs significantly better than any transcripts based approach on Breakfast dataset. On Hollywood extended, however, our approach is on par with few state-of-the-art transcript based approaches.

# Conclusion

In this paper, we proposed an approach that leverages additional knowledge from 'in-domain' unannotated videos to train a temporal action segmentation model when a limited annotation is available. Our approach is based on multistream distillation that repeatedly refines and finally com-bines their frame predictions. We further predicted the action order that is used as an order constraint while computing the frame labels of unannotated videos. Our experiments on two different action datasets showcase comparable performance to full-supervised approaches despite limited annotation. Additionally, we also demonstrate the impact of the proposed distillation and accumulation on full supervision, pointing towards its potential across various levels of supervision.

