# Introduction

Autonomous vehicles need to navigate in dynamic environments in a safe and comfortable manner. This requires predicting the future motions of other agents to understand how the scene will evolve. However, depending on each agent's intention (e.g. turning, lane-changing), the agents' future motions can involve complicated maneuvers such as yielding, nudging, and acceleration. Even worse, those intentions are not known a priori by the ego-robot, and agents may also change their minds based on behaviors of nearby agents. Therefore, even with access to the ground-truth trajectory histories of the agents, forecasting their motions is very challenging and is an unsolved problem.

By leveraging deep learning, the motion forecasting community has been making steady progress. Most stateof-the-art models share a similar design principle: using a single feature vector to characterize all the informa- tion related to an actor, as shown in Fig. 1, left. They typically first encode for each actor its past motions and the surrounding map (or other context information) into a feature vector, which is computed either by feeding a 2D rasterization to a convolutional neural network (CNN) [59,60,41,4,37,8], or directly using a recurrent neural network (RNN) [62,49,16,61,18,2]. Next, they exchange the information among actors to model interactions, e.g., via a fully-connected graph neural network (GNN) [54,6,41,49,7,16] or an attention mechanism [26,43,52,44,34]. Finally, they predict future motions per actor from its feature vector via a regression header [29, 49,41,59,8,30,55].

Although such a paradigm has shown competitive results, it has three main shortcomings: 1) Representing the context information of large regions of space, such as fast moving actors traversing possibly a hundred meters within five seconds, with a single vector is difficult. 2) Building a fully-connected interaction graph among actors ignores important map structures. For example, an unprotected left turn vehicle should yield to oncoming traffic, while two spatially nearby vehicles driving on opposite lanes barely interact with each other. 3) The regression header does not explicitly leverage the lane information, which could provide a good inductive bias for accurate predictions.  consequence, regression-based predictors sometimes forecast shooting-out-of-road trajectories, which are unrealistic.

In this paper, we propose a graph-centric motion forecasting model, i.e., LaneRCNN, to address the aforementioned issues. We represent an actor and its context in a distributed and map-aware manner by constructing an actor-specific graph, called Lane-graph Region-of-Interest (LaneRoI), along with node embeddings that encode the past motion and map semantics. In particular, we construct LaneRoI following the topology of lanes that are relevant to this actor, where nodes on this graph correspond to small spatial regions along these lanes, and edges represent the topological and spatial relations among regions. Compared to using a single vector to encode all the information of a large region, our LaneRoI naturally preserves the map structure and captures the more fine-grained information, as each node embedding only needs to represent the local context within a small region. To model interactions, we embed the LaneRoIs of all actors to a global lane graph and then propagate the information over this global graph. Since the LaneRoI's of interacting actors are highly relevant, those actors will share overlapping regions on the global graph, thus having more frequent communications during the information propagation compared to irrelevant actors. Importantly, this process neither requires any heuristics nor makes any oversimplified assumptions while learning interactions conditioned on maps. We then predict future motions on each LaneRoI in a fully-convolutional manner, such that small regions along lanes (nodes in LaneRoI) can serve as anchors and provide good priors . We demonstrate the effectiveness of our method on the large-scale Argoverse motion forecasting benchmark [10]. We achieve the first rank on the challenging Argoverse competition leaderboard [1], significantly outperforming previous results.

# Related Work

Motion Forecasting: Traditional methods use handcrafted features and rules based on human knowledge to model interactions and constraints in motion forecasting [12,11,14,21,33,57,32], which are sometimes oversimplified and not scalable. Recently, learning-based approaches employ the deep learning and significantly outperform traditional ones. Given the actors and the scene, a deep forecasting model first needs to design a format to encode the information. To do so, previous methods [41,4,37] often rasterize the trajectories of actors into a Birds-Eye-View (BEV) image, with different channels representing different observation timesteps, and then apply a CNN and RoI pooling [39,20] to extract actor features. Maps can be encoded similarly [59,60,8,4,49]. However, the square receptive fields of a CNN may not be efficient to encode actor movements [29], which are typically long curves. Moreover, the map rasterization may lose useful information like lane topologies. RNNs are an alternative way to encode actor kinematic information [62,49,16,61,18,2] compactly and efficiently. Recently, VectorNet [16] and LaneGCN [29] generalized such compact encodings to map representations. VectorNet treats a map as a collection of polylines and uses a RNN to encode them, while LaneGCN builds a graph of lanes and conducts convolutions over the graph. Different from all these work, we encode both actors and maps in an unified graph representation, which is more structured and powerful.

Modeling interactions among actors is also critical for a multi-agent system. Pioneering learning-based work design a social-pooling mechanism [2,18] to aggregate the information from nearby actors. However, such a pooling operation may potentially lose actor-specific information. To address this, attention-mechanism [43,52,44,48] or GNNbased methods [60,26,29,6,41,49,7,16] build actor interaction graphs (usually fully-connected with all actors or k-nearest neighbors based), and perform attention or message passing to update actor features. Social convolutional pooling [62,15,47] has also been explored, which maintains the spatial distribution of actors. However, most of these work do not explicitly consider map structures, which largely affects interactions among actors in reality.

To generate each actor's predicted futures, many works sample multi-modal futures under a conditional variational auto-encoder (CVAE) framework [25,40,49,41,7], or with a multi-head/mode regressor [29, 13,34]. Others output discrete sets of trajectory samples [60,37,9] or occupancy maps [22,42]. Recently, TNT [61] concurrently and independently designs a similar output parameterization as ours where lanes are used as priors for the forecasting. Note that, in addition to the parameterization, we contribute a novel graph representation and a powerful architecture which significantly outperforms their results.

Graph Neural Networks: Relying on operators like graph convolution and message passing, graph neural networks (GNNs) and their variants [45,5,28,24,19,31] generalize deep learning on regular graphs like grids to ones with irregular topologies. They have achieved great successes in learning useful graph representations for various tasks [35,38,50,27,17]. We draw inspiration from the general concept "ego-graph" and propose LaneRoI, which is specially designed for lane graphs and captures both the local map topologies and the past motion information of an individual actor. Moreover, to capture interactions among actors, we further propose an interaction module which effectively communicates information among LaneRoI graphs.

# LaneRCNN

Our goal is to predict the future motions of all actors in a scene, given their past motions and an HD map. Different from existing work, we represent an actor and its context with a LaneRoI, an actor-specific graph which is more structured and expressive than the single feature vector used in the literature. Based on this representation, we design LaneRCNN, a graph-centric motion forecasting model that encodes context, models interactions between actors, and predicts future motions all in a map topology aware manner. An overview of our model is shown in Fig. 2.

In the following, we first introduce our problem formulation and notations in Sec. 3.1. We then define our LaneRoI representations in Sec. 3.2. In Sec. 3.3, we explain how LaneRCNN processes features and models interactions via graph-based message-passing. Finally, we show our mapaware trajectory prediction and learning objectives in Sec. 3.4 and Sec. 3.5 respectively. Figure 3: The LaneRoI of the actor i is a collection of a graph G i (constructed following lane topology: nodes as lane segments and edges as segment connectivities) and node embeddings F i (encoding motions of the actor, as well as geometric and semantic properties of lane segments).

## Problem Formulation

We denote the past motion of the i-th actor as a set of 2D points encoding the center locations over the past L timesteps, i.e., (x -L i , y

, with (x, y) the 2D coordinates in bird's eye view (BEV). Our goal is to forecast the future motions of all actors in the scene

, where T is our prediction horizon and N is the number of actors.

In addition to the past kinematic information of the actors, maps also play an important role for motion forecasting since (i) actors usually follow lanes on the map, (ii) the map structure determines the right of way, which in turns affects the interactions among actors. As is common practice in self-driving, we assume an HD map is accessible, which contains lanes and associated semantic attributes, e.g., turning lane and lane controlled by traffic light. Each lane is composed of many consecutive lane segments i , which are short segments along the centerline of the lane. In addition, a lane segment i can have pairwise relationships with another segment j in the same lane or in another lane, such as i being a successor of j or a left neighbor.

## LaneRoI Representation

Graph Representation: One straight-forward way to represent an actor and its context (map) information is by first rasterizing both its trajectory as well as the map to form a 2D BEV image, and then cropping the underlying representation centered in the actor's location in BEV [60,49,62,6]. However, rasterizations are prone to information loss such as connectivities among lanes. Furthermore, it is a rather inefficient representation since actor motions are expanded typically in the direction along the lanes, not across them. Inspired by [29], we instead use a graph representation for our LaneRoI to preserve the structure while being compact. For each actor i in the scene, we first retrieve all relevant lanes that this actor can possibly go to in the prediction horizon T as well as come from in the observed history horizon L. We then convert the lanes into a directed graph G i = {V, {E suc , E pre , E left , E right }} where each node v ∈ V represents a lane segment within those lanes and the lane topology is represented by different types of edges E r , encoding the following relationships: predecessor, successor, left and right neighbor. Two nodes are connected by an edge e ∈ E r if the corresponding lane segments i , j have a relation r, e.g., lane segment i is a successor of lane segment j . Hereafter, we will use the term node interchangeably with the term lane segment.

# Graph Input Encoding:

The graph G i only characterizes map structures around the i-th actor without much information about the actor. We therefore augment the graph with a set of node embeddings to construct our LaneRoI . Recall that each node k in G i is associated with a lane segment k . We design its embedding f k ∈ R C to capture the geometric and semantic information of k , as well as its relations with the actor. In particular, geometric features include the center location, the orientation and the curvature of k ; semantic features include binary features indicating if k is a turning lane, if it is currently controlled by a red light, etc. To encode the actor information into f k , we note that the past motion of an actor can be identified as a set of 2D displacements, defining the movements between consecutive timesteps. Therefore, we also include the relative positions and orientations of these 2D displacements w.r.t. k into f k which encodes actor motions in a map-dependent manner. This is beneficial for understanding actor behaviors w.r.t. the map, e.g., a trajectory that steadily deviates from one lane and approaches the neighboring lane is highly likely a lane change. In practice, it is important to clamp the actor information, i.e., if k is more than 5 meters away from the actor we replace the actor motion embedding in f k with zeros. We hypothesize that such a restriction encourages the model to learn better representations via the message passing over the graph. To summarize, (G i , F i ) is the LaneRoI of the actor i, encoding the actor-specific information for motion forecasting, where F i ∈ R Mi×C is the collection of node embeddings f k and M i is the number of nodes in G i .

## LaneRCNN Backbone

As LaneRoIs have irregular graph structures, we can not apply standard 2D convolutions to obtain feature representations. In the following, we first introduce the lane convolution and pooling operators (Fig. 4), which serve similar purposes as their 2D counterparts while respecting the graph topology. Based on these operators, we then describe how our LaneRCNN updates features of each LaneRoI as well as handles interactions among all LaneRoIs (actors). Given a LaneRoI (G i , F i ), a lane convolution updates features F i by aggregating features from its neighborhood (in the graph). Formally, we use E i (r) to denote the binary adjacency matrix for G i under the relation r, i.e., the (p, q) entry in this matrix is 1 if lane segments p and q have the relation r and 0 otherwise. We denote the n-hop connectivity under the relation r as the matrix bool (E i (r)

# Lane Convolution Operator: We briefly introduce the lane convolution which was originally proposed in [29]

, where the operator bool sets any non-zero entries to one and otherwise keeps them as zero. The output node features are updated as follows,

where both W and W n,r are learnable parameters, Ψ(•) is a non-linearity consisted of LayerNorm [3] and ReLU [36], and the summation is over all possible relations r and hops n. In practice, we use n ∈ {1, 2, 4, 8, 16, 32}. Such a multi-hop mechanism mimics the dilated convolution [58] and effectively enlarges the receptive field.

Lane Pooling Operator: We design a lane pooling operator which is a learnable pooling function. Given a LaneRoI (G i , F i ), recall G i actually corresponds to a number of lanes spanned in the 2D plane (scene). For an arbitrary 2D vector v in the plane, a lane pooling operator pools, or 'interpolates', the feature of v from F i . Note that v can be a lane segment in another graph G j (spatially close to G i ). Therefore, lane pooling helps communicate information back and forth between graphs, which we will explain in the interaction part. To generate the feature f v of vector v, we first retrieve its 'neighboring nodes' in G i , by checking if the center distance between a lane segment k in G i and vector v is smaller than a certain threshold. A naive pooling strategy is to simply take a mean of those k . However, this ignores the fact that relations between k and v can vary a lot depending on their relative pose: a lane segment that is perpendicular to v (conflicting) and the one that is aligned with v have very different semantics. Inspired by the generalized convolution on graphs/manifolds [35,53,29], we use the relative pose and some non-linearities to learn a pooling function. In particular, we denote the set of surrounding nodes on G i as N , and the relative pose between v and k as ∆ vk which includes relative position and orientation. The pooled feature f v can then be written as,

where [• • • ] means concatenation and M is a two-layer multi-layer perceptron (MLP).

LaneRoI Encoder: Equipped with operators introduced above, we now describe how LaneRCNN processes features for each LaneRoI. Given a scene, we first construct a LaneRoI per actor and encode its input information into node embeddings as described in Sec. 3.2. Then, for each LaneRoI, we apply four lane convolution layers and get the updated node embeddings F i . Essentially, a lane convolution layer propagates information from a node to its (multihop) connected nodes. Stacking more layers builds larger receptive fields and has a larger model capacity. However, we find deeper networks do not necessarily lead to better performances in practice, possibly due to the well-known difficulty of learning long-term dependencies. To address this, we introduce a graph shortcut mechanism on LaneRoI.

The graph shortcut layer can be applied after any layer of lane convolution: we aggregate F i output from the previous layer into a global embedding with the same dimension as node embeddings, and then add it to embeddings of all nodes in G i . Recall that the actor past motions are a number of 2D vectors, i.e., movements between consecutive timesteps. We use the lane pooling to extract features for these 2D vectors. A 1D CNN with downsampling is then applied to these features to build the final shortcut embedding. Intuitively, a lane convolution may suffer from the diminishing information flow during the message-passing, while such a shortcut can provide an auxiliary path to communicate among far-away nodes efficiently. We will show that the shortcut significantly boosts the performance in the ablation study.

LaneRoI Interactor: So far, our LaneRoI encoder provides good features for a given actor, but it lacks the ability to model interactions among different actors, which is extremely important for the motion forecasting in a multiagent system. We now describe how we handle actor interactions under LaneRoI representations. After processing all LaneRoIs with the LaneRoI encoder (shared weights), we build a global lane graph G containing all lanes in the scene. Its node embeddings are constructed by projecting all LaneRoIs to G itself. We then apply four lane convolution layers on G to perform message passing. Finally, we distribute the 'global node' embeddings back to each LaneRoI . Our design is motivated by the fact that actors have interactions since they share the same space-time region. Similarly, in our model, all LaneRoIs share the same global graph G where they communicate with each other following map structures.

In particular, suppose we have a set of LaneRoIs

previous layers and a global lane graph G. For each node in G, we use a lane pooling to construct its embedding: retrieving its neighbors from all LaneRoIs as N , measured by center distance, and then applying Eq. 2. This ensures each global node has the information of all those actors that could interact with it. The distribute step is an inverse process: for each node in G i , find its neighbors, apply a lane pooling, and add the resulted embedding to original F i (serving as a skipconnection).

## Map-Relative Outputs Decoding

The future is innately multi-modal and an actor can take many different yet possible future motions. Fortunately, different modalities can be largely characterized by different goals of an actor. Here, a goal means a final position of an actor at the end of prediction horizon. Note that actors mostly follow lane structures and thus their goals are usually close to a lane segment . Therefore, our model can predict the final goals of an actor in a fully convolutional manner, based on its LaneRoI features. Namely, we apply a 2-layer MLP on each node feature f k , and output five values including the probability that k is the closest lane segment to final destination p( k = goal), as well as relative residues from k to the final destination x gtx k , y gty k , sin(θ gtθ k ), cos(θ gtθ k ).

Based on results of previous steps, we select the top K1 For each prediction, we use the position and the direction of the actor at t = 0 as well as those at the goal to interpolate a curve, using Bezier quadratic parameterization. We then unroll a constant acceleration kinematic model along this curve, and sample 2D points at each future timestep based on the curve and the kinematic information. These 2D points form a trajectory, which serves as an initial proposal of our final forecasting. Despite its simplicity, this parameterization gives us surprisingly good results.

Our final step is to refine those trajectory proposals using a learnable header. Similar to the shortcut layer introduced in Sec. 3 CNN to pool features of this trajectory. Finally, we decode a pair of values per timestep, representing the residue from the trajectory proposal to the ground-truth future position at this timestep (encoded in Frenet coordinate of this trajectory proposal). We provide more detailed definitions of our parameterization and output space in the supplementary A.

## Learning

We train our model end-to-end with a loss containing the goal classification, the goal regression, and the trajectory refinements. Specifically, we use

where α and β are hyparameters determining relative weights of different terms. As our model predicts the goal classification and regression results per node, we simply adopt a binary cross entropy loss for L cls with online hard example mining [46] and a smooth-L1 loss for L reg , where the L reg is only evaluated on positive nodes, i.e. closest lane segments to the ground-truth final positions. The L refine is also a smooth-L1 loss with training labels generated on the fly: projecting ground-truth future trajectories to the predicted trajectory proposals, and use the Frenet coordinate values as our regression targets.

# Experiment

We evaluate the effectiveness of LaneRCNN on the large-scale Argoverse motion forecasting benchmark (Argoverse), which is publicly available and provides annotations of both actor motions and HD maps. In the following, we first explain our experimental setup and then compare our method against state-of-the-art on the leaderboard. We also conduct ablation studies on each module of Lan-eRCNN to validate our design choices. Finally, we present some qualitative results.

## Experimental Settings

Dataset: Argoverse provides a large-scale dataset [10] for the purpose of training, validating and testing models, where the task is to forecast 3 seconds future motions given 2 seconds past observations. This dataset consists of more than 30K real-world driving sequences collected in Miami and Pittsburgh. Those sequences are further split into train, validation, and test sets without any geographical overlapping. Each of them has 205942, 39472, and 78143 sequences respectively. In particular, each sequence contains the positions of all actors in a scene within the past 2 seconds history, annotated at 10Hz. It also specifies one interesting actor in this scene, with type 'agent', whose future 3 seconds motions are used for the evaluation. The train and validation splits additionally provide future locations of all actors within 3 second horizon labeled at 10Hz, while annotations for test sequences are withheld from the public and used for the leaderboard evaluation. Besides, HD map information can be retrieved for all sequences.

Metrics: We follow the benchmark setting and use Miss-Rate (MR), Average Displacement Error (ADE) and Final Displacement Error (FDE), which are also widely used in the community. MR is defined as the ratio of data that none of the predictions has less than 2.0 meters L2 error at the final timestep. ADE is the averaged L2 errors of all future timesteps, while FDE only counts the final timestep. To evaluate the mutli-modal prediction, we also adopt the benchmark setting: predicting K=6 future trajectories per actor and evaluating the min K MR, min K ADE, min K FDE using the trajectory that is closest to the ground-truth.

Implementation Details: We train our model on the train set with the batch size of 64 and terminate at 30 epochs. We use Adam [23] optimizer with the learning rate initialized at 0.01 and decayed by 10 at 20 epochs. To normalize the data, we translate and rotate the coordinate system of each sequence so that the origin is at current position (t = 0) of 'agent' actor and x-axis is aligned with its current direction.

During training, we further apply a random rotation data augmentation within (- 

## Comparison with State-of-the-art

We compare our approach with top entries on Argoverse motion forecasting leaderboard [1] as well as official baselines provided by the dataset [10] as shown in Table 1. We only submit our final model once to the leaderboard and achieve state-of-the-art performance. 2 This is a very challenging benchmark with around 100 participants at the time of our submission. Note that for the official ranking metric MR (K=6), previous leading methods are extremely close to each other, implying the difficulty of further improving the performance. Nevertheless, we significantly boost the number which verifies the effectiveness of our method. Among the competitors, both Jean [34] and TNT [61] use RNNs to encode actor kinematic states and lane polylines. They then build a fully-connected interaction graph among all actors and lanes, and use either the attention or GNNs to model interactions. As a result, they represent each actor with a single feature vector, which is less expressive than our LaneRoI representations. Moreover, the fully-connected interaction graph may also discard valuable map structure information. Note that TNT shares a similar output parameterization as ours, yet we perform better on all metrics. This further validates the advantages of our LaneRoI compared against traditional representations. Unfortunately, since Poly team does not publish their method, we can not compare with it qualitatively. 2 Snapshot of the leaderboard at the submission time: Nov. 12, 2020.

## Ablation Studies

Ablations on LaneRoI Encoder: We first show the ablation study on one of our main contributions, i.e., LaneRoI , in the upper half of Table 2. The first row shows a representative of the traditional representations. Specifically, we first build embeddings for lane graph nodes using only the map information and 4 lane convolution layers. We then use a 1D CNN (U-net style) to extract a motion feature vector from actor kinematic states, concatenate it with every graph node embedding and make predictions. Conceptually, this is similar to TNT [61] except that we modify the backbone network to make comparisons fair. On the second row, we show the result of our LaneRoI representations with again four lane convolution layers (no shortcuts). Hence, the only difference is whether the actor is encoded with a single motion vector shared by all nodes, or encoded in a distributed and structured manner as ours. As shown in the table, our LaneRoI achieves similar or better results on all metrics, exhibiting its advantages. Note that this row is not yet our best result in terms of using LaneRoI representations, as the actor information is only exposed to a small region during the input encoding (clamping at input node embeddings) and can not efficiently propagate to the full LaneRoI without the help of the shortcut, which we will show next.  refers to average-pooling all node embeddings within a LaneRoI, and 'Center Pool' means we pool a feature from a LaneRoI using nodes that around the last observation of the actor and a lane pooling. As we can see, although these two approaches can possibly spread out information to every node in a LaneRoI (and thus build a shortcut), they barely improve the performance. On the contrary, ours achieve significant improvements. This is because we pool features along the past trajectory of the actor, which results in a larger and actor-motion-specific receptive field. Here, ×1 and ×2 refer to an encoder with 1 shortcut per 4 and 2 lane convolution layers respectively. This shows stacking more shortcuts provides some, but diminishing, benefits.

# Ablations on LaneRoI Interactor:

To verify the effectiveness of our map-aware interaction module, we compare against several model variants based on the fully-connected interaction graph among actors. Specifically, for each actor, we apply a LaneRoI encoder3 to process node embeddings, and then pool an actor-specific feature vector from LaneRoI via either the global average pooling or our shortcut mechanism. These actor features are then fed into a transformerstyle [51] attention module or a fully-connected GNN. Finally, we add the output actor features to nodes in their LaneRoI respectively and make predictions using our decoding module. As a result, these variants have the same pipeline as ours, with the only difference on how to communicate across actors. To make comparisons as fair as possible, both the attention and GNN have the same numbers of layers and channels as our LaneRoI Interactor. 4As shown in Table 2, all interaction-based models outperform the one without considering interactions (row 1) as expected. In addition, our approach significantly improves the performance compared to both the attention and GNN. Interestingly, all fully-connected interaction graph based model reach similar performance, which might imply such backbones may saturate the performance (as also shown by leading methods on the leaderboard). We also show that naively using the average pooling to embed features from LaneRoIs to global graph does not achieves good performance because it ignores local structures.

## Qualitative results

In Figure 5, we show some qualitative results on Argoverse validation set. We can see that our method generally follows the map very well and demonstrates good multimodalities. From left to right, we show 1) when an actor follows a curved lane, our model predicts two direction modes with different velocities; 2) when it is on a straight lane, our model covers the possibilities of lane changing; 3) when it's approaching an intersection, our model captures both the go-straight and the turn-right modes, especially with lower speeds for turning right, which are quite common in the real world; 4) when there is an actor blocking the path, we predict overtaking behaviors matching exactly with the groundtruth. Moreover, for the lane-following mode, we predict much slower speeds which are consistent with this scenario, showing the effectiveness of our interaction modeling. We provide more qualitative results in the supplementary E.

# Conclusion

In this paper, we propose LaneRCNN, a graph-centric motion forecasting model. Relying on learnable graph operators, LaneRCNN builds a distributed lane-graph-based representation (LaneROI) per actor to encode its past motion and the local map topology. Moreover, we propose an interaction module which effectively captures the interactions among actors within the shared global lane graph. And lastly, we parameterize the output trajectory using lane graphs which helps improve the prediction. We demonstrate that LaneRCNN achieves state-of-the-art performance on the challenging Argoverse motion forecasting benchmark.

# A. Map-Relative Output Decoding

Our output decoding can be divided into three steps: predicting the final goal of an actor based on node embeddings, propose an initial trajectory based on the goal and the initial pose, and refine the trajectory proposal with a learnable header. As the first step is straight-forward, we now explain how we perform the second and third steps in details.

# A.1. Trajectory Proposal

Given a predicted final pose x T , y T , dx T , dy T and initial pose x 0 , y 0 , dx 0 , dy 0 of an actor, where (x, y) is the 2D location and (dx, dy) is the tangent vector, we fit a Bezier quadratic curve satisfying these boundary conditions, i.e., zero-th and first order derivative values. Specifically, the curve can be parameterized by

Here, s is the normalized distance. As a result, each predicted goal uniquely defines a 2D curve.

Next, we unroll a velocity profile along this curve to get 2D waypoint proposals at every future timestamp. Assuming the actor is moving with a constant acceleration within the prediction horizon, we can compute the acceleration based on the initial velocity v and the traveled distance s (from (x 0 , y 0 ) to (x T , y T ) along the Bezier curve) using

Therefore, the future position of the actor at any timestamp t can be evaluated by querying the position along the curve at s(t) = vt + 1 2 at 2 .

# A.2. Trajectory Refinement

Simply using our trajectory proposals for motion forecasting will not be very accurate, as not all actors move with constant accelerations and follow Bezier curves, yet the proposals provide us good initial estimations which we can further refine. To do so, for each trajectory proposal, we construct its features using a shortcut layer on top our LaneRoI node embeddings. We then use a 2 layer MLP to decode a pair of values (s t , d t ) for each future timestamp, representing the actor position at time t in the Frenet Coordinate System [56]. The Cartesian coordinates (x t , y t ) can be mapped from (s t , d t ) by first traversing along the Bezier curve distance s t (a.k.a. longitudinal), and then deviating perpendicularly from the curve distance d t (a.k.a. lateral).

The sign of d t indicates the deviation is either to-the-left or to-the-right.

# B. LaneRoI Construction

To construct a LaneRoI, we need to retrieve all relevant lanes of a given actor. In our experiments, we use a simple heuristic for this purpose. Given an HD map of a scene and an arbitrary actor in this scene, we first uniformly sample segments k with 1 meter length along each lane's centerline . Then, for the actor's location at each past timestamp, we find the nearest lane segment and collect them together into a set. This is a simplified version of lane association and achieve very high recall of the true ego-lane. Finally, we retrieve predecessing and successing lane segments (within a range D) of those segments in the set to capture lanefollowing behaviors, as well as left and right neighbors of those predecessing and successing lanes which are necessary to capture lane-changing behaviors. The range D equals to an expected length of future movement by integrating the current velocity within the prediction horizon, plus a buffer value, e.g., 20 meters. Therefore, D is dynamically changed based on actor velocity. This is motivated by the fact that high speed actors travel larger distances and thus should have larger LaneRoIs to capture their motions as well as interactions with other actors.

# C. Architecture and Learning Details

Our LaneRCNN is constructed as follows: we first feed each input LaneRoI representation into an encoder, consists of 2 lane convolution layers and a shortcut layer, followed by another 2 lane convolution layers and a shortcut layer. We then use a lane pooling layer to build the node embeddings of the global graph (interactor), where the neighborhood threshold is set to 2 meters. Another four layers of lane convolution are applied on top of this global graph. Next, we distribute the global node embeddings to each LaneRoI by using a lane pooling layer and adding the pooled features to original LaneRoI embeddings (previous encoder outputs) as a skip-connection architecture. Another 4 layers of lane convolution and 2 layers of shortcut layers are applied afterwards. Finally, we use two layers of MLP to decode a classification score per node using its embeddings, and another two layer for regression branch as well. All layers except the final outputs have 64 channels. We use Layer Normalization [3] and ReLU [36] for normalization and non-linearity respectively.

During training, we apply online hard example mining [46] for L cls (Eq. 3.5). Recall each node predict a binary classification score indicating whether this node is the closest lane segment to the final actor position. We use the closest lane segment to the ground-truth location as The final L cls is the average of positive example loss plus the average of negative example loss. Finally, we add L reg and L ref ine with relative weights of α = 0.5 and β = 0.2 respectively to form our total loss L.

# D. Ablation Studies

When constructing our LaneRoI, we define a lane segment to be a node in the graph. However, there are different ways to sample lane segments and we find such a choice largely affects the final performance. In Table 3 we show different strategies of sampling lane segments. The first four rows refer to upsampling the original lane segment labels provided in the dataset. 5 Such a strategy provides segments with different lengths, e.g., shorter and denser segments where the geometry of the lanes changes more rapidly. The last four rows sample segments uniformly along lanes with a predefined length.

As we can observe from Table 3, even though the 'upsample' strategy can result in similar average segment length as the 'uniform' strategy, it performs much worse in all metrics. This is possible because different lengths of segments introduce additional variance and harm the representation learning process. We can also conclude from the table that using denser sampling can achieve better results. Besides, we show the effectiveness of adding a regression branch for each node in addition to a classification branch, shown in 'Reg' column.

Our output parameterization explicitly leverages the lane information and thus ease the training. In Fig. 6, we validate 5 Lanes are labeled in the format of polylines in Argoverse, thus points on those polylines naturally divide lanes into segments.

such an argument where we compare against a regressionbased variant of our model. In particular, we use the same backbone as ours, and then perform a shortcut layer on top of each LaneRoI to extract an actor-specific feature vector. We then build a multi-modal regression header which directly regresses future motions in the 2D Cartesian coordinates. 6 We can see from Fig. 6 that our model achieves decent performance when only small amounts of training data are available: with only 1% of training data, our method can achieve 20% of miss-rate. On the contrary, the regressionbased model requires much more data. This shows our method can exploit map structures as good priors and ease learning.

In Table 4, we summarize ablations on different trajectory parameterizations. We can see a constant acceleration rollout slightly improves over constant speed assumption, and the Bezier curve significantly outperforms a straight line parameterization, indicating it is more realistic. In addition, adding a learnable header to refine the trajectory proposals (e.g., Bezier curve) can further boost performance.

# E. Qualitative Results

Lastly, we provide more visualization of our model outputs, as we believe the metric numbers can only reveal part of the story. We present various scenarios in Fig. 7, including turning, following curved roads, interacting with other actors and some abnormal behaviors. On the first two rows, we show turning behaviors under various map topologies. We can see our motion forecasting results nicely capture possible modes: turning into different directions or occupying different lanes after turning. We also do well even if the actor is not strictly following the centerlines. On the third row, we show predictions following curved roads, which are difficult scenarios for auto-regressive predictors [59,8]. On Figure 6: Model performance when using different amounts of data for training. Our output parameterization explicitly leverages lanes as priors for motion forecasting, and thus significantly ease the learning compared to directly regressing the future motions in the 2D plane. the fourth row, we show that our model can predict breaking or overtaking behaviors when leading vehicles are blocking the ego-actor. Finally, we show in the fifth row that our model also works well when abnormal situations occur, e.g., driving out of roads. This is impressive as the model relies on lanes to make predictions, yet it shows capabilities to predict non-map-compliant behaviors. 

# Acknowledgement

We would like to sincerely thank Siva Manivasagam, Yun Chen, Bin Yang, Wei-Chiu Ma and Shenlong Wang for their valuable help on this paper.

