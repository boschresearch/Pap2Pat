{
    "id": "https://semopenalex.org/work/W4296957203",
    "authors": [
        "Jing Ren",
        "Yanzhi Wang",
        "Sheng Li",
        "Yong Geng",
        "Yanyu Li",
        "Sergey Tulyakov",
        "Xulong Tang",
        "Zhenglun Kong"
    ],
    "title": "Layer Freezing & Data Sieving: Missing Pieces of a Generic Framework for  Sparse Training",
    "date": "2022-09-22",
    "abstract": "Recently, sparse training has emerged as a promising paradigm for efficient deep learning on edge devices. The current research mainly devotes efforts to reducing training costs by further increasing model sparsity. However, increasing sparsity is not always ideal since it will inevitably introduce severe accuracy degradation at an extremely high sparsity level. This paper intends to explore other possible directions to effectively and efficiently reduce sparse training costs while preserving accuracy. To this end, we investigate two techniques, namely, layer freezing and data sieving. First, the layer freezing approach has shown its success in dense model training and fine-tuning, yet it has never been adopted in the sparse training domain. Nevertheless, the unique characteristics of sparse training may hinder the incorporation of layer freezing techniques. Therefore, we analyze the feasibility and potentiality of using the layer freezing technique in sparse training and find it has the potential to save considerable training costs. Second, we propose a data sieving method for dataset-efficient training, which further reduces training costs by ensuring only a partial dataset is used throughout the entire training process. We show that both techniques can be well incorporated into the sparse training algorithm to form a generic framework, which we dub SpFDE. Our extensive experiments demonstrate that SpFDE can significantly reduce training costs while preserving accuracy from three dimensions: weight sparsity, layer freezing, and dataset sieving.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Sparse training, as a promising solution for efficient training on edge devices, has drawn significant attention from both the industry and academia [1]. Recent studies have proposed various sparse training algorithms with computation and memory savings to achieve training acceleration. These sparse training approaches can be divided into two main categories. The first category is fixed-mask sparse training methods, aiming to find a better sparse structure in the initial phase and keep the sparse structure constant throughout the entire training process [2,3]. These approaches have a straightforward sparse training process but suffer from a higher accuracy degradation. Another category is Dynamic Sparse Training (DST), which usually starts the training from a randomly selected sparse structure [4,5]. DST methods tend to continuously update the sparse structure during the sparse training process while maintaining an overall sparsity ratio for the model. Compared with the fixed-mask sparse training, the state-of-the-art DST methods have shown their superiority in accuracy and recently become a more broadly adopted sparse training paradigm [6].",
                "However, although the existing sparse training approaches can reduce meaningful training costs, most of them devote their efforts to studying how to reduce training costs by further increasing sparsity while mitigating accuracy drop. As a result, the community tends to focus on the sparse training performance at an extremely high sparsity ratio, e.g., 95% and 98%. Nevertheless, even the most recent sparse training approaches still lead to severe performance drop at these high sparsity ratios. For instance, on the CIFAR-10 dataset [7], MEST [1] has a 2.5% and 4% accuracy drop at 95% and 98% sparsity, respectively. In fact, the network performance usually begins to drop dramatically at the extremely high sparsity, while the actual gains from weight sparsity, i.e., savings of computation and memory, tend to saturate. This indicates that reducing training costs by pushing sparsity towards extreme ratios at the cost of network performance may not always the desirable methodology when a certain sparsity level has been reached. Towards this end, we raise a fundamental question that has seldom been asked: Are there other ways that can be seamlessly combined with sparse training to further reduce training costs effectively while maintaining network performance?",
                "Table 1: The key features of SpFDE compared to representative sparse training works.",
                "To answer the question, we first take a step back to understand whether all layers in sparse networks are equally important. Recent studies reveal that not all the layers in dense Deep Neural Networks (DNNs) need to be trained equally [8,9,10]. Generally, the front layers in DNNs are responsible for extracting low-level features and usually have fewer parameters than the later layers. These make the front layers have higher representational similarity and converge faster during training [8,11]. Therefore, the layer freezing techniques are proposed, which stop the training (updating) of specific DNN layers early in the training process to save the training costs. The early work attempts the layer freezing technique in dense model training [8], while many following works focus on layer freezing in the fine-tuning process of the large transformer-based models [12,13,9,10,14,15]. Even with the progress, the layer freezing technique has never been explored in the sparse training.",
                "Layer freezing seems like a promising solution for sparse training that further reduces training costs. However, the conclusion is still too early to draw, given that sparse training has two critical characteristics that make it a unique domain compared with dense DNN training and fine-tuning. This might impede the incorporation of the layer freezing technique in sparse training. x The superiority of the DST method is attributed to its continuously changed sparse structure, which helps it end up with a better result [6]. This could inherently contradict the layer freezing that requires the layers to be unchangeable early in the training process. y The impact of the sparsity for each layer is unknown in terms of the convergence speed. These two characteristics directly affect the feasibility and potentiality of using the layer freezing techniques in sparse training.",
                "Back to the question we raise, whether there are other directions to effectively reduce sparse training costs besides increasing sparsity. To tackle the question, in this work, we propose two techniques that can be well incorporated with sparse training, namely layer freezing and data sieving.  [2], GraSP [3], RigL [5], ITOP [6], SET [19], DSR [4], and MEST [1], is provided in Tab. Three main sparsity schemes introduced in the area of network pruning consists of unstructured [20,21,22], structured [23,24,25,26,27,28,29,30,31,32,33,34,35,36], and fine-grained structured pruning [37,38,39,40,41,42,43,44,45,46]. Though network pruning is initially proposed for inference acceleration, it is widely adopted in sparse training to achieve the satisfactory trade-offs between network performance, e.g., classification accuracy, memory footprint, and training time.",
                "Most of these works follow the training pipeline of pretraining-pruning-retraining. Instead, we consider a generic sparsity training framework that works for edge devices by focusing on sparse networks trained from scratch, instead of training dense networks, which is not feasible on resource limited devices [1].",
                "Research on the sparse training strategies [47,48,49] can be categorized into fixed-mask and sparsemask sparse training, where the former aims to make it feasible that the training of the pruned models can be implemented on edge devices [2,3,50,51,52] and the latter studies how to reduce memory footprint along with the computation during training [53,19,4,54,5].  The Dynamic Sparse Training (DST) method shows its superior performance by continuously changing its sparse model structure during training to search for a better sparse architecture, making it a desirable sparse training paradigm [5]. However, the dynamically changed network structure seems essentially contradictory to the layer freezing technique, given the weights of frozen layers are fixed and not further trained. Thus, these layers cannot keep searching for better sparse structures, which may compromise the quality of the sparse model trained, leading to lower model accuracy.",
                "Assumption. Inspired by the convergence speed for various layers is different in conventional dense model training, we conjecture that, in DST, the front layers may also find desired sparse structure faster than the later layers. If true, we may be able to introduce the layer freezing technique in sparse training without compromising the sparse training accuracy.",
                "Experimental Setting. We investigate the assumption by tracking the structural similarity of the sparse model along the sparse training process. Specifically, we select the well-trained sparse model as the reference model and compare the intermediate sparse model obtained at each epoch with the reference model. We define structural similarity as the percentage of the common non-zero weight locations, i.e., indices, in both the intermediate sparse model and the final sparse model. For instance, the structural similarity of 70% indicates that 30% of the current intermediate sparse structure will be altered during the rest of the training process and will not be presented in the final model. The 100% structural similarity shows the sparse model structure is fully stabilized.",
                "Analysis Results. Fig. 1 (a) shows the trend of structural similarity of different layers within a model along the same sparse training process. We adopt the DST method from MEST [1] with 90% unstructured sparsity and evaluate the results using ResNet-32 on the CIFAR-100 dataset. Note that we check the structural similarity by choosing the locations of the 50% most significant non-zero weights from the intermediate models. The reason is that sparse training algorithms (e.g., MEST and RigL) may force less important weights/locations to be changed during sparse training, regardless of whether the sparse structure has already been stabilized. Additionally, the most significant weights play the most important role in the model's generalizability. Therefore, tracking the structural similarity using 50% most significant non-zero weights is reasonable and meaningful.",
                "Observation. From the results, we can observe that the structural similarity of the first layer converges at the very early stage of the training, i.e., 40 epochs, and the front layers' structural similarity converge faster than the later layers. The structural similarity of sparse training follows a similar pattern as in the dense model training. This indicates that the changing/searching of the front layers from the sparse models can be stopped earlier without compromising the quality of the final sparse model, providing the feasibility of applying the layer freezing technique in sparse training. "
            ],
            "subsections": [
                {
                    "title": "What Is the Impact of Model Sparsity on the Network Representation Learning Process?",
                    "paragraphs": [
                        "With the above exploration giving evidence that layer freezing is compatible with sparse training (Sec. 3.1), another critical question is to understand the impact of model sparsity on the neural network representation learning process. In other words, what is the convergence speed for the same layer under different sparsity? If the convergence speed is different under different sparsity, applying layer freezing would be much more complicated since deciding the stop criterion would be very challenging, and the potential gain we can have by using layer freezing could be limited.",
                        "Previous work shows that layer freezing can be used to effectively reduce training costs due to the ability of fast representation learning of the front layers in the network [8]. Another work observes that a wider network is easier to learn the representation to a saturated level [11]. Therefore, whether the sparsity would significantly slow down the representation learning or the convergence speed of the layers is unknown since the width of the layer can be changed during sparse training.",
                        "Experimental Setting. To explore the impact of sparsity on network representation learning speed, we adopt the centered kernel alignment (CKA) [11] as an indicator of representational similarity. We track the trend of the CKA between the final and intermediate model at each training epoch.",
                        "Analysis Results and Observation. The results are shown in Fig. 1 (b). We compare the CKA trends of the same layer, i.e., 10 th layer in ResNet-32, in dense model training and sparse training under different sparsity. Surprisingly, we find that, in sparse training, even under a high sparsity ratio (i.e., 90%), there is no apparent slowdown in the network representation learning speed. Similar observations can also be found in other layers (more analysis results in Appendix A). This indicates that the layer freezing technique can potentially be adopted in the sparse training process as early as in the dense model training process, thereby effectively reducing the training costs.",
                        "Takeaway. Considering the unique characteristics of sparse training, we first explore the feasibility and potentiality of using the layer freezing techniques in the sparse training domain. By investigating both the structural similarity and representational similarity of the sparse training, we tentatively conclude that the layer freezing technique is also suitable for sparse training and has the potential to save considerable training costs."
                    ],
                    "subsections": []
                },
                {
                    "title": "Overview of Sparse Training Framework",
                    "paragraphs": [
                        "The overall end-to-end training process can be logically divided into three stages, including the initial stage, the active training stage, and the progressive layer freezing stage. Motivated by the observation that the structural and representational similarity of front layers converges faster than later layers in sparse training (Sec. 3), we propose the progressive layer freezing approach to gradually freeze layers sequentially. Specifically, a layer can be frozen only if all the layers in front of this layer are frozen. The progressive manner brings the benefits for maximizing the saving of training costs since the entire frozen part of the model does not require computing backpropagation."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Initial",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Layer Freezing Algorithm",
                    "paragraphs": [
                        "Alg. 1 shows the training flow of SpFDE and the algorithm of progressive layer freezing. For a given DNN model with L layers, we divide it into N blocks, with each block consisting of several consecutive DNN layers, such as a bottleneck block in the ResNet [61]. We denote T as the total training epoch, \u2206\u03c4 as the sparse structure changing interval of dynamic sparse training, and T f rz (0 < T f rz < T ) as the epoch that we start the progressive layer freezing stage and freeze the first block. Then, for every \u2206\u03c4 epochs, we sequentially freeze the next block until the expected overall training FLOPs satisfy the target_f lops. We consider the frozen blocks still need to conduct forward propagation during training. Therefore, we compute the training FLOPs reduction of freezing a block as its sparse back-propagation computation FLOPs  (calculated by BpFlops(\u2022) in Alg. 1) multiplied by the total frozen epochs of the block. For detailed implementation, given the target training FLOPs target_f lops and the total number of layers to freeze, we empirically choose to freeze 2/3 layers of the model and the T f rz can be easily calculated.",
                        "To better combine with the DST and make sure the layers/blocks are appropriately trained before being frozen, we synchronize the progressive layer freezing interval to the structure changing interval, i.e., \u2206\u03c4 , of the sparse training, and adopt a layer/block-wise cosine learning rate schedule according to the total active training epoch of each layer/block."
                    ],
                    "subsections": [
                        {
                            "title": "Design Principles for Layer Freezing",
                            "paragraphs": [
                                "There are two key principles for deriving a layer freezing algorithm, the freezing scheme and the freezing criterion. Here we discuss the reasons that the proposed progressive layer freezing is rational.",
                                "Freezing Scheme. Since sparse training may target the resource-limited edge devices, it is desired to have the training method as simple as possible to reduce the system overhead and strictly meet the budget requirements. Therefore, we follow a cost-saving-oriented freezing principle to guarantee the target training costs and derive the layer freezing scheme, which can include the single-shot, single-shot & resume, periodically freezing, and delayed periodically freezing (as illustrated in Fig. 3 (a)). We adopt the single-shot scheme since it achieves the highest accuracy under the same training FLOPs saving (detailed results in Appendix B). The possible reason is that the single-shot freezing scheme has the longest active training epochs at the beginning of the training, which helps layers converge to a better sparse structure before freezing.",
                                "Freezing Criterion. With the freezing scheme decided, another important question is how to derive the freezing criterion, i.e., choosing which iterations or epochs to freeze the layers. Existing works have explored adaptive freezing methods by calculating and collecting the gradients during the fine-tuning of dense networks [10]. However, from our observations, the unique property of sparse training makes these approaches not applicable. For example, as shown in Fig. 3 (b), the difference of gradients norms from different layers decreases very fast at the beginning of the sparse training while it keeps fluctuating after some epochs because of the prune-and-grow weights. Abstracting the freezing criterion based on the gradients norm would inevitably introduce extra computation and system complexity since the changing patterns of the gradient norm difference are volatile. Therefore, our strategy of combining the layer freezing interval with the DST interval is more favorable."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "Circular Data Sieving",
                    "paragraphs": [
                        "We propose the data sieving method to achieve true dataset-efficient training throughout the sparse training process. As shown in Fig. We adopt the number of forgetting times as the criteria to indicate the complexity of each training sample. Specifically, for each training sample, we collect the number of forgetting times by counting the number of transitions from being correctly classified to being misclassified within each \u2206\u03c4 interval. We re-collect this number for each interval to ensure the newly added samples can be treated equally. Additionally, we use the structure changing frequency \u2206\u03c4 in sparse training as the dataset update frequency to minimize the impact of changed structure on the forgetting times.",
                        "We treat the removed dataset as a queue structure, retrieving samples from its head and adding the newly removed sample to its tail. After all the initial removed samples are retrieved, we shuffle the removed dataset after each update, making all the training samples can be used at least once. As a result, we can gradually sieve the relatively easier samples out and only use the important samples for dataset-efficient training. More analysis results are in the Appendix C."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experimental Results",
            "paragraphs": [
                "In this section, we evaluate our proposed SpFDE framework on benchmark datasets, including CIFAR-100 [7] and ImageNet [63], for the image classification task with ResNet-32 and ResNet-50. Note that we follow the previous work [1,3,2] using the 2\u00d7 widen version of ResNet-32. We compare the accuracy, training FLOPs, and memory costs of our framework with the most representative sparse training works [2,3,53,54,4,19,5, 1] at different sparsity ratios. Models are trained by using PyTorch [64] on an 8\u00d7A100 GPU server. We adopt the standard data augmentation and the momentum SGD optimizer. Layer-wise cosine annealing learning rate schedule is used according to the frozen epochs. To make a fair comparison with the reference works, we also use 160 training epochs on the CIFAR-100 dataset and 150 training epochs on the ImageNet dataset. We choose MEST+EM&S [1] as our training algorithm for weight sparsity since it does not involve any dense computations, making it desirable for edge device scenarios. We apply uniform unstructured sparsity across all the convolutional layers while only keeping the first layer dense. More experiments on other datasets and the detailed hyper-parameter setting are provided in the Appendix D."
            ],
            "subsections": [
                {
                    "title": "Comparison on Model Accuracy and Training FLOPs",
                    "paragraphs": [
                        "Tab. 2 shows the comparison of accuracy and computation FLOPs results on CIFAR-100 dataset using ResNet-32. Each accuracy result is averaged over 3 runs. We denote the configuration of our SpFDE using x% + y%, where x indicates the target training FLOPs reduction during layer freezing and y is the percentage of removed training data. Our SpFDE can consistently achieve higher or similar accuracy compared to the most recent sparse training methods while considerably reducing the training FLOPs. Specifically, at 90% sparsity ratio, SpFDE 20%+20% maintains similar accuracy as MEST [1], while achieving 27% training FLOPs reduction. When compared with DeepR [53], SET [19], and DSR [4], SpFDE 25%+25% achieves 27% FLOPs reduction and +1.36% \u223c +4.24% "
                    ],
                    "subsections": []
                },
                {
                    "title": "Reduction on Memory Cost",
                    "paragraphs": [
                        "From the Fig. 4, we can see the superior memory saving of our SpFDE framework. The memory costs indicate the memory footprint used during the sparse training process, including the weights, activations, and the gradient of weights and activations, using a 32-bit floating-point representation with a batch size of 64 on ResNet-32 using CIFAR-100. The \"SpFDE Min.\" stands for the training memory costs after all the target layers are frozen, while the \"SpFDE Avg.\" is the average memory costs throughout the entire training process. The baseline results of \"DST methods Min.\" only consider the minimum memory costs requirement for DST methods [2,3,53,54,4,19,5,1,6], which ignores the memory overhead such as the periodic dense back-propagation in RigL [5], dense sparse structure searching at initialization in [2,3], and the soft memory bound in MEST [1]. Even under this condition, our \"SpFDE Avg.\" can still outperform the \"DST methods Min.\" with a large margin (20% \u223c 25.3%). The \"SpFDE Min.\" results show our minimum memory costs can be reduced by 42.2% \u223c 43.9% compared to the \"DST methods Min.\" at different sparsity ratios. This significant reduction in memory costs is especially crucial to edge training."
                    ],
                    "subsections": []
                },
                {
                    "title": "Discussion and Limitation",
                    "paragraphs": [
                        "The reduction in training FLOPs of our method comes from three sources: weight sparsity, frozen layers, and shrunken dataset. However, the actual training acceleration depends on different factors, e.g., the support of the sparse computation, layer type and size, and system overhead. Generally, the same FLOPs reduction from the frozen layers and shrunken dataset can lead to higher actual training acceleration than weight sparsity (more details in Appendix F). This makes our layer freezing and data sieving method more valuable in sparse training. We use the overall computation FLOPs to measure the training acceleration, which may be considered a theoretical upper bound."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "This work investigates the layer freezing and data sieving technique in the sparse training domain.",
                "Based on the analysis of the feasibility and potentiality of using the layer freezing technique in sparse training, we introduce a progressive layer freezing method. Then, we propose a data sieving technique, which ensures end-to-end dataset-efficient training. We seamlessly incorporate layer freezing and data sieving methods into the sparse training algorithm to form a generic framework named SpFDE. Our extensive experiments demonstrate that our SpFDE consistently outperforms the prior arts and can significantly reduce training FLOPs and memory costs while preserving high accuracy. While this work mainly focuses on the classification task, a future direction is to further investigate the performance of our methods on other tasks and networks. Another exciting topic is studying the best trade-off between these techniques when considering accuracy, FLOPs, memory costs, and actual acceleration."
            ],
            "subsections": []
        },
        {
            "title": "Appendix",
            "paragraphs": [
                "A More Results for the Impact of Model Sparsity on the Network Representation Learning Process Sec. 3.2 of the main paper discusses the impact of model sparsity on the network representation learning process. Here we provide more experimental results. Specifically, we evaluate the representational similarity using the CKA value of the same layer from the model (ResNet-32) with different sparsity at each epoch and compare them with the final model. We choose the early (1 st and 3 rd ), middle (18 th ), and late layers (25 th and 32 nd ) to track their CKA trends. We evaluate three sparsity ratios, including medium (50%), medium-high (80%), and high (90%) sparsity. The results are shown in Fig. A1.  "
            ],
            "subsections": []
        },
        {
            "title": "B Ablation Analysis on Freezing Schemes",
            "paragraphs": [
                "In our work, we evaluate four different types of freezing schemes (Sec. 4.4.4 of the main paper), including the single-shot freezing, single-shot freezing & resume, periodically freezing, and delayed periodically freezing.",
                "Single-Shot Freezing Scheme. The single-shot freezing scheme is the default freezing scheme used in our progressive layer freezing method. For this scheme, we progressively freeze the layers/blocks in a sequential manner, as shown in Alg. 1 and Fig. 3 (a) in the main paper.",
                "Single-Shot Freezing & Resume Scheme. This scheme follows the same way to decide the per layer/block freezing epoch as the single-shot scheme, except that we make the freezing epoch for all layers/blocks t epochs earlier and resume (defrost) the training for all layers/blocks at the last t epochs. In this case, we can keep the single-shot & resume has the same FLOPs reduction as the single-shot scheme, and the entire network can be fine-tuned at the end of training with a small learning rate.",
                "Periodically Freezing Scheme. For the periodically freezing scheme, we let the selected layers freeze periodically with a given frequency so that all the layers/blocks are able to be updated at different stages of the training process. The basic idea is to let the front layers/blocks updated (trained) less frequently than the later layers. For example, we let the front layers/blocks only be updated for one epoch in every four epochs and let the middle layers/blocks only be updated for one epoch in every two epochs. Therefore, we consider the update frequency of the front and middle layers/blocks are 1/4 and 1/2, respectively. To ensure that when a layer is frozen, all the layers in front of it are frozen, we need to let the freezing period be the numbers of power of two (e.g.,",
                "In our experiments, we divide the ResNet32 into three blocks and set the update frequency for the first and second blocks to 1/4 and 1/2, respectively. The last block will not be frozen during the training. We control the number of layers in each block to satisfy the overall training FLOPs reduction requirement.",
                "Delayed Periodically Freezing Scheme. For this scheme, we first let all the layers/blocks be trained actively for certain epochs, then periodically freeze the layers used in the periodically freezing scheme.",
                "To achieve the same training FLOPs reduction as the periodically freezing scheme, more layers are needed to be included in the first and second blocks.    "
            ],
            "subsections": []
        },
        {
            "title": "E Ablation Study on Layer Freezing and Data Sieving",
            "paragraphs": [
                "We also conduct ablation studies for the impact of layer-freezing and data sieving on accuracy by themselves (Tab. A8 and Tab. A9). The results are obtained using ResNet-32 on the CIFAR-100 with the sparsity of 60% and 90%. The accuracy results are the average value of 3 runs using random seeds. From the experiments, we can further find some interesting observations:",
                "\u2022 Under both sparsity of 60% and 90%, saving up to 15% training costs (FLOPs) via either layer freezing or data sieving does not lead to any accuracy drop.",
                "\u2022 When under a higher sparsity ratio (90% vs. 60%), sparse training can tolerate a higher FLOPs reduction for both layer freezing and data sieving. For example, compared to the non-freezing case (i.e., None in the second column), the layer freezing with a 20% FLOPs reduction leads to a -0.01% and -0.21% accuracy drop for 90% sparsity and 60% sparsity, respectively. As for the data sieving, compared to the non-freezing case, under a 20% FLOPs reduction, there is a -0.19% and -0.31% accuracy drop for 90% sparsity and 60% sparsity, respectively. The possiable reason is that, under a higher sparsity ratio, the upper bound for model accuracy/generalization capability is decreased, mitigating the sensitivity to the number of training data or layer freezing.",
                "\u2022 With a relatively higher FLOPs reduction ratio (i.e., 30% 35%), data sieving preserves higher accuracy than layer freezing under the same FLOPs reduction ratio. This inspires that if people intend to pursue a more aggressive FLOPs reduction at the cost of accuracy degradation, removing more data via the data sieving method is a more desirable choice than freezing more layers.",
                "Furthermore, in Tab. A10, we show a comparison between only using layer-freezing or data sieving, or both of them to achieve similar FLOPs reductions. It can be observed that to achieve similar FLOPs reduction, using layer-freezing and data sieving together achieves much higher accuracy than by only using one of them individually, showing the importance of combining the two techniques. "
            ],
            "subsections": []
        },
        {
            "title": "F Discussion on Acceleration",
            "paragraphs": [
                "In our work, the reduction in training FLOPs comes from three sources: weight sparsity, frozen layers, and shrunken dataset. It is well-known that the acceleration based on weight sparsity is heavily affected by many different factors, such as the sparse computation support from a sparse matrix multiplication library or the dedicated compiler optimizations [39]. Besides, the sparsity schemes play an important role in the sparse computation acceleration. Currently, the actual acceleration by leveraging weight sparsity is still limited even at a very high sparsity ratio [1].",
                "We also evaluate the acceleration achieved by using our layer freezing and data sieving methods. We measure the training time over 50 consecutive training epochs for each configuration and calculate the average value.",
                "Tab. A11 and Tab. A12 show the acceleration results by using our layer freezing and data sieving methods, respectively. We compare the per epoch training latency with different FLOPs saving configurations (i.e., 10%, 15%, 20%, and 25%) with the baseline result (i.e., using whole dataset and without freezing). We can see that both methods achieve almost the linear training acceleration according to the FLOPs reduction. This indicates that both methods only introduce negligible overhead to the training process. Compared to the weight sparsity, this demonstrates the superiority of layer freezing and data sieving methods in the acceleration efficiency when under the same FLOPs reduction. Most importantly, the layer freezing and data sieving methods have a high degree of practicality since the acceleration can be easily achieved using native PyTorch/TensorFlow without additional support."
            ],
            "subsections": []
        }
    ]
}