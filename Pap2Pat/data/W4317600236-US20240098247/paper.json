{
    "id": "https://semopenalex.org/work/W4317600236",
    "authors": [
        "Christian Timmerer",
        "Hadi Amirpour",
        "Mohammad Ghanbari",
        "Vignesh V Menon",
        "Prajit T Rajendran"
    ],
    "title": "Content-adaptive Encoder Preset Prediction for Adaptive Live Streaming",
    "date": "2022-12-07",
    "abstract": "In live streaming applications, a fixed set of bitrate-resolution pairs (known as bitrate ladder) is generally used to avoid additional pre-processing run-time to analyze the complexity of every video content and determine the optimized bitrate ladder. Furthermore, live encoders use the fastest available preset for encoding to ensure the minimum possible latency in streaming. For live encoders, it is expected that the encoding speed is equal to the video framerate. An optimized encoding preset may result in (i) increased Quality of Experience (QoE) and (ii) improved CPU utilization while encoding. In this light, this paper introduces a Content-Adaptive encoder Preset prediction Scheme (CAPS) for adaptive live video streaming applications. In this scheme, the encoder preset is determined using Discrete Cosine Transform (DCT)-energy-based low-complexity spatial and temporal features for every video segment, the number of CPU threads allocated for each encoding instance, and the target encoding speed. Experimental results show that ChPS yields an overall quality improvement of 0.83 dB PSNR and 3.81 VMAF with the same bitrate, compared to the fastest preset encoding of the HTTP Live Streaming (HLS) bitrate ladder using $times265$ HEVC open-source encoder. This is achieved by maintaining the desired encoding speed and reducing CPU idle time.",
    "sections": [
        {
            "title": "I. INTRODUCTION",
            "paragraphs": [
                "Motivation: HTTP Adaptive Streaming (HAS) has become the de-facto standard in delivering video content for various clients regarding internet speeds and device types. The main idea behind HAS is to divide the video content into segments and encode each segment at various bitrates and resolutions, called representations, which are stored in plain HTTP servers. These representations are stored to continuously adapt the video delivery to the network conditions and device capabilities of the client [1]. Traditionally, a fixed bitrate ladder, e.g., HTTP Live Streaming (HLS) bitrate ladder 1 , is used in live streaming. Furthermore, for every representation, maintaining a fixed encoding speed, which is the same as the video framerate, independent of the video content, is a key goal for a live encoder. Although the compression efficiency (in terms of the obtained quality and bitrate) of the output video is an important metric for the encoder, maintaining the encoding speed takes precedence in the live scenario. This is because a reduction in encoding speed may lead to the unacceptable outcome of dropped frames during transmission, eventually decreasing the Quality of Experience (QoE) [2]. Traditional open-source encoders like x2642 , and x265 3 have pre-defined sets of encoding parameters (termed as presets) which present a trade-off between the encoding time and compression efficiency 4 . The preset for the fastest encoding (ultrafast for x264 and x265) is then used as the encoding preset for all live content, independent of the dynamic complexity of the content. Though this conservative technique achieves the intended result of achieving a live encoding, the resulting encode is sub-optimal, especially when the type of the content is dynamically changing, which is the typical use-case for live streams [4]. Furthermore, when the content becomes easier to encode (i.e., slow-moving videos or videos that have simpler textures are easy to encode as predicting the current frame from a previous frame is simpler, resulting in smaller residuals), the encoder would achieve a higher encoding speed than the target encoding speed. This, in turn, introduces unnecessary CPU idle time as it waits for the video feed. If the encoder preset is configured such that this higher encoding speed can be reduced while still being compatible with the expected live encoding speed, the quality of the encoded content achieved by the encoder can be improved. Subsequently, when the content becomes complex again, the Fig. 2: The encoding pipeline using CAPS envisioned in this paper.",
                "encoder preset need to be reconfigured to move back to the faster configuration that achieves live encoding speed [5]. Furthermore, the encoding speed also depends on the encoding resolution and bitrate. Fig. 1 shows the encoding time measurement of HLS bitrate ladder encoding of the Wood s000 sequence [3] using ultrafast preset of x265 3 and eight CPU threads. Since the video segment has a duration of 5 seconds, the target encoding time (T ) is 5 seconds [2]. The CPU utilization is poor in the lower bitrate representation encodings; e.g., for 0.145 Mbps to 3.4 Mbps representations, an encoding time of less than one second is observed. In the adaptive live streaming scenarios, lower bitrates typically select lower resolutions which might need slower presets to achieve the desired target encoding speed while utilizing the CPU more efficiently.",
                "Goal: This paper aims to present an encoding scheme that determines the encoding preset configuration dynamically, adaptive to the video content, to maximize the CPU utilization and maximize the efficiency of adaptive live streaming for a given target encoding speed.",
                "Contributions: In this paper, a content-adaptive encoder preset prediction scheme (CAPS) is proposed for live video streaming applications to provide low latency streaming while reducing the CPU idle time. To this light, content-aware features, i.e., (i) Discrete Cosine Transform (DCT)-energy-based low-complexity spatial and temporal features, are extracted to determine video segments' characteristics. (ii) Based on these features, encoding time is predicted for every preset supported by the encoder using XGBoost [6] models. Using this information, optimized encoder presets are determined to maintain the target encoding speed.",
                "Paper outline: In Section II, the proposed CAPS scheme is explained. In Section III, the scheme's performance is validated, and the corresponding experimental results are presented. Finally, Section IV concludes the paper."
            ],
            "subsections": []
        },
        {
            "title": "II. CAPS ARCHITECTURE",
            "paragraphs": [
                "The architecture of CAPS for streaming applications is presented in Fig. 2, according to which the encoder preset for each video segment is predicted using the spatial and temporal features (i.e., E, h, and L) of the video segment, the target video encoding speed (f ), the number of CPU threads used for encoding (c) and the set of pre-defined resolutions (R) and bitrates (B) of the bitrate ladder. The encoding process is carried out with the predicted optimized encoder preset from the set of supported presets (P ) for each video segment. CAPS is classified into two steps: (i) video complexity feature extraction and (ii) encoder preset prediction, explained in Section II-A, and Section II-B, respectively."
            ],
            "subsections": [
                {
                    "title": "A. Video Complexity Feature Extraction",
                    "paragraphs": [
                        "In live streaming applications, selecting low-complexity features is critical to ensure low-latency video streaming without disruptions. In this paper, the optimized encoder preset (p) is determined as a function of three DCT-energybased features [7], (i) the average texture energy (E), (ii) the average gradient of the texture energy (h), and (iii) the average luminescence (L) [8]- [12], the target bitrate (b), resolution (r), the target video encoding speed (f ) and the number of CPU threads used for encoding (c).",
                        "The three DCT-energy-based features are determined as follows. The block-wise texture of each frame is defined as:",
                        "where k is the block address in the s th frame, w \u00d7 w pixels is the size of the block, and DCT (i, j) is the (i, j) th DCT component when i + j > 0, and 0 otherwise [13].",
                        "where K represents the number of blocks per frame, and S denotes the number of frames in the segment. Furthermore, the average temporal energy (h) is defined as follows:",
                        "Preset selection p Fig. 3: Encoding Preset Prediction for a video framerate (f ) and the number of CPU threads (c).",
                        "The luminescence of non-overlapping blocks k of s th frame is defined as:",
                        "where DCT (0, 0) is the DC component in the DCT calculation. The block-wise luminescence is averaged per segment denoted as L as shown below."
                    ],
                    "subsections": []
                },
                {
                    "title": "B. Encoding Preset Prediction",
                    "paragraphs": [
                        "The objective of selecting the optimized encoding preset based on resolution, bitrate, and video complexity is decomposed into two parts: (i) the former deals with designing models to predict the encoding time for each corresponding input, and (ii) the latter involves developing a function to obtain the optimized preset based on the predicted encoding times for each available encoding preset. As explained above, Fig. 3 illustrates the architecture proposed in this paper to facilitate the encoding preset prediction in a two-step manner. The input to the encoding time prediction models designed for a given encoder, number of CPU threads for encoding (c), and target encoding speed (f ) comprises complexity metrics (E, h, and L) along with the resolution (r) and bitrate (b). r and b are input in logarithmic scale to the encoding time prediction models to reduce internal covariate shift [14]. The model set is trained to predict the encoding times for the pre-defined set of encoding presets (P ). The minimum and maximum encoder preset (p min and p max , respectively) are chosen based on the target encoder. For example, x265 HEVC [15] encoder supports encoding presets ranging from 0 to 9 (i.e., ultrafast to placebo). The model set predicts the encoding times for each of the presets in P as tpmin to tpmax . The predicted encoding times are fed into the preset selection function, which chooses the optimized preset to ensure that the encoding time matches the target video encoding speed. This is accomplished by selecting the preset which is closest to the target encoding time T , which is defined as:",
                        "where n represents the number of frames in the segment. Apart from this, the function should ensure that the encoding time is not greater than the target encoding time T . This can be represented mathematically as follows:",
                        "where p is the selected optimum preset and t is the encoding time for p.",
                        "Implementation of prediction models: In this paper, the XGBoost [6] algorithm is used to train encoding time prediction models for various resolution and preset combinations. XGBoost models are an implementation of gradient boosted decision trees that boast of a significant level of speed and performance as opposed to other machine learning models. (p max -p min + 1) \u2022 n r models are trained, where n r denotes the number of resolutions supported by the streaming service provider. Training prediction models for each resolution and preset combination ensures scalability, as more resolutions and presets can be added to CAPS architecture in the future with minimal re-training. The input vector passed to the model set (cf. Fig. 3) is [E, h, L, log(r), log(b)]. Inside the model set, prediction models corresponding to the resolution r is used to predict the encoding times for all presets in P for the target bitrate b. Thus, the output from the model set is the predicted encoding time for all presets, i.e., tp i , \u2200i \u2208 [p min , p max ]. Since the encoding time prediction is a regression problem, the loss function employed is mean absolute error."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "III. EVALUATION",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "A. Test Methodology",
                    "paragraphs": [
                        "In this paper, four hundred sequences (80% of the sequences) from the Video Complexity Dataset [3] are used as the training dataset, and the remaining (20%) is used as the test dataset. The sequences are encoded at 24fps using x265 v3.5 3 , i.e., T = 5 seconds. The presets defined in x265: 0 (ultrafast) to 8 (veryslow) are used for evaluation. All experiments are run on a dual-processor server with Intel Xeon Gold 5218R (80 cores, frequency at 2.10 GHz), where each encoding instance uses 8 CPU threads (i.e., c = 8) with multi-threading and x86 SIMD [16] optimizations. The DCT-energy-based features, E, h, and L are extracted using VCA [17] running as a preprocessor using 8 CPU threads with multi-threading and x86 SIMD optimizations. The bitrate ladder, as shown in Table I is considered in the evaluation.",
                        "The resulting overall quality in Peak Signal to Noise Ratio (PSNR) and Video Multimethod Assessment Fusion  0.97 0.96 0.97 0.97 0.97 0.96 0.97 0.98 0.98 MAE 0.06 0.09 0.12 0.17 0.22 0.31 0.33 0.41 0.49 Fig. 4: Average relative importance of features in the encoding time prediction of 2160p encoding for all presets.",
                        "(VMAF) 5 , and the achieved bitrate are compared for each test sequence. Bj\u00f8ntegaard Delta values [18] BD-PSNR and BD-VMAF refer to the average increase in PSNR and VMAF of the representations compared with the ultrafast preset encoding with the same bitrate, respectively. A positive BD-PSNR and BD-VMAF indicate a gain in coding efficiency of CAPS compared to the ultrafast preset encoding."
                    ],
                    "subsections": []
                },
                {
                    "title": "B. Experimental Results",
                    "paragraphs": [
                        "Firstly, the overall time taken to predict optimized preset is evaluated. E, h, and L features are extracted at an average speed of 370 frames per second. The inference time of the XGBoost model is 0.5ms. Hence, the overall speed of preset prediction is 240 frames per second. Secondly, the encoding time prediction accuracy of the XGBoost models for each preset is observed in terms of R 2 score and Mean Absolute Error (MAE), as shown in Table II. The average R 2 score and MAE are 0.97 and 0.21, respectively. This paper also evaluates the relative importance of features in the prediction models for each resolution in terms of SHAP values [19], as shown in Fig. 4. The target bitrate in the logarithmic scale (log(b)) is the most critical feature for encoding time prediction, followed by the h, L, and E features. Fig. 5 shows the average preset chosen for each representation of the HLS bitrate ladder across the test dataset. On  6a shows the average video encoding time for every representation in the bitrate ladder using ultrafast preset and CAPS. On average, the 11.6 Mbps and 16.8 Mbps representations take more than 5 seconds for encoding using ultrafast preset. Hence, these representations cannot be encoded faster than the existing presets. For the other representations, it is observed that the encodings are finished within the bound of T = 5. It is also observed that using ultrafast preset for all representations introduces significant CPU idle time for lower bitrate representations. However, CAPS yield lower CPU idle time when the encodings are carried out concurrently. Fig. 6b and Fig. 6c show the average PSNR and VMAF for each representation, respectively. It is observed that the visual quality improves significantly at lower bitrate representations. Using CAPS yields an overall BD-VMAF of 3.81 and BD-PSNR of 0.83 dB."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "IV. CONCLUSIONS",
            "paragraphs": [
                "This paper proposed CAPS, a content-adaptive encoder preset prediction scheme for adaptive live streaming applications. CAPS predicts the optimized encoder preset for a given target bitrate, resolution, and video framerate for each segment, which helps improve the quality of video encodings. DCT-energy-based features are used to determine segments' spatial and temporal complexity. The performance of CAPS is analyzed using the x265 open-source HEVC encoder for the HLS bitrate ladder encoding. It is observed that CAPS yield lower idle time and an overall quality improvement of 0.83dB PSNR and 3.81 VMAF score with the same bitrate, compared to the fastest preset encoding of the reference HTTP Live Streaming (HLS) bitrate ladder. "
            ],
            "subsections": []
        }
    ]
}