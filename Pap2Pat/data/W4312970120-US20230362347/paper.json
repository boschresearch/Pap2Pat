{
    "id": "https://semopenalex.org/work/W4312970120",
    "authors": [
        "Justin Johnson",
        "Chris Rockwell",
        "Ang Cao"
    ],
    "title": "FWD: Real-time Novel View Synthesis with Forward Warping and Depth",
    "date": "2022-06-01",
    "abstract": "Novel view synthesis (NVS) is a challenging task requiring systems to generate photorealistic images of scenes from new viewpoints, where both quality and speed are important for applications. Previous image-based rendering (IBR) methods are fast, but have poor quality when input views are sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give impressive results but are not real-time. In our paper, we propose a generalizable NVS method with sparse inputs, called FWD, which gives high-quality synthesis in real-time. With explicit depth and differentiable rendering, it achieves competitive results to the SOTA methods with 130-1000\u00d7 speedup and better perceptual quality. If available, we can seamlessly integrate sensor depth during either training or inference to improve image quality while retaining real-time speed. With the growing prevalence of depths sensors, we hope that methods making use of depth will become increasingly useful.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Given several posed images, novel view synthesis (NVS) aims to generate photorealistic images depicting the scene from unseen viewpoints. This long-standing task has applications in graphics, VR/AR, bringing life to still images. It requires a deep visual understanding of geometry and semantics, making it appealing to test visual understanding.",
                "Early work on NVS focused on image-based rendering (IBR), where models generate target views from a set of input images. Light field [39] or proxy geometry (like mesh surfaces) [12,24,61,62] are typically constructed from posed inputs, and target views are synthesized by resampling or blending warped inputs. Requiring dense input images, these methods are limited by 3D reconstruction quality, and can perform poorly with sparse input images.",
                "Recently, Neural Radiance Fields (NeRF) [48] have become the leading methods for NVS, using MLPs to represent the 5D radiance field of the scene implicitly. The color and density of each sampling point are queried from the network and aggregated by volumetric rendering to get the  Real-time Novel View Synthesis. We present a realtime and generalizable method to synthesize images from sparse inputs. NeRF variants model the scene via an MLP, which is queried millions of times during rendering and leads to low speeds. Our method utilizes explicit depths and point cloud renderers for fast rendering, inspired by SynSin [82]. The model is trained endto-end with a novel fusion transformer to give high-quality results, where regressed depths and features are optimized for synthesis.",
                "pixel color. With dense sampling points and differentiable renderer, explicit geometry isn't needed, and densities optimized for synthesis quality are learned. Despite impressive results, they are not generalizable, requiring MLP fitting for each scene with dense inputs. Also, they are extremely slow because of tremendous MLP query times for a single image. Generalizable NeRF variants like PixelNeRF [89], IBR-Net [78] and MVSNeRF [9] emerge very recently, synthesizing novel views of unseen scenes without per-scene optimization by modeling an MLP conditioned on sparse inputs. However, they still query the MLP millions of times, leading to slow speeds. Albeit the progress of accelerating NeRF with per-scene optimization [88,27,18], fast and generalizable NeRF variants are still under-explored.",
                "In this paper, we target a generalizable NVS method with sparse inputs, refraining dense view collections. Both realtime speed and high-quality synthesis are expected, allowing interactive applications. Classical IBR methods are fast but require dense input views for good results. Generalizable NeRF variants show excellent quality without perscene optimization but require intense computations, leading to slow speeds. Our method, termed FWD, achieves this target by Forward Warping features based on Depths.",
                "Our key insight is that explicitly representing the depth of each input pixel allows us to apply forward warping to each input view using a differentiable point cloud renderer. This avoids the expensive volumetric sampling used in NeRF-like methods, enabling real-time speed while maintaining high image quality. This idea is deeply inspired by the success of SynSin [82], which employs a differentiable point cloud renderer for single image NVS. Our paper extends SynSin to multiple inputs settings and explores effective and efficient methods to fuse multi-view information.",
                "Like prior NVS methods, our approach can be trained with RGB data only, but it can be progressively enhanced if noisy sensor depth data is available during training or inference. Depth sensors are becoming more prevalent in consumer devices such as the iPhone 13 Pro and the LG G8 ThinQ, making RGB-D data more accessible than ever. For this reason, we believe that methods making use of RGB-D will become increasingly useful over time.",
                "Our method estimates depths for each input view to build a point cloud of latent features, then synthesizes novel views via a point cloud renderer. To alleviate the inconsistencies between observations from various viewpoints, we introduce a view-dependent feature MLP into point clouds to model view-dependent effects. We also propose a novel Transformer-based fusion module to effectively combine features from multiple inputs. A refinement module is employed to inpaint missing regions and further improve synthesis quality. The whole model is trained end-to-end to minimize photometric and perceptual losses, learning depth and features optimized for synthesis quality.",
                "Our design possesses several advantages compared with existing methods. First, it gives both high-quality and highspeed synthesis. Using explicit point clouds enables realtime rendering. In the meanwhile, differentiable renderer and end-to-end training empower high-quality synthesis results. Also, compared to NeRF-like methods, which cannot synthesize whole images during training because of intensive computations, our method could easily utilize perceptual loss and refinement module, which noticeably improves the visual quality of synthesis. Moreover, our model can seamlessly integrate sensor depths to further improve synthesis quality. Experimental results support these analyses.",
                "We evaluate our method on the ShapeNet and DTU datasets, comparing it with representative NeRF-variants and IBR methods. It outperforms existing methods, considering speed and quality jointly: compared to IBR methods we improve both speed and quality; compared to recent NeRF-based methods we achieve competitive quality at realtime speeds (130-1000\u00d7 speedup). A user study demonstrates that our method gives the most perceptually pleasing results among all methods. The code is available at https://github.com/Caoang327/fwd_code."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Novel view synthesis is a long-standing problem in computer vision, allowing for the generation of novel views given several scene images. A variety of 3D representations (both implicit and explicit) have been used for NVS, including depth and multi-plane images [74,94,72,57,7,67], voxels [69,21], meshes [61,23,28,62], point clouds [82,40,64] and neural scene representations [65,41,19,34,47,55,48]. In this work, we use point clouds as our 3D representations for computational and memory efficiency. Image-based Rendering. IBR synthesizes novel views from a set of reference images by weighted blending [15,39,20,24,57,61,12,62]. They generally estimate proxy geometry from dense captured images for synthesis. For instance, Riegler et al. [61] uses multi-view stereo [66,87,77,77,45,29] to produce scene mesh surface and warps source view images to target views based on proxy geometry. Despite promising results in some cases, they are essentially limited by the quality of 3D reconstructions, where dense inputs (tens to hundreds) with large overlap and reasonable baselines are necessary for decent results. These methods estimate geometry as an intermediate task not directly optimized for image quality. In contrast we input sparse views and learn depth jointly to optimize for synthesis quality. Neural Scene Representations. Recent work uses implicit scene representations for view synthesis [65,41,19,34,47,55]. Given many views, neural radiance fields (NeRF) show impressive results [48,92,46,56,81], but require expensive per-scene optimization. Recent methods [78,89,75,9,31] generalize NeRF without per-scene optimization by learning a shared prior, with sparse inputs. However these methods require expensive ray sampling and therefore are very slow. In contrast, we achieve significant speedups using explicit representations. Some concurrent work accelerates NeRF by reformulating the computation [18], using precomputation [88,27], or adding view dependence to explicit 3D representations [41,83,2,8,49]; unlike ours, these all require dense input views and per-scene optimization. Utilizing RGB-D in NVS. The growing availability of annotated depth maps [13,5,10,1,71,68] facilitates depth utilization in NVS [54,40,26], which serves as extra supervision or input to networks. Our method utilizes explicit depths as 3D representations, allowing using sensor depths as additional inputs for better quality. Given the increasing popularity of depth sensors, integrating sensor depths is a promising direction for real-world applications.",
                "Depth has been used in neural scene representations for speedups [51,73], spaser inputs [16] and dynamic scenes [84]. However, these works still require per-scene optimization. Utilizing RGB-D inputs to accelerate generalizable NeRF like [89,78] is still an open problem. Differentiable Rendering and Refinement. We use advances in differentiable rendering [42,35,11,52,43] to learn 3D end-to-end. Learned geometries rely heavily on rendering and refinement [90,86,3,79] to quickly synthesize realistic results. Refinement has improved dramatically owing to generative modeling [38,36,91,95] and rendering frameworks [60,32,50,30]. Instead of aggregating information across viewpoints before rendering [44], we render viewpoints separately and fuse using a Transformer [76,17,4], enabling attention across input views."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [
                "Given a sparse set of input images {I i } N i=1 and corresponding camera poses {R i , T i }, our goal is to synthesize a novel view with camera pose {R t , T t } fast and effectively. The depths {D sen i } of I i captured from sensors are optionally available, which are generally incomplete and noisy.",
                "The insight of our method is that using explicit depths and forward warping enables real-time rendering speed and tremendous accelerations. Meanwhile, to alleviate quality degradations caused by inaccurate depth estimations, a differentiable renderer and well-designed fusion & refinement modules are employed, encouraging the model to learn geometry and features optimized for synthesis quality.",
                "As illustrated in Figure 2, with estimated depths, input view I i is converted to a 3D point cloud P i containing geometries and view-dependent semantics of the view. A differentiable neural point cloud renderer \u03c0 is used to project point clouds to target viewpoints. Rather than directly aggregating point clouds across views before rendering, we propose a Transformer-based module T fusing rendered re-sults at target view. Finally, a refinement module R is employed to generate final outputs. The whole model is trained end-to-end with photometric and perceptual loss."
            ],
            "subsections": [
                {
                    "title": "Point Cloud Construction",
                    "paragraphs": [
                        "We use point clouds to represent scenes due to their efficiency, compact memory usage, and scalability to complex scenes. For input view I i , point cloud P i is constructed by estimating depth D i and feature vectors F i for each pixel in the input image, then projecting the feature vectors into 3D space using known camera intrinsics. The depth D i is estimated by a depth network d; features F i are computed by a spatial feature encoder f and view-dependent MLP \u03c8. Spatial Feature Encoder f . Scene semantics of input view I i are mapped to per-pixel feature vectors F i by spatial feature encoder f . Each feature vector in F i is 61-dimensions and is concatenated with RGB channels, which is 64 dimensions in total. f is built on BigGAN architecture [3]. Depth Network d. Estimating depth from a single image has scaling/shifting ambiguity, losing valuable multi-view cues and leading to inconsistent estimations across views. Applying multi-view stereo algorithms (MVS) [66,87,77,85] solely on sparse inputs is challenging because of limited overlap and huge baselines between input views, leading to inaccurate and low-confidence estimations. Therefore, we employ a hybrid design cascading a U-Net after the MVS module. The U-Net takes image I i and estimated depths from the MVS module as inputs, refining depths with multiview stereo cues and image cues. PatchmatchNet [77] is utilized as the MVS module, which is fast and lightweight. Depth Estimation with sensor depths. As stated, U-Net receives an initial depth estimation from the MVS module and outputs a refined depth used to build the point cloud. If sensor depth D sen i is available, it is directly input to the U-Net as the initial depth estimations. In this setting, U-Net servers as completion and refinement module taking D sen i and I i as inputs, since D sen i is usually noisy and incomplete. During training, loss L s is employed to encourage the U-Net output to match the sensor depth.",
                        "where M i is a binary mask indicating valid sensor depths. View-Dependent Feature MLP \u03c8. The appearance of the point could vary across views because of lighting and view direction, causing inconsistency between multiple views. Therefore, we propose to insert view direction changes into scene semantics to model this view-dependent effects. An MLP \u03c8 is designed to compute view-dependent features F i by taking F i and relative view changes \u2206v from input to target view as inputs. For each point in the cloud, \u2206v is calculated based on normalized view directions v i and v t , from the point to camera centers of input view i and target view t. The relative view direction change is calculated as:",
                        "and the view-dependent feature F i is:",
                        "where \u03b4 is a two-layer MLP mapping \u2206v to a 32dimensions vector and \u03c8 is also a two-layer MLP."
                    ],
                    "subsections": []
                },
                {
                    "title": "Point Cloud Renderer",
                    "paragraphs": [
                        "To observe the constructed point cloud P i at target views, we employ a neural point cloud renderer \u03c0. P i is first transformed to target view coordinates based on camera poses and then rendered by \u03c0. The rendered feature maps Fi share the same dimension as feature F i at each pixel. With explicit geometry transformation, our rendered results are geometrically consistent and correct across views.",
                        "We use the differentiable renderer design of [82], which splats 3D points to the image plane and gets pixel values by blending point features. The blending weights are computed based on z-buffer depths and distances between pixel and point centers. It is implemented using Pytorch3D [60].",
                        "This fully differentiable renderer allows our model to be trained end-to-end, where photometric and perceptual loss gradients can be propagated to points' position and features. In this way, the model learns to estimate depths and features optimized for synthesis quality, leading to superior quality. We show the effectiveness of it in experiments.",
                        "Image feature geometry feature"
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Query Token",
            "paragraphs": [
                "Transformer T"
            ],
            "subsections": []
        },
        {
            "title": "MLP",
            "paragraphs": [
                "Per-pixel Feature Vector Figure 3. Fusion Transformer. We use a lightweight transformer T to fuse the features from N input views on each pixel. We use a learnable token to query the fusion results."
            ],
            "subsections": [
                {
                    "title": "Fusion and Refinement",
                    "paragraphs": [
                        "Unlike SynSin [82] using a single image for NVS, fusing multi-view inputs is required in our method. A na\u00efve fusion transforms each point cloud to target view and aggregates them into a large one for rendering. Despite high efficiency, it is vulnerable to inaccurate depths since points with wrong depths may occlude points from other views, leading to degraded results. Methods like PointNet [58] may be feasible to apply on the aggregated point cloud for refinement, but they are not efficient with significant point numbers.",
                        "Instead, we render each point cloud individually at target viewpoints and fuse rendered results by a fusion Transformer T . A refinement module R is used to inpaint missing regions, decode feature maps and improve synthesis quality. Fusion Transformer T . Given an arbitrary number of rendered feature maps { Fi }, fusion should be effective, fast, and permutation invariant. Inspired by the success of Transformers, we propose a pixel-wise Transformer T for fusion, detailed in Figure 3. At each pixel, T inputs rendered feature vectors and queries fused results using a learnable \"token\". Applied on features, T utilizes semantics for fusion.",
                        "Rendered results may lose geometry cues for fusion when rendered from 3D to 2D. For instance, depths may reveal occlusion relationships across views, and relative view changes from input to target views relate to each input's importance for fusion. Therefore, we also explored to use geometry features as position encoding while not helpful. Refinement Module R. Built with 8 ResNet [22] blocks, R decodes fused feature maps F to RGB images \u0128 at target views. It inpaints regions invisible for inputs in a semantically and geometrically meaningful manner. Also, it corrects local errors caused by inaccurate depths and improves perceptual quality based on semantics contained by feature maps, leading to coherent and high-quality synthesis."
                    ],
                    "subsections": []
                },
                {
                    "title": "Training and Implementation Details",
                    "paragraphs": [
                        "Our model is trained end-to-end with photometric L l2 and perceptual L c losses between generated and groundtruth target images. The whole loss function is: ",
                        "where \u03bb l2 = 5.0, \u03bb c = 1.0. The model is trained endto-end on 4 2080Ti GPUs for 3 days, using Adam [37] with learning rate 10 -4 and \u03b2 1 =0.9, \u03b2 2 =0.999. When sensors depths are available as inputs, L s is used with \u03bb s = 5.0."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "The goal of our paper is real-time and generalizable novel view synthesis with sparse inputs, which can optionally use sensor depths. To this end, our experiments aim to identify the speed and quality at which our method can synthesize novel images and explore the advantage of explicit depths. We evaluate our methods on ShapeNet [6] and DTU [33] datasets, comparing results with the SOTA methods and alternative approaches. Experiments take place with held-out test scenes and no per-scene optimization. We conduct ablations to validate the effectiveness of designs. Metrics. We conduct A/B test to measure the visual quality, in which workers select the image most similar to the ground truth from competing methods. Automatic image quality metrics including PSNR, SSIM [80] and LPIPS [93] are also reported, and we find LPIPS best reflects the image quality as perceived by humans. Frames per second (FPS) during rendering is measured on the same platform (single 2080Ti GPU with 4 CPU cores). All evaluations are conducted using the same protocol (same inputs and outputs). Model Variants. Three models are evaluated with various accessibility to depths for training and test, as defined in Table 1. FWD utilizes a pretrained PatchmatchNet [77] as the MVS module for depth estimations, which is also updated during end-to-end training with photometric and perceptual loss. FWD-U learns depth estimations in an Unsupervised manner, sharing the same model and settings as FWD while PatchmatchNet is randomly initialized without any pretraining. FWD-D takes sensor depths as additional inputs during both training and inference. It doesn't use any MVS module since sensor depths provide abundant geometry cues. For pretraining PatchmatchNet, we train it following typical MVS settings and using the same data splitting as NVS."
            ],
            "subsections": [
                {
                    "title": "ShapeNet Benchmarks",
                    "paragraphs": [
                        "We first evaluate our model for category-agnostic synthesis task on ShapeNet [6]. Following the setting of [89], we train and evaluate a single model on 13 ShapeNet [6] categories. Each instance contains 24 4, where FWD-U gets noticeably superior results. Our synthesized results are more realistic and closely matching to target views, while PixelNeRF's results tend to be blurry. We observe the same trend in the DTU benchmark and evaluate the visual quality quantitatively there.",
                        "We show quantitative results in Table 2, adding SRN [70] and DVR [53] as other baselines. Our method outperforms others significantly for LPIPS, indicating a much better perceptual quality, as corroborated by qualitative results. Pixel-NeRF has a slightly better PSNR while its results are blurry. Most importantly, FWD-U runs at a speed of over 300 FPS, which is 300\u00d7 faster than PixelNeRF."
                    ],
                    "subsections": []
                },
                {
                    "title": "DTU MVS Benchmarks",
                    "paragraphs": [
                        "We also evaluate our model on DTU MVS dataset [33], which is a real scene dataset consisting of 103 scenes. Each scene contains one or multiple objects placed on a table, while images and incomplete depths are collected by the camera and structured light scanner mounted on an industrial robot arm. Corresponding camera poses are provided. As stated in [89], this dataset is challenging since it consists of complex real scenes without apparent semantic similarities across scenes. Also, images are taken under varying lighting conditions with distinct color inconsistencies between views. Moreover, with only under 100 scenes available for training, it is prone to overfitting in training.",
                        "We follow the same training and evaluation pipelines as PixelNeRF [89] for all methods to give a fair comparison. The data consists of 88 training and 15 test scenes, between which there are no shared or highly similar scenes. Images are down-sampled to a resolution of 300 \u00d7 400. For training, three input views are randomly sampled, with the rest as target views. For inference, we choose three fixed informative input views and synthesize other views of the scene. Baselines. We evaluate a set of representatives of generalizable NeRF and IBR methods in two different scenarios: with RGB or RGB-D available as inputs during inference.",
                        "PixelNeRF [89], IBRNet [78] and MVSNeRF [9] are the SOTA generalizable NeRF variants, taking RGB as inputs. We use the official PixelNeRF model trained on DTU MVS and carefully retrain IBRNet and MVSNeRF with the same 3-input-view settings. PixelNeRF-DS is also included as reported in [16], which is PixelNeRF supervised with depths. Please note that our settings are very different from evaluations used in original papers of IBRNet and MVSNeRF.",
                        "A series of IBR methods are also evaluated. Since COLMAP [66] fails to give reasonable outputs with sparse input images, methods using COLMAP like FVS [61], DeepBlending [25] cannot estimate scene geometry in this setting. For these methods, we use depths captured by sensors as estimated depths, which should give upper-bound performance of these methods. To better cope with missing regions, we add our refinement model to DeepBlending [25] and retrain it on DTU dataset, termed Blending-R.",
                        "For fairness, we evaluate all methods using the same protocol, distinct from some of their original settings. Although we try our best to adopt these methods, our reported results may still not perfectly reflect their true capacity. We show comparisons to baselines in Figure 6. Our methods provide noticeably better results than baselines across different depth settings. For models without depths in test, IBRNet and PixelNeRF give blurry results in areas of high detail such as the buildings in the top row, while our FWD-U and FWD give more realistic and sharper images. With sensor depths in test, baseline Blending-R produces more cogent outputs, but still struggles to distinguish objects from the background, such as in the middle row, while FWD-D gives faithfully synthesis and clear boundaries. Quantitative Results. We evaluate synthesis quality quantitatively by user study following a standard A/B paradigm. Workers choose the closest to a ground truth image between competing methods, and are monitored using a qualifier and sentinel examples. All views in the test set (690 in total) are evaluated, and each view is judged by three workers.",
                        "In Figure 7, user study results support qualitative observations. Among all baselines with and without test depths , users choose our method as more closely matching ground truth images than others most of the time. FWD-U is selected over PixelNeRF in 65.6% of examples, and 77.8% compared to IBRNet. Also, over 90% workers prefer FWD-D to FWD , showing advantage of using sensor depths.",
                        "We show automated view synthesis metrics and speeds in Table 3. Across all depth availability settings, our method is competitive with the SOTA baselines while significantly faster. FWD-D runs in real-time and gives substantially better image quality than others.  weaker PSNR and SSIM of our unsupervised FWD-U against PixelNeRF and IBRNet. However, FWD-U has noticeably better perceptual quality with the best LPIPS, and human raters prefer it to other methods in A/B tests. The visual quality in figure 6 also illustrates the disparity between comparisons using PSNR and LPIPS. Meanwhile, FWD-U is above 1000\u00d7 faster than PixelNeRF and above 100\u00d7 faster than IBRNet. Depth estimations, rendering and CNN would introduce tiny pixel shiftings, which harm the PSNR of our method. NeRF-like methods are trained to optimize L2 loss for each pixel independently, leading to blur results.",
                        "Among all methods without test depths, FWD has the best results. Although it uses a pretrained MVS module, we think this comparison is still reasonable since pretrained depth module is easy to get. Also, training depths can be easily calculated from training images since they are dense.",
                        "Baseline comparisons also show that IBR methods are fast, but do not give images that are competitive with our method. Our method outperforms them in both perceptual quality and standard metrics, showing the efficacy of proposed methods. Note that Blending+R doesn't support variable number inputs and our refinement module improves its results significantly. We also compare FWD-U with SynSin [82] which only receives a single input image, showing the benefits of using multi-view inputs in NVS.",
                        "Table 3.",
                        "Quantitative comparison on DTU real images. We compare our method with representatives of generalizable NeRF variants and IBR methods for image quality and rendering speed. Our method achieves significantly better speed-quality tradeoff, indicating the effectiveness and efficiency of our design.",
                        "\u2020 Unlike other methods, SynSin receives only one image as input. "
                    ],
                    "subsections": []
                },
                {
                    "title": "Ablations and Analysis",
                    "paragraphs": [
                        "We evaluate the effectiveness of our designs and study depth in more detail through ablation experiments. Effects of Fusion Transformer. We design a model without Transformer, which concatenates point clouds across views into a bigger one for later rendering and refinement. Its results in FWD-U settings are shown in Figure 8. The ablated version is vulnerable to inaccurate depths learned in unsupervised manner and synthesizes \"ghost objects\" since points with bad depths occlude other views' points.",
                        "We repeat the same ablation in FWD-D settings, shown in Table 4, which settings give much better depth estimations with sensor depths. The ablated model has notably worse results for all metrics, indicating that the proposed method is not only powerful to tackle inaccurate depth estimations, but also fuse semantic features effectively. Effects of View Dependent MLP. For ablation, we remove the view-dependent feature MLP and report its results in Table 4. Removing this module reduces model's ability to produce view-dependent appearances, leading to worse per-  formance for all metrics. We show more results in Supp.",
                        "Depth Analysis and Ablations. We visualize depths in Figure 9. Estimating depths from sparse inputs is challenging and gives less accurate results because of the huge baselines between inputs. We show estimated depths from PatchmatchNet here, filtered based on the confidence scores. Therefore, refinement is essential in our design to propagate multi-view geometry cues to the whole image.",
                        "Our end-to-end model learns it by synthesis losses. We ablate the depth network in Table 5 and report depth error \u03b4 3cm , which is the percentage of estimated depths within 3 cm of sensor depths. MVS module is critical (row 2) to give geometrically consistent depths. U-Net further refines depths and improves the synthesis quality (row 3). PatchmatchNet has its own shallow refinement layer, already giving decent refinements. Learning unsupervised MVS and NVS jointly from scratch is challenging (row 4), and training depth network without supervision [14] first may give a good initialization for further jointly training."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We propose a real-time and generalizable method for NVS with sparse inputs by using explicit depths. This method inherits the core idea of SynSin while extending it to multi-view input settings, which is more challenging. Our experiments show that estimating depths can give impressive results with a real-time speed, outperforming existing methods. Moreover, the proposed method could utilize sensor depths seamlessly and improve synthesis quality significantly. With the increasing availability of mobile depth sensors, we believe our method has exciting real-world 3D applications. We acknowledge there could be the potential  for the technology to be used for negative purposes by nefarious actors, like synthesizing fake images for cheating.",
                "There are also challenges and limitations yet to be explored. 1) Although using explicit depths gives tremendous speedups, it potentially inflicts depth reliance on our model. We designed a hybrid depth regressor to improve the quality of depth by combining MVS and single image depth estimations. We also employed an effective fusion and refinement module to reduce the degrades caused by inaccurate depths. Despite these designs, the depth estimator may still work poorly in some challenging settings (like very wide camera baselines), and it would influence the synthesis results. Exploring other depth estimation methods like MiDaS [59] could be an interesting direction for future work.",
                "2) The potential capacity of our method is not fully explored. Like SynSin [82], our model (depth/feature network and refinement module especially) is suitable and beneficial from large-scale training data, while the DTU MVS dataset is not big enough and easy to overfit during training. Evaluating our method on large-scale datasets like Hypersim [63] would potentially reveal more advantages of our model, which dataset is very challenging for NeRF-like methods.",
                "3) Although our method gives more visually appealing results, our PSNR and SSIM are lower than NeRF-like methods. We hypothesize that our refinement module is not perfectly trained to decode RGB colors from feature vectors because of limited training data. Also, tiny misalignments caused during the rendering process may also harm the PSNR, although it is not perceptually visible. "
            ],
            "subsections": []
        },
        {
            "title": "Model Architectures and Training details",
            "paragraphs": [
                "As stated in the paper, our model consists of spatial feature network f , depth network d, view-dependent feature MLP \u03c8, neural point cloud renderer \u03c0, fusion transformer T and refinement module R. We show architecture details. Spatial Feature Network f . The spatial feature network f contains 8 ResNet blocks, with output channels of 32,32,32,64,64,64,64,61 and no downsampling. Each ResNet block utilizes 3 \u00d7 3 /stride 1/padding 1 convolution followed by instance norm and ReLU. Similar to [3,82], spectral normalization is utilized for each convolution for stable training. The input images are padded by 0 to fit network. Depth Network d. We utilize a classical U-Net for depth refinement, consisting 8 downsampling blocks and 8 upsampling blocks. We pad constant zero to make the input have feasible shape. For PatchMatchNet [77] to estimate the initial depths, we follow the original pipeline, in which we downsample the input images into 4 scales and predict the depths in a coarse-to-fine manner. View-dependent feature MLP \u03c8. The relative view direction change vector is first passed through a two-layer MLP without 16 and 32 output features perspectively to get a 32dims feature embedding. Then this 32 dims feature embedding is concatenated with the original 64 dims feature vector and passed through another two-layer MLP with output 64 and 64 output features. We use ReLU as activation function following MLP and no normalization layers. Neural point cloud renderer \u03c0. The point cloud renderer is implemented in Pytorch3D [60], which takes a point cloud and pose P and project it to a 150 \u00d7 200 feature map, where each pixel has 64-dims feature. The renderer fills zero with 64-dims for pixels which are invisible.",
                "The blending weight \u03b1 of the 3D point x for pixel l is",
                "where s is the Euclidean distance between point x's splatted center and pixel l; r is the radius of point x in NDC coordinate. We set r = 1.5 pixels in our experiments. To render the value of each pixel, we employ alpha-compositing to blend all feasible points. The rendered feature F l of pixel l is:",
                "where F i , \u03b1 i is the feature and alpha of 3D point i. The rendering is based on points' depths and we blend the Top K points with the nearest depths. We use K = 16 in our paper, meaning we blend at most 16 points to get the results of one pixel. We compute D l , the depth of pixel l using the blending weights by:",
                "where d i is the depth of point i. "
            ],
            "subsections": []
        },
        {
            "title": "License Discussions",
            "paragraphs": [
                "We discuss the licenses of assets."
            ],
            "subsections": [
                {
                    "title": "Dataset",
                    "paragraphs": [
                        "ShapeNet Dataset [6] . We conduct our experiments on the ShapeNet dataset and we cite the paper [6] as required by the author. More specifically, we download the data from NMR, which is hosted by DVR [52] authors. DTU MVS Dataset [1]. We conduct our experiments on the DTU MVS dataset. This dataset doesn't include any license. On the other hand, we cite the paper [1] which is required by the paper."
                    ],
                    "subsections": []
                },
                {
                    "title": "Methods",
                    "paragraphs": [
                        "PixelNeRF [89] . We evaluate the official code of Pix-elNeRF [89] for comparison. The code is hosted in the github page https://github.com/sxyu/pixel-nerf, which uses the BSD 2-Clause \"Simplified\" License.",
                        "IBRNet [78] .",
                        "We use the official code of IBR-Net [78] for comparison, which is hosted at https://github.com/googleinterns/IBRNet. This project has the Apache License 2.0. MVSNeRF [9]. We use the official code of MVS-NeRF [9] for comparison, which is hosted at https://github.com/apchenstu/mvsnerf with MIT License. SynSin [82]. The code of SynSin [82] is hosted at https://github.com/facebookresearch/synsin with Copyright (c) 2020, Facebook All rights reserved. Stable View Synthesis (SVS) [62]. We use the code hosted at https://github.com/isl-org/StableViewSynthesis for eval- DeepBlending [25]. We implement it according to the code at https://github.com/Phog/DeepBlending. This work is under Apache License. PixelNeRF-DS [16]. We use the number reported in [16].",
                        "Pytorch3D. We use the code from Pytorch3D [?]: https://github.com/facebookresearch/pytorch3d for our differentiable renderer. The code is licensed with BSD 3-Clause License."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "User Study Details",
            "paragraphs": [
                "As detailed in the paper, we conduct user study to evaluate perceptual quality of synthesized images. We provide more information about user study here.",
                "We employ the standard A/B test paradigm as our user study format, which asks workers to select the closest result to a ground truth image between two competing results. Results of method A and B and ground truth target view are available during test. All views in the test set (690 views in total) are evaluated and each view is judged by 3 workers.",
                "All tests are conducted using thehive.ai, a website similar to Amazon Mechanical Turk. Workers were given instructions and examples of tests, and then they were given a test to identify whether they understand the task. Only workers passing the test were allowed to conduct A/B test. Three images of the same view are shown in the test, where the first image is the ground truth target views and the rest two images are synthesized images. Workers are asked to select \"left\" or \"right\" image to indicate their preference. Results of method A and B are randomly placed for every test for fairness. We show the instructions and examples below."
            ],
            "subsections": []
        },
        {
            "title": "Additional Results",
            "paragraphs": [
                "Again, please see attached videos for comparison between ours and other methods."
            ],
            "subsections": [
                {
                    "title": "Time statistics.",
                    "paragraphs": [
                        "We show times spent on each component of FWD-D model during a single forward pass in Table 6. As shown in the Table, only 30 percent and 12 percent of times are spent on the Rendering process and Fusion process, indicating that our renderer and Transformer are highly-efficient. Moreover, we compare the time distributions of our method with PixelNeRF and IBRNet in Table 7.  We show more synthesis results in the following. For comparison, we show results of the same scene and views. We first show the results of FWD-U in Figure 11, FWD in Figure 12 and FWD-D in Figure 13. We also show baseline results: PixelNeRF in Figure 14, IBRNet in Figure 15, MVSNeRF in Figure 16, FVS in Figure 17 and Blending-R in Figure 18. Again, please see attached videos for comparison between ours and other methods.  "
                    ],
                    "subsections": []
                }
            ]
        }
    ]
}