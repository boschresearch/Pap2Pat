{
    "id": "US20230362347",
    "authors": [
        "Justin Johnson",
        "Ang Cao",
        "Chris Rockwell"
    ],
    "title": "Real-Time Novel View Synthesis With Forward Warping And Depth",
    "date": "2022-05-09 00:00:00",
    "abstract": "A fast and generalizable novel view synthesis method with sparse inputs is disclosed. The method may comprise: accessing at least a first input image with a first view of a subject in the first input image, and a second input image with a second view of the subject in the second input image using a computer system; estimating depths for pixels in the at least first and second input images; constructing a point cloud of image features from the estimated depths; and synthesizing a novel view by forward warping by using a point cloud rendering of the constructed point cloud.",
    "sections": [
        {
            "title": "DESCRIPTION",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "BACKGROUND OF THE INVENTION",
                    "paragraphs": [],
                    "subsections": [
                        {
                            "title": "1. Field of the Invention",
                            "paragraphs": [
                                "This invention relates to real-time novel view synthesis with forward warping and depth."
                            ],
                            "subsections": [],
                            "outline_long": [
                                "define field"
                            ],
                            "num_characters": 88,
                            "outline_medium": [
                                "define field"
                            ],
                            "outline_short": [
                                "define field"
                            ]
                        },
                        {
                            "title": "2. Description of the Related Art",
                            "paragraphs": [
                                "Novel view synthesis (NVS) is a challenging task requiring systems to generate photorealistic images of scenes from new viewpoints, where both quality and speeds are desired for real applications. Previous image-based rendering (IBR) possesses fast rendering speeds but limited quality when input views are sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give impressive results but suffer extremely low speeds.",
                                "Therefore, there is a need for a fast and generalizable novel view synthesis method with sparse inputs."
                            ],
                            "subsections": [],
                            "outline_long": [
                                "summarize limitations of NVS"
                            ],
                            "num_characters": 540,
                            "outline_medium": [
                                "motivate novel view synthesis"
                            ],
                            "outline_short": [
                                "summarize limitations of NVS"
                            ]
                        }
                    ],
                    "outline_long": [],
                    "num_characters": 0,
                    "outline_medium": [],
                    "outline_short": []
                },
                {
                    "title": "SUMMARY OF THE INVENTION",
                    "paragraphs": [
                        "In this disclosure, we provide a fast and generalizable novel view synthesis method with sparse inputs, which enjoys both high quality and fast running for synthesis. With explicit depth and differentiable rendering, the method achieves competitive quality with the state of the art NeRF-based methods with 140-1000\u00d7 speed up. Moreover, it allows seamless integration of sensor depths to improve the synthesis quality significantly. With the growing prevalence of depths sensors, the method can contribute to real applications.",
                        "In one configuration, a method is provided for novel view synthesis. The method includes accessing a first input image with a first view of a subject in the first input image, and a second input image with a second view of the subject in the second input image using a computer system. The method also includes estimating depths for pixels in the at least first and second input images. The method also includes constructing a point cloud of image features from the estimated depths. The method also includes synthesizing a novel view by forward warping by using a point cloud rendering of the constructed point cloud.",
                        "In one configuration, a system is provided for novel view synthesis. The system includes a computer system configured to: i) access a first input image with a first view of a subject in the first input image, and a second input image with a second view of the subject in the second input image; ii) estimate depths for pixels in the at least first and second input images; iii) construct a point cloud of image features from the estimated depths; and iv) synthesize a novel view by forward warping by using a point cloud rendering of the constructed point cloud.",
                        "These and other features, aspects, and advantages of the present disclosure will become better understood upon consideration of the following detailed description, drawings, and appended claims."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "motivate novel view synthesis",
                        "summarize method",
                        "highlight advantages"
                    ],
                    "num_characters": 1907,
                    "outline_medium": [
                        "summarize invention"
                    ],
                    "outline_short": [
                        "introduce novel view synthesis method"
                    ]
                },
                {
                    "title": "DETAILED DESCRIPTION OF THE INVENTION",
                    "paragraphs": [
                        "Before the present invention is described in further detail, it is to be understood that the invention is not limited to the particular embodiments described. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only, and is not intended to be limiting. The scope of the present invention will be limited only by the claims. As used herein, the singular forms \u201ca\u201d, \u201can\u201d, and \u201cthe\u201d include plural embodiments unless the context clearly dictates otherwise.",
                        "It should be apparent to those skilled in the art that many additional modifications beside those already described are possible without departing from the inventive concepts. In interpreting this disclosure, all terms should be interpreted in the broadest possible manner consistent with the context. Variations of the term \u201ccomprising\u201d, \u201cincluding\u201d, or \u201chaving\u201d should be interpreted as referring to elements, components, or steps in a non-exclusive manner, so the referenced elements, components, or steps may be combined with other elements, components, or steps that are not expressly referenced. Embodiments referenced as \u201ccomprising\u201d, \u201cincluding\u201d, or \u201chaving\u201d certain elements are also contemplated as \u201cconsisting essentially of\u201d and \u201cconsisting of\u201d those elements, unless the context clearly dictates otherwise. It should be appreciated that aspects of the disclosure that are described with respect to a system are applicable to the methods, and vice versa, unless the context explicitly dictates otherwise.",
                        "The present invention provides systems and methods for fast and generalizable novel view synthesis (NVS) with sparse inputs, which enjoys both high quality and fast running for synthesis. (NVS) aims to generate photorealistic images depicting a scene or image from unseen viewpoints. Sparse inputs may include where a limited number of views of a scene are available, or where the angular difference between views is minimal, thus making a reproduction of the scene from a novel viewpoint challenging. With explicit depth and differentiable rendering, the systems and methods may achieve competitive quality with the state of the art Neural Radiance Fields (NeRF) based methods with significant speed up. Seamless integration of sensor depths may be provided to improve the synthesis quality.",
                        "Previous methods have required dense input views, such as a large number of views from different viewpoints, to produce good results. Some methods have shown excellent quality without per-scene optimization, but require intense computation leading to slow speeds.",
                        "In some configurations, the systems and methods in accordance with the present disclosure use Forward Warping features based on estimated Depths (FWD) to achieve high quality, fast results. Estimating explicit depth for input views can provide synthesis results with real-time speed. After estimating depths, a differentiable point cloud renderer may be used to project input images to a target view, which may provide for fast rendering speed and enabling end-to-end-training. In some configurations, training may be performed with RGB data only. Training may be progressively enhanced if noisy sensor depth data is available during training or inference.",
                        "Referring to FIG. 10, a flowchart of non-limiting example steps for a method of novel view synthesis is shown. An input view or a number of input views may be accessed or acquired at step 1002. In some configurations, at least a first and second input view are used. Depths may be estimated for each input image at step 1004. Explicit depths may be in the form of 3D representations, which may provide for using sensor depths as additional inputs for better quality. Explicit depth may provide for fast and high-quality NVS.",
                        "A point cloud of image features may be constructed at step 1006 based on the estimated depths. Forward warping may be used to synthesize novel views at step 1008 using a point cloud renderer. View-dependent effects, such as missing pixel data, for the synthesized novel views may be modeled at step 1010. In some configurations, the view-dependent effects may be modeled using a feature-dependent MLP operating on estimated point clouds. Addressing view-dependent effects, such as missing regions or pixel data, may be performed with image fusion of the synthesized novel views and inpainting the missing data. Fused data may be generated by fusing multiple synthesized views at step 1012. In some configurations, fused data may be generated using a Transformer-based fusion module. Missing regions may be in-painted to generate output pixels at step 1014, such as by using a refinement module. In some configurations, a model may be trained end-to-end to minimize photometric and perceptual losses, which may provide for learning depth and features optimized for synthesis quality.",
                        "In some configurations, a sparse set of input images may be represented by {Ii}i=1N and corresponding camera poses {Ri, Ti}. A novel view with camera pose {Rt, Tt} may be synthesized. The depths {Disen} of Ii may be estimated, or may be optionally captured from sensors. Using explicit depths and forward warping may provide for real-time rendering speed and tremendous accelerations. In some configurations, quality degradations caused by inaccurate depth estimations may be mitigated by using a differentiable renderer, fusion & refinement modules, and encouraging the model to learn geometry and features optimized for synthesis quality.",
                        "In some configurations for estimating depths, input view Ii may be converted to a 3D point cloud Pi containing geometry and view-dependent semantics of the view. View-dependent semantics may include the angle of the view to the subject in the scene, pixel values in the scene, and the like. A differentiable neural point cloud renderer \u03c0 may be used to project point clouds to target viewpoints. Rather than directly aggregating point clouds across views before rendering, a Transformer-based module T may be used for fusing rendered results at a target view. A refinement module R may be employed to generate final outputs. The whole model may be trained end-to-end with photometric and perceptual loss.",
                        "Point cloud construction may include using an explicit 3D representation to inject 3D priors into a model. Point clouds may be used to represent scenes due to their efficiency, compact memory usage, and scalability to complex scenes. For input view a point cloud Pi may be constructed by estimating per-pixel depth Di and per-pixel feature vectors F\u2032i at the same resolution as the input image, then projecting the feature vectors into 3D space using known camera intrinsics. The depth Di may be estimated by a depth network d, giving 3D scene structure; features F\u2032i may be computed by a spatial feature encoder f and view-dependent MLP \u03c8 representing scene semantics.",
                        "In a non-limiting example of a spatial feature encoder f, scene semantics of input view Ii are mapped to pixel-specific feature vectors Fi by spatial feature encoder f. Each feature vector in Fi may include a determined number of dimensions and may be concatenated with RGB channels.",
                        "In a non-limiting example of depth network d, depth may be estimated from a single image, but using a single image may introduce scaling/shifting ambiguity, losing valuable multi-view cues and leading to inconsistent estimations across views. Applying conventional multi-view stereo algorithms (MVS) solely on sparse inputs is challenging because of limited overlap and huge baselines between input views, leading to inaccurate and low-confidence estimations. In some configurations, cascading a U-Net after the MVS module may address these challenges. The U-Net takes image Ii and estimates depths from the MVS module as inputs, refining depths with multiview stereo cues and image cues.",
                        "In some configurations, depth estimation may include sensor depths. A U-Net may receive an initial depth estimation from an MVS module and may output a refined depth used to build the point cloud. If sensor depth Disen is available, it may be directly input to the U-Net as the initial depth estimations. A U-Net may serve as a completion and refinement module taking Disen and Ii as inputs, since Disen is usually noisy and incomplete. During training, loss Ls may be employed to encourage the U-Net output to match the sensor depth, as determined by eq.(1) below.",
                        "For a view-dependent feature MLP \u03c8, the appearance of the same object may vary across views, such as because of lighting and view direction changes introducing inconsistency between multiple views. The scene semantics may be represented by spatial features Fi and translated to the target view for synthesis. View direction changes from input to target views may be inserted into scene semantics to model the view-dependent effects. An MLP \u03c8 may be used to compute view-dependent spatial features Fi by taking Fi and relative view changes \u0394v as inputs. For each point in the cloud, \u0394v may be calculated based on normalized view directions vi and vt, from the point to camera centers of input view i and target view t. The relative view direction change may be determined by eq. (2) below, and view-dependent feature F\u2032i may be determined by eq. (3) below.",
                        "In some configurations for a point cloud renderer, after constructing a point cloud Pi from view i containing the geometry and semantics of the scene, Pi may be viewed at target views for synthesis. A neural point cloud \u03c0 may be used. Pi may be first transformed to target view coordinates based on relative camera poses and then rendered by \u03c0. The rendered results \u02dcFi may be spatial feature maps, sharing the same dimension as feature F\u2032i in Pi at each pixel. With explicit geometry transformation and renderer satisfying geometry rules, rendered results may be geometrically consistent and correct across views. A renderer may provide for assigning points to a region and accumulate them based on blending weights for each pixel. The weight may be computed based on depth, distance to rays cast from the pixel, sphere radius, and the like.",
                        "A fully differentiable renderer may be used and may provide for a model to be trained end-to-end, where photometric and perceptual loss gradients can be propagated to points' position and features. The model may be trained to learn to estimate depths and features optimized for synthesis quality.",
                        "The point clouds constructed from each input view for synthesis may be fused to generate fused data. Each point cloud may be rendered individually at the target viewpoint and the rendered results may be fused to form fused data. A Transformer-based fusion module T may be used to fuse arbitrary size inputs and a refinement module R may be used to synthesize final results based on fused features.",
                        "In some configurations of fusion Transformer T, a set of feature maps {{tilde over (F)}i} may be rendered from point clouds and fused into one feature map, which may be decoded into an RGB image by a refinement module. The fusion operation may consider scene semantics, be fast, and may support an arbitrary number of inputs in any order. A pixel-wise Transformer T may be used for fusion. T extracts feature vectors from {{tilde over (F)}i} as inputs and output a fused one at each pixel. A standard multi-head attention may be applied to the sequence of feature vectors and queries using an extra learnable \u201ctoken\u201d. Applied on features, T may utilize scene semantics for fusion. Depths at a target view may be rendered for each point cloud and the relative view changes may be determined from input to target views. These may be concatenated as geometry features and used as position encoding of the Transformer.",
                        "In some configurations of refinement Module R, fused feature maps {tilde over (F)} may be decoded to RGB images I at target view. Regions not visible in the input views may be inpainted to be semantically meaningful and geometrically accurate. Inpainting to be semantically meaningful may include missing portions of a feature or object in an image, such as a couch, should be filled in with similar texture. Inpainting to be geometrically accurate may include where a feature or an object that has straight lines should continue to be straight. In some configurations, inpainting includes assigning pixel values to missing pixel data based upon interpolating between nearest neighbor pixel regions, by incorporating pixel data from similar novel views or regions in the image, performing pixel weighting, and the like. Local errors caused by inaccurate depths may be corrected and perceptual quality may be improved based on semantics contained by feature maps, leading to coherent and high-quality synthesis.",
                        "Training of a model may be performed end-to-end with photometric l2 and perceptual c losses between generated and ground-truth target images. The whole loss function may be determined by eq. (4) below. The model may be trained for a period of time, such as over a period of days, and may include using GPUs.",
                        "In some configurations, the effectiveness of the novel view synthesis may be determined, such as by using a ShapeNet or DTU benchmark, and the like, and comparing the results with representative NeRF-variants and IBR methods. Novel view synthesis in accordance with the present disclosure may outperform existing methods, considering both speed and quality.",
                        "FIG. 11 shows an example 1100 of a system for automatically performing novel view synthesis using input image data in accordance with some embodiments of the disclosed subject matter. As shown in FIG. 11, a computing device 1110 can receive multiple images or multiple types of image data from an image source 1102. In some configurations, computing device 1110 can execute at least a portion of an automatic novel view synthesis system 1104 to automatically novel views based on input images. Novel view synthesis system 1104 may include a transformer-based fusion module 1132, multi-view stereo module 1134, and refinement module 1136.",
                        "Additionally or alternatively, in some embodiments, computing device 1110 can communicate information about image data received from image source 1102 to a server 1120 over a communication network 1108, which can execute at least a portion of automatic novel view synthesis system 1104 to automatically generate novel views. In such embodiments, server 1120 can return information to computing device 1110 (and/or any other suitable computing device) indicative of an output of automatic novel view synthesis system 1104 to generate novel views.",
                        "In some embodiments, computing device 1110 and/or server 1120 can be any suitable computing device or combination of devices, such as a desktop computer, a laptop computer, a smartphone, a tablet computer, a wearable computer, a server computer, a virtual machine being executed by a physical computing device, etc. In some configurations, automatic novel view synthesis system 1104 can generate novel views from input image data using a model trained by a neural network, such as a convolutional neural network (CNN). In some embodiments, training image data can be used to train a model, such as a support vector machine (SVM), to inpaint missing image features in the novel view. In some embodiments, automatic novel view synthesis system 1104 can provide input image data to the trained model and can present a novel view synthesis based on the output of the model.",
                        "In some embodiments, image source 1102 can be any suitable source of image data, such as a camera system, a vehicle camera system, or another computing device (e.g., a server storing image data), etc. In some embodiments, image source 1102 can be local to computing device 1110. For example, image source 1102 can be incorporated with computing device 1110 (e.g., computing device 1110 can be configured as part of a device for capturing and/or storing images). As another example, image source 1102 can be connected to computing device 1110 by a cable, a direct wireless link, etc. Additionally or alternatively, in some embodiments, image source 1102 can be located locally and/or remotely from computing device 1110, and can communicate image data to computing device 1110 (and/or server 1120) via a communication network (e.g., communication network 1108).",
                        "In some embodiments, communication network 1108 can be any suitable communication network or combination of communication networks. For example, communication network 1108 can include a Wi-Fi network (which can include one or more wireless routers, one or more switches, etc.), a peer-to-peer network (e.g., a Bluetooth network), a cellular network (e.g., a 3G network, a 4G network, etc., complying with any suitable standard, such as CDMA, GSM, LTE, LTE Advanced, WiMAX, etc.), a wired network, etc. In some embodiments, communication network 1108 can be a local area network, a wide area network, a public network (e.g., the Internet), a private or semi-private network (e.g., a corporate or university intranet), any other suitable type of network, or any suitable combination of networks. Communications links shown in FIG. 11 can each be any suitable communications link or combination of communications links, such as wired links, fiber optic links, Wi-Fi links, Bluetooth links, cellular links, etc.",
                        "FIG. 12 shows an example 1200 of hardware that can be used to implement image source 1102, computing device 1110, and/or server 1120 in accordance with some embodiments of the disclosed subject matter. As shown in FIG. 12, in some embodiments, computing device 1110 can include a processor 1202, a display 1204, one or more inputs 1206, one or more communication systems 1208, and/or memory 1210. In some embodiments, processor 1202 can be any suitable hardware processor or combination of processors, such as a central processing unit (CPU), a graphics processing unit (GPU), etc. In some embodiments, display 1204 can include any suitable display devices, such as a computer monitor, a touchscreen, a television, etc. In some embodiments, inputs 1206 can include any suitable input devices and/or sensors that can be used to receive user input, such as a keyboard, a mouse, a touchscreen, a microphone, etc.",
                        "In some embodiments, communications systems 1208 can include any suitable hardware, firmware, and/or software for communicating information over communication network 1108 and/or any other suitable communication networks. For example, communications systems 1208 can include one or more transceivers, one or more communication chips and/or chip sets, etc. In a more particular example, communications systems 1208 can include hardware, firmware and/or software that can be used to establish a Wi-Fi connection, a Bluetooth connection, a cellular connection, an Ethernet connection, etc.",
                        "In some embodiments, memory 1210 can include any suitable storage device or devices that can be used to store instructions, values, etc., that can be used, for example, by processor 1202 to present content using display 1204, to communicate with server 1120 via communications system(s) 1208, etc. Memory 1210 can include any suitable volatile memory, non-volatile memory, storage, or any suitable combination thereof. For example, memory 1210 can include RAM, ROM, EEPROM, one or more flash drives, one or more hard disks, one or more solid state drives, one or more optical drives, etc. In some embodiments, memory 1210 can have encoded thereon a computer program for controlling operation of computing device 1110. In such embodiments, processor 1202 can execute at least a portion of the computer program to present content (e.g., camera 2D images, 3D images, user interfaces, graphics, tables, etc.), receive content from server 1120, transmit information to server 1120, etc.",
                        "In some embodiments, server 1120 can include a processor 1212, a display 1214, one or more inputs 1216, one or more communications systems 1218, and/or memory 1220. In some embodiments, processor 1212 can be any suitable hardware processor or combination of processors, such as a CPU, a GPU, etc. In some embodiments, display 1214 can include any suitable display devices, such as a computer monitor, a touchscreen, a television, etc. In some embodiments, inputs 1216 can include any suitable input devices and/or sensors that can be used to receive user input, such as a keyboard, a mouse, a touchscreen, a microphone, etc.",
                        "In some embodiments, communications systems 1218 can include any suitable hardware, firmware, and/or software for communicating information over communication network 1108 and/or any other suitable communication networks. For example, communications systems 1218 can include one or more transceivers, one or more communication chips and/or chip sets, etc. In a more particular example, communications systems 1218 can include hardware, firmware and/or software that can be used to establish a Wi-Fi connection, a Bluetooth connection, a cellular connection, an Ethernet connection, etc.",
                        "In some embodiments, memory 1220 can include any suitable storage device or devices that can be used to store instructions, values, etc., that can be used, for example, by processor 1212 to present content using display 1214, to communicate with one or more computing devices 1110, etc. Memory 1220 can include any suitable volatile memory, non-volatile memory, storage, or any suitable combination thereof. For example, memory 1220 can include RAM, ROM, EEPROM, one or more flash drives, one or more hard disks, one or more solid state drives, one or more optical drives, etc. In some embodiments, memory 1220 can have encoded thereon a server program for controlling operation of server 1120. In such embodiments, processor 1212 can execute at least a portion of the server program to transmit information and/or content (e.g., image data, a user interface, etc.) to one or more computing devices 1110, receive information and/or content from one or more computing devices 1110, receive instructions from one or more devices (e.g., a personal computer, a laptop computer, a tablet computer, a smartphone, etc.), etc.",
                        "In some embodiments, image source 1102 can include a processor 1222, imaging components 1224, one or more communications systems 1226, and/or memory 1228. In some embodiments, processor 1222 can be any suitable hardware processor or combination of processors, such as a CPU, a GPU, etc. In some embodiments, imaging components 1224 can be any suitable components to generate image data.",
                        "Note that, although not shown, image source 1102 can include any suitable inputs and/or outputs. For example, image source 1102 can include a storage device, such as an SD card, thumb drive, and the like, or input devices and/or sensors that can be used to receive user input, such as a keyboard, a mouse, a touchscreen, a microphone, a trackpad, a trackball, hardware buttons, software buttons, etc. As another example, image source 1102 can include any suitable display devices, such as a computer monitor, a touchscreen, a television, etc., one or more speakers, etc.",
                        "In some embodiments, communications systems 1226 can include any suitable hardware, firmware, and/or software for communicating information to computing device 1110 (and, in some embodiments, over communication network 1108 and/or any other suitable communication networks). For example, communications systems 1226 can include one or more transceivers, one or more communication chips and/or chip sets, etc. In a more particular example, communications systems 1226 can include hardware, firmware and/or software that can be used to establish a wired connection using any suitable port and/or communication standard (e.g., VGA, DVI video, USB, RS-232, etc.), Wi-Fi connection, a Bluetooth connection, a cellular connection, an Ethernet connection, etc.",
                        "In some embodiments, memory 1228 can include any suitable storage device or devices that can be used to store instructions, values, image data, etc., that can be used, for example, by processor 1222 to: control imaging components 1224, and/or receive image data from imaging components 1224; generate images; present content (e.g., images, a user interface, etc.) using a display; communicate with one or more computing devices 1110; etc. Memory 1228 can include any suitable volatile memory, non-volatile memory, storage, or any suitable combination thereof. For example, memory 1228 can include RAM, ROM, EEPROM, one or more flash drives, one or more hard disks, one or more solid state drives, one or more optical drives, etc. In some embodiments, memory 1228 can have encoded thereon a program for controlling operation of image source 1102. In such embodiments, processor 1222 can execute at least a portion of the program to generate images, transmit information and/or content (e.g., image data) to one or more computing devices 1110, receive information and/or content from one or more computing devices 1110, receive instructions from one or more devices (e.g., a personal computer, a laptop computer, a tablet computer, a smartphone, etc.), etc."
                    ],
                    "subsections": [
                        {
                            "title": "Example",
                            "paragraphs": [
                                "The following Example is provided in order to demonstrate and further illustrate certain embodiments and aspects of the present invention and is not to be construed as limiting the scope of the invention.",
                                "Novel view synthesis (NVS) is a challenging task requiring systems to generate photorealistic images of scenes from new viewpoints, where both quality and speeds are desired for real applications. Previous image-based rendering (IBR) possesses fast rendering speeds but limited quality when input views are sparse. Recent Neural Radiance Fields (NeRF) and generalizable variants give impressive results but suffer extremely low speeds. In this Example, we describe a fast and generalizable NVS method with sparse inputs, called FWD-U, which enjoys both high quality and fast running for synthesis. With explicit depth and differentiable rendering, it achieves competitive quality with the state of the art NeRF-based methods with 140-1000\u00d7 speed up. Moreover, it allows seamless integration of sensor depths to improve the synthesis quality significantly. With the growing prevalence of depth sensors, our method can contribute to real applications.",
                                "**1. Introduction**",
                                "Given several posed images, novel view synthesis (NVS) aims to generate photorealistic images depicting the scene from unseen viewpoints. This long-standing task has applications in graphics, VR/AR, bringing life to still images. It requires a deep visual understanding of geometry and semantics, making it appealing to test visual understanding.",
                                "Early work on NVS focused on image-based rendering (IBR), where models learn to blend a set of reference images for novel views. They typically construct light field [Ref. 37] or proxy geometry (like mesh surfaces) [Ref. 10, 22, 56, 57] from posed inputs and synthesize target views by resampling or blending warped source views. These methods tend to require many input images that densely cover the scene, and are limited by 3D reconstruction performance. They can have artifacts caused by low-quality geometry estimates, and tend to perform poorly for sparse input images.",
                                "Recently, Neural Radiance Fields (NeRF) [Ref. 46] have become a leading methods for NVS. They use an MLP to implicitly represent the 5D radiance field of the scene, and query the color and density of every sampled point from this network. A volumetric renderer aggregates sampled points along rays to give pixel colors. With many sampled points and a differentiable renderer, they don't need explicit geometry and learn densities optimized for synthesis quality. Despite impressive results, they are not generalizable, fitting a separate MLP for each scene with dense view images. Finally, they are slow since generating images requires querying the learned MLP tremendously many times.",
                                "Following works like PixelNeRF [Ref. 82], IBRNet [Ref. 72] and MVSNeRF [Ref. 7] emerged as generalizable NeRF variants, which generate views of unseen scenes without per-scene optimization by modeling MLPs conditioned on sparse input views. However, they still query the MLP millions of times, leading to slow speed. Thus, despite the progress of accelerating NeRF with per-scene optimization, fast and generalizable NeRF variants are still under-explored.",
                                "Our Example targets a generalizable NVS method with sparse inputs, having both real-time speed and high-quality results. Classical IBR methods possess are fast but require dense input views for good results. Generalizable NeRF variants show excellent quality without per-scene optimization, but require intense computation leading to slow speeds. Our method, termed FWD, achieves this target by Forward Warping features based on estimated Depths.",
                                "Our key insight is that estimating explicit depth for input views can give impressive synthesis results with real-time speed. After estimating depths, we use a differentiable point cloud renderer to project input images to a target view; this gives fast rendering speed and enables end-to-end-training.",
                                "Like prior NVS methods our approach can be trained with RGB data only, but it can be progressively enhanced if noisy sensor depth data is available during training or inference. Depth sensors are becoming more prevalent in consumer devices such as the iPhone 13 Pro and the LG G8 ThinQ, making RGB-D data more accessible than ever. For this reason, we believe that methods making use of RGB-D will become increasingly useful over time.",
                                "Our method estimates depths for each input view to build a point cloud of image features, then synthesizes novel views via a point cloud renderer. We model view-dependent effects with a feature-dependent MLP operating on estimated point clouds. We fuse data from multiple input views with a novel Transformer-based fusion module, then use a refinement module to inpaint missing regions and generate output pixels. The whole model is trained end-to-end to minimize photometric and perceptual losses, learning depth and features optimized for synthesis quality.",
                                "We validate the effectiveness of our method on the ShapeNet and DTU benchmarks, comparing it with representative NeRF-variants and IBR methods. It significantly outperforms existing methods, considering both speed and quality: compared to IBR methods we improve both speed and quality; compared to recent NeRF-based methods we achieve competitive quality at real-time speeds (140-1000\u00d7 speedup). A user study demonstrates that our method gives the most perceptually pleasing results among all methods. Some main contributions are:\n\n\n- - We propose a generalizable and end-to-end NVS method with sparse\n    inputs, which gives high-quality synthesis at a real-time speed. It\n    outperforms existing methods significantly for speed and quality.\n  - We show that explicit depth is sufficient to give fast and\n    high-quality NVS, and is easy to integrate sensor depths for better\n    results. Experiments study performance across different depth\n    availabilities.\n  - We propose a novel Transformer-based fusion module and introduce a\n    view-dependent MLP into point cloud, significantly improving\n    synthesis quality.",
                                "**2. Related Work**",
                                "Novel view synthesis is a long-standing problem in computer vision, allowing for the generation of novel views given several scene images. A variety of 3D representations (both implicit and explicit) have been used for NVS, including depth and multi-plane images [Ref. 68, 87, 66, 54, 6, 61], voxels [Ref. 63, 19], meshes [Ref. 56, 21, 26, 57], point clouds [Ref. 76, 38, 58] and neural scene representations [Ref. 59, 39, 17, 32, 45, 52, 46]. In this Example, we use point clouds for computational and memory efficiency.",
                                "Image-based Rendering. IBR synthesizes novel views from a set of reference images by weighted blending [Ref. 13, 37, 18, 22, 54, 56, 10, 57]. They generally estimate proxy geometry from dense captured images for synthesis. For instance, Riegler et al. [Ref. 56] uses multi-view stereo [Ref. 60, 80, 71, 71, 43, 27] to produce scene mesh surface and warps source view images to target views based on proxy geometry. Despite promising results in some cases, they are essentially limited by the quality of 3D reconstructions, where dense inputs (tens to hundreds) with large overlap and reasonable baselines are necessary for decent results. These methods estimate geometry as an intermediate task not directly optimized for image quality. In contrast, we input sparse views and learn depth jointly to optimize for synthesis quality.",
                                "Neural Scene Representations. Recent work uses implicit scene representations for view synthesis [Ref. 59, 39, 17, 32, 45, 52]. Given many views, neural radiance fields (NeRF) show impressive results [Ref. 46, 85, 44, 53, 75], but require expensive per-scene optimization. Recent methods [Ref. 72, 82, 69, 7, 29] generalize NeRF without per-scene optimization by learning a shared prior, focusing on very sparse input views. However these methods require expensive ray sampling and therefore are very slow. In contrast, we achieve significant speedup (140-1000\u00d7) using explicit representations. Some concurrent work accelerates NeRF by reformulating the computation [Ref. 16], using precomputation [Ref. 81, 25], or adding view dependence to explicit 3D representations [Ref. 39, 77]; unlike ours, these all require dense input views and per-scene optimization.",
                                "Utilizing RGB-D in NVS. The growing availability of annotated depth maps [Ref. 11, 4, 8, 1, 65, 62] facilitates depth utilization in NVS [Ref. 51, 38, 24], which serves as extra supervision or input to networks. Our method utilizes explicit depths as 3D representations, allowing using sensor depths as additional inputs for better quality. Given the increasing popularity of depth sensors, integrating sensor depths is a promising direction for real-world applications. Depth has been used in neural scene representations for speedups [Ref. 48, 67], spaser inputs [Ref. 14] and dynamic scenes [Ref. 78]. However, these works still require per-scene optimization. Utilizing RGB-D inputs to accelerate generalizable NeRF like [Ref. 82, 72] is still an open problem.",
                                "Differentiable Rendering and Refinement. We use advances in differentiable rendering [Ref. 40, 33, 9, 49, 41] to learn 3D end-to-end. Learned geometric representations rely heavily on rendering and refinement [Ref. 83, 79, 2, 73] to quickly synthesize realistic results. Refinement has improved dramatically owing to advances in generative modeling [Ref. 36, 34, 84, 88] and rendering frameworks [Ref. 55, 30, 47, 28]. Although information across viewpoints is typically aggregated before rendering [Ref. 42], we propose to render viewpoints separately and combine using a transformer [Ref. 70, 15, 3], enabling attention across input views.",
                                "Given a sparse set of input images {Ii}i=1N and corresponding camera poses {Ri, Ti}, our goal is to synthesize a novel view with camera pose {Rt, Tt} fast and effectively. The depths {Disen} of Ii captured from sensors are optionally available, which are generally incomplete and noisy.",
                                "The insight of our method is that using explicit depths and forward warping enables real-time rendering speed and tremendous accelerations. Meanwhile, to alleviate quality degradations caused by inaccurate depth estimations, a differentiable renderer and well-designed fusion & refinement modules are required, encouraging the model to learn geometry and features optimized for synthesis quality.",
                                "As illustrated in FIG. 2, with estimated depths, input view Ii is converted to a 3D point cloud Pi containing geometry and view-dependent semantics of the view. A differentiable neural point cloud renderer \u03c0 is used to project point clouds to target viewpoints. Rather than directly aggregating point clouds across views before rendering, we propose a Transformer-based module T fusing rendered results at target view. Finally, a refinement module R is employed to generate final outputs. The whole model is trained end-to-end with photometric and perceptual loss.",
                                "**3.1. Point Cloud Construction**",
                                "Using an explicit 3D representation is a natural way to inject 3D priors into the model. We use point clouds to represent scenes due to their efficiency, compact memory usage, and scalability to complex scenes. For input view we construct point cloud Pi by estimating per-pixel depth Di and per-pixel feature vectors F\u2032i at the same resolution as the input image, then projecting the feature vectors into 3D space using known camera intrinsics. The depth Di is estimated by a depth network d, giving 3D scene structure; features F\u2032i are computed by a spatial feature encoder f and view-dependent MLP \u03c8, representing scene semantics.",
                                "Spatial Feature Encoder f. Scene semantics of input view Ii are mapped to pixel-specific feature vectors Fi by spatial feature encoder f. Each feature vector in Fi is 61-dimensions and is concatenated with RGB channels for 64 dimensions. f is built on BigGAN architecture [Ref. 2].",
                                "Depth Network d. Estimating depth from a single image has scaling/shifting ambiguity, losing valuable multi-view cues and leading to inconsistent estimations across views. Applying multi-view stereo algorithms (MVS) [Ref. 60, 80, 71] solely on sparse inputs is challenging because of limited overlap and huge baselines between input views, leading to inaccurate and low-confidence estimations. Therefore, we employ a hybrid design cascading a U-Net after the MVS module. The U-Net takes image Ii and estimated depths from the MVS module as inputs, refining depths with multiview stereo cues and image cues. PatchmatchNet [Ref. 71] is utilized as the MVS module, which is fast and lightweight.",
                                "Depth Estimation with sensor depths. As stated, U-Net receives an initial depth estimation from the MVS module and outputs a refined depth used to build the point cloud. If sensor depth Disen is available, it is directly input to the U-Net as the initial depth estimations. In this setting, U-Net servers as completion and refinement module taking Disen and Ii as inputs, since Disen is usually noisy and incomplete. During training, loss Ls is employed to encourage the U-Net output to match the sensor depth.",
                                "s=\u2225Mi\u2299Di=Mi\u2514Disen\u2225\u2003\u2003(1)",
                                "where Mi is a binary mask indicating valid sensor depths.",
                                "View-Dependent Feature MLP \u03c8. The appearance of the same object varies across views because of lighting and view direction changes, introducing inconsistency between multiple views. The scene semantics is represented by spatial features Fi and translated to the target view for synthesis. Therefore, view direction changes from input to target views should be inserted into scene semantics to model the view-dependent effects. We design an MLP \u03c8 to compute view-dependent spatial features Fi by taking Fi and relative view changes \u0394v as inputs. For each point in the cloud, \u0394v is calculated based on normalized view directions vi and vt, from the point to camera centers of input view i and target view t. The relative view direction change is calculated as:",
                                "\u0394v=[(vi\u2212vt)/\u2225vi\u2212vt\u2225,vi\u00b7vt],vi,vt\u22083.\u2003\u2003(2)",
                                "and the view-dependent feature F\u2032i is:",
                                "F\u2032i=\u03c8(Fi,\u03b4(\u0394v))\u2003\u2003(3)",
                                "where \u03b4 is a two-layer MLP mapping \u0394v to a 32-dimensions vector and \u03c8 is also a two-layer MLP.",
                                "**3.2. Point Cloud Renderer**",
                                "After constructing a point cloud Pi from view i containing the geometry and semantics of the scene, we expect to view Pi at target views for synthesis. We use a neural point cloud \u03c0 to achieve it. Pi is first transformed to target view coordinates based on relative camera poses and then rendered by \u03c0. The rendered results \u02dcFi are spatial feature maps, sharing the same dimension as feature F\u2032i in Pi at each pixel. With explicit geometry transformation and renderer satisfying geometry rules, our rendered results are geometrically consistent and correct across views.",
                                "As discussed, we expect this renderer to be fast and differentiable for speed and quality. We use the highly efficient renderer in Pytorch3D [Ref. 55], which splats points to a region and accumulates them based on blending weights for each pixel. The weight is computed based on depth, distance to rays cast from the pixel, and sphere radius.",
                                "This fully differentiable renderer allows our model to be trained end-to-end, where photometric and perceptual loss gradients can be propagated to points' position and features. In this way, the model learns to estimate depths and features optimized for synthesis quality, leading to superior quality. We show the effectiveness of it in experiments.",
                                "**3.3. Fusion and Refinement**",
                                "We fuse the point clouds constructed from each input view for synthesis. A naive fusion translates each point cloud to target view coordinates and aggregates them into a large one for rendering. Despite high efficiency, it is vulnerable to inaccurate depths since points with wrong depths may occlude points from other views, leading to degraded performance. Some regularizers like PointNet may be feasible to apply on the aggregated point cloud for refinement, but they are not efficient when point number is large.",
                                "Therefore, we instead render each point cloud individually at the target viewpoint and fuse the rendered results. We propose a novel Transformer-based fusion module T to fuse arbitrary size inputs and use a refinement module R to synthesize final results based on fused features.",
                                "Fusion Transformer T. Given a set of feature maps {{tilde over (F)}i} rendered from point clouds, we fuse them into one feature map, which will be decoded into an RGB image by refinement module. The fusion operation should understand scene semantics, be fast and support arbitrary number of inputs in any order. We take advantage of progress on the Transformer and propose a pixel-wise Transformer T for fusion, which is detailed in FIG. 3. T extracts feature vectors from {{tilde over (F)}i} as inputs and output fused one at each pixel. It applies standard multi-head attention to the sequence of feature vectors and queries using an extra learnable \u201ctoken\u201d. Applied on features, T utilizes scene semantics for fusion.",
                                "Fusing rendered feature maps may lose some helpful geometry information since it projects 3D to 2D. For instance, point clouds from different views may have meaningful occlusion relationships based on their relative depths. Also, rendered feature maps from views closer to target views are more robust and important than ones from distant views. To this end, we render depths at target view for each point cloud and compute the relative view changes from input to target views. We concatenate them as geometry features and use them as position encoding of the Transformer.",
                                "Refinement Module R. Refinement module R decodes fused feature maps {tilde over (F)} to RGB images \u0128 at target view. It will inpaint regions not visible in the input views in a semantically meaningful and geometrically accurate manner. Also, it corrects local errors caused by inaccurate depths and improves perceptual quality based on semantics contained by feature maps, leading to coherent and high-quality synthesis. R is built with 8 ResNet [Ref. 20] blocks.",
                                "**3.4. Training and Implementation Details**",
                                "Our model is trained end-to-end with photometric l2 and perceptual c losses between generated and ground-truth target images. The whole loss function is:",
                                "=\u03bbll+\u03bbcc\u2003\u2003(4)",
                                "where \u03bbl2=5:0; \u03bbc=1:0. The model is trained end-to-end on 4 2080Ti GPUs for 2 days, using Adam [Ref. 35] with learning rate 10\u22124 and \u03b21=0.9; \u03b22=0:999. When sensors depths are available as inputs, 8 is used with \u03bbs=5:0.",
                                "**4. Experiments**",
                                "The goal of our Example is real-time and generalizable novel view synthesis with sparse inputs, which can optionally use sensor depths. To this end, our experiments aim to identify the speed and quality at which our method can synthesize novel images and explore the advantage of explicit depths. We evaluate our methods on ShapeNet [Ref. 5] and DTU [Ref. 31] datasets, comparing results with the state of the art methods and alternative approaches. Experiments take place with held-out test scenes and no per-scene optimization. We conduct ablations to validate the effectiveness of designs.",
                                "Metrics. To measure image quality, we report the standard image quality metrics PSNR and SSIM [Ref. 74]. We also report LPIPS [Ref. 86], and find this metric best reflects the image quality as perceived by humans. Visual quality is also evaluated by conducting A/B testing, in which workers select the image most similar to the ground truth from competing methods. Inference speeds are measured in frames per second (FPS). All evaluations are conducted using the same protocol (same inputs and outputs), and rendering speed are measured on the same platform (1 2080Ti GPU with 4 CPU cores).",
                                "Model Variants. We evaluate three models with various accessibility to depths for training and inference, as defined in Table 1.",
                                "FWD utilizes PatchmatchNet [Ref. 71] as the MVS module for depth estimations in complex scenes. PatchmatchNet is initialized from officially pre-trained weights on DTU dataset and updated during end-to-end training with photometric and perceptual loss. FWD-U learns depth estimations in an Unsupervised manner from scratch, sharing the same model and settings as FWD while PatchmatchNet is randomly initialized without any pretraining. FWD-D takes sensor depths as additional inputs during both training and inference. It doesn't use any MVS module since sensor depths provide abundant geometry cues.",
                                "PatchmatchNet is pre-trained following typical MVS settings, which has distinct domain gaps between our settings since our input images share larger view change. We show the estimated depths from pre-trained PatchmatchNet in FIG. 9, which are inaccurate and relatively incomplete after filtering low-confidence regions.",
                                "**4.1. ShapeNet Benchmarks**",
                                "We first evaluate our approach on the category-agnostic view synthesis task on ShapeNet. Following the setting of [Ref. 82], we train and evaluate a single model on 13 ShapeNet categories. Each instance contains 24 fixed views of 64\u00d764 resolution. During training, one random view is selected as input and the rest are served as target views. For testing, we synthesize all other views from a fixed informative view. The model is finetuned with two random input views for 2-view experiments. We find that U-Net is sufficient for good results on this synthetic dataset without the MVS module.",
                                "We show qualitative comparisons to PixelNeRF in FIG. 4, where FWD-U gets noticeably superior results. Our synthesized results are more realistic and closely matching to target views, while PixelNeRF's results tend to be blurry. We observe the same trend in the DTU benchmark and evaluate the visual quality quantitatively there.",
                                "We show quantitative results in Table 2, adding SRN [Ref. 64] and DVR [Ref. 50] as other baselines. Our method outperforms others significantly for LPIPS, indicating a much better perceptual quality, as corroborated by qualitative results. PixelNeRF has a slightly better PSNR while its results are blurry. Most importantly, FWD-U runs at a speed of over 300 FPS, which is 300\u00d7faster than PixelNeRF.",
                                "**4.2. DTU MVS Benchmarks**",
                                "We further evaluate models on DTU MVS dataset [Ref. 31], which is a real scene dataset consisting of 103 scenes. Each scene contains one or multiple objects placed on a table, while images and incomplete depths are collected by the camera and structured light scanner mounted on an industrial robot arm. Corresponding camera poses are provided.",
                                "As stated in [Ref. 82], this dataset is challenging since it consists of complex real scenes without apparent semantic similarities across scenes. Also, images are taken under varying lighting conditions with distinct color inconsistencies between views. Moreover, with only under 100 scenes available for training, it is prone to overfitting in training.",
                                "We follow the same training and evaluation pipelines as PixelNeRF [Ref. 82] for all methods to give a fair comparison. The data consists of 88 training and 15 test scenes, between which there are no shared or highly similar scenes. Images are down-sampled to a resolution of 300\u00d7400. For training, three input views are randomly sampled, with the rest as target views. For inference, we choose three fixed informative input views and synthesize other views of the scene.",
                                "Baselines. We evaluate a set of representatives of generalizable NeRF and IBR methods in two different scenarios: with RGB or RGB-D available as inputs during inference.",
                                "PixelNeRF [Ref. 82], IBRNet [Ref. 72] and MVSNeRF [Ref. 7] are the state-of-the-art generalizable NeRF variants, taking only RGB as inputs. We use the official PixelNeRF model trained on DTU MVS dataset and carefully retrain the IBRNet and MVSNeRF with the same 3-input-view settings. We also evaluate PixelNeRF-DS, which is PixelNeRF [Ref. 82] supervised with depths as reported in [Ref. 14]. Please note that IBRNet and MVSNeRF use different evaluations in their paper, where IBRNet uses 10 views for synthesis and MVSNeRF selects different input views for each target view.",
                                "A series of IBR methods are also evaluated. Since COLMAP [Ref. 60] fails to give reasonable outputs with sparse input images, methods using COLMAP like FVS [Ref. 56], DeepBlending [Ref. 23] cannot estimate scene geometry in this setting. For these methods, we use depths captured by sensors as estimated depths, which should give upper-bound performance of these methods. To better cope with missing regions, we add our refinement model to DeepBlending [Ref. 23] and retrain it on DTU dataset, termed Blending-R.",
                                "Qualitative Results. Synthesis results are shown in FIG. 5, where high-quality and geometrically correct novel views are synthesized in real-time (over 35 FPS) under significant viewpoint changes. Our refinement model faithfully inpaints invisible regions; also, synthesized images have good shadows, light reflection, and varying appearance across views, showing the efficacy of feature-dependent MLP. With sensor depths, results can be further improved.",
                                "We show comparisons to baselines in FIG. 6. FWD provides noticeably better results than baselines across different depth settings. For models without depths in test, baselines IBRNet and PixelNeRF become blurry in areas of high detail such as the buildings in the top row, while our FWD-U and FWD give more realistic and sharper images. With sensor depths in test, baseline Blending-R produces more cogent outputs, but still struggles to distinguish objects from the background, such as in the middle row, while FWD-D gives faithfully synthesis and clear boundaries.",
                                "Quantitative Results. We first evaluate synthesis quality by user study. We fellow a standard NB paradigm in which workers choose between competing methods the closest to a ground truth image. Workers are monitored using a qualifier and sentinel examples. All views in the test set (690 in total) are evaluated, and each view is judged by three workers.",
                                "User study results are consistent with qualitative observations, as shown in FIG. 7. Among all baselines with and without depths during test, users choose FWD as more closely matching ground truth images than others most of the time. For instance, FWD-U is selected over PixelNeRF in 65.6% of examples, and 77.8% compared to IBRNet. Also, over 90% workers prefer FWD-D to FWD or FWD-U, showing a huge advantage of integrating sensor depths.",
                                "We show automated view synthesis metrics and speed in Table 3.",
                                "Across all three depth availability settings, FWD is competitive with the state of the art baselines while significantly faster. FWD-D runs in real-time and gives substantially better image quality than others. FWD has competitive metrics to PixelNeRF-DS while 1000\u00d7faster. Notably, NeRF variants such as PixelNeRF, IBRNet, MVSNeRF, and PixelNeRF-DS are two orders of magnitude slower.",
                                "The exception to highly competitive performance is weaker PSNR and SSIM of our unsupervised FWD-U against PixelNeRF and IBRNet. However, FWD-U has better perceptual quality since it has the best LPIPS, and human raters prefer it to other methods in NB tests. FIG. 6 also illustrates the disparity between comparisons using PSNR and LPIPS. Meanwhile, FWD-U is above 1000\u00d7 faster than PixelNeRF and above 100\u00d7 faster than IBRNet. In our method, depth estimations, rendering and even CNN would introduce tiny pixel shiftings, which harm the PSNR. PixelNeRF and IBRNet synthesize every pixel independently supervised by L2 loss, leading to blurred results.",
                                "Among all methods without test depths, FWD has the best perceptual quality, LPIPS, and second-best PSNR and SSIM. Although it uses a pretrained MVS module, we think this comparison is still reasonable since pretrained depth module is easy to get. Also, training depths can be easily calculated from training images since they are dense.",
                                "Baseline comparisons also show that IBR methods are fast, but do not give images that are competitive with our method. Our method outperforms them in both perceptual quality and standard metrics, showing the efficacy of proposed methods. We also compare FWD-U with SynSin [Ref. 76] which only receives a single input image, showing the benefits of using multi-view inputs in NVS.",
                                "**4.3. Ablations and Analysis**",
                                "We evaluate the effectiveness of our designs and study depth in more detail through ablation experiments.",
                                "Effects of Fusion Transformer. We design a model without Transformer which concatenates point clouds across views into a bigger one for later rendering and refinement. Its results in FWD-U settings are shown in FIG. 8. The ablated version cannot deal with inaccurate depths learned in unsupervised manner and synthesize \u201cghost objects\u201d since points with bad depths occlude other views' points.",
                                "We repeat this ablation for FWD-D in Table 4 which should give much better depth estimations with sensor depths inputs. The ablated model has notably worse results for all metrics. Ablation results indicate that the proposed method is powerful to tackle inaccurate depth estimations and fuse semantic features across views as well.",
                                "Effects of View Dependent MLP. For ablation, we remove the view-dependent feature MLP and report its results in Table 4.",
                                "Removing this module reduces model's ability to produce view-dependent appearance, leading to worse performance for all metrics.",
                                "Depth Analysis and Ablations. We visualize depths in FIG. 9. Estimating depth from sparse inputs is challenging and less accurate because of huge baselines between inputs. We show estimated depths by pretrained PatchmatchNet, which are filtered based on the confidence scores. Therefore, propagating multi-view geometry cues to the whole image by refinement is important in our model. Our end-to-end model learns it by synthesis losses.",
                                "We ablate the depth network in Table 5, and report the difference between estimated and sensor depths as errors. MVS module is important (row 2), providing geometrically consistent depths and resolving scale/shift ambiguity. U-Net further improves the synthesis quality (row 3) and refines depths. PatchmatchNet has a shallow refinement layer in its model, giving decent refinements without our U-Net. Learning unsupervised depth estimations and view synthesis jointly from scratch is challenging (row 4).",
                                "Training depth network without supervision [Ref. 12] first may give a good initialization for jointly training. We hypothesize that consistency between depths may be more important than errors, given that they are not perfectly aligned to quality."
                            ],
                            "subsections": [],
                            "outline_long": [
                                "introduce novel view synthesis (NVS) task",
                                "motivate NVS applications",
                                "describe limitations of image-based rendering (IBR)",
                                "describe limitations of Neural Radiance Fields (NeRF)",
                                "introduce FWD-U method",
                                "highlight advantages of FWD-U",
                                "describe explicit depth estimation",
                                "describe differentiable point cloud renderer",
                                "describe end-to-end training",
                                "describe sensor depth integration",
                                "describe growing prevalence of depth sensors",
                                "describe method overview",
                                "describe point cloud construction",
                                "describe feature-dependent MLP",
                                "describe Transformer-based fusion module",
                                "describe refinement module",
                                "describe training objectives",
                                "describe validation on benchmarks",
                                "describe user study",
                                "describe main contributions",
                                "introduce related work",
                                "describe image-based rendering methods",
                                "describe neural scene representations",
                                "describe utilizing RGB-D in NVS",
                                "describe differentiable rendering and refinement",
                                "describe problem formulation",
                                "describe point cloud construction",
                                "describe spatial feature encoder",
                                "describe depth network",
                                "describe depth estimation with sensor depths",
                                "describe view-dependent feature MLP",
                                "describe point cloud renderer",
                                "describe fusion and refinement",
                                "motivate novel view synthesis",
                                "limitations of rendered feature maps",
                                "introduce refinement module R",
                                "describe architecture of R",
                                "introduce training and implementation details",
                                "define loss function",
                                "specify training settings",
                                "introduce experiments",
                                "state goal of experiments",
                                "describe evaluation metrics",
                                "introduce model variants",
                                "describe FWD model",
                                "describe FWD-U model",
                                "describe FWD-D model",
                                "introduce ShapeNet benchmarks",
                                "describe evaluation protocol",
                                "show qualitative comparisons",
                                "show quantitative results",
                                "introduce DTU MVS benchmarks",
                                "describe evaluation protocol",
                                "show qualitative results",
                                "show quantitative results",
                                "introduce ablation experiments",
                                "evaluate effectiveness of fusion transformer",
                                "evaluate effectiveness of view-dependent MLP",
                                "analyze depth estimation"
                            ],
                            "num_characters": 30590,
                            "outline_medium": [
                                "introduce novel view synthesis (NVS) task",
                                "motivate NVS applications",
                                "describe limitations of image-based rendering (IBR)",
                                "describe limitations of Neural Radiance Fields (NeRF)",
                                "introduce FWD-U method",
                                "describe key insight of FWD-U",
                                "describe FWD-U architecture",
                                "describe depth estimation module",
                                "describe point cloud construction",
                                "describe differentiable point cloud renderer",
                                "describe fusion module",
                                "describe refinement module",
                                "describe training losses",
                                "describe experimental setup",
                                "describe results",
                                "conclude FWD-U method",
                                "motivate novel view synthesis",
                                "introduce refinement module R",
                                "describe training and implementation details",
                                "introduce experiments",
                                "define metrics",
                                "describe model variants",
                                "evaluate on ShapeNet benchmarks",
                                "evaluate on DTU MVS benchmarks",
                                "show qualitative results",
                                "show quantitative results",
                                "evaluate baselines",
                                "perform ablation study",
                                "analyze depth estimation"
                            ],
                            "outline_short": [
                                "introduce novel view synthesis task",
                                "motivate fast and generalizable NVS method",
                                "describe FWD-U method with sparse inputs",
                                "highlight advantages of FWD-U over NeRF and IBR",
                                "outline key components of FWD-U method",
                                "describe point cloud construction and depth estimation",
                                "explain point cloud renderer and fusion module",
                                "summarize contributions of FWD-U method",
                                "motivate novel view synthesis",
                                "describe refinement module R",
                                "outline training and implementation details",
                                "summarize experiments and evaluation metrics",
                                "describe model variants and ablation studies",
                                "analyze results and discuss limitations"
                            ]
                        }
                    ],
                    "outline_long": [
                        "define scope of invention",
                        "introduce terminology",
                        "describe prior art limitations",
                        "introduce novel view synthesis (NVS) with sparse inputs",
                        "describe advantages of NVS",
                        "introduce Forward Warping features based on estimated Depths (FWD)",
                        "describe estimation of explicit depth for input views",
                        "introduce differentiable point cloud renderer",
                        "describe fast rendering speed and end-to-end training",
                        "introduce training with RGB data only",
                        "describe progressive enhancement with noisy sensor depth data",
                        "introduce flowchart of non-limiting example steps for NVS",
                        "access or acquire input view or multiple input views",
                        "estimate depths for each input image",
                        "construct point cloud of image features",
                        "synthesize novel views using point cloud renderer",
                        "model view-dependent effects",
                        "fuse multiple synthesized views",
                        "in-paint missing regions",
                        "generate output pixels",
                        "describe sparse set of input images and corresponding camera poses",
                        "synthesize novel view with camera pose",
                        "estimate depths of input images",
                        "describe view-dependent semantics",
                        "introduce differentiable neural point cloud renderer",
                        "fuse rendered results at target view",
                        "employ refinement module",
                        "train end-to-end with photometric and perceptual loss",
                        "describe point cloud construction",
                        "estimate per-pixel depth and per-pixel feature vectors",
                        "project feature vectors into 3D space",
                        "describe spatial feature encoder",
                        "map scene semantics to pixel-specific feature vectors",
                        "describe depth network",
                        "estimate depth from single image",
                        "address scaling/shifting ambiguity",
                        "apply conventional multi-view stereo algorithms",
                        "cascade U-Net after MVS module",
                        "refine depths with multiview stereo cues and image cues",
                        "describe sensor depths",
                        "input sensor depth to U-Net",
                        "complete and refine depth estimation",
                        "describe view-dependent feature MLP",
                        "compute view-dependent spatial features",
                        "translate scene semantics to target view",
                        "describe point cloud renderer",
                        "render point cloud at target view",
                        "describe Transformer-based fusion module",
                        "fuse feature maps from point clouds",
                        "decode fused feature maps to RGB images"
                    ],
                    "num_characters": 25134,
                    "outline_medium": [
                        "define scope of invention",
                        "introduce terminology",
                        "describe prior art limitations",
                        "introduce novel view synthesis (NVS) with sparse inputs",
                        "describe advantages of NVS",
                        "introduce Forward Warping features based on estimated Depths (FWD)",
                        "describe estimating explicit depth for input views",
                        "introduce differentiable point cloud renderer",
                        "describe training with RGB data only",
                        "describe training with noisy sensor depth data",
                        "introduce flowchart of non-limiting example steps for NVS",
                        "access or acquire input views",
                        "estimate depths for each input image",
                        "construct point cloud of image features",
                        "synthesize novel views using point cloud renderer",
                        "model view-dependent effects",
                        "fuse multiple synthesized views",
                        "in-paint missing regions",
                        "describe sparse set of input images and camera poses",
                        "estimate depths using a U-Net",
                        "describe view-dependent feature MLP",
                        "introduce point cloud renderer",
                        "describe fusion Transformer module",
                        "describe refinement module",
                        "describe training of a model end-to-end with photometric and perceptual losses"
                    ],
                    "outline_short": [
                        "define scope of invention",
                        "introduce novel view synthesis (NVS) with sparse inputs",
                        "describe limitations of previous methods",
                        "introduce Forward Warping features based on estimated Depths (FWD) for high quality, fast results",
                        "describe point cloud construction using estimated depths and feature vectors",
                        "explain differentiable neural point cloud renderer for fast rendering speed",
                        "describe Transformer-based fusion module for fusing rendered results",
                        "introduce refinement module for generating final outputs",
                        "describe training of model end-to-end with photometric and perceptual loss",
                        "illustrate system for automatically performing novel view synthesis",
                        "describe hardware implementation of image source, computing device, and server",
                        "provide example of hardware components for image source, computing device, and server"
                    ]
                },
                {
                    "title": "5. CONCLUSION",
                    "paragraphs": [
                        "We propose a real-time and generalizable method for NVS with sparse inputs by using explicit depths. Our experiments show that estimating depths can give impressive results with a real-time speed, outperforming existing methods. Moreover, the proposed method could utilize sensor depths seamlessly and improve synthesis quality significantly. With the increasing availability of mobile depth sensors, we believe our method has exciting real-world 3D applications."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "summarize method advantages"
                    ],
                    "num_characters": 463,
                    "outline_medium": [
                        "summarize method advantages"
                    ],
                    "outline_short": [
                        "summarize method advantages"
                    ]
                }
            ],
            "outline_long": [],
            "num_characters": 0,
            "outline_medium": [],
            "outline_short": []
        }
    ],
    "claims": [
        "1. A method for novel view synthesis, the method comprising:\naccessing at least a first input image with a first view of a subject in the first input image, and a second input image with a second view of the subject in the second input image using a computer system;\nestimating depths for pixels in the at least first and second input images;\nconstructing a point cloud of image features from the estimated depths; and\nsynthesizing a novel view by forward warping by using a point cloud rendering of the constructed point cloud.",
        "2. The method of claim 1, further comprising modeling view-dependent effects from the synthesized novel view.",
        "3. The method of claim 2, wherein view-dependent effects include missing pixel data in the synthesized novel view.",
        "4. The method of claim 2, further comprising generating fused data by fusing the at least first input image and the second input image.",
        "5. The method of claim 4, wherein generating fused data includes using a fusion Transformer T, and rendering a set of feature maps {{tilde over (F)}i} from the point cloud and fused into a feature map.",
        "6. The method of claim 5, wherein the feature map is decoded into an RGB image by a refinement module.",
        "7. The method of claim 5, wherein the fusion Transformer T extracts feature vectors from {{tilde over (F)}i} as inputs and output a fused one at each pixel.",
        "8. The method of claim 4, further comprising generating output pixels for the synthesized novel view by inpainting missing pixel data based on the fused data.",
        "9. The method of claim 1, wherein the synthesized novel view includes a viewpoint of the subject different from that at least first input and second input image",
        "10. The method of claim 1, wherein the computer system is further configured to access a plurality of input images.",
        "11. A system for novel view synthesis, the system comprising:\na computer system configured to:\ni) access at least a first input image with a first view of a subject in the first input image, and a second input image with a second view of the subject in the second input image;\nii) estimate depths for pixels in the at least first and second input images;\niii) construct a point cloud of image features from the estimated depths; and\niv) synthesize a novel view by forward warping by using a point cloud rendering of the constructed point cloud.",
        "12. The system of claim 11, wherein the computer system is further configured to model view-dependent effects from the synthesized novel view.",
        "13. The system of claim 12, wherein view-dependent effects include missing pixel data in the synthesized novel view.",
        "14. The system of claim 12, wherein the computer system is further configured to generate fused data by fusing the at least first input image and the second input image.",
        "15. The system of claim 14, wherein the computer system is further configured to generate fused data using a fusion Transformer T, and rendering a set of feature maps {{tilde over (F)}i} from the point cloud and fused into a feature map.",
        "16. The system of claim 15, wherein the computer system is further configured to decode the feature map into an RGB image using a refinement module.",
        "17. The system of claim 15, wherein the computer system is further configured to extract feature vectors from as inputs and output a fused one at each pixel using the fusion Transformer T.",
        "18. The system of claim 14, wherein the computer system is further configured to generate output pixels for the synthesized novel view by inpainting missing pixel data based on the fused data.",
        "19. The system of claim 11, wherein the synthesized novel view includes a viewpoint of the subject different from that at least first input and second input image",
        "20. The system of claim 11, wherein the computer system is further configured to access a plurality of input images."
    ]
}