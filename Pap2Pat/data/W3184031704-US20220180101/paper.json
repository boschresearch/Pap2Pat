{
    "id": "https://semopenalex.org/work/W3184031704",
    "authors": [
        "Adrien Gaidon",
        "Juan Carlos Niebles",
        "Kuan\u2010Hui Lee",
        "Ehsan Adeli",
        "Nishant Rai"
    ],
    "title": "CoCon: Cooperative-Contrastive Learning",
    "date": "2021-06-01",
    "abstract": "Labeling videos at scale is impractical. Consequently, self-supervised visual representation learning is key for efficient video analysis. Recent success in learning image representations suggest contrastive learning is a promising framework to tackle this challenge. However, when applied to real-world videos, contrastive learning may unknowingly lead to separation of instances that contain semantically similar events. In our work, we introduce a cooperative variant of contrastive learning to utilize complementary information across views and address this issue. We use data-driven sampling to leverage implicit relationships between multiple input video views, whether observed (e.g. RGB) or inferred (e.g. flow, segmentation masks, poses). We are one of the firsts to explore exploiting inter-instance relationships to drive learning. We experimentally evaluate our representations on the downstream task of action recognition. Our method achieves competitive performance on standard benchmarks (UCF101, HMDB51, Kinetics400). Furthermore, qualitative experiments illustrate that our models can capture higher-order class relationships. The code is available at http://github.com/nishantrai18/CoCon.",
    "sections": [
        {
            "title": "Keypoint Embeddings Flow Embeddings",
            "paragraphs": [
                "Figure 1: Given a pair of instances (e.g. people doing squats) and corresponding multiple views, features are computed using viewspecific deep encoders f 's. Different instances may have contrasting similarities in different views. For instance, V0 (left) and V1 (right) have similar optical-flow o = f f low and pose keypoints (keypoint) p = f keypoint features but their image i = f rgb features are far apart. CoCon leverages these inconsistencies by encouraging the distances in all views to become similar. High similarity of o0, o1 and p0, p1 nudges i0, i1 towards each other in the RGB space."
            ],
            "subsections": []
        },
        {
            "title": "Introduction",
            "paragraphs": [
                "There has recently been a surge in interest for approaches utilizing self-supervised methods for visual representation learning. Recent advances in visual representation learning have demonstrated impressive performance compared to their supervised counterparts [3,14]. Fresh development in the video domain have attempted to make similar improvements [10,16,25,35].",
                "Videos are a rich source for self-supervision, due to the inherent temporal consistency in neighboring frames. A natural approach to exploit this temporal structure is pre-dicting future context as done in [10,16,25,27]. Such approaches perform future prediction in mainly two ways: (1) predicting a reconstruction of future frames [25,27,39], (2) predicting features representing the future frames [10,16]. If the goal is learning high-level semantic features for other downstream tasks, then complete reconstruction of frames is unnecessary. Inspired by developments in language modelling [29], recent work [41] propose losses that only focus on the latent embedding using frame-level context. One of the more recent approaches [10] propose utilizing spatiotemporal context to learn meaningful representations. Even though such developments have led to improved performance, the quality of the learned features is still lagging behind that of their supervised counterparts.",
                "Due to the lack of labels in self-supervised settings, it is impossible to make direct associations between different training instances. Instead, prior work has learned associations based on structure, either in the form of temporal [10,20,23,26,44] or spatial proximity [10,18,20,30] of patches extracted from training images or videos. However, the contrastive losses utilized enforce similarity constraints between instances from same videos while pushing instances from other videos far away even if they represent the same semantic content. This inherent drawback forces learning of features with limited semantic knowledge and encourage performing low-level discrimination between different videos. Recent approaches suffer from this restriction leading to poor representations.",
                "The idea of utilizing multiple views of information has been a well-established one with roots in human perception [4,15]. It's argued that useful higher order semantics are present throughout different views and are consistent across them. At the same time, different views provide complementary information which can be utilized to aid learning in other views. Multi-view learning has been a popular direction [35,40] utilizing these traits to improve representation quality. Recent approaches learn features utilizing multiple views with the motivation that information shared across views has valuable semantic meaning. A majority of these approaches directly utilize core ideas such as contrastive learning [31] and mutual information maximization [2,24,46]. Although the fusion of views leads to improved representations, such approaches also utilize contrastive losses, consequently suffering from the same drawback of low-level discrimination between similar instances.",
                "We propose Cooperative Contrastive Learning (CoCon), which overcomes this shortcoming and leads to improved visual representations. Our main motivation is that each view sees a specific pattern, which can be useful to guide other views and improve representations. Our approach utilizes inter-view information to avoid the drawback of discriminating similar instances discussed earlier. To this end, each view sees a different aspect of the videos, allowing it to suggest potentially similar instances to other views. This allows us to infer implicit relationships between instances in a self-supervised multi-view setting, something which we are the first to explore. These associations are then used in order to learn better representations for downstream applications such as video classification and action recognition. Fig. 1 shows an overview of CoCon. It is worth noting that although CoCon utilizes building blocks currently used in self-supervised representation learning, it is applicable to other tasks utilizing contrastive learning and be used in conjunction with other recently proposed methods.",
                "We use 'freely' available views of the input such as RGB frames and Optical Flow. We also explore the benefit of using high-level inferred semantics as additional noisy views, such as human pose keypoints and segmentation masks generated using off-the-shelf models [45]. These views are not independent, as they can be derived from the original input images. However, they are complementary and lead to significant gains, demonstrating CoCon's effectiveness even with noisy related views. The extensible nature of our framework and the 'freely' available views used make it possible to use CoCon with any publicly available video dataset and other contrastive learning approaches."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Self-supervised Learning from images Recent approaches have tackled image representation learning by exploiting color information [22,47] and spatial relationships [30,34], where relative positions between image patches are exploited as supervisory signals. Several approaches apply self-supervision to super-resolution [6,19] or even to multitask [5] and cross-domain [33] learning frameworks.",
                "Self-supervised Learning from videos Multiple approaches [10,16,25,27,39] perform self-supervision through 'predicting' future frames. However, the term 'predicting' is overloaded, as they do not directly predict and reconstruct frames but instead operate on latent representations. This ignores stochasticity of frame appearance, e.g., illumination changes, camera motion, appearance changes due to reflections and so on, allowing the model to focus on higher-order semantic features. Recent work [10,40] utilize Noise Contrastive Estimation to perform prediction of the latent representations rather than the exact future frames, vastly improving performance. Yet, another class of proxy tasks are based on temporal ordering of frames [28,44]. Temporal coherence [17,43] and 3D puzzle [20] were used as proxy loss to exploit spatio/temporal structures.",
                "Multi-view learning Multiple views of videos are rich sources of information for self-supervised learning [35,40,42]. Two stream networks for action recognition [37] have led to many competitive approaches, which demonstrate using even derivable views such as optical flow helps improve performance considerably. There have been approaches [26,35,40,42] utilizing diverse views, sometimes derivable from one other, to learn better representations. However, these approaches utilize inter-view links by maximizing mutual information between them. Although this leads to improved performance, we believe the rich inter-view linkages can be utilized more effectively by utilizing them to uncover implicit relationships between instances.",
                "Multi-View Self-supervised learning Multiple recent approaches [1,11,12,32] have tackled the challenge of multi-modal self-supervised learning achieving impressive performance. However, these approaches suffer from the same drawback of discriminating between similar instances, leaving potential to benefit from inter-sample relationships.",
                "Most approaches above perform self-supervision using positive and negative pairs mined through structural constraints, e.g., temporal and spatial proximity. Although this results in representations that capture some degree of semantic information, it incorrectly leads to treating similar actions differently due to the inherent nature of their pairmining. For instance, clip pairs in different videos are considered negatives, even if they represent the same action. We argue that utilizing different views and inter-instance relationships to propose positive pairs to aid training can lead to improvement of all views simultaneously."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [
                "We describe cooperative contrastive learning (CoCon) and intuition behind our designs in this section. Additional details regarding architecture and implementation are present in the appendix. In the following sections, we build our framework borrowing the learning framework present in [10] which learns video representations through spatiotemporal contrastive losses. It should be noted that even though we use this particular self-supervised backbone in our experiments, our approach is not restricted by the choice of the underlying self-supervised task. CoCon can be used in conjunction with any other frameworks currently present and allow them to be extended to a multi-view setting.",
                "A video V is a sequence of T frames (not necessarily RGB images) with resolution H \u00d7 W and C channels, {i 1 , i 2 , . . . , i T }, where i t \u2208 R H\u00d7W \u00d7C . Assume T = N * K, where N is the number of blocks and K denotes the number of frames per block. We partition a video clip V into N disjoint blocks V = {x 1 , x 2 , . . . , x N }, where x j \u2208 R K\u00d7H\u00d7W \u00d7C and a non-linear encoder f (.) transforms each input block x j into its latent representation z j = f (x j ). An aggregation function, g(.) takes a sequence {z 1 , z 2 , . . . , z j } as input and generates a context representation c j = g(z 1 , z 2 , . . . , z j ). In our setup, z j \u2208 R H \u00d7W \u00d7D and c j \u2208 R D . D represents the embedding size and H , W represent down-sampled resolutions as different regions in z j represent features for different spatial locations. We define z j = P ool(z j ) where z j \u2208 R D and c = F (V ) where F (.) = g(f (.)).",
                "Similar to [10], we create a prediction task involving predicting z of future blocks. Details are provided in the appendix. For multiple views, we define c v = F v (V v ), where V v , c v and F v represent the input, context feature and composite encoder for view v respectively. Contrastive Loss Noise Contrastive Estimation (NCE) [9,29,31] constructs a binary classification task where a classifier is fed with real and noisy samples with the training objective being distinguishing them. Similar to [10,31], we use an NCE loss over our feature embeddings described in Eq 1. z i,k represents the feature embedding for the i th timestep and the k th spatial location. Recall z j \u2208 R H \u00d7W \u00d7D which preserves the spatial layout. We normalize z i,k to lie on the unit hypersphere. Eq 1 is a cross-entropy loss distinguishing one positive pair from all the negative pairs present in a video. We use temperature \u03c4 = 0.005 in our experiments. In a batch setting with multiple video clips, it is possible to have more inter-clip negative pairs.",
                "To extend this to multiple views, we utilize different encoders \u03c6 v for each view v. We train these encoders by utilizing L cpc for each of them independently, giving us,",
                "Cooperative Multi-View Learning Recent approaches [12,35,40] tackle multi-view self-supervised learning by maximizing mutual information across views. They involve using positive and negative pairs generated using structural constraints, e.g., spatio-temporal proximity in videos [10,11,35,40]. Although such representations capture semantic content, they unintentionally encourage discriminating video clips containing semantically similar content due to the inherent nature of pair generation, i.e. video clips from different videos are negatives. We utilize interinstance relationships to alleviate some of these issues. We soften this constraint by indirectly deriving pair proposals using different views. Such a co-operative scheme benefits all models as each individual view gradually improves. Better models are able to generate better proposals, improving performance of all views creating a positive feedback loop. Our belief is that significant semantic features should be universal across views, therefore, potential incorrect proposals from one view should cancel out through proposals from other views.",
                "We achieve the above by computing view-specific distances and synchronizing them across all views. We enforce a consistency loss between distances from each view. Looking at it from another perspective, we are encouraging relationships between instances to be the same across views i.e. similar pairs in one view should be a similar pair in  other views as well. Treating this as inter-view graph regularization, we create a graph similarity matrix W v of size K \u00d7 K, using some distance metric. We represent our distance metric by D(.). In our experiments, we use the cosine distance which translates to W v ab = z z \u2022 z b . Assume h a v denotes the representation for the v th view of instance a. In our experiments, we use h = z giving us block level features. Our resultant loss becomes the inconsistency between similarity matrices across views. The resultant graph regularization loss becomes v0,v1 W v0 -W v1 which is simplified in Eq 2.",
                "Building on top of our earlier intuition, in order to have sensible proposals, we need to have discriminative scores, i.e. we should have both positive (D \u2192 0) and negative (D \u2192 1) pairs. To promote well distributed distances, we utilize the hinge loss described in Eq 3.",
                "L sim is the hinge loss, where the first term pushes representations of the same instance in different views closer; while the second term pushes different instances apart. Since the number of structural negative pairs are much larger than the positives, we introduce \u00b5 in order to balance the loss weights. We choose \u00b5 such that the first and second components contribute equally to the loss.",
                "Note that L sim entangles different views together. An alternative would be defining such a loss individually for each view. However, diversity is inherently encouraged through L cpc , and interactions between views have the side-effect of increasing their mutual information (MI), which leads to improved performance [35,40].",
                "We combine the above losses to get our cooperative loss, L coop = L sync + \u03b1 \u2022 L sim . We use \u03b1 = 1.0 for our experiments and observe roughly similar performance for different values of \u03b1. The overall loss of our model is given by",
                "L cpc encourages our model to learn good features for each view, while L coop nudges it to learn higher-level features using all views while respecting the similarity structure across them."
            ],
            "subsections": []
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "The goal of our framework is to learn video representations which can be leveraged for video analysis tasks. Therefore, we perform experiments validating the quality of our representations. We measure downstream action classification to objectively measure model effectiveness and ana- lyze impact of our designs through controlled ablation studies. We also conduct qualitative experiments to gain deeper insights into our approach. In this section, we briefly go over our experiment framework. Additional details and discussions for each component are provided in the appendix. Datasets Our approach is a self-supervised learning framework for any dataset with multiple views. However, we discuss its relevance to video action classification in our experiments. We focus on human action datasets i.e. UCF101, HMDB51 and Kinetics400. UCF101 contains 13K videos spanning over 101 human action classes. HMDB51 contains 7K video clips mostly from movies for 51 classes. Kinetics-400 (K400) is a large video dataset with 306K video clips from 400 classes.",
                "Views We utilize different views in our experiments. For Kinetics-400, we learn encoders for RGB and Optical Flow. We use Farneback flow (FF) [7] instead of the commonly used TVL1-Flow as it is quicker to compute lowering our computation budget. Although FF leads to lower performance compared to TVL1, the essence of our claims remain unaffected. For UCF101 and HMDB51, we learn encoders for RGB, TVL1 Optical Flow, Pose Heatmaps (PoseHMs) and Human Segmentation Masks (SegMasks). A few visual samples for each view are provided in 2. PoseHMs and SegMasks are generated using an off-the-shelf detector [45] without any form of pre/post-processing.",
                "Implementation Details We choose a 3D-ResNet similar to [10,13] as the encoder f (.). We choose N = 8 and K = 5 in our experiments. We subsample the input by uniformly choosing one out of every 3 frames. Our predictive task involves predicting the last three blocks using the first five blocks. We use standard data augmentations during training whose details are provided in the appendix. We train our models using Adam [21] optimizer with an initial learning rate of 10 -3 , decreased upon loss plateauing. We use 4 GPUs with a batch size of 16 samples per GPU. Multiple spatio-temporal samples ensure sufficient negative examples despite the small batch size used for training.",
                "Action Classification We measure the effectiveness of our learned representations using the downstream task of action classification. We follow the standard evaluation protocol of using self-supervised model weights as initializa-tion for supervised learning. The architecture is then finetuned end-to-end using class label supervision. We finally report the fine-tuned accuracies on UCF101 and HMDB51. While fine-tuning, we use the learned composite function F (.) in order to generate context representations for the video blocks. The context feature is further passed through a spatial pooling layer followed by a fully-connected layer and a multi-way softmax for action classification."
            ],
            "subsections": [
                {
                    "title": "Quantitative Results",
                    "paragraphs": [
                        "We analyze various aspects of CoCon through ablation studies, experiments on multiple datasets, controlled variation of views and comparison to comparable methods. We objectively evaluate model performance using downstream classification accuracy as a proxy for learned representation quality. Pre-training is performed on either UCF101 or Kinetics400. We propose two baselines for comparison. Ablation Study We have motivated the utility of our various loss components. We now perform experiments to quantify the impact of each. The pre-training dataset used is the 1 st split of UCF101, and downstream classification accuracy is computed on the same. Table 1 summarizes the results of our experiment. As expected, all cross-view approaches comfortably perform better than CPC; demonstrating the utility of multi-view training.",
                        "Using L cpc sync leads to no performance improvements, as only using L sync leads to the model collapsing by squashing all D scores to have similar values, thus necessitating L sim to counter-balance this tendency. L cpc sim leads to improved performance wrt L cpc as it learns better features by effectively maximizing mutual information between views. CoCon i.e L cocon achieves the same by also regularizing manifolds across views, leading to even better performance across all views. The important comparison to observe is between L cpc sim and L cocon . As L cpc sim is the most similar baseline to other multi-view approaches, e.g., CMC [40]. However, we argue this baseline is even stronger as it in-volves both single-view and multi-view components compared to [40], which only uses a contrastive multi-view loss to learn representations.",
                        "Effect of Datasets A critical benefit of self-supervised approaches is the ability to run on large unlabelled datasets. To simulate such a setting, we perform pre-training using UCF101 or Kinetics4001 without labels utilizing the 1 st splits of UCF101 and HMDB51 for evaluation. Table 2 confirms pre-training with a larger dataset leads to better performance. It is also worth noting that CoCon pretrained with UCF101 outperforms CPC trained on Kinet-ics400 even though CoCon on UCF101 uses only around 10% data compared to Kinetics. Further demonstrating the potential of utilizing multiple views as opposed to training with larger and diverse datasets.",
                        "When comparing the Random baseline and CoCon pre-trained on Kinetics400, we observe higher performance gains for RGB (+25.4%) compared to Optical-Flow (+6.9%). We argue this is due to higher variance and complexity of RGB compared to Flow, allowing a randomly initialized network to perform relatively better with Flow. While comparing our approach with CPC, we again observe higher gains in RGB (+4.1%) compared to Flow (+2.7%). This can be explained by the potential capability of RGB to capture flow-like features when learned jointly.",
                        "Effect of cooperative training We compare benefits of cooperative training with varying views. We look at co-training of RGB, Flow, SegMasks and PoseHMs. Recall that these additional views are generated using off-theshelf models without any additional post-processing. Even though they are somewhat redundant i.e. Flow, PoseHM, SegMasks are actually derived from RGB Images; using them simultaneously still leads to a large performance increase. We also note that although SegMasks and PoseHMs are sparse low-dimensional features, they still help improve performance across all views.",
                        "Table 3 summarizes downstream action recognition performance of each view under different approaches. We see improved performance with increase in the number of views used. Consistent gains for views such as Flow, SegMasks, PoseHM, which are not as expressive as RGB points towards extraction of higher-order features even from low dimensional inputs. We observe PoseHM and Seg-Mask have lower performance gains when evaluated on HMDB51. This can be attributed to the large degree of noise in PoseHMs and SegMasks for HMDB51. HMDB is a challenging and diverse dataset, leading to poor predictions from our off-the-shelf detector. In conclusion, the benefits of joint training are apparent as CoCon leads to a performance improvement for all the views involved."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Effect of additional views",
            "paragraphs": [
                "CoCon hinges on the assumption that multi-view information helps in improving overall representation quality. To verify our hypothesis, we study co-training with different number of views. We consider two scenarios, 1) Joint training of RGB and Flow streams, and 2) Joint training of RGB, Flow, SegMasks and PoseHMs. Table 5 shows a consistent increase across views when increasing the number of views used during training. We should note that both SegMasks and PoseHMs contain significant noise as the off-the-shelf models incorrectly detects and misses humans in numerous videos. However, we see a consistent mutual increase in performance for all the involved views despite the prevalence of noise.",
                "Comparison with comparable approaches We summarize comparisons of CoCon with comparable state-ofthe-art approaches in Table 6. CoCon-Ensemble refers to an ensemble of models for all the involved views. We observe a few major trends, (1) When pre-training on UCF101, using multiple views allows us to outperform the nearest comparable approach by around 10.4%. This demonstrates the potential of cooperatively utilizing multiple views to learn representations. (2) We see considerable gains while training on Kinetics400 as well, however, the increase is smaller compared to UCF101. We argue the reasons are, a) we only utilize two views for co-training. b) the flow we utilize for Kinetics400 is Farneback Flow instead of TVL1 flow used for UCF101 and HMDB51. (3) Our method comfortably outperforms recent multi-view approaches consistently on UCF101 and HMDB51. (4) An interesting observation is that using multiple views of a small dataset (UCF101) performs better (71.0%) than pre-training on a large dataset, Kinetics400 (68.2%). This suggests that utilizing different views can be better than merely training on larger datasets.",
                "Comparison with recent approaches A few very recent approaches [1,11,12,32] have tackled multi-modal self-supervised achieving impressive performance. CoCon differs from them as it considers inter-instance relationships to aid learning in addition to relationships between views. Due to resource constraints, it was not possible to have a fair comparison due to the significant difference in the amount of GPUs, number of epochs trained and the backbones used. However, we hope our carefully constructed experiments given earlier provide deeper insights into CoCon's benefits even with lower resource requirements."
            ],
            "subsections": [
                {
                    "title": "Qualitative Results",
                    "paragraphs": [
                        "We motivate CoCon arguing about the benefits of preserving similarities across view-specific spaces. We observe respecting structure across views results in emergence of higher-order semantics without additional supervision e.g. sensible class relationships and good feature representations. Jointly training with views known to perform well for video action understanding allows us to learn good video     Effect of action classes on performance Figure 4 shows the classes which observe the least and highest performance improvements when co-trained with multiple views. We observe a loose pattern where action classes involving distinguishable physical movements see larger improvements. We can argue this is because we use views which are suitable for physically intensive actions.",
                        "Inter-Class Relationships In order to study consistency of structure across different views, we look at relationships between classes by inferring their similarities through our learned features. We compare cosine similarities across videos from different classes and compute the most similar four classes for each action. We repeat the process for all views and look at the consistency of the results. We only display classes which are amongst the closest ones across all views. Table 4 summarizes our results. We see the detected nearest actions are semantically related to the original actions. In the cases of PlayingCello, we encounter a cluster of categories involving playing instruments. Similarly for BasketBall, we can see emergence of sports-based relationships even though there isn't any visual commonality between the categories. It's worth noting that as these nearest classes are consistent across different views, our approach cannot cheat to generate them i.e. it cannot look at 'background crowd' or 'green field' and infer that a video clip is related to sports. Since views such as Optical-Flow, SegMasks and KeypointHeatmap do not have such information and are very low-dimensional.",
                        "Action Alignment Even though we only use selfsupervision, our embeddings are able to capture relevant semantics through our multi-view approach allowing loose alignment between videos. To compute this soft alignment, we divide each video into 18 blocks and compute blocklevel features. We then utilize relative cosine similarities to infer associations between the videos. Figure 5 highlights a few examples. Notice the periodicity implicitly present in some actions (e.g. pullups) captured through the heatmap allowing us to perform non-linear alignment."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We propose a cooperative version of contrastive learning, called CoCon, for self-supervised video representation learning. By leveraging relationships across views, we encourage our self-supervised learning objective to be aligned with the underlying semantics. We demonstrate the effectiveness of our approach on the downstream task of action classification, and illustrate the semantic structure of our representation. We show that additional input views generated by off-the-shelf computer vision algorithms can lead to significant improvements, even though they are noisy and derived from existing modality i.e. RGB. As these views are 'freely' available, this shows the feasibility of utilizing multi-view approaches on datasets which are not traditionally considered multi-view. We hope this enables the ability to leverage multi-view learning algorithms and observe performance gains even on single-view datasets."
            ],
            "subsections": []
        },
        {
            "title": "A.2. Datasets",
            "paragraphs": [
                "Kinetics400 contains 400 human action classes, with at least 400 real-world video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focused and cover a broad range of classes including human-object and human-human interactions. The large diversity and variance in the dataset make it an extremely challenging dataset.",
                "HMDB51 dataset contains around 6800 real-world video clips from 51 action classes. These action classes cover a wide range of actions -facial actions, facial action with object manipulations, general body movement, and general body movements with human interactions. This dataset is challenging as it contains many poor quality video with significant camera motions and also the number of samples are not enough to effectively train a deep network. We report classification accuracy for 51 classes across 3 splits provided by the authors.",
                "UCF101 dataset contains 13320 videos from 101 action classes that are divided into 5 categories -human-object interaction, body-movement only, human-human interaction, playing musical instruments and sports. Action classification in this datasets is challenging owing to variations in pose, camera motion, viewpoint and spatio-temporal extent of an action."
            ],
            "subsections": []
        },
        {
            "title": "A.3. Views",
            "paragraphs": [
                "We simultaneously learn encoders for RGB and Optical Flow while training on Kinetics-400. Instead of using the commonly used TVL1-Flow, we rely on Farneback flow which usually results in lower performance for action classification, however is much faster to compute. For UCF101 and HMDB51, we simultaneously learn encoders for RGB, TVL1 Optical Flow, Pose Heatmaps and Semantic Maps.",
                "We give a brief overview of the views utilized and their generation.",
                "\u2022 RGB Images, RGB -We directly use sequences of RGB frames present in videos",
                "\u2022 Optical Flow, Flow -We use the popular TVL1 flow for UCF101 and HMDB51 and Farneback Flow (FF) for Kinetics400. FF is known to perform worse than TVL1-Flow on visual recognition tasks, however, it is quicker to compute. This view mismatch views leads to lesser gains when using Kinetics pre-trained flow weights for UCF101 and HMDB51.",
                "\u2022 Pose Keypoint Heatmaps, PoseHMs -We use an offthe-shelf keypoint detector [45] and extract confidence heatmaps for each keypoint. Note that we perform no pre/post-processing on the results and directly use this as input to our model. The input modality is inherently very noisy, however, we still observe improved performance.",
                "\u2022 Human Segmentation Masks, SegMasks -Similar to the above, we use an off-the-shelf semantic segmentation network [45] and extract confidence scores for human segmentation. Similar to pose keypoint heatmaps, this input modality is inherently very noisy. Fig. 7 shows examples of different views. Note the prevalence of noise in a few samples, specially SegMasks. There are multiple other instances where PoseHMs are noisy as we're unable to even localize the actor accurately."
            ],
            "subsections": []
        },
        {
            "title": "A.4. Implementation Details",
            "paragraphs": [
                "We choose to use a 3D-ResNet similar to [13] as the encoder f (.). Following [10] we only expand the convolutional kernels present in the last two residual blocks to be 3D ones. We used 3D-ResNet18 for our experiments, denoted as ResNet18. We use a weak aggregation function g(.) in order to learn a strong encoder f (.). Specifically, we use a one-layer Convolutional Gated Recurrent Unit (Con-vGRU) with kernel size (1, 1) as g(.). The weights are shared amongst all spatial positions in the feature map. This design allows the aggregation function to propagate features in the temporal axis. A dropout [38] with p = 0.1 is used when computing the hidden state at each time step. A shallow two-layer perceptron is used as the predictive function \u03c6(.). Recall z j = P ool(z j )wherez j \u2208 R D . We utilize stacked max pool layers as P ool(.).",
                "To construct blocks to pass to the network, we uniformly choose one out of every 3 frames. We then group these into 8 blocks containing 5 frames each. Since the videos we use are usually 30fps, each block roughly covers 0.5 seconds worth of content. The predictive task we design involves predicting the last three blocks using the first five. Therefore, we effectively predict the next 1.5 seconds based on the first 2.5 seconds.",
                "We perform random cropping, random horizontal flipping, random greying, and color jittering to perform data augmentation in the case of images. For optical flow, we only perform random cropping on the image. As discussed earlier, Keypoint Heatmaps and Segmentation Confidence Masks are modelled as images, therefore we perform random cropping and horizontal flipping in their case. Note that random cropping and flipping is applied for the entire block in a consistent way. Random greying and color jittering are applied in a frame-wise manner to prevent the network from learning low-level features such as optical flow. Therefore, each video block may contain both colored and grey-scale image with different contrast.",
                "All individual view-specific models are trained independently using only L cpc . After which we proceed to train all view-specific models simultaneously using L cocon . All models are trained end-to-end using Adam [21] optimizer with an initial learning rate 10 -3 and weight decay 10 -5 . Learning rate is decayed to 10 -4 when validation loss plateaus. A batch size of 16 samples per GPU is used, and our experiments use 4 GPUs. We train models on UCF101 for 100 epochs using L cpc , after which they are collectively trained together for 60 epochs using L cocon . We repeat the same for Kinetics400 with reduced epochs. We train models on Kinetics400 for 80 epochs using L cpc and further for 40 epochs using L cocon .",
                "The learned representations are evaluated by their performance on the downstream task of action classification. We follow the evaluation practice from recent works and use the weights learned through our self-supervised framework as initialization for supervised learning. The whole setup is then fine-tuned end-to-end using class label supervision. We finally report the fine-tuned accuracies on UCF101 and HMDB51. We use the learned composite function F (.) to generate context representations for video blocks. The context feature is further passed through a spatial pooling layer followed by a fully-connected layer and a multi-way softmax for action classification. We use dropout with p = 0.7 for classification. The models are fine-tuned for 100 epochs with learning rate decreasing at different steps. During inference, video clips from the validation set are densely sampled from an input video and cut into blocks with halflength overlapping. The softmax probabilities are averaged to give the final classification result. "
            ],
            "subsections": []
        },
        {
            "title": "B. Additional Results",
            "paragraphs": [
                "We motivate CoCon arguing about the benefits of preserving similarities across view-specific feature spaces. We observe respecting structure across views results in emergence of higher-order semantics without additional supervision e.g. sensible class relationships and good feature representations. We go over different results in the following sections."
            ],
            "subsections": []
        },
        {
            "title": "B.1. t-SNE Visualization",
            "paragraphs": [
                "We explore t-SNE visualizations of our learned representations on the 1 st test split of UCF101 extracted using F (.). Our model is trained on the corresponding train split to ensure we're testing out of sample quality. For clarity, only 21 action classes are displayed. We loosely order the action classes according to their relationships. Classes having similar colors are semantically similar. Results are displayed in Fig 8 . Even though we operate in a self-supervised setting, our approach is able to uncover deeper semantic features allowing us to uncover inter-class relationships. We can see a much more concise and consistent clustering in CoCon compared to CPC. We also observe the distinct improvement in the compactness of the clusters as we increase the number of views."
            ],
            "subsections": []
        },
        {
            "title": "B.2. Inter-Class Relationships",
            "paragraphs": [
                "In order to study the manifold consistency across different views, we look at relationships between classes by inferring their similarities through the learned features. We compare cosine similarities across video clips from different classes. We then compute the most similar five classes for each action. We repeat the process for all views and look at the consistency of the results. Ideally, semantically similar classes should be consistent across all views, assuming the views reasonably capture the essence of the task we're interested in.",
                "We observe that CoCon leads to much higher consistency across different views. Specifically, we see 41 classes which have at least four out of five top-classes consistent in all views; as opposed to 10 classes in CPC. Similar patterns are seen when we consider other thresholds. In order to confirm that the nearest classes are actually sensible, we mention the most-similar classes for a few action classes.",
                "We can see that the nearest actions generated are semantically related to the original actions. In the cases of PlayingCello, we encounter a cluster of categories involving playing instruments. Similarly for BasketBall, we can see emergence of sports-based relationships even though there is no visual commonality between categories. We also see a few seemingly unrelated classes as well, e.g., Boxing-PunchingBag and YoYo; SalsaSpin and WalkingWithDog. A deeper inspection into the samples is required to comment whether this truly makes sense. It is worth noting that as these nearest action classes are mostly consistent across different views, our approach cannot cheat to generate them i.e. it cannot look at 'background crowd' or 'green field' and infer that the video clip is related to sports. Since views such as Optical-Flow, SegMasks and KeypointHeatmap do not have such information and are much low-dimensional."
            ],
            "subsections": []
        },
        {
            "title": "C. Action Alignment",
            "paragraphs": [
                "An interesting side-effect of improved representations for actions is the possibility of performing loose action alignment. Even though we only use self-supervision, Co-Con embeddings are able to capture relevant semantics through our multi-view approach allowing loose alignment between videos. To compute this soft alignment, we divide each video into 18 blocks and compute block-level features z . We then utilize relative cosine similarities to infer associations between the videos. We smoothen the heatmap in order to make it visually appealing. Figure 5 shows alignment between different videos. Figure 9 highlights a few examples when we perform alignment between same videos. Notice the periodicity implicitly present in these actions captured through the heatmap."
            ],
            "subsections": []
        },
        {
            "title": "C.1. Cosine similarity",
            "paragraphs": [
                "This section highlights the ability of representation generated through CoCon to capture meaningful semantics going beyond low-level features. We look at cosine similarity distributions of video representation from UCF101. We extract one context representation for each video and pool it into a vector. We then compute the cosine similarity for each pair of video features across the unseen UCF101 test set. The cosine distance is summarized by a histogram, where the 'blue' histogram represents the score distribution for positives i.e. videos belonging to the same class; and the 'orange' one shows the distribution for negatives i.e. videos from different classes."
            ],
            "subsections": []
        },
        {
            "title": "C.2. Nearest Neighbors",
            "paragraphs": [
                "We utilize CoCon to perform video retrieval for different query videos. Note that CoCon is able to look past purely visual background features and focus on actions even though it only used RGB inputs. For example, we see that we are able to retrieve close neighbors for BenchPress, even   "
            ],
            "subsections": []
        },
        {
            "title": "A.1. Model Overview",
            "paragraphs": [
                "We build our framework borrowing the learning framework present in [10] which learns video representations through spatio-temporal contrastive losses. It should be noted that even though we use this particular self-supervised backbone in our experiments, our approach is not restricted by the choice of the underlying self-supervised task.",
                "A video V is a sequence of T frames (not necessarily RGB images) with resolution H \u00d7 W and C channels, {i 1 , i 2 , . . . , i T }, where i t \u2208 R H\u00d7W \u00d7C . Assume T = N * K, where N is the number of blocks and K denotes the number of frames per block. We partition a video clip V into N disjoint blocks V = {x 1 , x 2 , . . . , x N }, where x j \u2208 R K\u00d7H\u00d7W \u00d7C and a non-linear encoder f (.) transforms each input block x j into its latent representation",
                "An aggregation function, g(.) takes a sequence {z 1 , z 2 , . . . , z j } as input and generates a context representation c j = g(z 1 , z 2 , . . . , z j ). In our setup, z j \u2208 R H \u00d7W \u00d7D and c j \u2208 R D . D represents the embedding size and H , W represent down-sampled resolutions as different regions in z j represent features for different spatial locations. We define z j = P ool(z j ) where z j \u2208 R D and c = F (V ) where F (.) = g(f (.)). In our experiments, H = 4, W = 4, D = 256.",
                "To learn effective representations, we create a prediction task involving predicting z of future blocks similar to [10]. In the ideal scenario, the task should force our model to capture all the necessary contextual semantics in c t and all frame level semantics in z t . We define \u03c6(.) which takes as input c t and predicts the latent state of the future frames. The formulation is given in Eq. ( 4).",
                "where \u03c6(.) takes c t as input and predicts the latent state of the future frames. We then utilize the predicted z t+1 to compute c t+1 . We can repeat this for as many steps as we want, in our experiments we restrict ourselves to predict till 3 steps in to the future.",
                "Note that we use the predicted z t+1 while predicting z t+2 to force the model to capture long range semantics. We can repeat this for a varying number of steps, although the difficulty increases tremendously as the number of steps Figure 6: A diagram of the learning framework utilized. We look at features in a sequential manner while simultaneously trying to predict representations for future states. increases as seen in [10]. In our experiments, we predict the next three blocks using the first five blocks."
            ],
            "subsections": []
        }
    ]
}