{
    "id": "https://semopenalex.org/work/W4313068277",
    "authors": [
        "Hsin-Ying Lee",
        "Subhransu Maji",
        "Sergey Tulyakov",
        "Zeng Huang",
        "Jinchang Ren",
        "Menglei Chai",
        "Zezhou Cheng",
        "Kyle Olszewski"
    ],
    "title": "Cross-modal 3D Shape Generation and Manipulation",
    "date": "2022-01-01",
    "abstract": "Creating and editing the shape and color of 3D objects require tremendous human effort and expertise. Compared to direct manipulation in 3D interfaces, 2D interactions such as sketches and scribbles are usually much more natural and intuitive for the users. In this paper, we propose a generic multi-modal generative model that couples the 2D modalities and implicit 3D representations through shared latent spaces. With the proposed model, versatile 3D generation and manipulation are enabled by simply propagating the editing from a specific 2D controlling modality through the latent spaces. For example, editing the 3D shape by drawing a sketch, re-colorizing the 3D surface via painting color scribbles on the 2D rendering, or generating 3D shapes of a certain category given one or a few reference images. Unlike prior works, our model does not require re-training or fine-tuning per editing task and is also conceptually simple, easy to implement, robust to input domain shifts, and flexible to diverse reconstruction on partial 2D inputs. We evaluate our framework on two representative 2D modalities of grayscale line sketches and rendered color images, and demonstrate that our method enables various shape manipulation and generation tasks with these 2D modalities.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "With the growth in 3D acquisition and visualization technology, there is an increasing need of tools for 3D content creation and editing tasks such as deforming the shape of an object, changing the color of a part, or inserting or removing a component. The graphics and vision community has proposed a number of tools for these tasks [46,13,2,41]. Yet, manipulating 3D still requires tremendous human labor and expertise, prohibiting wide-scale adoption by nonprofessionals. Compared to the traditional 3D user interfaces, 2D interactions on view-dependent image planes can be a more intuitive way to edit the shape. This has motivated the community to leverage advances in shape representations using deep networks [40,37,9,50] for 3D shape manipulation with 2D controls, such as mesh reconstruction from sketches [20] and color editing with scribbles [36]. We propose a multi-modal generative model that bridges multiple 2D (e.g., sketch, color views) and 3D modalities via shared latent spaces (left). Versatile 3D shape generation and manipulation tasks can be tackled via simple latent optimization method (right).",
                "However, most prior works on 2D-to-3D shape manipulation are tailored to a particular editing task and interaction format, which makes generalization to new editing tasks or controls challenging, or even infeasible. This is important because there is often no single interaction that fits every use case -the preferred 2D user control depends on the editing goals, scenarios, devices, or targeted users.",
                "Motivated by this, we propose a 2D-to-3D framework that not only works on a single control modality but also enjoys the flexibility of handling various types of 2D interactions without the need for changing the architecture or even re-training (Fig. 1 left). Our framework bridges various 2D interaction modalities and the target 3D shape through a uniform editing propagation mechanism. The key is to construct a shared latent representation across generative models of each of the 2D and 3D modalities. The shared latent representation enforces that an arbitrary latent code corresponds to a 3D model that is consistent with every modality, in terms of both shape and color. With our model, any editing can be achieved by an objective that aims to match the corresponding editing modality and backpropagating the error to estimate the latent code. Moreover, different editing operations and modalities can be combined and interleaved leading to a versatile tool for editing the shape (Fig. 1 right). The approach can be extended to a new user control by simply adding a generator for the corresponding modality in the framework.",
                "We evaluate our framework on two representative 2D modalities, i.e., grayscale line sketches, and rendered color images. We provide extensive quantitative and qualitative results in shape and color editing with sketches and scribbles, as well as single-view, few-shot, or even partial-view cross-modal shape generation. The proposed method is conceptually simple, easy to implement, robust to input domain shifts, and generalizable to new modalities with no special requirement on the network architecture. ",
                "resents one representation (e.g., images, text) of underlying signals. Multi-modal VAEs [52,56,57,49,29] learn a joint distribution p \u03b8 (x 0 , . . . , x n | z) conditioned on common latent variables z \u2208 Z. Without the assumption of paired multi-modal data, multi-modal GANs [33,10,17] learn the joint distribution by sharing a latent space and model parameters across modalities. These multi-modal generative models have enabled versatile applications such as cross-modal image translation [10,33] and domain adaptation [33]. Similar to these works, we build a multi-modal generative model that bridges multiple modalities via a shared latent space. However, we generate and edit 3D shapes with sparse 2D inputs (e.g., scribbles, sketches) and build a 2D-3D generative model based on variational auto-decoders (VADs) [60,22]. Prior work [60] has shown that VADs excel at generative modeling from incomplete data. In this work, we demonstrate that the multi-modal VADs (MM-VADs) are ideally suited for the task of 3D generation and manipulation from sparse 2D inputs (e.g., color scribble or partial inputs). Shape and Appearance Reconstruction. Extensive works have explored the problem of 3D reconstruction from different modalities, such as RGB images [27,11], videos [59], sketches [26,20,64,63], or even text [8]. This problem has also been explored under diverse representations [11,15,35,54,14,40,9,37,50,58] and different levels of supervision [11,15,27,16,59]. Despite the diverse settings of this problem, the encoder-decoder network, which maps the source modalities to 3D shape directly in a feed-forward manner, remains the most popular 3D reconstruction model [11,54,27,40]. However, such feed-forward networks are not robust to input domain shift (e.g., incomplete data). In this work, we demonstrate that the proposed MM-VADs perform more robustly and could provide multiple 3D reconstructions that fit the given input (e.g., partial 2D views). Shape and Appearance Manipulation. Numerous interactive tools have been developed for image editing [31,62,44,32,18,30] and 3D shape manipulations [46,13,2,41]. More recently, generative modeling of natural images [17,51] has became a \"Swiss knife\" for image editing problems [65,48,47,19,1,4,5,39,45]. Similar to these works, we build a multi-modal generative model that is able to tackle versatile 3D shape generation and editing tasks with 2D inputs. Novel interactive tools have also been proposed recently to edit implicit 3D representations [40,38]. For example, DualSDF [22] edits the SDFs [40] via shape primitives (e.g., spheres). Sketch2Mesh [20] reconstructs shapes from sketch with an encoder-decoder network and refines 3D shapes via differentiable rendering.",
                "EditNeRF [36] edits the radiance field [38] by fine-tuning the network weights based on user's scribbles. Tab. 5 summarizes the commons and differences between our work and recent efforts [22,20,36] on 3D manipulation and generation. Similar to Sketch2Mesh [20], we edit and reconstruct 3D shape from 2D sketch. However, we tackle this problem via a novel multi-modal generative model that performs more robust to input domain shift (e.g., partial input, sparse color scribble). Furthermore, the shape and color edits can be combined and interleaved with our model; Like EditNeRF, we edit the appearance of 3D shapes via 2D color scribbles. However, we conduct the 3D editing via a simple latent optimization, instead of finetuning the network weights per edit; Akin to DualSDF [22], we build a generative model for 3D manipulation, yet we generate and edit shapes from 2D modalities which is more intuitive to edit the shape than using 3D primitives. Moreover, our generative model can be adapted to generate 3D shapes of a certain category (e.g., armchairs) given a few 2D examples, namely, few-shot cross-modal shape generation."
            ],
            "subsections": []
        },
        {
            "title": "Method",
            "paragraphs": [
                "We describe the Variational Auto-Decoders (VADs) [60] in \u00a7 3.1, introduce the proposed VAD-based multi-modal generative model (dubbed MM-VADs) in \u00a7 3.2, and illustrate the application of MM-VADs in cross-modal 3D shape generation and manipulation tasks in \u00a7 3.3."
            ],
            "subsections": [
                {
                    "title": "Background: Variational Auto-Decoder",
                    "paragraphs": [
                        "Given observation variables x \u223c p(x) and latent variables z \u223c p(z), a variational auto-decoder (VAD) approximates the data distribution p(x) via a parametric family of distributions p \u03b8 (x | z) with parameters \u03b8. Similar to variational autoencoders (VAEs) [29], VADs are trained by maximizing the marginal distribution p(x) = p \u03b8 (x | z)p(z)dz. In practice this integral is expensive or intractable, so the model parameters \u03b8 are learned instead by maximizing the Evidence Lower Bound (ELBO):",
                        "where KL(\u2022 \u2225 \u2022) is the Kullback-Leibler divergence that encourages the posterior distribution to follow the latent prior p(z), and q \u03d5 (z | x) is an approximation of the posterior p(z | x). In VAEs, q \u03d5 (z | x) is parametrized by a neural network and \u03d5 are the parameters of the encoder. In VADs, \u03d5 are instead learnable similar to the parameters \u03b8 in the decoder p \u03b8 (x | z). For example, the multivariate Gaussian approximate posterior for a data instance x i is defined as:",
                        "where \u03d5 = {\u00b5 i , \u03a3 i }. The reparametrization trick is applied in order to backpropagate the gradients to the mean \u00b5 i and variance \u03a3 i in VADs. In comparison, VAEs back-propagate the gradients through the mean \u00b5 i and variance \u03a3 i to learn the parameters of the encoder. At inference time, the parameters \u03d5 of the approximate posterior distribution can be estimated by maximizing the ELBO in Eqn. 1 while the parameters \u03b8 of the decoder are frozen:",
                        "Despite the similarity between VAEs and VADs, prior works [60] demonstrate that VADs perform approximate posterior inference more robustly on incomplete data and input domain shifts than VAEs."
                    ],
                    "subsections": []
                },
                {
                    "title": "Multi-Modal Variational Auto-Decoder",
                    "paragraphs": [
                        "We consider two modalities x, w and an i.i.d. dataset with paired instances (X, W ) = {(x 0 , w 0 ), . . . , (x N , w N )}. We target at learning a joint distribution of both modalities p(x, w). Like VADs [60], the multi-modal VADs (MM-VADs) are trained by maximizing the ELBO:",
                        "where z is the latent variable shared by the two modalities x and w, p \u03b8 (x, w | z) = p \u03b8x (x | z)p \u03b8w (w | z) under the assumption that the two modalities x and w are independent conditioned on the latent variable z (i.e., x \u22a5 \u22a5 w | z). In practice, p \u03b8x (x | z) or p \u03b8w (w | z) can be parameterized by different networks for the two modalities x and w respectively. The parameters \u03d5 of the approximate posterior distribution q \u03d5 (z | x, w) are learnable parameters where \u03d5 = {\u00b5, \u03a3} under the assumption of multivariate Gaussian posterior distribution. At inference time, the parameters \u03d5 are estimated via maximizing the ELBO with frozen decoder parameters \u03b8:",
                        "When one of the modalities is missing during inference, the inputs of the missing modalities are simply set to zero. This is the case when we want to infer one modality from the other (e.g., 3D reconstruction from 2D sketch). This framework can be trivially extended to learn a joint distribution of more than two modalities."
                    ],
                    "subsections": []
                },
                {
                    "title": "Learning a Joint 2D-3D Prior with MM-VADs",
                    "paragraphs": [
                        "Here we introduce the application of MM-VADs in cross-modal 3D shape generation and manipulation. Specifically, we learn a joint distribution of 2D and 3D modalities with MM-VADs. Once trained, MM-VADs can be applied to versatile shape generation and editing tasks via a simple posterior inference (or latent optimization). We explore three representative modalities, including 3D shape with colorful surface, 2D sketch in grayscale, and 2D rendered image in < l a t e x i t s h a 1 _ b a s e 6 4 = \" s e 9 I 3 1 q",
                        "We propose a multi-modal variational auto-decoder consisting of compact shape and color latent spaces shared across multiple 2D (e.g., sketch, RGB views) or 3D modalities (e.g., signed distance function and 3D surface color).",
                        "RGB color, donated as C, S, R respectively. Given a dataset {(C i , S i , R i )}, we target at learning a joint distribution of the three modalities p(C, S, R). Fig. 2 presents the overview of the MM-VADs framework. We provide more details in the following sections. Joint Latent Space. The MM-VADs share a common latent space Z across different modalities (Eqn. 4). Targeting at editing 3D shape and surface color independently, we further disentangle the shared latent space into the shape and color subspaces, denoted as Z s and Z c respectively. Therefore, each latent code z = z s \u2295 z c , where z s \u2208 Z s , z c \u2208 Z c , and \u2295 denotes the concatenation operator. 3D Colorful Shape. Targeting at generating and editing 3D shapes and their appearance, we use the 3D colorful shape as one of our modalities. Among various representations of 3D shapes (e.g., voxel, mesh, point clouds), the implicit representations [40,37,50] model 3D shapes as isosurfaces of functions and are capable of capturing high-level details. We adopt the DeepSDF [40] to regress the signed distance functions (SDFs) from point samples directly using a MLP-based 3D shape network F \u03b1 (z s \u2295 p), whose input is a shape latent code z s \u2208 Z s and 3D coordinates p \u2208 R 3 . We predict the surface color with another feed-forward 3D color network F \u03b2 (z c \u2295 z k s ), whose input is a color latent code z c \u2208 Z c and the intermediate features from the k-th layer of 3D shape network F \u03b1 . The generator of the 3D modality G C is the combination of the 3D shape and color network:",
                        "Both networks are trained using the same set of spatial points. The objective function L C for G C is the L 1 loss defined between the prediction and the groundtruth SDF values and surface colors on the sampled points. 2D Sketch. The 2D sketch depicts the 3D structures and provides a natural way for the user to manipulate the 3D shapes. For the purpose of generalization, we adopt a simple and standard fully convolutional network [42] as our sketch generator G S (z s \u2295 v) with the shape code z s \u2208 Z s and the viewpoint v as input. The objective function L S is defined as a cross-entropy loss between the reconstructed and ground-truth sketches. 2D Rendering. The 2D color rendering reflects a view-dependent appearance of the 3D surface. Drawing 2D scribbles on the renderings provides an efficient and straightforward interactive tool for the user to edit the 3D surface color. Similar to the 2D sketch modality, we use the standard fully convolutional architecture [42] as our 2D rendering generator G R (z s \u2295 z c \u2295 v), which takes the concatenation of the shape code z s \u2208 Z s , the color code z c \u2208 Z c and the viewpoint v. We adopt Laplacian-L 1 loss [3] to train G R :",
                        "where z i is the concatenation of the shape and color codes for the target image R i , N is the total number of pixels in the image R i , J is the total number of levels of the Laplacian pyramid (e.g., 3 by default), and L j (x) is the j-th level in the pyramid of image x [6]. This loss encourages sharper output [3] compared to the standard L 1 or MSE loss.",
                        "Summary. The proposed MM-VAD framework for learning the joint distribution of the three modalities can be learned with the following objective:",
                        "where the first term regularizes the posterior distribution to a latent prior (e.g., N (0, I)), and the second term can be factorized into three components under the assumption that modalities are independent conditioned on the shared latent variable z:",
                        "where each term corresponds to the reconstruction loss per modality as described above. Notice that the 3D shape modality C contains all the information in the latent variable z, therefore q \u03d5 (z | C, S, R) = q \u03d5 (z | C)."
                    ],
                    "subsections": []
                },
                {
                    "title": "Cross-Modal Shape Manipulation with MM-VADs",
                    "paragraphs": [
                        "Given an initial latent code z 0 that corresponds to the initial 3D shape G C (z 0 ) and any 2D control G M (z 0 ) of the 2D modality M \u2208 {S, R}, the shape manipulation is conducted by optimizing within the latent space to get the updated code \u1e91 such that G( \u1e91) M matches the 2D edits e M :",
                        "where L edit could be any loss (e.g., L 1 loss) that encourages the 2D modalities G( \u1e91) M to match the 2D edits e M , and L reg (z) encourages the latent code to stay in the latent prior of MM-VADs. We apply the regularization loss proposed in DualSDF [22]:",
                        "where \u03b3 and \u03b2 controls the strength of the regularization loss. The latent optimization is closely related to the posterior inference (Eqn. 5) of MM-VADs. MM-VADs allows free-form edits e M . For example, the edits e M could be local modifications on the sketch or sparse color scribbles on 2D renderings. This makes the MM-VADs ideally suited for the interactive 3D manipulation tasks. In comparison, the encoder-decoder networks [20] are not robust to the input domain shift (e.g., incomplete data [60]) and require re-training per type of user interactions (e.g., sketch, color scribble)."
                    ],
                    "subsections": []
                },
                {
                    "title": "Cross-Modal Shape Generation with MM-VADs",
                    "paragraphs": [
                        "Single-View Reconstruction. Given a single input x M of the 2D modality M \u2208 {C, R}, the task of single-view cross-modal shape generation is to reconstruct the corresponding 3D shape satisfying the 2D constraint. Without the need of training one model per pair of 2D and 3D modalities [20,53] or designing differentiable renderers [34] for each 2D modalities [20], like shape manipulation ( \u00a73.4), this task can be tackled via the latent optimization:",
                        "Partial-View Reconstruction. The MM-VADs are flexible to reconstruct 3D shapes from partially visible inputs. More interestingly, when the input is ambiguous, it provides diverse 3D reconstructions by performing the latent optimization with different initialization of the latent code z. This property has practical applications. For example, the MM-VAD could provide multiple 3D shape suggestions interactively while the user is drawing sketches. Few-Shot Generation. Given a few 2D images spanning a subspace in the 3D distribution that represents a certain semantic attribute (e.g., armchairs, red chairs), the task of few-shot shape generation is to learn a 3D shape generative model that conceptually aligns with the provided 2D images. Given our pretrained MM-VAD, we tackle this task by steering the latent space with adversarial loss, borrowing the idea from MineGAN [55]. Specifically, we learn a mapping function h \u03c9 (z) that maps the prior distribution of the latent space z \u223c p(z) (i.e., N (0, I)) to a new distribution such that samples from the 2D generators G M (h \u03c9 (z)) aligns the target data distribution x \u223c p(x) depicted by the provided 2D images. We apply the WGAN-GP loss [21] with frozen generators to learn the mapping function h \u03c9 (z):",
                        "where both the mapping function h \u03c9 and the discriminator D are trained from scratch."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "This section provides qualitative and quantitative results of the proposed MM-VADs in versatile tasks of 3D shape manipulation ( \u00a7 4.1) and generation ( \u00a7 4.2)."
            ],
            "subsections": []
        },
        {
            "title": "Remove engines Add engines Remove curve Add curve",
            "paragraphs": [
                "Before editing After editing Fig. 3. Editing shape via sketch. The proposed method enables fine-grained editing of shape geometry, e.g., removing the engine of an airplane or reshaping the back of a chair. Interestingly, new engines often appears at the tail of airplane after removing the engines on the wing. This is because airplanes without any engines rarely exist in the domain of our generative model. The edited local regions are highlighted in red bounding boxes.",
                "Dataset. We conduct evaluations and comparisons mainly on 3D ShapeNet dataset [7]. For 3D shapes, We follow DeepSDF [40] to sample 3D points and their signed distances to the object surface. The points that are far from the surface (i.e., with the absolute distance higher than a threshold) are assigned with a pre-defined background color (e.g., white) while points surrounding the surface are assigned with the color of the nearest surface point. For 2D sketches, we use suggestive contours [12] to generate the synthetic sketches. For 2D renderings, we randomize the surface color of 3D shapes per semantic part. We use ShapeNet chairs and airplanes with the same training and test splits as DeepSDF [40]. Implementation Details. We use an 8-layer MLP as the 3D shape network which outputs SDF and a 3-layer MLP as the 3D color network which predicts RGB. We concatenate the features from the 6-th hidden layer of the 3D shape network with the color code as the input to the 3D color network. We train our MM-VADs using Adam [28]. We present more implementation details in the Appendix.",
                "Baselines. We use the following state-of-the-arts as our baselines:",
                "-Encoder-Decoder Networks [20]. This model is trained per task of 3D generation from 2D modalities (sketches or RGB images). We do not use the differentiable rendering proposed in [20] which requires auxiliary information (e.g., segmentation mask, depth) and is applicable to MM-VADs. -EditNeRF [36]. This model edits 3D neural radiance field (including shape and color) by updating the neural network weights based on the user's scribbles. We make comparisons with the pre-trained EditNeRF models."
            ],
            "subsections": [
                {
                    "title": "Cross-modal Shape Manipulation",
                    "paragraphs": [
                        "Sketch-Based Shape Manipulation. The proposed MM-VADs allow users to edit the fine geometric structures via 2D sketches, as described in \u00a7 3.4. We provide  users with an interactive interface where users can edit the initial sketch by adding or removing a certain part or even deforming a contour line. Fig. 3 presents some qualitative results of sketch-based shape manipulation. Interestingly, we find that our manipulation is semantics-aware. For example, removing the airplane engines on the wings will automatically add new engines to the tail. Such shape priors are absent in non-generative models (e.g., EditNeRF [36]).",
                        "It is challenging to quantitatively evaluate the sketch-based shape editing due to the lack of ground-truth paired 3D shapes before and after editing. For this reason, prior works [20] report the quantitative results of 3D reconstruction from sketches as a proxy. We follow prior works and report the same quantitative evaluations in Sec. 4.2. Furthermore, we manually edit the 3D shapes presented in Fig. 3 such that their sketches align with the human edits. Tab. 2 reports the Chamfer distance (CD) between the manually edited shapes and our editing results. We see that CD improves when removing a part, but adding parts unfortunately increases the CD as it induces more changes to the overall shape. This is often desirable, but the CD metric does not reflect that.",
                        "Fig. 4 provides a comparison with DualSDF [22]. A fair comparison is not possible, as DualSDF edits shapes via 3D primitives instead of 2D views. We find that DualSDF requires users to select right primitives to achieve certain edits (e.g., adding a curve to the chair back). In comparison, our sketch-based shape editing is more intuitive. Scribble-Based Color Manipulation. MM-VADs allow users to edit the appearance of 3D shapes via color scribbles. Fig. 5 shows that MM-VADs propagate the sparse color scribbles into desired regions (e.g., from the left wing of the airplanes to the right, from the left leg of chairs to the right). We provide more color editing results with diverse color scribbles in the appendix. As a quantitative [36]",
                        "Ours Fig. 6. Comparison with EditNeRF. Our model (bottom) achieves comparable editing performance with EditNeRF [36] (top). We provide three color edits on 2D views (odd columns), each followed by the 3D editing result (even columns).",
                        "evaluation ,we select 10 shapes per category (including airplanes and chairs) and edit the surface color to make it visually similar to reference shapes with same geometry yet different surface color. The editing quality is measured by the similarity between the renderings of the edited 3D shapes and the reference shapes. Tab. 3 reports the PSNR and LPIPS [61] metrics of the evaluation. The surface color of 3D shapes is much closer to the reference after editing, compared to the initial shapes, suggesting the effectiveness of our MM-VAD model in editing color via scribbles.",
                        "A similar task has recently been explored in EditNeRF [36]. However, an apple-to-apple comparison with EditNeRF is not possible due to the intrinsically different 3D representations (NeRF [38] vs SDFs [40]). Moreover, the proposed MM-VADs are generative models while EditNeRF is non-generative; The MM-VADs bridge 2D and 3D via shared latent spaces while EditNeRF relies on differentiable rendering. We present more detailed comparisons in the appendix. We provide qualitative comparisons with EditNeRF on chairs with similar structures using their pre-trained models. Fig. 6 shows that the color editing from MM-VADs is on par with EditNeRF. The MM-VADs achieve the editing via simple latent optimization (Eqn. 12), while EditNeRF requires updating the network weights per instance and fails to generate meaningful color editing results via optimizing the color code alone. Furthermore, MM-VADs take 0.06 seconds per edit and 6.78 seconds to render our 3D shapes into 256 \u00d7 256 RGB images, while EditNeRF takes over a minute per edit including rendering. Table 3. Quantitative results of editing 3D via 2D scribbles. We edit the surface color of 3D shape based on reference shapes, and report the similarity between the editing results and the target (bottom row). As a reference, we also report the metrics before editing (top row).   In comparison, the encoder-decoder networks [20] trained on full-view sketches are not robust to the domain shift induced by occlusion and unable to provide multiple 3D shapes given partial views. Notice that the predictions of surface color is not available in the encoder-decoder networks from the prior work [20]."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Methods",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Cross-Modal Shape Generation",
                    "paragraphs": [
                        "Single-View and Partial-View Shape Reconstruction. Fig. 7 compares the performance of our model and the encoder-decoder networks [20] under different occlusion ratios in the lower part of the objects in 2D views. The proposed model only has a slight performance drop as the occluded parts increase (Fig. 7a), mainly because of the ambiguity of 3D reconstruction given partial views. In fact, our reconstructions results fit the partial views quite well. Even though our model performs slightly worse than the encoder-decoder networks on full-view inputs, the proposed model is more robust to the input domain shift. This is because compared to task-specific training, our model achieves a better trade-off between reconstruction accuracy and domain generalization. More interestingly, our model can achieve diverse and reasonable 3D reconstruction by sampling different initialization for latent optimization (Fig. 7b). Few-Shot Shape Generation. The proposed method is able to adapt the pre-trained multi-modal generative model with as few as 10 training samples of a specific 2D modality. Fig. 8 presents some of the few-shot cross-modal shape Table 4. Quantitative results of few-shot cross-modal shape generation. We report Frechet Inception Distance (FID) (lower is better ) and classification error (Cls. Err) (lower is better ). We effectively adapt the pretrained multi-modal VAD model using a few 2D images to a desired 3D shape generator. As a reference, we report the metrics before the few-shot adaptation (top row). generation results. To quantitatively evaluate the few-shot shape generation performance, we render the 3D shapes into 2D RGB images and report the Frechet Inception Distance (FID) scores [24] between the rendered images and the ground-truth samples. Since the FID score is not sensitive to the semantic difference between two image sets, we also report the classification error on the random samples from the model before and after the adaptation. Specifically, we train a binary image classifier to identify the target image categories (e.g., armchairs vs. other chairs), and we run the trained classifier on the 2D renderings of the 3D samples before and after the adaptation. As presented in Tab. 4, our pre-trained generative model can be effectively adapted to a certain shape subspace given as few as 10 2D examples. This capability allows us to agilely adapt our generative model to a subspace defined by a few unlabelled samples, so that users can easily narrow down the target shape during the manipulation by providing a few samples of a common attribute, such as a specific category, style, or color. We are unaware of any prior works that can tackle this task in the literature. The 2D examples used to adapt the pre-trained generative model are provided in our appendix. Shape and Color Transfer. Transferring shape and color across different 3D instances can be achieved by simply swapping the latent codes. Fig. 9 shows that the shape and color are well disentangled in the proposed generative model. The  transfer results also are semantically meaningful, i.e., the color is only transferred across the same semantic parts (e.g., seats for the chair, wings for the airplane) even though the geometry of the source and target instances are quite different."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Stage",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Case Study on Real Images",
                    "paragraphs": [
                        "The workflow of 3D designers usually starts by drawing a 2D sketch to portray the coarse 3D geometry and then colorizes the sketch to depict the 3D appearance. These 2D arts are used as a reference to build 3D objects. Undoubtedly this procedure requires extensive human efforts and expertise. Such tasks can be automated with our MM-VADs. As shown in Fig. 10, we first reconstruct the 3D shape from a hand-drawn sketch. We then assign a surface color by randomly sampling a color code from the latent space of the MM-VADs, which can be easily edited by drawing color scribbles on the surface. Our model does not require any re-training on each of these steps and provides a tool to conduct shape generation and color editing consecutively. Such a task is infeasible with the existing works that train an encoder-decoder network to predict 3D shape from sketch [20]."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Discussion",
            "paragraphs": [
                "We propose a multi-modal generative model which bridges multiple 2D and 3D modalities through a shared latent space. One limitation of the proposed method is that we are only able to provide editing results in the prior distribution of our generative model (see appendix for more details). Despite this limitation, our model has enabled versatile cross-modal 3D generation and manipulation tasks without the need of re-training per task and demonstrates strong robustness to input domain shift. multiple trials as the final results of the latent optimization. We find this simple strategy significantly stabilizes our model in the 3D reconstruction task. For example, the mean Chamfer distance decreases from 5.50 to 1.73 in the task of 3D reconstruction from single-view sketch on ShapeNet airplanes and from 9.10 to 4.70 on ShapeNet chairs. In 3D shape manipulation, the latent optimization starts from a known latent code corresponding to the target shape to be edited, and we only run the latent optimization once."
            ],
            "subsections": []
        },
        {
            "title": "C Baselines",
            "paragraphs": [
                "Here we present more details about the baselines used in our experiments.",
                "-Encoder-Decoder Networks [20,43]. This model is originally designed for predicting 3D shapes from sketches, followed by a shape refinement step based on differentiable rendering. We re-purpose this model to reconstruct 3D shapes from RGB images by simply modifying the input channels in the first convolutional layer. We use the official implementations with default hyperparameter settings3 . -EditNeRF [36] edits a conditional radiance field representation of 3D scenes with sparse scribbles as input. The shape and color of 3D objects are edited by updating the neural network weights. We make qualitative comparisons with the EditNeRF using their pre-trained models4 . Our model shares many similarities with EditNeRF (e.g., network architecture, scribble-based interaction). However, the proposed model is significantly different from EditNeRF in terms of shape representation (SDFs [40] vs NeRF [38]), shape manipulation method (latent optimization vs network fine-tuning), and the way to bridge the 3D and 2D modalities (shared latent spaces vs differentiable rendering).",
                "Tab. 5 provides detailed comparisons between EditNeRF and our model. We train the proposed multi-modal variational auto-decoders (MM-VADs) on the ShapeNet dataset [7]. The training and testing split is the same as DeepSDF [40] and DualSDF [22]. We use the same pre-trained MM-VADs throughout our experiments. For airplanes, there are 1780 shapes for training and 456 shapes for testing. For chairs, there are 3281 training shapes and 833 testing instances. For 3D shape manipulation, we present the results on known shapes (i.e., shapes from training data), similar to EditNeRF [36] and DualSDF [22].",
                "D.2 3D reconstruction from Sketch or RGB modalities.",
                "Table 6 presents quantitative evaluations of the 3D reconstruction from sketch and RGB inputs under different occlusion ratios, corresponding to the curves in Fig. 7 in the main manuscript. We report results on both vertically and horizontally occluded inputs. Since 3D shapes and their 2D views are generally symmetric horizontally, the proposed model has almost no performance drop when masking out the right-half regions of the inputs. In comparison, the encoder-decoder networks [20] that is trained on full-view inputs suffers from the input domain shift induced by the occlusion."
            ],
            "subsections": []
        },
        {
            "title": "D.3 Few-shot 3D Generation",
            "paragraphs": [
                "Fig. 12 presents the 2D examples used in our few-shot shape generation experiments. For each category (e.g., armchair), we randomly sample 10 images from our training data. We then adapt a pre-trained MM-VAD using these 2D Table 6. Quantitative results of single-view reconstruction. We report the average Chamfer Distance (30, 000 points) multiplied by 10 3 between the reconstructed 3D shapes and the groundtruth (lower is better ). The performance of the proposed model is slightly worse than the encoder-decoder networks [20] trained on the full-view inputs. However, MM-VADs perform more robustly to the input domain shift (e.g., only partial view of input is available). The first column presents the occlusion rate in the input, where \"Full\" means no occlusion in the input, \"1/2-horizontal\" the left half of the input is visible, and \"3/4-vertical\" the top 3/4 region of the object is available. Superscripts in the last row denote the performance drop under the input domain shift (lower is better ). This table corresponds to Fig. 7 "
            ],
            "subsections": []
        },
        {
            "title": "E Limitations",
            "paragraphs": [
                "3D reconstruction from 2D modalities. The proposed model fails to reconstruct fine structures of 3D shapes from sketches or RGB views, for example, the holes on the back of chairs (Fig. 13a, b, g, h), fine textures on the seat of chairs (Fig. 13e), or the wheelbase of desk chairs (Fig. 13c,f). The capability of modeling fine structures is mainly determined by the 3D shape representation (i.e., SDFs [40]), training samples of SDFs, and the capacity of the proposed generative model. This issue can be potentially relieved by sampling more 3D training points surrounding the surface or increasing the capacity of the proposed model (e.g., enlarging the dimension of the latent space, increasing the depth of 3D shape networks)",
                "3D manipulation with 2D color scribble. Similar to GAN-based image manipulation models [65,19,5,39], we are only able to provide editing results within the prior distribution of a pre-trained MM-VAD. For example, in the task of  editing shape with color scribbles, if there are multiple scribbles of different colors on the same part of a shape (e.g., the seat of a chair), our model either edits the shape based on one of the scribbles or generates a surface color that is completely different from all scribbles, as shown in Fig. 14. We notice that the editing results of EditNeRF [36] are similar to ours based on their released demo5 . Our model may produce unexpected color editing results, for example, the edited 3D surface color may not match the 2D color scribbles provided by the user (Fig. 14d), probably due to bad initialization of the latent code or suboptimal hyperparameter settings. The multi-trial latent optimization described in Sec. B may relieve this issue.",
                "3D manipulation via 2D sketch. In this task, the major issue is that editing one part of a shape usually leads to changes in other parts. For example, removing the engines on the wing of airplanes results in new engines on the tail in many cases, as shown in Fig. 3 in the main text. Fig. 15 in this section provides more examples. This is mainly because editing shapes via latent optimization can only produce new shapes in the prior distribution of the generative model. It is potentially useful to add more constraints upon the latent optimization, e.g., enforcing the output of the 2D sketch generator to be as similar as possible to the original sketch. However, our preliminary experiments show that the latent optimization with such constraint typically under-fits the edited parts of the sketch and fails to achieve desired edits in 3D shape. In addition, the proposed model fails to add more complicated structures into the shape, for example, adding holes onto the back of chairs (Fig. 15c). We will investigate these issues further in our future work. Few-shot shape generation. We are unable to adapt a pre-trained MM-VAD to generate shapes of fine-grained categories (e.g., single-engine airplanes) using a few 2D RGB images. We also fail to adapt a pre-trained MM-VAD using a few 2D sketches. We hypothesize that this is because the discriminator is trained from scratch and unable to learn discriminative representations among fine-grained categories or sparse inputs (e.g., sketches) with limited 2D examples. These issues may be relieved by initializing the discriminator with a pre-trained classifier. We leave this in our future work.",
                "F Diverse color scribbles. "
            ],
            "subsections": []
        },
        {
            "title": "Appendix A Implementation Details",
            "paragraphs": [
                "The implementation details of 3D shape and color networks are included in the main text. Here we provide additional implementation details.",
                "-Joint latent space. The shape and color latent codes are both of dimension 128 throughout our experiments. We observe that lower-dimensional latent codes (e.g., 32) lead to worse shape reconstruction. -2D sketch and renderings. The image resolution of all 2D modalities is set to 128 \u00d7 128. We use the generator architecture from DCGAN [42] for all 2D modalities. -Few-shot shape generation. We use the discriminator from DCGAN [42].",
                "The mapping function h w (z) in the MineGAN framework [55] is a two-layer MLP with batch normalization [25] and a ReLU activation function. -Latent optimization. In the task of shape and appearance manipulation, we conduct the latent optimization for 5 steps starting from a known initial latent code that corresponds to the initial 2D and 3D instances. The hyperparameter \u03b3 and \u03b2 in Eqn.11 is 0.02 and 0.5 respectively by default. For the single-view shape generation tasks, we run multiple trials of latent optimization from different randomly sampled latent codes. The optimized code with minimal reconstruction loss is used as the final result. We observe that such multi-trial optimization significantly stabilizes the performance of 3D reconstruction (see Sec. B for more details)."
            ],
            "subsections": []
        },
        {
            "title": "B Ablation study",
            "paragraphs": [
                "The latent optimization is crucial to the performance of our shape reconstruction and manipulation tasks. In this section, we provide ablation studies on the regularization loss (Eqn. 10 in the main text) and the multi-trial latent optimization method (as described in Sec. A)."
            ],
            "subsections": []
        }
    ]
}