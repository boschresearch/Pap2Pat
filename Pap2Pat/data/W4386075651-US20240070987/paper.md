# Introduction

Stylized 3D characters , such as those in Fig. 1, are commonly used in animation, movies, and video games. Deforming these characters to mimic natural human or animal poses has been a long-standing task in computer graphics. Different from the 3D models of natural humans and animals, stylized 3D characters are created by professional artists through imagination and exaggeration. As a result, each stylized character has a distinct skeleton, shape, mesh arXiv:2306.00200v1 [cs.CV] 31 May 2023 topology, and usually include various accessories, such as a cloak or wings (see Fig. 1). These variations hinder the process of matching the pose of a stylized 3D character to that of a reference avatar, generally making manual rigging a requirement. Unfortunately, rigging is a tedious process that requires manual effort to create the skeleton and skinning weights for each character. Even when provided with manually annotated rigs, transferring poses from a source avatar onto stylized characters is not trivial when the source and target skeletons differ. Automating this procedure is still an open research problem and is the focus of many recent works [2,4,24,53]. Meanwhile, non-stylized 3D humans and animals have been well-studied by numerous prior works [35,41,55,63,70]. A few methods generously provide readily available annotated datasets [11,12,42,70], or carefully designed parametric models [41,52,70]. By taking advantage of these datasets [12,42], several learningbased methods [7, 14,35,63,69] disentangle and transfer poses between human meshes using neural networks. However, these methods (referred to as "part-level" in the following) carry out pose transfer by either globally deforming the whole body mesh [14,22,48,69] or by transforming body parts [35,49], both of which lead to overfitting on the training human meshes and fail to generalize to stylized characters with significantly different body part shapes. Interestingly, classical mesh deformation methods [56,57] (referred to as "local" in the following) can transfer poses between a pair of meshes with significant shape differences by computing and transferring per-triangle transformations through correspondence. Though these methods require manual correspondence annotation between the source and target meshes, they provide a key insight that by transforming individual triangles instead of body parts, the mesh deformation methods are more agnostic to a part's shape and can generalize to meshes with different shapes.

We marry the benefits of learning-based methods [7, 14,35,63,69] with the classic local deformation approach [56] and present a model for unrigged, stylized character deformation guided by a non-stylized biped or quadruped avatar. Notably, our model only requires easily accessible posed human or animal meshes for training and can be directly applied to deform 3D stylized characters with a significantly different shape at inference. To this end, we implicitly operationalize the key insight from the local deformation method [56] by modeling the shape and pose of a 3D character with a correspondence-aware shape understanding module and an implicit pose deformation module. The shape understanding module learns to predict the part segmentation label (i.e., the coarse-level correspondence) for each surface point, besides representing the shape of a 3D character as a latent shape code. The pose deformation module is conditioned on the shape code and deforms individual surface point guided by a target pose code sampled from a prior pose latent space [51]. Furthermore, to encourage realistic deformation and generalize to rare poses, we propose a novel volume-based test-time training procedure that can be efficiently applied to unseen stylized characters.

During inference, by mapping biped or quadruped poses from videos, in addition to meshes to the prior pose latent space using existing works [32,52,54], we can transfer poses from different modalities onto unrigged 3D stylized characters. Our main contributions are:

• We propose a solution to a practical and challenging task -learning a model for stylized 3D character deformation with only posed human or animal meshes. 

# Related Work

Deformation Transfer. Deformation transfer is a longstanding problem in the computer graphics community [3,6,8,9,56,66]. Sumner et al. [56] apply an affine transformation to each triangle of the mesh to solve an optimization problem that matches the deformation of the source mesh while maintaining the shape of the target mesh. Ben-Chen et al. [9] enclose the source and target shapes with two cages and transfer the Jacobians of the source deformation to the target shape. However, these methods need tedious human efforts to annotate the correspondence between the source and target shapes. More recently, several deep learning methods are developed to solve the deformation transfer task. However, they either require manually providing the correspondence [67] or cannot generalize [14,22,69] to stylized characters with different shapes. Gao et al. [22] propose a VAE-GAN based method to leverage the cycle consistency between the source and target shapes. Nonetheless, it can only work on shapes used in training. Wang et al. [63] introduce conditional normalization used in style transfer for 3D deformation transfer. But the method is limited to clothed-humans and cannot handle the large shape variations of stylized characters.

We argue that these learning-based methods cannot generalize to stylized characters because they rely on encoding their global information (e.g., body or parts), which is different from traditional works that focus on local deformation, e.g., the affine transformation applied to each triangle in [56]. Using a neural network to encode the global information easily leads to overfitting. For example, models trained on human meshes cannot generalize to a stylized humanoid character. At the same time, early works only focus on local information and cannot model global information such as correspondence between the source and target shapes, which is why they all need human effort to annotate the correspondence. Our method tries to learn the correspondence and deform locally at the same time.

Skeleton-based Pose Transfer. Besides mesh deformation transfer, an alternative way to transfer pose is to utilize skeletons. Motion retargeting is also a common name used for transferring poses from one motion sequence to another. Gleicher et al. [24] propose a space-time constrained solver aiming to satisfy the kinematics-level constraints and to preserve the characters' original identity. Following works [5,19,33] try to solve inverse-kinematics or inverse rate control to achieve pose transfer. There are also dynamics-based methods [4, 60] that consider physics during the retargeting process. Recently, learning-based methods [20,27,38,61,62] train deep neural networks to predict the transformation of the skeleton. Aberman et al. [2] propose a pooling-based method to transfer poses between meshes with different skeletons.

All these works highly rely on the skeleton for pose transfer. Other works try to estimate the rigging of the template shape [7, 40,53,64,65] when a skeleton is not available. But if the prediction of the skinning weights fails, the retargeting fails as well. Liao et al. [37] propose a model that learns to predict the skinning weights and pose transfer jointly using ground truth skinning weights and paired motion data as supervision, which limits the generalization of this method to categories where annotations are more scarce compared to humans (e.g., quadrupeds). Instead, our method uses posed human or animal meshes for training and deforms stylized characters of different shapes at inference. Implicit 3D shape representation. Implicit 3D shape representations have shown great success in reconstructing static shapes [13,16,18,21,23,29,43,44,50] and deformable ones [10,28,34,[45][46][47][48][49]59]. DeepSDF [50] proposes to use an MLP to predict the signed distance field (SDF) value of a query point in 3D space, where a shape code is jointly optimized in an auto-decoding manner. Occupancy flow [46] generalizes the Occupancy Networks [43] to learn a temporally and spatially continuous vector field with a Neu-ralODE [15]. Inspired by parameteric models, NPMs [48] disentangles and represents the shape and pose of dynamic humans by learning an implicit shape and pose function, respectively. Different from these implicit shape representation works that focus on reconstructing static or deformable meshes, we further exploit the inherent continuity and locality of implicit functions to deform stylized characters to match a target pose in a zero-shot manner.

# Method

We aim to transfer the pose of a biped or quadruped avatar to an unrigged, stylized 3D character. We tackle this problem by modeling the shape and pose of a 3D character using a correspondence-aware shape understanding module and an implicit pose deformation module, inspired by classical mesh deformation methods [56,57]. The shape understanding module (Sec. 3.1, Fig. 2) predicts a latent shape code and part segmentation label of a 3D character in rest pose, while the pose deformation module (Sec. 3.2, Fig. 3) deforms the character in the rest pose given the predicted shape code and a target pose code. Moreover, to produce natural deformations and generalize to rare poses unseen at training, we introduce an efficient volume-based test-time training procedure (Sec 3.3) for unseen stylized characters. All three modules, trained only with posed, unclothed human meshes, and unrigged, stylized characters in a rest pose, are directly applied to unseen stylized characters at inference. We explain our method for humans, and describe how we extend it to quadrupeds in Sec. 4.6.

## Correspondence-Aware Shape Understanding

Given a 3D character in rest pose, we propose a shape understanding module to represent its shape information as a latent code, and to predict a body part segmentation label for each surface point.

To learn a representative shape code, we employ an implicit auto-decoder [48,50] that reconstructs the 3D character taking the shape code as input. During training, we jointly optimize the shape code of each training sample and the decoder. Given an unseen character (i.e., a stylized 3D character) during inference, we obtain its shape code by freezing the decoder and optimizing the shape code to reconstruct the given character. Specifically, as shown in Fig. 2, given the concatenation of a query point x ∈ R 3 and the shape code s ∈ R d , we first obtain an embedding e ∈ R d via an MLP denoted as F. Conditioned on the embedding e, the occupancy ôx ∈ R of x is then predicted by another MLP denoted as O. The occupancy indicates if the query point x is inside or outside the body surface and can be supervised by the ground truth occupancy as:

where o x is the ground truth occupancy at point x.

Since our shape code eventually serves as a condition for the pose deformation module, we argue that it should also capture the part correspondence knowledge across different instances, in addition to the shape information (e.g., height, weight, and shape of each body part). This insight has been utilized by early local mesh deformation method [56], which explicitly utilizes correspondence to transfer local transformations between the source and target meshes. Our pose deformation process could also benefit from learning part correspondence. Take the various headgear, hats, and horns on the stylized characters's heads in Fig. 1 as an example. If these components can be "understood" as extensions of the character's heads by their shape codes, they will move smoothly with the character's heads during pose deformation. Thus, besides mesh reconstruction, we effectively task our shape understanding module with an additional objective: predicting part-level correspondence instantiated as the part segmentation label. Specifically, we propose to utilize an MLP P to additionally predict a part label p x = (p 1 x , ..., p K x ) T ∈ R K for each surface point x. Thanks to the densely annotated human mesh dataset, we can also supervise part segmentation learning with ground truth labels via:

where K is the total number of body parts, and 1 k x = 1 if x belongs to the k th part and 1 k x = 0 otherwise. To prepare the shape understanding module for stylized characters during inference, besides unclothed human meshes, we also include unrigged 3D stylized characters in rest pose during training. These characters in rest pose are easily accessible and do not require any annotation. For shape reconstruction, Eq. 1 can be similarly applied to the stylized characters. However, as there is no part segmentation annotation for stylized characters, we propose a selfsupervised inverse constraint inspired by correspondence learning methods [17,39] to facilitate part segmentation prediction on these characters. Specifically, we reconstruct the query point's coordinates from the concatenation of the shape code s and the embedding e through an MLP Q and add an auxiliary objective as:

(

Intuitively, for stylized characters without part annotation, the model learned without this objective may converge to a trivial solution where similar embeddings are predicted for points with the same occupancy value, even when they are far away from each other, and belong to different body parts. Tab. 4 further quantitatively verifies the effectiveness of this constraint. Beyond facilitating shape understanding, the predicted part segmentation label is further utilized in the volume-based test-time training module which will be introduced in Sec. 3.3.

## Implicit Pose Deformation Module

Given the learned shape code and a target pose, the pose deformation module deforms each surface point of the character to match the target pose. In the following, we first describe how we represent a human pose and then introduce the implicit function used for pose deformation.

Instead of learning a latent pose space from scratch as in [37,48], we propose to represent a human pose by the corresponding pose code in the latent space of VPoser [52]. Our intuition is that VPoser is trained with an abundance of posed humans from the large-scale AMASS dataset [42]. This facilitates faster training and provides robustness to overfitting. Furthermore, human poses can be successfully estimated from different modalities (e.g., videos or meshes), and mapped to the latent space of VPoser by existing methods [32,52,54]. By taking advantage of these works, our model can be applied to transfer poses from various modalities to an unrigged stylized character without any additional effort. A few examples can be found in the supplementary.

To deform a character to match the given pose, we learn a neural implicit function M that takes the sampled pose code m ∈ R 32 , the learned shape code, and a query point x around the character's surface as inputs and outputs the offset (denoted as ∆x ∈ R 3 ) of x in 3D space. Given the densely annotated human mesh dataset, we directly use the ground truth offset ∆x as supervision. The training objective for our pose deformation module is defined as:

(4)

Essentially, our implicit pose deformation module is similar in spirit to early local mesh deformation methods [56] and has two key advantages compared to the partlevel pose transfer methods [22,37,63]. First, our implicit pose deformation network is agnostic to mesh topology and resolution. Thus our model can be directly applied to unseen 3D stylized characters with significantly different resolutions and mesh topology compared to the training human meshes during inference. Second, stylized characters often include distinct body part shapes compared to humans. For example, the characters shown in Fig. 1 include big heads or various accessories. Previous part-level methods [37] that learn to predict a bone transformation and skinning weight for each body part usually fail on these unique body parts, since they are different from the corresponding human body parts used for training. In contrast, by learning to deform individual surface point, implicit functions are more agnostic to the overall shape of a body part and thus can generalize better to stylized characters with significantly different body part shapes. Fig. 4 and Fig. 6 show these advantages.

## Volume-based Test-time Training

The shape understanding and pose deformation modules discussed above are trained with only posed human meshes and unrigged 3D stylized characters in rest pose. When applied to unseen characters with significantly different shapes, we observe surface distortion introduced by the pose deformation module. Moreover, it is challenging for the module to fully capture the long tail of the pose distribution. To resolve these issues, we propose to apply test-time training [58] and fine-tune the pose deformation module on unseen stylized characters.

To encourage natural pose deformation, we further propose a volume-preserving constraint during test-time training. Our key insight is that preserving the volume of each part in the rest pose mesh during pose deformation results in less distortion [35,63]. However, it is non-trivial to compute the precise volume of each body part, which can have complex geometry. Instead, we propose to preserve the Eu-clidean distance between pairs of vertices sampled from the surface of the mesh, as a proxy for constraining the volume. Specifically, given a mesh in rest pose, we randomly sample two points x c i and x c j on the surface within the same part c using the part segmentation prediction from the shape understanding module. We calculate the offset of these two points ∆x c i and ∆x c j using our pose deformation module and minimize the change in the distance between them by:

(5) By sampling a large number of point pairs within a part and minimizing Eq. 5, we can approximately maintain the volume of each body part during pose deformation.

Furthermore, in order to generalize the pose deformation module to long-tail poses that are rarely seen during training, we propose to utilize the source character in rest pose and its deformed shape as paired training data during testtime training. Specifically, we take the source character in rest pose, its target pose code, and its optimized shape code as inputs and we output the movement ∆x dr , where x dr is a query point from the source character. We minimize the L2 distance between the predicted movement ∆x dr and the ground truth movement ∆x dr ,

Besides the volume-preserving constraint and the reconstruction of the source character, we also employ the edge loss L e used in [25,37,63]. Overall, the objectives for the test-time training procedure are L T = λ v L v + λ e L e + λ dr L dr , where λ v , λ e , and λ dr are hyper-parameters balancing the loss weights.

# Experiments

## Datasets

To train the shape understanding module, we use 40 human meshes sampled from the SMPL [41] parametric model. We use both the occupancy and part segmentation label of these meshes as supervision (see Sec. 3.1). To generalize the shape understanding module to stylized characters, we further include 600 stylized characters from RigNet [64]. Note that we only use the rest pose mesh (i.e., occupancy label) of the characters in [64] for training. To train our pose deformation module, we construct paired training data by deforming each of the 40 SMPL characters discussed above with 5000 pose codes sampled from the VPoser's [51] latent space. In total, we collect 200,000 training pairs, with each pair including an unclothed human mesh in rest pose and the same human mesh in target pose.

After training the shape understanding and pose deformation modules, we test them on the Mixamo [1] dataset, which includes challenging stylized characters, and the MGN [11] dataset, which includes clothed humans. The characters in both datasets have different shapes compared to the unclothed SMPL meshes we used for training, demonstrating the generalization ability of the proposed method. Following [37], we test on 19 stylized characters, with each deformed by 28 motion sequences from the Mixamo dataset. For the MGN dataset, we test on 16 clothed characters, with each deformed by 200 target poses. Both the testing characters and poses are unseen during training.

For quadrupeds, since there is no dataset including largescale paired stylized quadrupeds for quantitative evaluation, we split all characters from the SMAL [70] dataset and use the first 34 shapes (i.e., cats, dogs, and horses) for training. We further collect 81 stylized quadrupeds in rest pose from the RigNet [64] to improve generalization of the shape understanding module. Similarly to the human category, we use occupancy and part segmentation supervision for the SMAL shapes and only the occupancy supervision for RigNet meshes. To train the pose deformation module, we deform each of the 34 characters in SMAL by 2000 poses sampled from the latent space of BARC [55], a 3D reconstruction model trained for the dog category. We quantitatively evaluate our model on the hippo meshes from the SMAL dataset, which have larger shape variance compared to the cats, dogs, and horses used for training. We produce the testing data by deforming each hippo mesh with 500 unseen target poses from SMAL [70]. We show qualitative pose transfer on stylized quadrupeds in Fig. 1.

## Implementation Details

We use the ADAM [30] optimizer to train both the shape understanding and pose deformation modules. For the shape understanding module, we use a learning rate of 1e -4 for both the decoder and shape code optimization, with a batch size of 64. Given a new character at inference time, we fix the decoder and only optimize the shape code for the new character with the same optimizer and learning rate. For the pose deformation module, we use a learning rate of 3e -4 with a batch size of 128. For test-time training, we use a batch size of 1 and a learning rate of 5e -3 with the ADAM optimizer. We set λ v , λ e , and λ dr (See Sec. 3.3) as 0.05, 0.01, and 1 respectively.

## Metrics and Baselines for Comparison

Metrics. We use Point-wise Mesh Euclidean Distance (PMD) [37,63] to evaluate pose transfer error. The PMD metric reveals pose similarity of the predicted deformation compared to its ground truth. However, as shown in Fig. 4, PMD can not fully show the smoothness and realism of the generated results. Thus, we adopt an edge length score (ELS) metric to evaluate the character's smoothness after the deformation. Specifically, we compare each edge's Our method achieves the lowest PMD with the highest ELS. We provide the performance of the SPT*(full) method, which uses more supervision than the other methods as a reference. Our method is even better or comparable to it.

length in the deformed mesh with the corresponding edge's length in the ground truth mesh. We define the score as

where E indicates all edges of the mesh, |E| is the number of the edges in the mesh. Vi and Vj are the vertices in the deformed mesh. V i and V j are the vertices in the ground truth mesh. For all the evaluation metrics, we scale the template character to be 1 meter tall, following [37].

Baselines.

We compare our method with Neural Blend Shapes (NBS) [35] and Skeleton-free Pose Transfer (SPT) [37]. NBS is a rigging prediction method trained on the SMPL and MGN datasets, which include naked and clothed human meshes with ground truth rigging information. For SPT, we show the results of two versions, one is trained only on the AMASS dataset, named SPT, which has a comparable level of supervision to our method. We also test the SPT*(full) version, which is trained on the AMASS, RigNet and Mixamo datasets, using both stylized characters' skinning weights as supervision and paired stylized characters in rest pose and target pose.

## Human-like Character Pose Transfer

We report the PMD metric on the MGN and Mixamo datasets in Tab. 1. We also include the performance of SPT*(full) for reference. On the MGN dataset which includes clothed humans, our method which is trained with only unclothed humans achieve the best PMD score than all baseline methods, including baselines trained with more supervision (i.e., the NBS [35] learned with clothed humans and the SPT*(full) [37] learned with skinning weight and paired motion data). For the stylized characters, our method outperforms the SPT baseline learned with a comparable amount of supervision and gets competitive results with the NBS [35] and SPT*(full) baseline trained with more supervision. Furthermore, when testing on the more challenging, less human-like characters (e.g., a mouse with a big head in Fig. 1), the baselines produce noticeable artifacts and rough surfaces, which can be observed in the qualitative comparisons in Fig. 4. We provide the PMD value for each character in the supplementary. We show the ELS score comparison of different methods on the MGN and Mixamo datasets in Tab. 1. For both clothed humans and stylized characters, our method can generate more realistic results which are consistent with the target mesh and achieves the best ELS score.

We visually compare our method and the baseline methods in Fig. 4 on the Mixamo dataset. Although NBS is trained with a clothed-human dataset, when testing on the human-like characters, it still fails on parts that are separate from the body such as the hair and the pants. When using only naked human meshes as supervision, SPT cannot generalize to challenging human-like characters, producing rough mesh surface with spikes.

Ours SPT [37] GT Table 2. Part prediction accuracy on Mixamo [1]. Our method achieves the best part segmentation accuracy.

## Part Understanding Comparison

As discussed in Sec. 3.1, part segmentation plays an important role in both shape understanding and pose deformation. Though NBS [35] and SPT [37] do not explicitly predict part segmentation label, they are both skinning weightbased methods and we can derive the part segmentation label from the predicted skinning weights. Specifically, by selecting the maximum weight of each vertex, we can convert the skinning weight prediction to part segmentation labels for the vertices. We compare our part prediction results with those derived from SPT and NBS. We report the part segmentation accuracy on the Mixamo datasets in Tab. 2 w/o inv w/o v Ours GT Figure 7. Qualitative comparison for ablation study. Removing the constraint (eq. 1) in shape understanding leads to wrong pose deformation results. The volume preserving loss (eq. 5) helps to maintain the identity, e.g., the thickness of the arms in first row.

Metric SPT [37] Ours Metric SPT [37] Ours PMD ↓ 10.28 8.28 ELS ↑ 0.28 0.86 Table 3. Comparison on Hippos from SMAL [70]. Our method achieves better pose transfer accuracy with more smooth results.

and visualize the part segmentation results in Fig. 5. Even trained with only part segmentation supervision of human meshes, our method can successfully segment each part for the stylized characters. On the contrary, SPT uses graph convolution network [31] to predict the skinning weights.

When training only with human meshes, it often fails to distinguish different parts. As shown in Fig. 5, it mixes up the right and left upper legs, and incorrectly classifies the shoulder as the head. Though NBS is trained with clothed humans, it always classifies human hair as the human body for characters from Mixamo. This is because that NBS uses the MeshCNN [26] as the shape encoder. As a result, it is sensitive to mesh topology and cannot generalize to meshes with disconnected parts (e.g., disconnected hair and head). Tab. 2 further quantitatively demonstrates that our method achieves the best part segmentation accuracy, demonstrating its ability to correctly interpret the shape and part information in stylized characters.

## Quadrupedal Pose Transfer Comparison

To further show the generalization ability of our method, we conduct experiments on quadrupeds. We report the PMD and ELS score of our method and the SPT [37] in Tab. 3. When testing on hippos with large shape gap from the training meshes, SPT has a hard time generalizing both in terms of pose transfer accuracy and natural deformation. While our method achieves both better qualitative and quantitative results. We visualize the qualitative comparisons in Fig. 6. SPT produces obvious artifacts on the hippo's mouth and legs, while our method achieves accurate pose transfer and maintains the shape characteristics of the original character at the same time. We provide more results in the supplementary. We also show the part segmentation results on stylized characters by our method in Fig. 8. Even for unique parts such as the hats and antlers, our method correctly assigns them to the head part.

## Ablation Study

To evaluate the key components of our method, we conduct ablation studies on the MGN dataset by removing the inverse constraint (Eq. 3) in the shape understanding module and the volume-preserving loss (Eq. 5) used during the test-time training produce, we name them as "ours w/o inv" and "ours w/o v" respectively. We report the PMD and ELS metrics in Tab. 4. The model learned without the inverse constraint or volume-preserving loss has worse PMD and ELS score than our full model, indicating the contribution of these two objectives. We also provide qualitative results in Fig. 7. We use red boxes to point out the artifacts. As shown in Fig. 7, our model trained without the inverse constraint produces less accurate pose transfer results. Moreover, adding the volume-preserving loss helps to maintain the character's local details such as the thickness of the arms.

# Conclusion

In this paper, we present a model that deforms unrigged, stylized characters guided by a biped or quadruped avatar. Our model is trained with only easily accessible posed human or animal meshes, yet can be applied to unseen stylized characters in a zero-shot manner during inference. To this end, we draw key insights from classic mesh deformation method and develop a correspondence-aware shape understanding module, an implicit pose deformation module and a volume-based test-time training procedure. We carry out extensive experiments on both the biped and quadruped category and show that our method produces more realistic and accurate deformation compared to baselines learned with comparable or more supervision.

# A. Evaluation Data Curation

Mixamo. Because the preprocessed Mixamo [1] testing sequences used in [37] are not publicly available, we follow the instructions in [37] and download the testing data from the Mixamo website [1]. In [37], 20 stylized characters and 28 motion sequences are used for evaluation. Among the 20 characters, the "liam" character is not publicly available on the Mixamo website, thus we evaluate our method and the baselines on the other 19 stylized characters. Moreover, some evaluation motions (e.g., "Teeter") include more than one motion sequence on the Mixamo website with the same name. However, it is not public information as to what exact sequences were used for evaluation in the prior work [37]. Thus, we download all motion sequences with the same name and randomly pick one for evaluation. Given a character in rest pose and the desired pose, we use the linear blend skinning algorithm to obtain the ground truth deformed mesh. We then compare the prediction from each method with the ground truth mesh by computing the PMD and ELS scores as discussed in Sec.4.3 in the main paper. For a fair comparison, all poses in the evaluation motion sequences are not used during training. All methods are evaluated using these collected testing pairs. MGN. We follow NBS [35] and download the MGN dataset 1 , which includes 96 clothed human characters. We use the same evaluation set (i.e., the last 16 human characters) as in NBS. To obtain the ground truth deformed characters, we sample 200 poses (unseen during training) and deform each of the 16 clothed characters using the Multi-Garment Net [11].

Pose code extraction from Mixamo characters. To obtain target poses from the Mixamo motion sequences, we apply a similar fitting procedure introduced in [36]. We optimize the SMPL parameters to minimize the L2 distance between the SMPL joints and the Mixamo joints. Different from [36], we also add a constraint to minimize the Chamfer distance between the SMPL shape vertices and the Mixamo shape vertices. Similarly as [54], we directly optimize the pose code in the VPoser's [51] latent space, instead of the parameters in SMPL. We fit the SMPL shape to the "marker man" character in Mixamo to get all the testing poses. 

# B. Implementation Details

Shape code computation. We use an off-the-shelf method2 that computes occupancy with "virtual laser scans" and does not require a watertight mesh. We sample 10,000 points in a unit space, which takes 2.35s on average. Then, we use the occupancy of each query point as supervision to optimize the shape code. We run 2,000 iterations with a batch size of 2,000 to get the shape code, which takes 3.41s on average. For each character, we only compute its shape code once and use it to transfer poses from different motion sequences. All the time cost reported in this supplementary was measured on a laptop with I7-11700h and a RTX 3060.

Detailed test-time training (TTT) procedure. Following the inference procedure in [37], TTT takes a stylized character in T-pose, and a source human character in T-pose and target pose as inputs. TTT finetunes the pose module to perform two tasks: a) the T-pose stylized character is deformed to the target pose, while being constrained by the self-supervised volume-preserving loss L v . b) the source human character in T-pose is deformed to the target pose, while being supervised by the ground truth human character in the target pose (L dr ). TTT further refines the results' smoothness and resemblance to driving poses. L dr helps the pose module understand and generalize to the target pose, rather than enforcing that the human and stylized character have similar offsets. TTT is carried out for each pair of stylized character and target pose. It is highly efficient and only requires fine-tuning the pose module for 20 iterations, which takes 18ms without batching. We can speed it up to 12ms for each pair with a batch size of 8.

# C. Baseline Methods Implementation

NBS [35]. We evaluate NBS using its publicly available code and pre-trained model 3 . NBS [35] takes the SMPL pose parameters as input, thus we feed the optimized SMPL parameters discussed above to NBS.

SPT [37]. To evaluate both SPT(full) and SPT on human-like stylized characters, we use the publicly available code4 and pre-trained models generously provided by the authors. For the quadruped category, we train and evaluate the SPT model using its public code on the dataset discussed in Sec.4.1 in the main paper. Specifically, we utilize the SMAL model [70] to produce motion pairs, including an animal mesh in rest pose and the desired pose. We also supervise SPT with the ground truth skinning weights from SMAL. Note that our model is trained and evaluated using the same quadruped dataset as SPT.

# D. Visualization

We provide more visualizations, including qualitative comparisons (Fig. 9), deformation results by using source poses from in-the-wild videos for both human-like (Fig. 10 and Fig. 11) and quadupeds (Fig. 12). To obtain the pose code from a video frame, we apply PyMAF [68] for human and BARC [55] for quadupeds. We provide more visualizations in the supplementary video.

# E. Limitation

Although our approach exhibits good generalization performance for bipedal and quadrupedal characters, modeling other categories whose poses are not being studied well remains difficult. Additionally, our method is unable to solve the articulation of hands and just treats them as rigid parts. Target Source NBS [35] SPT [37] Ours GT Figure 9. Qualitative comparisons on Mixamo [1].

# Source Results

Figure 10. Transferring poses from in-the-wild videos to stylized characters.

# Appendix

In this appendix, we introduce more details about the evaluation data curation procedure, the implementation of our method and the baseline methods, more qualitative results and the limitations of our method.

# Source Results

Figure 12. Transferring animal poses from in-the-wild videos to stylized quadrupedal characters.

