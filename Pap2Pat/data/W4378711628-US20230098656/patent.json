{
    "id": "US20230098656",
    "authors": [
        "Aonan Zhang",
        "Jiankai Sun",
        "Ruocheng Guo",
        "Taiqing Wang",
        "Xiaohui Chen"
    ],
    "title": "DATA SUBSAMPLING FOR RECOMMENDATION SYSTEMS",
    "date": "2022-11-28 00:00:00",
    "abstract": "The present disclosure describes techniques for improving data subsampling for recommendation systems. A user-item graph associated with training data may be constructed. An importance of user-item interactions may be estimated via graph conductance based on the user-item graph. An importance of the training data may be measured via sample hardness using a pre-trained pilot model. A subsampling rate may be generated based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model.",
    "sections": [
        {
            "title": "DESCRIPTION",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "BACKGROUND",
                    "paragraphs": [
                        "Machine learning models are increasingly being used across a variety of industries to perform a variety of different tasks. Such tasks may include making predictions or recommendations about data. Improved techniques for utilizing machine learning models are desirable."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "motivate machine learning"
                    ],
                    "num_characters": 269,
                    "outline_medium": [
                        "motivate machine learning"
                    ],
                    "outline_short": [
                        "motivate machine learning"
                    ]
                },
                {
                    "title": "DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS",
                    "paragraphs": [
                        "Recommendation systems may learn user preferences through user-item interactions (e.g., user clicks on various items). For example, a user click may be considered as a positive sample that indicates the user's interest in the clicked item. Conversely, if a user does not click on an item, such as a no-click may be considered as a negative sample. Clickthrough rate (CTR) prediction models may be configured to output click probabilities of user-item pairs, and such probabilities may be used to rank recommended items for a user in response to a user request. Such CTR models may be trained using data collected from online platforms, where \u201cno-click\u201d user-item pairs dominate. Due to this imbalance of the training dataset, negative sampling may be performed. Negative sampling may down-samples negative samples. Down-sampling negative samples may significantly reduce model training costs.",
                        "In embodiments, instead of treating all data points as equally important, non-uniform data subsampling aims to retain more informative samples (and ignore less informative samples). Previous techniques for non-uniform data subsampling utilize a pilot model to assess the importance of data. When the pilot model is correctly pre-trained, one can achieve an optimal sampling rate. Such pilot-model techniques may be configured to measure the importance of data using pilot prediction scores together with first and second-order derivatives of a loss function. As optimal negative sampling rates are proportional to the pilot model's prediction score, a high sampling rate may indicate an inaccurate model prediction. The sampling strategy may be interpreted as using hard negative samples (HNS).",
                        "Model-based sampling algorithms may not be applicable in real-life scenarios. A real-life recommendation system may deploy data subsampling in response to a user initiating a request to an online serving model. The user may receive recommendations returned by a server. If the user clicks a particular recommended item, then a positive instance associated with that recommended item may be collected. Otherwise, if the user does not click a particular recommended item within a period of time, a negative instance associated with that recommended item may be collected. A pilot prediction score and other statistics may be recorded in each instance to calculate a sampling rate for the data subsampling. All instances may be filtered by a data subsampling module before input/output (I/O) to reduce the I/O and network bandwidth bottleneck. One or more offline models may be trained with historical data before being deployed online. However, for model-based methods, because subsampling rates are determined by online models, such subsampling rates may be sub-optimal for offline training purposes. When the model trained offline using such subsampling rates is deployed online, data subsampling is affected. As a result, inconsistent subsampling rates may be produced.",
                        "In these real-life scenarios, two unavoidable obstacles to the application of model-based sampling exist. First, offline model training is vulnerable to model misspecification. Model misspecification may cause inferior results. Unfortunately, model misspecification is persistent due to an online-offline discrepancy, especially in continuous integration and deployment (Cl/CD) in real systems. Second, the coupling of data subsampling and model training introduces extra dependencies across system modules. Such extra dependencies may increase system maintenance cost and cause extra technical debt. Thus, techniques for improving data subsampling of recommendation systems are needed.",
                        "Described here are techniques for improving data subsampling of recommendation systems using model-agnostic data subsampling methods. The topology of a user-item graph may be used to estimate the importance of each user-item interaction (an edge in the user-item graph), such as via graph conductance. After the importance of each user-item interaction is estimated, a propagation step may be performed to smooth out the estimated importance values. As the techniques described herein are model agnostic, the merits of both model agnostic and model-based subsampling methods may be combined in certain embodiments.",
                        "FIG. 1 illustrates an example data subsampling system 100 that may be used in accordance with the present disclosure. The system 100 may comprise a user-item graph sub-system 104, a pilot model sub-system 106, a smoothing sub-system 108, and an ensemble sub-system 110. The system 100 may be configured to generate a subsampling rate that may be used to sample a set of training data for training an offline recommendation model.",
                        "The user-item graph sub-system 104 may be configured to receive a set of training data 102. The set of training data 102 may be associated with user-item interactions. The set of training data 102 may comprise a plurality of positive instances and a plurality of negative instances. For example, if a user interacted with (e.g., clicked) a particular item, then a positive instance associated with that item may be collected. Otherwise, if the user did not click a particular item within a period of time, a negative instance associated with that item may be collected.",
                        "For example, the system 100 may be configured to solve a binary classification problem, where D={(xn, yn)}n=1N is a training set (e.g., set of training data 102) of size N, and xn and yn are the feature vector and label of instance n, respectively. The generalized logistic regression (GLM) model, where the target model corresponds to the offline model (before deployment) may be represented as",
                        "\\({{{f\\left( {x;\\theta} \\right)}:} = {{p\\left( {{y = \\left. 1 \\middle| x \\right.},\\ \\theta} \\right)} = \\frac{1}{1 + e^{- {g({x;\\theta})}}}}},\\)",
                        "where the log-odd g(x; \u03b8) is implemented by a predictive model. N0 may denote the number of negative instances and N1=N\u2212N0 may denote the number of positive instances in the set of training data 102. The set of training data 102 may be imbalanced, in that the number of negative instances may greatly outnumber the number of positive instances (e.g., N0>>N1).",
                        "FIG. 2 shows the set of training data 102 in more detail. As shown in the example of FIG. 2, the set of training data 102 comprises five instances. The first three instances in the set of training data 102 are positive instances. For example, the first instance indicates that the user u1 clicked on the item v1. The second instance indicates that the user u2 clicked on item v1. The third instance indicates that the user u1 clicked on item v2. The last two instances in the set of training data 102 are negative instances. For example, the fourth instance indicates that the user u2 clicked on the item v2. The fifth instance indicates that the user u3 clicked on item v2. While the set of training data 102 shown in the example of FIG. 2 only includes five instances, it should be appreciated that the number of instances in the set of training data 102 may include a much larger number of instances, such as hundreds, thousands, or millions of instances. The number of negative instances in the set of training data 102 may greatly outnumber the number of positive instances in the set of training data 102.",
                        "Referring back to FIG. 1, as information is sparsely distributed over a large number of negative instances, negative sampling may be used to reduce the dataset size and to boost training efficiency. A negative sampling algorithm may be configured to weigh each negative instance with some measurement of its importance. For example, the measure of importance for a negative instance x may be represented as \u03c0(x). The importance for a negative instance may be used as the negative sampling rate of the negative instance.",
                        "In embodiments, the measure of importance may be assigned to a negative instance by exploiting \u201chard negative samples.\u201d Sampling rates may be proportional to non-negative hardness scores h(\u22c5):",
                        "\u03c0(xn) \u221d h(xn), s\u00b7t\u00b7\u03a3n=1N\u03c0(xn)=N0\u03b1\u2003\u2003Equation 1",
                        "where \u03b1 \u2208 (0, 1] is a pre-set average subsampling rate of negative instances. An online pilot model {tilde over (f)}(\u22c5) may equal f(\u22c5; \u03b8*). For example, the online pilot model may have the same functional form as the target model, and \u03b8* may be the true parameter. Thus, a model-based hardness score hb(xn)={tilde over (f)}(xn)=f(xn; \u03b8*) may be set to get a near-optimal sampling rate \u03c0(xn) by Equation 1. A negative instance predicted with a higher score by the pilot model {tilde over (f)}(\u22c5) is more \u201csurprising\u201d and thus is harder for the target model f(\u22c5; \u03b8). For a positive instance, \u03c0(x) may be used to denote its counterfactual negative sampling rate. The hard negative sampling procedure may be demonstrated as follows:",
                        "Since data distribution shifts after subsampling, the log odds may be corrected to get an unbiased estimation:",
                        "\\(\\begin{matrix}\n{\\overset{\\hat{}}{\\theta} = {\\underset{\\theta}{argmax}{\\sum\\limits_{n = 1}^{N}{\\delta_{n}\\left\\lbrack {{y_{n}g{\\pi\\left( {x_{n};\\theta} \\right)}} - {\\log\\left( {1 + e^{{g({x_{n};\\theta})} - l_{n}}} \\right.}} \\right\\rbrack}}}} & {{Equation}2}\n\\end{matrix}\\)",
                        "where \u03b4n \u2208 {0, 1} is the subsampling indicator and ln:=log \u03c0(xn). Log odds correction may be more efficient than the inverse probability weighting estimator. However, when the pilot model is misspecified, optimal negative sampling with pilot models may not be achieved. As described above, deploying model-based hard negative sampling methods may be error prone, as model misspecification problems persistently exist due to online-offline model discrepancy and continuous integration and deployment.",
                        "Thus, a model-agnostic hardness score ha(\u22c5) may be utilized to maintain a scalable and sustainable data subsampling service. In the binary classification problem described above, each feature xn=(ui, vj, cn), where cn represents context features and the label of instance n is yn. The model-agnostic hardness score ha(\u22c5) of negative samples may be determined without referring to a pilot model.",
                        "To determine the model-agnostic hardness ha(\u22c5) of negative samples without referring to a pilot model, sample hardness may be related to graph topology. The user-item graph sub-system 104 may be configured to generate a user-item bipartite graph based on the set of training data 102. The user-item bipartite graph may comprise two sets of nodes. One of the two sets of nodes may represent users and the other of the two sets of nodes may represent items. The user-item bipartite graph may comprise edges, with each edge representing interactions between a user node and an item node. For example, the user-item bipartite graph may be represented as (U, V, E), where the node set U={ui}i=1M represents M users, the node set V={vj}j=1Q represents Q items, and the edge set E={(ui, vj)}n=1N represents N user-item pairs. For each node pair n, yn \u2208 {0, 1} represents whether there is a positive interaction between uiand vj.",
                        "In embodiments, the user-item graph sub-system 104 may be configured to determine an effective conductance associated with the edges in the user-item bipartite graph. The edges of the user-item bipartite graph may be treated as instances for subsampling. In the context of the bipartite graph, hard negative sampling may be performed using the concept of effective conductance.",
                        "For example, the user-item bipartite graph may be imagined as an electricity network, where each edge (ui, vj) is a conductor with conductance G(ui, vj). The conductance measures the edge's ability to transfer \u201celectrical current.\u201d G(ui, vj) may be large when a user uiexpresses direct preference of item vj. In particular, G(ui, vj) may be set to equal yn. Thus, the conductance may be equal to one if there is a direct preference, and the conductance may be equal to zero if there is not a direct preference expressed. The effective conductance Geff(ui, vj) between uiand vjmay represent the network's ability to transfer \u201ccurrent\u201d from uito vj(or vice versa). The effective conductance Geff(ui, vj) is the reciprocal of effective resistance Reff(ui, vj). Geff(ui, vj) and Reff(ui, vj) may be defined as follows:",
                        "\\(\\begin{matrix}\n{{{R_{eff}\\left( {u_{i_{n}},v_{j_{n}}} \\right)} = {\\left( {{e\\left\\lbrack u_{i_{n}} \\right\\rbrack} - {e\\left\\lbrack v_{j_{n}} \\right\\rbrack}} \\right)^{T}{L^{+}\\left( {{e\\left\\lbrack u_{i_{n}} \\right\\rbrack} - {e\\left\\lbrack v_{j_{n}} \\right\\rbrack}} \\right)}}},{{G_{eff}\\left( {u_{i_{n}},v_{j_{n}}} \\right)} = \\frac{1}{R_{eff}\\left( {u_{i_{n}},v_{j_{n}}} \\right)}}} & {{Equation}3}\n\\end{matrix}\\)",
                        "where e[\u22c5] \u2208 {0, 1}M+Q is the one-hot encoding of a node in the user-item bipartite graph, and L+ is the pseudo inverse of the Laplacian of the user-item bipartite graph. If there are many conductible paths between uiand vj, then the effective conductance Geff(ui, vj) may be large.",
                        "FIG. 3 shows an example framework 300 for interpreting effective conductance on a bipartite graph 302. The bipartite graph 302 may comprise two sets of nodes. One set of nodes {u1, u2, u3} may represent users and the other set of nodes {v1, v2} may represent items. The user-item bipartite graph may comprise edges (labeled 1-5), with each of the five edges representing interactions between a user node and an item node. For example, the edge labeled 1 represents an interaction between user node u1 and item node v1. The edge labeled 2 represents an interaction between user node u2 and item node v1. The edge labeled 3 represents an interaction between user node u1 and item node v2. The edge labeled 4 represents an interaction between user node u2 and item node v2. The edge labeled 5 represents an interaction between user node u3 and item node v1. The positive edges (labeled 1-3) may indicate that the corresponding user clicked on the corresponding item, while the negative edges (labeled 4-5) may indicate that the corresponding user did not click on the corresponding item (such as within a certain time frame).",
                        "The effective conductance associated with each edge in the user-item bipartite graph 302 may be determined. The table 304 of FIG. 3 shows the effective conductance associated with each of the five edges in the user-item bipartite graph 302. For example, the table 304 shows that the edge labeled 1 has an effective conductance of 0.18. The edge labeled 2 has an effective conductance of 0.21. The edge labeled 3 has an effective conductance of 0.68. The edge labeled 4 has an effective conductance of 0.39. The edge labeled 5 has an effective conductance of 0.12.",
                        "For example, each of the positive edges may be assigned a conductance G=1, and all negative edges may be assigned a conductance G=0. The user-item pair (u2, v2) may have an effective conductance Geff(u2, v1)=\u2153 and the user-item pair (u3, v2) may have an effective conductance Geff(u3, v2)=0. Effective conductance may demonstrate user preference. A 3-hop path exists between u2 and v2 (u2\u2192v1\u2192u1\u2192v2), but no path exists between u3 and v2. Thus, u3 may prefer v2 more than u2 prefers v2. The user item pair (u2, v2), which is represented by a negative edge, may correspond to a harder negative sample than (u3, v2), which is also represented by a negative edge.",
                        "Referring back to FIG. 1, sample hardness may be estimated via effective conductance. Effective conductance positively relates to sample hardness. The hardness score may be defined as:",
                        "ha(xn)=Geff(ui, vj)\u2212G(ui, vj) \u2003\u2003Equation 4",
                        "for a negative sample, G(ui, vj)=0. The effective conductance Geff(ui, vj) may be high when there are multiple high-conductance paths from uito vj, demonstrating a user's indirect preference to the item. When the indirect preference is high but (ui, vj) turns out to be negative, the instance may be identified as a hard negative sample. For a positive sample, ha(xn) denotes its counterfactual hardness score by subtracting the direct conductor G(ui, vj) from Geff(ui, vj) to eliminate the prior information given by the label. The hardness score may be used to calculate the counterfactual negative sampling rate for log odds correction in Equation 2. Positive samples may not be dropped.",
                        "In embodiments, the direct calculation of effective conductance may be time-consuming. Instead of directly calculating effective conductance, the commute time distance comm(u, v) may first be approximated through random walk using scientific computing tools. Then the transformation Geff(u, v)=2|E|/comm(u, v) may be used to convert the commute time into effective conductance.",
                        "Some hard instances may be overlooked by model-agnostic methods. For example, estimating sample hardness via effective conductance in the manner described above (e.g., a model-agnostic method) may cause some hard instances to be overlooked. The hard instances that may be overlooked by model-agnostic methods may be captured by model-based methods, such as by a pre-trained pilot model. In embodiments, the pilot model sub-system 106 may be configured to determine a pilot prediction as a hardness score. In the example framework 400 depicted in FIG. 4, the pilot model sub-system 106 may cause a pre-trained pilot model 402 to generate a pilot prediction as a hardness score for each user-item pair. The table 404 of FIG. 4 shows the pilot prediction as a hardness score for each user-item pair in the set of training data 102. For example, the table 404 shows that the user-item pair (u1, v1) has a pilot hardness score of 0.24. The user-item pair (u2, v1) has a pilot hardness score of 0.96. The user-item pair (u1, v2) has a pilot hardness score of 0.41. The user-item pair (u2, v2) has a pilot hardness score of 0.18. The user-item pair (u3, v2) has a pilot hardness score of 0.29.",
                        "Referring back to FIG. 1, in embodiments, the smoothing subsystem 108 may be configured to smooth hardness scores. The smoothing subsystem 108 may be configured to smooth hardness scores associated with both model-agnostic and model-based methods. The smoothing subsystem 108 may be configured to smooth hardness scores associated with both model-agnostic and model-based methods based on a line graph transformation of the user-item bipartite graph and graph propagation.",
                        "In embodiments, smoothing the hardness scores associated with the model-agnostic methods may comprise smoothing the hardness score associated with each of the negative instances. Smoothing the hardness score associated with each of the negative instances may comprise determining an average effective conductance associated with neighboring negative edges of each negative edge. Then, for each negative edge, a weighted sum of the average effective conductance and a corresponding effective conductance may be calculated. The weighted sum may be equal to the final model-agnostic hardness score for that negative edge.",
                        "As shown in the example framework 500 of FIG. 5, the smoothing subsystem 108 may generate a line-graph transformation 502 of the user-item bipartite graph (e.g., the graph 302). The smoothing subsystem 108 may use the line-graph transformation 502 of the user-item bipartite graph to smooth the model-agnostic hardness scores (e.g., the effective conductance scores shown in the table 304 of FIG. 3). The table 504 shows the smoothed model-agnostic hardness scores (e.g., final model-agnostic hardness scores) generated by the smoothing subsystem 108. Likewise, the smoothing subsystem 108 may use the line-graph transformation 502 of the user-item bipartite graph to smooth the model-based hardness scores (e.g., the pilot prediction as hardness scores shown in the table 404 of FIG. 4). The table 506 shows the smoothed model-based hardness scores (e.g., final model-based hardness scores) generated by the smoothing subsystem 108.",
                        "To smooth the hardness scores associated with both model-agnostic and model-based methods, the smoothing subsystem 108 may utilize graph propagation techniques. The edge effective conductance derived from the graph may be noisy, thus leading to an inaccurate estimation of hardness scores. Graph propagation may be used to smooth the hardness score. Edge propagation may be reduced to node propagation by transforming the user-item bipartite graph (U, V, E) into its corresponding line graph L(U, V, E)=(VL,EL), where VL=E and EL is the collection of edge pairs that share the same node.",
                        "In embodiments, the model-agnostic hardness scores may be smoothed by propagating uncertainty. Geff:=Geff(ui, vj)n=1N \u2208 RN and Y:=(yn)n=1N \u2208 {0,1}N may be denoted as the vector of the effective conductance scores and edge labels, respectively. The effective conductance Geff may be normalized as the estimated score Z and the uncertainty score B may be calculated as the absolute residual between Z and Y:",
                        "\\(\\begin{matrix}\n{{Z = \\frac{G_{eff} - {\\min\\left( G_{eff} \\right)}}{{\\max\\left( G_{eff} \\right)} - {\\min\\left( G_{eff} \\right)}}},{B = {{\u2758{Y - Z}\u2758}.}}} & {{Equation}5}\n\\end{matrix}\\)",
                        "The min-max normalization may be used to restrict the hardness score to be within the range [0, 1]. It may be denoted that S=DL\u2212\u00bdALDL\u2212\u00bd, where AL and DL are the adjacency matrix and the degree matrix of the line graph, respectively. The uncertainty may be smoothed by solving the following optimization problem:",
                        "\\(\\begin{matrix}\n{\\overset{\\hat{}}{B} = {{\\underset{W}{argmin}{{tr}\\left( {{W^{T}\\left( {I - S} \\right)}W} \\right)}} + {\\mu{{\uf605{W - B}\uf606}_{F}^{2}.}}}} & {{Equation}6}\n\\end{matrix}\\)",
                        "The first term in Equation 6 restricts the difference of uncertainties in neighboring nodes. The second term in Equation 6 constrains the smoothed uncertainty to be close to the initial uncertainty, with the coefficient \u03bc controlling the strength of the constraint. With the smoothed uncertainty vector {circumflex over (B)}, the hardness estimation may be corrected by reversing Equation 5.",
                        "{circumflex over (Z)}=Y+(\u22121)Y{circumflex over (B)}\u2003\u2003Equation 7",
                        "An iterative approximation approach may be used. If \u03b3=1/(1+\u03bc) and Bt+1=(1\u2212\u03b3)B+\u03b3SBt, B0=B, then Bt\u2192{circumflex over (B)} when t\u2192\u221e. However, this iterative approach is not scalable as the transformed line graph has |EL|=(\u03a3u\u2208UDeg(ui)2+\u03a3v\u2208VDeg(vj)2)/2\u2212N edges in total, where Deg(\u22c5) represents node degree. Alternatively, edge uncertainty may be directly propagated along the original graph (U, V, E), which only contains |E|=(\u03a3u\u2208UDeg(ui)+\u03a3v\u2208V Deg(vj)/2\u2212N edges in total. The propagation rule over edges is as follows:",
                        "\\(\\begin{matrix}\n{{{B_{n}^{t + 1} = {{\\left( {1 - \\gamma} \\right)B_{n}} + {\\gamma\\frac{{m^{t}\\left( u_{i_{n}} \\right)} + {m^{t}\\left( v_{j_{n}} \\right)} - {2Z_{n}}}{{De{g\\left( u_{i_{n}} \\right)}} + {De{g\\left( v_{j_{n}} \\right)}} - 2}}}},{where}}{{{m^{t}(u)} = {\\sum_{{n:u} = u_{i_{n}}}B_{n}^{t}}},{{m^{t}(v)} = {\\sum_{{n:v} = v_{j_{n}}}B_{n}^{t}}},{B_{n}^{0} = {B_{n}.}}}} & \n\\end{matrix}\\)",
                        "Using message passing mechanisms, the aggregated uncertainty mt(u) may be stored in u and then the uncertainty Bt+1 may be updated by applying the rule above.",
                        "In embodiments, the hardness scores associated with both model-agnostic and model-based methods may be smoothed by propagating scores. Instead of propagating uncertainty, we can directly propagate the scores {circumflex over (Z)} by iterating Zt+1=(1\u2212\u03b3) {circumflex over (Z)}+\u03b3SZt, Z0={circumflex over (Z)} until convergence. After obtaining the final hardness scores, the final hardness scores may be rescaled to match the average subsampling rates \u03b1.",
                        "In embodiments, the ensemble sub-system 110 may be configured to generate a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model. For example, the ensemble sub-system 110 may use both the final model-agnostic hardness scores and the final model-based hardness scores to determine a final subsampling rate. As shown in the example framework 600 of FIG. 6, the ensemble sub-system 110 may be configured to determine a maximum between the final model-agnostic hardness scores shown in the table 504 and the final model-based hardness scores shown in the table 506. For example, for each instance, the ensemble sub-system 110 may be configured to determine whether the final model-agnostic hardness score or the final model-based hardness score is greater. For the instance labeled \u201c1,\u201d the ensemble sub-system 110 may be configured to determine whether the final model-agnostic hardness score of 0.21 or the final model-based hardness score of 0.29 is greater. As the final model-based hardness score of 0.29 is greater, the final hardness score of 0.29 may be used to calculate the subsampling rate associated with the instance labeled \u201c1.\u201d The ensemble sub-system 110 may be configured to make such a determination for each instance. The ensemble sub-system 110 may be configured to generate the subsampling rate based on the final hardness score associated with each instance.",
                        "In embodiments, the final subsampling rate may be determined based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model. For example, given a sample x, both model-agnostic and model-based subsampling methods may be used to calculate their corresponding sampling rate \u03c0D(x) and \u03c0\u03d5(x) respectively. Particularly \u03c0\u03d5(x) is the subsampling rate for x by using a pre-trained pilot model hb(\u22c5):=({tilde over (f)}(\u22c5; \u03d5), such as the pre-trained pilot model 402. \u03c0D(x) may be the subsampling rate using model agnostic hardness score ha(\u22c5) in Equation 4:",
                        "\u03c0\u03d5(x) \u221d max (min(p\u03d5hb(x), \u03b5\u03d5, 1),",
                        "\u03c0D(x) \u221d max (min(pDha(x), \u03b5D, 1),",
                        "where (\u03b5\u03d5, \u03b5D) is the minimum sampling rate and (p\u03d5, pD) tunable linear scaling parameters to meet the average subsampling rate \u03b1.",
                        "In embodiments, three simple yet effective heuristic strategies may be used to combine the model-agnostic and model-based subsampling methods to generate a final sampling rate: maximum, mean, and product.",
                        "\u03c0max(x)=pmax max(\u03c0D(x), \u03c0\u03d5(x));",
                        "\u03c0mean(x)=(\u03c0D(x)+\u03c0\u03d5(x))/2;",
                        "\u03c0prod(x)=min(max(pprod\u03c0D(x)\u03c0\u03d5(x)), \u03b5prod), 1), \u2003\u2003Equation 8",
                        "where \u03b5prod is an extra hyperparameter used when applying product combination, and pmax and pprod are tuned to normalize the average sample rate to \u03b1. After subsampling rate combination, each x may be sampled with probability in Equation 8. Each of the sampled instances may follow the normal training protocol to optimize the training objective as shown in Equation 2, which guarantees the final result to be well-calibrated.",
                        "In embodiments, the final sampling rate may be used to subsample the negative instances in the training data 102. The negative instances in the training data 102 may be subsampled based on the final sampling rate. The subsampled negative instances and all of the positive instances in the training data 102 may collectively make up the final subsampled training set 112. FIG. 7 shows an example subsampled training set 112. As shown in the example of FIG. 7, the negative instance labeled \u201c5\u201d in the training data 102 is no longer present in the subsampled training set 112. Thus, the negative instance labeled \u201c5\u201d in the training data 102 was not selected during the subsampling process. The negative instance labeled \u201c4\u201d in the training data 102 and the positive instances labeled \u201c1-3\u201d in the training data 102 are still present in the subsampled training set 112.",
                        "In embodiments, an offline recommendation model may be trained using positive instances in the training data 102 and the subsampled negative instances. For example, the offline recommendation model 102 may be trained using the subsampled training set 112. The trained offline recommendation model may be deployed. The deployed offline recommendation model may be configured to recommend items to users.",
                        "FIG. 8 illustrates an example process 800 of improving data subsampling for recommendation systems. For example, the system 100 may perform the process 800. Although depicted as a sequence of operations in FIG. 8, those of ordinary skill in the art will appreciate that various embodiments may add, remove, reorder, or modify the depicted operations.",
                        "A set of training data may be associated with user-item interactions. The set of training data may comprise a plurality of positive instances and a plurality of negative instances. For example, if a user interacted with (e.g., clicked) a particular item, then a positive instance associated with that item may be collected. Otherwise, if the user did not click a particular item within a period of time, a negative instance associated with that item may be collected. The set of training data may be imbalanced, in that the number of negative instances may greatly outnumber the number of positive instances.",
                        "At 802, a user-item graph associated with the training data may be constructed. The user-item bipartite graph may comprise two sets of nodes. One of the two sets of nodes may represent users and the other of the two sets of nodes may represent items. The user-item bipartite graph may comprise edges, with each edge representing interactions between a user node and an item node. The positive edges may indicate that the corresponding user clicked on the corresponding item, while the negative edges may indicate that the corresponding user did not click on the corresponding item (such as within a certain time frame).",
                        "As information is sparsely distributed over the large number of negative instances, negative sampling may be used to reduce the dataset size and to boost training efficiency. A negative sampling algorithm may be configured to weigh each negative instance with some measurement of its importance. For example, the measure of importance for a negative instance x may be represented as \u03c0(x). The importance for a negative instance may be used as the negative sampling rate of the negative instance. At 804, the importance of user-item interactions may be estimated via graph conductance based on the user-item graph. Some hard instances may be overlooked by model-agnostic methods. The hard instances that may be overlooked by model-agnostic methods may be captured by model-based methods, such as by a pre-trained pilot model. At 806, the importance of the training data may be measured via sample hardness using a pre-trained pilot model. The pre-trained pilot model may generate a pilot prediction as a hardness score for each user-item pair in the training data.",
                        "In embodiments, the final subsampling rate may be determined based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model. At 808, a subsampling rate may be generated based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model. For example, given a sample x, both model-agnostic and model-based subsampling methods may be used to calculate their corresponding sampling rate \u03c0D(x) and \u03c0\u03d5(x) respectively. In embodiments, three simple yet effective heuristic strategies may be used to combine the model-agnostic and model-based subsampling methods to generate a final sampling rate: maximum, mean, and product. After subsampling rate combination, each x may be sampled with probability in Equation 8. Each of the sampled instances may follow the normal training protocol to optimize the training objective as shown in Equation 2, which guarantees the final result to be well-calibrated.",
                        "FIG. 9 illustrates an example process 900 of improving data subsampling for recommendation systems. For example, the system 100 may perform the process 900. Although depicted as a sequence of operations in FIG. 9, those of ordinary skill in the art will appreciate that various embodiments may add, remove, reorder, or modify the depicted operations.",
                        "In embodiments, the final subsampling rate may be determined based on an importance estimated from the user-item graph and an importance measured by the pre-trained pilot model. At 902, a subsampling rate may be generated based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model. For example, given a sample x, both model-agnostic and model-based subsampling methods may be used to calculate their corresponding sampling rate \u03c0D(x) and \u03c0\u03d5(x) respectively. In embodiments, three simple yet effective heuristic strategies may be used to combine the model-agnostic and model-based subsampling methods to generate a final sampling rate: maximum, mean, and product. After subsampling rate combination, each x may be sampled with probability in Equation 8. Each of the sampled instances may follow the normal training protocol to optimize the training objective as shown in Equation 2, which guarantees the final result to be well-calibrated.",
                        "The final sampling rate may be used to subsample the negative instances in training data. At 904, negative instances in training data may be subsampled based on the final subsampling rate. The subsampled negative instances and all of the positive instances in the training data may collectively make up the final subsampled training set. An offline recommendation model may be trained using positive instances in the training data and the subsampled negative instances. At 906, an an offline recommendation model may be trained using positive instances in the training data and the subsampled negative instances. For example, the offline recommendation model may be trained using the subsampled training set. The trained offline recommendation model may be deployed. The deployed offline recommendation model may be configured to recommend items to users.",
                        "FIG. 10 illustrates an example process 1000 of improving data subsampling for recommendation systems. For example, the system 100 may perform the process 1000. Although depicted as a sequence of operations in FIG. 10, those of ordinary skill in the art will appreciate that various embodiments may add, remove, reorder, or modify the depicted operations.",
                        "A set of training data may be associated with user-item interactions. The set of training data may comprise a plurality of positive instances and a plurality of negative instances. For example, if a user interacted with (e.g., clicked) a particular item, then a positive instance associated with that item may be collected. Otherwise, if the user did not click a particular item within a period of time, a negative instance associated with that item may be collected. The set of training data may be imbalanced, in that the number of negative instances may greatly outnumber the number of positive instances.",
                        "At 1002, a user-item graph associated with the training data may be constructed. The user-item bipartite graph may comprise two sets of nodes. One of the two sets of nodes may represent users and the other of the two sets of nodes may represent items. The user-item bipartite graph may comprise edges, with each edge representing interactions between a user node and an item node. The positive edges may correspond to positives instances in the training data, and the negative edges may correspond to negative instances in the training data.",
                        "At 1004, a hardness score associated with each of the negative instances may be estimated by calculating an effective conductance corresponding to each negative edge. For example, the user-item bipartite graph may be imagined as an electricity network, where each edge (ui, vj) is a conductor with conductance G(ui, vj). The conductance measures the edge's ability to transfer \u201celectrical current.\u201d G(ui, vj) may be large when a user uiexpresses direct preference of item vj. In particular, G(ui, vj) may be set to equal yn. Thus, the conductance may be equal to one if there is a direct preference, and the conductance may be equal to zero if there is not a direct preference expressed. The effective conductance Geff(ui, vj) between uiand vjmay represent the network's ability to transfer \u201ccurrent\u201d from uito vj(or vice versa). If there are many conductible paths between uiand vj, then the effective conductance Geff(ui, vj) may be large. Effective conductance may demonstrate user preference. Sample hardness may be estimated via effective conductance. Effective conductance positively relates to sample hardness.",
                        "At 1006, the hardness score associated with each of the negative instances may be smoothed using graph propagation. The edge effective conductance derived from the graph may be noisy, thus leading to an inaccurate estimation of hardness scores. Graph propagation may be used to smooth the hardness score. Edge propagation may be reduced to node propagation by transforming the user-item bipartite graph (U, V, E) into its corresponding line graph L(U, V, E)=(VL,EL), where VL=E and EL is the collection of edge pairs that share the same node.",
                        "FIG. 11 illustrates an example process 1100 of improving data subsampling for recommendation systems. For example, the system 100 may perform the process 1100. Although depicted as a sequence of operations in FIG. 11, those of ordinary skill in the art will appreciate that various embodiments may add, remove, reorder, or modify the depicted operations.",
                        "The edge effective conductance derived from a graph may be noisy, thus leading to an inaccurate estimation of hardness scores. Graph propagation may be used to smooth the hardness score. Edge propagation may be reduced to node propagation by transforming the user-item bipartite graph (U, V, E) into its corresponding line graph L(U, V, E)=(VL,EL), where VL=E and EL is the collection of edge pairs that share the same node.",
                        "The hardness score associated with each of the negative instances may be smoothed using graph propagation. Smoothing the hardness score associated with each of the negative instances may comprise determining an average effective conductance associated with neighboring negative edges of each negative edge. At 1102, an average effective conductance associated with neighboring negative edges of each negative edge may be determined. Then, for each negative edge, a weighted sum of the average effective conductance and a corresponding effective conductance may be calculated. At 1104, a weighted sum of the average effective conductance and a corresponding effective conductance may be calculated for each negative edge. The weighted sum may be equal to the final model-agnostic hardness score for that negative edge.",
                        "FIG. 12 illustrates an example process 1200 of improving data subsampling for recommendation systems. For example, the system 100 may perform the process 1200. Although depicted as a sequence of operations in FIG. 12, those of ordinary skill in the art will appreciate that various embodiments may add, remove, reorder, or modify the depicted operations.",
                        "Some hard instances may be overlooked by model-agnostic methods. For example, estimating sample hardness via effective conductance in the manner described above (e.g., a model-agnostic method) may cause some hard instances to be overlooked. The hard instances that may be overlooked by model-agnostic methods may be captured by model-based methods, such as by a pre-trained pilot model. At 1202, a hardness score associated with each negative instance in training data may be generated. The hardness score associated with each negative instance in training data may be generated using a pre-trained pilot model. At 1204, the hardness score associated with each of the negative instances may be smoothed. For example, the hardness score associated with each of the negative instances may be smoothed based on a line graph transformation of the user-item bipartite graph and graph propagation.",
                        "FIG. 13 illustrates an example process 1300 of improving data subsampling for recommendation systems. For example, the system 100 may perform the process 1300. Although depicted as a sequence of operations in FIG. 13, those of ordinary skill in the art will appreciate that various embodiments may add, remove, reorder, or modify the depicted operations.",
                        "A line-graph transformation of the user-item bipartite graph may be used to smooth model-agnostic hardness scores and model-based hardness scores. To smooth the hardness scores associated with both model-agnostic and model-based methods, graph propagation techniques may be utilized. The edge effective conductance derived from the graph may be noisy, thus leading to an inaccurate estimation of hardness scores. Graph propagation may be used to smooth the hardness score.",
                        "At 1302, a final hardness score associated with each negative instances in the training data may be determined. The final hardness score may be determined based on a corresponding smoothed hardness score determined based on a user-item graph and a corresponding smoothed hardness score determined by a pre-trained pilot model. For example, a maximum between the final model-agnostic hardness scores and the final model-based hardness scores may be determined. For example, for each instance, it may be determined whether the final model-agnostic hardness score or the final model-based hardness score is greater.",
                        "A subsampling rate may be generated based on the final hardness score associated with each instance. At 1304, a subsampling rate of the negative instances in the training data may be generated based on the final hardness score associated with each of the negative instances. Negative instances in the training data may be subsampled based on the subsampling rate. An offline recommendation model may be trained using positive instances in the training data and the subsampled negative instances.",
                        "As described above, for model-based sampling, pilot misspecification may lead to discrepancies in model performance. Described below are the results of experiments showing that pilot misspecification may lead to discrepancies in model performance. Also described below are the empirical results over two datasets that demonstrate the superiority of the model agnostic subsampling method described herein. The results of extensive ablation studies that were conducted to investigate the effectiveness of model-agnostic hardness score, score propagation, and the benefit of ensembling model-agnostic and model-based methods are also described below. Finally, effective resistance and its relationship to negative sampling is discussed below.",
                        "The empirical results described below are demonstrated using a first data set (e.g., KuaiRec) and a second data set (e.g., Microsoft News Dataset (MIND)). For both datasets, 80% of the data was used for training, 10% was used for validation, and 10% was used for testing. All experimental results are reported for 8 runs with random initializations on one random data split. The average subsampling rate was \u03b1=0.2 on training data for both datasets.",
                        "The first data set is a recommendation dataset collected from a video-sharing mobile app. The first dataset is generally a sparse user-item interaction matrix with a fully observed small submatrix. The fully observed submatrix was cropped and only the rest of entries in the sparse matrix were considered as those data are collected under natural settings. The label \u201cwatch ratio\u201d was used, which represents the total duration of a user watching a video divided by the video duration. In the experiment, a user is considered to like a video (positive instance) if the \u201cwatch ratio\u201d is larger than 3. The second data set is a large-scale news recommendation dataset with binary labels indicating users' impressions of recommended news. The content data in each news corpus was not used during the experiment. The second data set did not require extra pre-processing.",
                        "Regarding the baselines and model selections for the experiments, two baseline subsampling methods were considered. The first baseline subsampling method is the model-agnostic uniform negative sampling and the second is a model-based near-optimal sampling method (Opt-Sampling), which relies on the prediction scores of a pilot model as the hardness score to calculate sample rates. Regarding the model architecture, the wide and deep model was chosen as the training target model to validate the effectiveness of the model-agnostic method described herein.",
                        "For model-based subsampling methods, a pilot model was pre-trained to estimate the sample hardness. To test the effect of pilot misspecification, five types of pilot models were considered. Those pilot models are: the wide and deep model (W&D), a linear logistic regression model (LR), an automatic feature interaction selection model (AFI), a neural factorization machines model (NFM), and a deep factorization machine model (DFM). In the remainder of the experiment description, unless otherwise specified, the W&D is used as the pilot model as it shares the same architecture as the target model (consistent pilot). All pilot models were trained using 10% of the training data.",
                        "As described above, model-based subsampling may rely on a correctly specified pilot model. This concept was studied on the first dataset when the pilot model is misspecified by using the same model-based subsampling approach while changing pilot model architectures. FIG. 14 shows a box plot 1400 reporting the target model performance. As shown in the graph 1400, the target model's area under the curve (AUC) obtained from different pilot models varies from 0.8557 (AFI) to 0.8577 (LR). As the standard deviation is around 0.001, the AUC difference is significant, demonstrating a potential loss in large-scale recommendation systems processing millions of data points on a daily basis. This result consolidates the effect of pilot misspecification, which justifies using model-agnostic subsampling approaches, such as the techniques described herein.",
                        "The target model was trained with different data sampling strategies. FIG. 15 shows a set of box plots 1500 showing the AUC performance of all training configurations on the two datasets. The graph on the left shows the AUC performance of all training configurations on the first data set, while the graph on the right shows the AUC performance of all training configurations on the second data set. As shown in FIG. 15, the model-agnostic effective conductance (MA-EC) sampling strategy consistently outperforms the uniform sampling baseline over both of the two datasets. In the first data set, MA-EC achieves comparable results to Opt-Sampling. In the second data set, Opt-Sampling does not improve over uniform sampling and is worse than MA-EC. As also shown in FIG. 15, extra performance is gained by smoothing the hardness estimation via propagation. The performance of ensembling the model-agnostic method and the model-based method via the maximum strategy is also shown in FIG. 15 (referred to as \u201cComb(Max.)) in the set of box plots 1500). FIG. 15 shows that the ensemble performs better than every single approach on both datasets. Similarly, the smoothed scores from Opt-Sampling and MA-EC may be ensembled, achieving the best performance demonstrated in the last column of the set of box plots 1500.",
                        "The first data set was used to conduct extensive ablation studies. Regarding the subsampling rate, uniform sampling was compared with the best of the methods described herein by ensembling smoothed scores from Opt-Sampling and MA-EC. FIG. 16 shows a box plot 1600. The box plot 1600 demonstrates that the methods described herein consistently outperform uniform sampling under different subsampling rates. For example, the box plot 1600 shows that the AUC for the methods described herein is consistently higher than the AUC for uniform sampling under different subsampling rates.",
                        "Regarding the ensemble strategies, to investigate whether hardness scores from MA-EC and Opt-Sampling complement each other, a control experiment was designed. Instances were assigned with subsampling rates from each method. For instances that have inconsistent subsampling rates between two methods, their subsampling rates were flipped into the other method. For example, instances that have \u03c0D(x)<0.2 and \u03c0\u03d5(x)>0.8 were assigned with the subsampling rate \u03c0\u03d5(x), and the rest of the instances were assigned with the subsampling rate \u03c0D(x). The result of the experiment shows that we achieve better model performance by assigning most of the sample with one set of scores and flipping part of the sample scores. This verifies that some hard negative instances might be overlooked by one method and can be discovered by the other.",
                        "The control experiment justifies ensembling MA-EC and Opt-Sampling. The ensemble strategies (maximum, mean, and product) were experimented on. FIG. 17a shows a set of box plots 1700 of the three ensemble methods. For each method, nine configurations of the hyperparameters (QD, Q\u03d5) are presented, where QD \u2208 {0.1, 0.12, 0.14} and Q\u03d5 \u2208{0.005, 0.01, 0.03}. For product strategy, hyperparameter Qprod=0.005 for all experiments. From the box plots 1700, it can be observed that the maximum strategy consistently gives comparable or better results than Opt-Sampling and MA-EC. While in mean and product strategies significant improvement is not observed. For the product strategy, the model performance even deteriorates. MA-EC needs to compute effective conductance to calculate subsampling rates. Effective conductance computation is not a bottleneck since it can be reused once computed. MA-EC is model-agnostic, thus can support training with different target models.",
                        "The effectiveness of correcting the hardness scores via graph propagation was investigated. The application of score correction in Opt-Sampling and MA-EC and their ensemble was of interest. Additionally, in applying both score correction and score ensemble, either ensembling corrected scores or correcting ensemble scores may be attempted. The latter consistently results in worse performance, so presented herein is only the result of the former in this work. FIG. 17b shows a set of box plots 1702 reporting the model performance of the ablation study. For the experiments of correcting scores estimated from Opt-Sampling and MA-EC, the propagation coefficient \u03b3 \u2208 {0.05, 0.1, 0.2, 0.3, 0.4} was explored. For each coefficient, iteration to smooth the scores until convergence was performed. Uncertainty propagation significantly improves model performance for both subsampling approaches. In score propagation, which runs on scores corrected by uncertainty propagation, model performance slightly improves in Opt-Sampling and worsens in MA-EC. In the ensemble of corrected scores, the hardness scores from the best configurations of both methods were combined via the maximum strategy described above. The reported result in the box plots 1702 demonstrates that the ensemble strategy improves model performance over not only the original scores but also the corrected scores.",
                        "The definition provided for effective resistance Reff in Equation 3 is often used for graph sparsification. An edge with high effective resistance is considered important in maintaining graph topology. Since the definitions of edge importance using effective conductance and effective resistance run against each other, it can be shown that defining edge importance with effective resistance is not applicable to the scenario described herein. For example, two model-agnostic subsampling methods with effective resistance (MA-ER) and effective conductance (MA-EC) as the hardness scores were compared. The effective resistance was computed on the graph where all edges have unit resistances. MA-ER fails to capture hard negative instances. On the first data set, MA-ER yields an average test AUC of 0.8535, which is worse than uniform sampling (0.8553). To unravel how MA-ER and MA-EC affect model training, a run from each method was randomly selected to visualize the model training metrics. As shown in the set of graphs 1800 of FIG. 18, when using MA-ER, training AUC remains the same as testing AUC before convergence. Besides, the model converges earlier. In sharp contrast, in MA-EC, there is a huge gap between training AUC and testing AUC. The gap shows that training instances are overall harder than those in the test set. This verifies that MA-EC discovers hard negatives while MA-ER does not.",
                        "FIG. 19 illustrates a computing device that may be used in various aspects, such as the services, networks, modules, and/or devices depicted in FIG. 1. With regard to the example architecture of FIG. 1, the cloud network (and any of its components), the client devices, and/or the network may each be implemented by one or more instance of a computing device 1900 of FIG. 19. The computer architecture shown in FIG. 19 shows a conventional server computer, workstation, desktop computer, laptop, tablet, network appliance, PDA, e-reader, digital cellular phone, or other computing node, and may be utilized to execute any aspects of the computers described herein, such as to implement the methods described herein.",
                        "The computing device 1900 may include a baseboard, or \u201cmotherboard,\u201d which is a printed circuit board to which a multitude of components or devices may be connected by way of a system bus or other electrical communication paths. One or more central processing units (CPUs) 1904 may operate in conjunction with a chipset 1906. The CPU(s) 1904 may be standard programmable processors that perform arithmetic and logical operations necessary for the operation of the computing device 1900.",
                        "The CPU(s) 1904 may perform the necessary operations by transitioning from one discrete physical state to the next through the manipulation of switching elements that differentiate between and change these states. Switching elements may generally include electronic circuits that maintain one of two binary states, such as flip-flops, and electronic circuits that provide an output state based on the logical combination of the states of one or more other switching elements, such as logic gates. These basic switching elements may be combined to create more complex logic circuits including registers, adders-subtractors, arithmetic logic units, floating-point units, and the like.",
                        "The CPU(s) 1904 may be augmented with or replaced by other processing units, such as GPU(s) 1905. The GPU(s) 1905 may comprise processing units specialized for but not necessarily limited to highly parallel computations, such as graphics and other visualization-related processing.",
                        "A chipset 1906 may provide an interface between the CPU(s) 1904 and the remainder of the components and devices on the baseboard. The chipset 1906 may provide an interface to a random-access memory (RAM) 1908 used as the main memory in the computing device 1900. The chipset 1906 may further provide an interface to a computer-readable storage medium, such as a read-only memory (ROM) 1920 or non-volatile RAM (NVRAM) (not shown), for storing basic routines that may help to start up the computing device 1900 and to transfer information between the various components and devices. ROM 1920 or NVRAM may also store other software components necessary for the operation of the computing device 1900 in accordance with the aspects described herein.",
                        "The computing device 1900 may operate in a networked environment using logical connections to remote computing nodes and computer systems through local area network (LAN). The chipset 1906 may include functionality for providing network connectivity through a network interface controller (NIC) 1922, such as a gigabit Ethernet adapter. A NIC 1922 may be capable of connecting the computing device 1900 to other computing nodes over a network 1916. It should be appreciated that multiple NICs 1922 may be present in the computing device 1900, connecting the computing device to other types of networks and remote computer systems.",
                        "The computing device 1900 may be connected to a mass storage device 1928 that provides non-volatile storage for the computer. The mass storage device 1928 may store system programs, application programs, other program modules, and data, which have been described in greater detail herein. The mass storage device 1928 may be connected to the computing device 1900 through a storage controller 1924 connected to the chipset 1906. The mass storage device 1928 may consist of one or more physical storage units. The mass storage device 1928 may comprise a management component. A storage controller 1924 may interface with the physical storage units through a serial attached SCSI (SAS) interface, a serial advanced technology attachment (SATA) interface, a fiber channel (FC) interface, or other type of interface for physically connecting and transferring data between computers and physical storage units.",
                        "The computing device 1900 may store data on the mass storage device 1928 by transforming the physical state of the physical storage units to reflect the information being stored. The specific transformation of a physical state may depend on various factors and on different implementations of this description. Examples of such factors may include, but are not limited to, the technology used to implement the physical storage units and whether the mass storage device 1928 is characterized as primary or secondary storage and the like.",
                        "For example, the computing device 1900 may store information to the mass storage device 1928 by issuing instructions through a storage controller 1924 to alter the magnetic characteristics of a particular location within a magnetic disk drive unit, the reflective or refractive characteristics of a particular location in an optical storage unit, or the electrical characteristics of a particular capacitor, transistor, or other discrete component in a solid-state storage unit. Other transformations of physical media are possible without departing from the scope and spirit of the present description, with the foregoing examples provided only to facilitate this description. The computing device 1900 may further read information from the mass storage device 1928 by detecting the physical states or characteristics of one or more particular locations within the physical storage units.",
                        "In addition to the mass storage device 1928 described above, the computing device 1900 may have access to other computer-readable storage media to store and retrieve information, such as program modules, data structures, or other data. It should be appreciated by those skilled in the art that computer-readable storage media may be any available media that provides for the storage of non-transitory data and that may be accessed by the computing device 1900.",
                        "By way of example and not limitation, computer-readable storage media may include volatile and non-volatile, transitory computer-readable storage media and non-transitory computer-readable storage media, and removable and non-removable media implemented in any method or technology. Computer-readable storage media includes, but is not limited to, RAM, ROM, erasable programmable ROM (\u201cEPROM\u201d), electrically erasable programmable ROM (\u201cEEPROM\u201d), flash memory or other solid-state memory technology, compact disc ROM (\u201cCD-ROM\u201d), digital versatile disk (\u201cDVD\u201d), high definition DVD (\u201cHD-DVD\u201d), BLU-RAY, or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage, other magnetic storage devices, or any other medium that may be used to store the desired information in a non-transitory fashion.",
                        "A mass storage device, such as the mass storage device 1928 depicted in FIG. 19, may store an operating system utilized to control the operation of the computing device 1900. The operating system may comprise a version of the LINUX operating system. The operating system may comprise a version of the WINDOWS SERVER operating system from the MICROSOFT Corporation. According to further aspects, the operating system may comprise a version of the UNIX operating system. Various mobile phone operating systems, such as IOS and ANDROID, may also be utilized. It should be appreciated that other operating systems may also be utilized. The mass storage device 1928 may store other system or application programs and data utilized by the computing device 1900.",
                        "The mass storage device 1928 or other computer-readable storage media may also be encoded with computer-executable instructions, which, when loaded into the computing device 1900, transforms the computing device from a general-purpose computing system into a special-purpose computer capable of implementing the aspects described herein. These computer-executable instructions transform the computing device 1900 by specifying how the CPU(s) 1904 transition between states, as described above. The computing device 1900 may have access to computer-readable storage media storing computer-executable instructions, which, when executed by the computing device 1900, may perform the methods described herein.",
                        "A computing device, such as the computing device 1900 depicted in FIG. 19, may also include an input/output controller 1932 for receiving and processing input from a number of input devices, such as a keyboard, a mouse, a touchpad, a touch screen, an electronic stylus, or other type of input device. Similarly, an input/output controller 1932 may provide output to a display, such as a computer monitor, a flat-panel display, a digital projector, a printer, a plotter, or other type of output device. It will be appreciated that the computing device 1900 may not include all of the components shown in FIG. 19, may include other components that are not explicitly shown in FIG. 19, or may utilize an architecture completely different than that shown in FIG. 19.",
                        "As described herein, a computing device may be a physical computing device, such as the computing device 1900 of FIG. 19. A computing node may also include a virtual machine host process and one or more virtual machine instances. Computer-executable instructions may be executed by the physical hardware of a computing device indirectly through interpretation and/or execution of instructions stored and executed in the context of a virtual machine.",
                        "It is to be understood that the methods and systems are not limited to specific methods, specific components, or to particular implementations. It is also to be understood that the terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting.",
                        "As used in the specification and the appended claims, the singular forms \u201ca,\u201d \u201can,\u201d and \u201cthe\u201d include plural referents unless the context clearly dictates otherwise. Ranges may be expressed herein as from \u201cabout\u201d one particular value, and/or to \u201cabout\u201d another particular value. When such a range is expressed, another embodiment includes from the one particular value and/or to the other particular value. Similarly, when values are expressed as approximations, by use of the antecedent \u201cabout,\u201d it will be understood that the particular value forms another embodiment. It will be further understood that the endpoints of each of the ranges are significant both in relation to the other endpoint, and independently of the other endpoint.",
                        "\u201cOptional\u201d or \u201coptionally\u201d means that the subsequently described event or circumstance may or may not occur, and that the description includes instances where said event or circumstance occurs and instances where it does not.",
                        "Throughout the description and claims of this specification, the word \u201ccomprise\u201d and variations of the word, such as \u201ccomprising\u201d and \u201ccomprises,\u201d means \u201cincluding but not limited to,\u201d and is not intended to exclude, for example, other components, integers or steps. \u201cExemplary\u201d means \u201can example of\u201d and is not intended to convey an indication of a preferred or ideal embodiment. \u201cSuch as\u201d is not used in a restrictive sense, but for explanatory purposes.",
                        "Components are described that may be used to perform the described methods and systems. When combinations, subsets, interactions, groups, etc., of these components are described, it is understood that while specific references to each of the various individual and collective combinations and permutations of these may not be explicitly described, each is specifically contemplated and described herein, for all methods and systems. This applies to all aspects of this application including, but not limited to, operations in described methods. Thus, if there are a variety of additional operations that may be performed it is understood that each of these additional operations may be performed with any specific embodiment or combination of embodiments of the described methods.",
                        "The present methods and systems may be understood more readily by reference to the following detailed description of preferred embodiments and the examples included therein and to the Figures and their descriptions.",
                        "As will be appreciated by one skilled in the art, the methods and systems may take the form of an entirely hardware embodiment, an entirely software embodiment, or an embodiment combining software and hardware aspects. Furthermore, the methods and systems may take the form of a computer program product on a computer-readable storage medium having computer-readable program instructions (e.g., computer software) embodied in the storage medium. More particularly, the present methods and systems may take the form of web-implemented computer software. Any suitable computer-readable storage medium may be utilized including hard disks, CD-ROMs, optical storage devices, or magnetic storage devices.",
                        "Embodiments of the methods and systems are described below with reference to block diagrams and flowchart illustrations of methods, systems, apparatuses and computer program products. It will be understood that each block of the block diagrams and flowchart illustrations, and combinations of blocks in the block diagrams and flowchart illustrations, respectively, may be implemented by computer program instructions. These computer program instructions may be loaded on a general-purpose computer, special-purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions which execute on the computer or other programmable data processing apparatus create a means for implementing the functions specified in the flowchart block or blocks.",
                        "These computer program instructions may also be stored in a computer-readable memory that may direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable memory produce an article of manufacture including computer-readable instructions for implementing the function specified in the flowchart block or blocks. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer-implemented process such that the instructions that execute on the computer or other programmable apparatus provide steps for implementing the functions specified in the flowchart block or blocks.",
                        "The various features and processes described above may be used independently of one another, or may be combined in various ways. All possible combinations and sub-combinations are intended to fall within the scope of this disclosure. In addition, certain methods or process blocks may be omitted in some implementations. The methods and processes described herein are also not limited to any particular sequence, and the blocks or states relating thereto may be performed in other sequences that are appropriate. For example, described blocks or states may be performed in an order other than that specifically described, or multiple blocks or states may be combined in a single block or state. The example blocks or states may be performed in serial, in parallel, or in some other manner. Blocks or states may be added to or removed from the described example embodiments. The example systems and components described herein may be configured differently than described. For example, elements may be added to, removed from, or rearranged compared to the described example embodiments.",
                        "It will also be appreciated that various items are illustrated as being stored in memory or on storage while being used, and that these items or portions thereof may be transferred between memory and other storage devices for purposes of memory management and data integrity. Alternatively, in other embodiments, some or all of the software modules and/or systems may execute in memory on another device and communicate with the illustrated computing systems via inter-computer communication. Furthermore, in some embodiments, some or all of the systems and/or modules may be implemented or provided in other ways, such as at least partially in firmware and/or hardware, including, but not limited to, one or more application-specific integrated circuits (\u201cASICs\u201d), standard integrated circuits, controllers (e.g., by executing appropriate instructions, and including microcontrollers and/or embedded controllers), field-programmable gate arrays (\u201cFPGAs\u201d), complex programmable logic devices (\u201cCPLDs\u201d), etc. Some or all of the modules, systems, and data structures may also be stored (e.g., as software instructions or structured data) on a computer-readable medium, such as a hard disk, a memory, a network, or a portable media article to be read by an appropriate device or via an appropriate connection. The systems, modules, and data structures may also be transmitted as generated data signals (e.g., as part of a carrier wave or other analog or digital propagated signal) on a variety of computer-readable transmission media, including wireless-based and wired/cable-based media, and may take a variety of forms (e.g., as part of a single or multiplexed analog signal, or as multiple discrete digital packets or frames). Such computer program products may also take other forms in other embodiments. Accordingly, the present invention may be practiced with other computer system configurations.",
                        "While the methods and systems have been described in connection with preferred embodiments and specific examples, it is not intended that the scope be limited to the particular embodiments set forth, as the embodiments herein are intended in all respects to be illustrative rather than restrictive.",
                        "Unless otherwise expressly stated, it is in no way intended that any method set forth herein be construed as requiring that its operations be performed in a specific order. Accordingly, where a method claim does not actually recite an order to be followed by its operations or it is not otherwise specifically stated in the claims or descriptions that the operations are to be limited to a specific order, it is no way intended that an order be inferred, in any respect. This holds for any possible non-express basis for interpretation, including: matters of logic with respect to arrangement of steps or operational flow; plain meaning derived from grammatical organization or punctuation; and the number or type of embodiments described in the specification.",
                        "It will be apparent to those skilled in the art that various modifications and variations may be made without departing from the scope or spirit of the present disclosure. Other embodiments will be apparent to those skilled in the art from consideration of the specification and practices described herein. It is intended that the specification and example figures be considered as exemplary only, with a true scope and spirit being indicated by the following claims."
                    ],
                    "subsections": [],
                    "outline_long": [
                        "introduce recommendation systems",
                        "describe clickthrough rate prediction models",
                        "explain negative sampling",
                        "motivate non-uniform data subsampling",
                        "describe pilot-model techniques",
                        "explain limitations of model-based sampling",
                        "introduce real-life scenarios",
                        "describe online-offline discrepancy",
                        "explain extra dependencies across system modules",
                        "motivate model-agnostic data subsampling methods",
                        "describe topology of user-item graph",
                        "estimate importance of user-item interactions",
                        "perform propagation step",
                        "describe data subsampling system",
                        "explain user-item graph sub-system",
                        "describe pilot model sub-system",
                        "explain smoothing sub-system",
                        "describe ensemble sub-system",
                        "generate subsampling rate",
                        "describe set of training data",
                        "explain binary classification problem",
                        "describe generalized logistic regression model",
                        "explain imbalanced set of training data",
                        "describe negative sampling algorithm",
                        "explain measure of importance for negative instances",
                        "describe hard negative sampling procedure",
                        "correct log odds",
                        "explain model-agnostic hardness score",
                        "relate sample hardness to graph topology",
                        "determine effective conductance",
                        "estimate sample hardness via effective conductance",
                        "define hardness score",
                        "introduce commute time distance",
                        "approximate commute time distance using random walk",
                        "transform commute time into effective conductance",
                        "motivate model-agnostic methods",
                        "describe limitations of model-agnostic methods",
                        "introduce pilot model sub-system",
                        "generate pilot prediction as hardness score",
                        "describe table of pilot predictions",
                        "introduce smoothing subsystem",
                        "smooth hardness scores associated with model-agnostic methods",
                        "smooth hardness scores associated with model-based methods",
                        "describe line-graph transformation",
                        "smooth model-agnostic hardness scores using line-graph transformation",
                        "smooth model-based hardness scores using line-graph transformation",
                        "describe graph propagation techniques",
                        "smooth hardness scores using graph propagation",
                        "introduce iterative approximation approach",
                        "describe edge uncertainty propagation",
                        "introduce ensemble sub-system",
                        "generate subsampling rate based on importance",
                        "describe final subsampling rate determination",
                        "introduce heuristic strategies for combining subsampling methods",
                        "subsample negative instances in training data",
                        "train offline recommendation model using subsampled training set",
                        "construct user-item graph",
                        "use negative sampling to reduce dataset size",
                        "estimate importance of user-item interactions via graph conductance",
                        "measure importance of training data via sample hardness using pre-trained pilot model",
                        "determine final subsampling rate based on importance estimated from user-item graph and importance measured by pre-trained pilot model",
                        "generate subsampling rate based on importance estimated from user-item graph and importance measured by pre-trained pilot model",
                        "illustrate example process of improving data subsampling for recommendation systems",
                        "construct user-item graph associated with training data",
                        "estimate hardness score associated with each of negative instances",
                        "smooth hardness score associated with each of negative instances using graph propagation",
                        "illustrate example process of improving data subsampling for recommendation systems",
                        "smooth hardness score associated with each of negative instances using graph propagation",
                        "determine average effective conductance associated with neighboring negative edges of each negative edge",
                        "calculate weighted sum of average effective conductance and corresponding effective conductance for each negative edge",
                        "illustrate example process of improving data subsampling for recommendation systems",
                        "generate hardness score associated with each negative instance in training data using pre-trained pilot model",
                        "smooth hardness score associated with each of negative instances",
                        "illustrate example process of improving data subsampling for recommendation systems",
                        "smooth model-agnostic hardness scores and model-based hardness scores using line-graph transformation of user-item bipartite graph and graph propagation",
                        "determine final hardness score associated with each negative instance in training data",
                        "generate subsampling rate of negative instances in training data based on final hardness score associated with each of negative instances",
                        "subsample negative instances in training data based on subsampling rate",
                        "train offline recommendation model using positive instances in training data and subsampled negative instances",
                        "describe results of experiments showing pilot misspecification leads to discrepancies in model performance",
                        "describe empirical results over two datasets demonstrating superiority of model-agnostic subsampling method",
                        "describe results of extensive ablation studies investigating effectiveness of model-agnostic hardness score, score propagation, and benefit of ensembling model-agnostic and model-based",
                        "discuss effective resistance and its relationship to negative sampling",
                        "describe experimental setup for empirical results",
                        "describe baseline subsampling methods and model selections for experiments",
                        "describe effect of pilot misspecification on model performance",
                        "describe target model performance with different data sampling strategies",
                        "describe ablation studies on subsampling rate",
                        "describe ensemble strategies for combining model-agnostic and model-based methods",
                        "describe control experiment investigating whether hardness scores from MA-EC and Opt-Sampling complement each other",
                        "describe results of control experiment",
                        "describe ensemble strategies for combining model-agnostic and model-based methods",
                        "describe results of ensemble strategies",
                        "describe advantage of MA-EC",
                        "describe computation of effective conductance",
                        "describe model-agnostic property of MA-EC",
                        "conclude description of patent application",
                        "investigate effectiveness of correcting hardness scores via graph propagation",
                        "apply score correction in Opt-Sampling and MA-EC and their ensemble",
                        "explore propagation coefficient \u03b3",
                        "perform iteration to smooth scores until convergence",
                        "demonstrate improvement in model performance",
                        "define effective resistance Reff for graph sparsification",
                        "compare edge importance using effective resistance and effective conductance",
                        "show limitations of MA-ER in capturing hard negative instances",
                        "visualize model training metrics for MA-ER and MA-EC",
                        "illustrate computing device architecture",
                        "describe central processing units (CPUs)",
                        "explain chipset functionality",
                        "detail random-access memory (RAM) and computer-readable storage medium",
                        "describe network interface controller (NIC) and network connectivity",
                        "illustrate mass storage device and storage controller",
                        "explain data storage and retrieval",
                        "describe operating system and its functionality",
                        "detail computer-executable instructions and their transformation of computing device",
                        "describe input/output controller and input/output devices",
                        "explain virtual machine host process and virtual machine instances",
                        "provide disclaimer on limitations of methods and systems",
                        "define terminology and its usage",
                        "explain ranges and approximations",
                        "define \"optional\" and \"optionally\"",
                        "explain \"comprise\" and variations",
                        "define \"exemplary\"",
                        "explain \"such as\"",
                        "describe components and their combinations",
                        "explain methods and systems in hardware, software, and combined embodiments",
                        "describe computer program product on computer-readable storage medium",
                        "explain web-implemented computer software",
                        "describe block diagrams and flowchart illustrations",
                        "explain computer program instructions and their loading",
                        "describe computer-readable memory and its functionality",
                        "explain article of manufacture including computer-readable instructions",
                        "describe computer-implemented process",
                        "explain independence of features and processes",
                        "describe omission of methods or process blocks",
                        "explain sequence of blocks or states",
                        "describe addition or removal of blocks or states",
                        "explain configuration of systems and components",
                        "describe memory management and data integrity",
                        "explain implementation in firmware and/or hardware",
                        "describe transmission of systems, modules, and data structures"
                    ],
                    "num_characters": 71589,
                    "outline_medium": [
                        "introduce recommendation systems",
                        "describe clickthrough rate prediction models",
                        "explain negative sampling",
                        "motivate non-uniform data subsampling",
                        "describe pilot-model techniques",
                        "explain limitations of model-based sampling",
                        "introduce model-agnostic data subsampling methods",
                        "describe topology of user-item graph",
                        "explain estimation of importance of user-item interactions",
                        "describe propagation step to smooth out importance values",
                        "illustrate example data subsampling system",
                        "describe user-item graph sub-system",
                        "explain negative sampling algorithm",
                        "describe model-agnostic hardness score",
                        "relate sample hardness to graph topology",
                        "illustrate example framework for interpreting effective conductance",
                        "approximate commute time distance",
                        "transform commute time into effective conductance",
                        "motivate model-agnostic methods",
                        "motivate model-based methods",
                        "smooth hardness scores using line graph transformation",
                        "smooth hardness scores using graph propagation",
                        "propagate uncertainty",
                        "propagate scores",
                        "combine model-agnostic and model-based hardness scores",
                        "determine final subsampling rate",
                        "subsample negative instances",
                        "train offline recommendation model",
                        "construct user-item graph",
                        "apply negative sampling",
                        "estimate importance of user-item interactions",
                        "measure importance of training data",
                        "determine final subsampling rate",
                        "subsample negative instances",
                        "train offline recommendation model",
                        "illustrate process of improving data subsampling",
                        "construct user-item graph",
                        "estimate hardness score",
                        "smooth hardness score",
                        "illustrate process of improving data subsampling",
                        "smooth hardness score using graph propagation",
                        "determine final hardness score",
                        "generate subsampling rate",
                        "subsample negative instances",
                        "train offline recommendation model",
                        "discuss pilot misspecification",
                        "describe empirical results",
                        "discuss effective resistance and negative sampling",
                        "investigate effectiveness of correcting hardness scores via graph propagation",
                        "explore application of score correction in Opt-Sampling and MA-EC",
                        "compare ensemble of corrected scores with original scores",
                        "define effective resistance Reff for graph sparsification",
                        "show limitations of defining edge importance with effective resistance",
                        "compare MA-ER and MA-EC for capturing hard negative instances",
                        "visualize model training metrics for MA-ER and MA-EC",
                        "illustrate computing device architecture",
                        "describe CPU and chipset components",
                        "explain GPU and RAM components",
                        "describe ROM and NVRAM components",
                        "explain network connectivity and NIC components",
                        "describe mass storage device and storage controller components",
                        "explain data storage and retrieval on mass storage device",
                        "describe operating system and its functions",
                        "explain computer-executable instructions and their effects",
                        "describe input/output controller and its functions",
                        "explain virtual machine host process and virtual machine instances",
                        "provide general understanding of methods and systems",
                        "describe block diagrams and flowchart illustrations",
                        "explain computer program products and their implementations",
                        "provide disclaimer and scope of the invention"
                    ],
                    "outline_short": [
                        "motivate recommendation systems",
                        "limitations of model-based sampling",
                        "introduce model-agnostic data subsampling methods",
                        "describe data subsampling system architecture",
                        "define generalized logistic regression model",
                        "explain hard negative sampling procedure",
                        "relate sample hardness to graph topology",
                        "estimate sample hardness via effective conductance",
                        "approximate commute time distance",
                        "capture hard instances by model-based methods",
                        "smooth hardness scores using line graph transformation",
                        "smooth hardness scores using graph propagation",
                        "generate subsampling rate based on importance",
                        "subsample negative instances and train offline recommendation model",
                        "construct user-item graph",
                        "estimate importance of user-item interactions",
                        "measure importance of training data via sample hardness",
                        "determine final subsampling rate",
                        "subsample negative instances in training data",
                        "train offline recommendation model",
                        "estimate hardness score associated with each negative instance",
                        "smooth hardness score using graph propagation",
                        "generate final hardness score and subsampling rate",
                        "discuss experimental results and ablation studies",
                        "investigate effectiveness of score correction via graph propagation",
                        "compare edge importance definitions using effective resistance and conductance",
                        "describe computing device architecture",
                        "detail CPU and chipset components",
                        "explain GPU and RAM components",
                        "describe storage controller and mass storage device components",
                        "detail network interface controller and network connectivity",
                        "explain operating system and program storage",
                        "describe input/output controller and input/output devices",
                        "discuss virtual machine instances and computer-executable instructions",
                        "provide general descriptions and disclaimers"
                    ]
                }
            ],
            "outline_long": [],
            "num_characters": 0,
            "outline_medium": [],
            "outline_short": []
        }
    ],
    "claims": [
        "1. A method of improving data subsampling for recommendation systems, comprising:\nconstructing a user-item graph associated with training data;\nestimating importance of user-item interactions via graph conductance based on the user-item graph;\nmeasuring importance of the training data via sample hardness using a pre-trained pilot model; and\ngenerating a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model.",
        "2. The method of claim 1, further comprising:\nsubsampling negative instances in the training data based on the subsampling rate.",
        "3. The method of claim 2, further comprising:\ntraining an offline recommendation model using positive instances in the training data and the subsampled negative instances.",
        "4. The method of claim 1, wherein the constructing a user-item graph associated with training data further comprises:\nconstructing a bipartite graph from the training data, wherein the bipartite graph comprises positive edges and negative edges, the positive edges corresponding to positives instances in the training data, and the negative edges corresponding to negative instances in the training data.",
        "5. The method of claim 4, wherein the estimating importance of user-item interactions via graph conductance based on the user-item graph further comprises:\nestimating a hardness score associated with each of the negative instances by calculating an effective conductance corresponding to each negative edge; and\nsmoothing the hardness score associated with each of the negative instances using graph propagation.",
        "6. The method of claim 5, wherein the smoothing the hardness score associated with each of the negative instances using graph propagation further comprises:\ndetermining an average effective conductance associated with neighboring negative edges of each negative edge; and\ncalculating, for each negative edge, a weighted sum of the average effective conductance and a corresponding effective conductance.",
        "7. The method of claim 1, wherein the measuring importance of the training data via sample hardness using a pre-trained pilot model further comprises:\ngenerating a hardness score associated with each negative instance in the training data using the pre-trained pilot model; and\nsmoothing the hardness score associated with each of the negative instances.",
        "8. The method of claim 1, wherein the generating a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model further comprises:\ndetermining a final hardness score associated with each negative instance in the training data based on a corresponding smoothed hardness score determined based on the user-item graph and a corresponding smoothed hardness score determined by the pre-trained pilot model; and\ngenerating the subsampling rate of negative instances in the training data based on the final hardness score associated with each of the negative instances.",
        "9. A system, comprising:\nat least one processor; and\nat least one memory comprising computer-readable instructions that upon execution by the at least one processor cause the system to perform operations comprising:\nconstructing a user-item graph associated with training data;\nestimating importance of user-item interactions via graph conductance based on the user-item graph;\nmeasuring importance of the training data via sample hardness using a pre-trained pilot model; and\ngenerating a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model.",
        "10. The system of claim 9, wherein the constructing a user-item graph associated with training data further comprises:\nconstructing a bipartite graph from the training data, wherein the bipartite graph comprises positive edges and negative edges, the positive edges corresponding to positives instances in the training data, and the negative edges corresponding to negative instances in the training data.",
        "11. The system of claim 10, wherein the estimating importance of user-item interactions via graph conductance based on the user-item graph further comprises:\nestimating a hardness score associated with each of the negative instances by calculating an effective conductance corresponding to each negative edge; and\nsmoothing the hardness score associated with each of the negative instances using graph propagation.",
        "12. The system of claim 11, wherein the smoothing the hardness score associated with each of the negative instances using graph propagation further comprises:\ndetermining an average effective conductance associated with neighboring negative edges of each negative edge; and\ncalculating, for each negative edge, a weighted sum of the average effective conductance and a corresponding effective conductance.",
        "13. The system of claim 9, wherein the measuring importance of the training data via sample hardness using a pre-trained pilot model further comprises:\ngenerating a hardness score associated with each negative instance in the training data using the pre-trained pilot model; and\nsmoothing the hardness score associated with each of the negative instances.",
        "14. The system of claim 9, wherein the generating a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model further comprises:\ndetermining a final hardness score associated with each negative instance in the training data based on a corresponding smoothed hardness score determined based on the user-item graph and a corresponding smoothed hardness score determined by the pre-trained pilot model; and\ngenerating the sub sampling rate of negative instances in the training data based on the final hardness score associated with each of the negative instances.",
        "15. A non-transitory computer-readable storage medium, storing computer-readable instructions that upon execution by a processor cause the processor to implement operations, the operation comprising:\nconstructing a user-item graph associated with training data;\nestimating importance of user-item interactions via graph conductance based on the user-item graph;\nmeasuring importance of the training data via sample hardness using a pre-trained pilot model; and\ngenerating a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model.",
        "16. The non-transitory computer-readable storage medium of claim 15, wherein the constructing a user-item graph associated with training data further comprises:\nconstructing a bipartite graph from the training data, wherein the bipartite graph comprises positive edges and negative edges, the positive edges corresponding to positives instances in the training data, and the negative edges corresponding to negative instances in the training data.",
        "17. The non-transitory computer-readable storage medium of claim 16, wherein the estimating importance of user-item interactions via graph conductance based on the user-item graph further comprises:\nestimating a hardness score associated with each of the negative instances by calculating an effective conductance corresponding to each negative edge; and\nsmoothing the hardness score associated with each of the negative instances using graph propagation.",
        "18. The non-transitory computer-readable storage medium of claim 17, wherein the smoothing the hardness score associated with each of the negative instances using graph propagation further comprises:\ndetermining an average effective conductance associated with neighboring negative edges of each negative edge; and\ncalculating, for each negative edge, a weighted sum of the average effective conductance and a corresponding effective conductance.",
        "19. The non-transitory computer-readable storage medium of claim 15, wherein the measuring importance of the training data via sample hardness using a pre-trained pilot model further comprises:\ngenerating a hardness score associated with each negative instance in the training data using the pre-trained pilot model; and\nsmoothing the hardness score associated with each of the negative instances.",
        "20. The non-transitory computer-readable storage medium of claim 15, wherein the generating a subsampling rate based on the importance estimated from the user-item graph and the importance measured by the pre-trained pilot model further comprises:\ndetermining a final hardness score associated with each negative instance in the training data based on a corresponding smoothed hardness score determined based on the user-item graph and a corresponding smoothed hardness score determined by the pre-trained pilot model; and\ngenerating the sub sampling rate of negative instances in the training data based on the final hardness score associated with each of the negative instances."
    ]
}