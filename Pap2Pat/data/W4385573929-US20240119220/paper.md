# Introduction

Contracts are legal documents used in several business workflows. They consist of paragraphs of text (clauses) outlining the terms and conditions for the involved parties. Prior to signing a contract, the parties need to understand the clauses, to ensure that they are aware of what they are agreeing to.

Contract clauses are usually very long, domainspecific, and contain several complex phrases (Table 1). Table 2 shows a linguistic comparison of legal language from SEC 2 contract clauses (Tuggener et al., 2020) and simple English Wikipedia (Coster and Kauchak, 2011); the average number of tokens in legal clauses is 129.73, while that in Simple Wikipedia (Coster and Kauchak, 2011) is 18.16, and similarly, the average sentence length of the former is 3.5 times that of the latter. Readability metrics such as Flesch Kincaid (FK) (Kincaid et al., 1975) and Automatic Readability Index (ARI) (Senter and Smith, 1967), and the tree depth of the syn-In the event that the Landlord shall deem it necessary or be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building in which the demised premises are located (unless the same result from Tenant's act, neglect, default or mode of operation in which event Tenant shall make all such repairs, alterations and improvements), then the same shall be made by the Landlord with reasonable dispatch, however, such obligation of Tenant shall not extend to maintenance, repairs or replacements necessitated by the intentional wrongdoing or gross negligence of Landlord. tactic parse trees of legal sentences, also indicate that legal language is much more complex compared to simple language in Wikipedia, and may be particularly difficult to read for laypeople without much legal background (our target readers). Further, obtaining legal aid to help interpret and review such language may be expensive for such readers.

We believe natural language processing (NLP) techniques for text simplification (TS) can be of particular utility to aid legal document understanding for laypeople without much legal knowledge, who are the target readers for this work.

The objective of TS is to provide simpler translations for complex input texts. TS is performed at different levels. Lexical simplification aims to replace complex words in a given text with simpler alternatives with equivalent meaning (Gooding and Kochmar, 2019;Qiang et al., 2020). Syntactic simplification typically involves splitting long sentences in shorter ones (Niklaus et al., 2019a,b). (Zhu et al., 2010;Wubben et al., 2012;Martin et al., 2020) use seq2seq-based supervised methods for TS, owing to the availability of parallel datasets (Xu et al., 2015;Niklaus et al., 2019b). There have also been recent advancements in unsupervised TS without the need for parallel datasets (Surya et al., 2019;Laban et al., 2021). However, we believe they may not be readily suited to legal TS, due to the extremely complex nature of legal text as op-posed to the complex text seen in general news or Wikipedia-like datasets. While prior works on challenges in TS (Xu et al., 2015;Štajner, 2021) focus on the quality of the TS datasets and evaluation metrics, we focus on the generalizability of existing TS systems to legal domain, and challenges in using existing evaluation metrics for legal TS.

In this paper, we aim to address two main research questions. (1) How do existing simplification methods perform (in the absence of legal parallel datasets) on the task of legal TS? We specifically examine three types of simplification, namely lexical TS, sentence splitting, and end-to-end TS (split-and-rephrase).

(2) What are the challenges, if any, in using existing automatic evaluation metrics for legal TS? To this end, we investigate three stateof-the-art (SoTA) unsupervised TS methods in the legal domain ( §2.1): (a) a BERT-based method for lexical simplification (Qiang et al., 2020), (b) a rulebased discourse-aware sentence splitting framework (Niklaus et al., 2019a), and (c) a rewardbased simplification method that learns to balance fluency, salience, and simplicity of output translations (Laban et al., 2021). We also investigate sequence-to-sequence-based supervised methods (Lewis et al., 2020) trained on three recently released parallel datasets for TS ( §2.2). To address the second question, we use several reference-free automatic metrics in the TS literature for simplicity, meaning preservation, and fluency on the model outputs, and conduct human studies to analyze their effectiveness. Finally, we outline some of the challenges in adapting existing methods and metrics to the legal domain, and present a few preliminary research questions that need to be addressed for furthering the research in the space of legal TS.

# Text Simplification for Legal Domain

We use several unsupervised and supervised methods. We briefly describe them below (please refer to Appendix B for further details).

## Unsupervised Text Simplification

Lexical simplification (LS) aims to replace complex words in a given sentence with simpler words with equivalent meaning to make the resulting text more readable. We use a recent SoTA unsupervised LS method BERT-LS3 (Qiang et al., 2020) that uses the pre-trained Transformer language model BERT (Devlin et al., 2019) to find simplification candidates for given complex words. Given a complex word w in a sequence S, a new sequence S ′ is constructed with w masked. The original and new sequences are concatenated and fed into BERT to obtain the probability distribution of the vocabulary p(•|S, S ′ \{w}) corresponding to the masked word. The top 10 words from p(•|S, S ′ \{w}) are selected as simplification candidates, excluding any morphological derivations. The candidates are ranked based on features such as BERT prediction probability, semantic similarity with complex word, and the candidate with the highest average rank is selected as the replacement. We associate complexity of a word with its commonness in a large corpus (Biran et al., 2011;Glavaš and Štajner, 2015), and identify complex words based on their frequency (<10K) in normal Wikipedia (Coster and Kauchak, 2011). Further details are provided in Appendix B. Sentence splitting involves the segmentation of a sentence into two or more shorter sentences that can be better processed by NLP systems. We use DISSIM, a discourse-aware syntactic TS framework, that breaks down a complex source sentence into a set of minimal propositions (Niklaus et al., 2019a). 4 Specifically, given a source sentence, it applies recursive transformations based on a set of 35 hand-crafted grammar rules based on syntactic and lexical patterns to split and rephrase the input sentence into structurally simplified sentences, and establish a semantic hierarchy among them. Sentence simplification. We use a recent SoTA reward-based text simplification method KEEPIT-SIMPLE (KIS) (Laban et al., 2021) that uses a generative model GPT-2 (Radford et al., 2019) to transform a complex sentence into a simpler version, while balancing rewards for fluency, salience, and simplicity using reference-free scorers in a reinforcement learning setup. 5 For fluency, perplexity is used from GPT-2; for simplicity, the Fleish-Kincaid Grade Level (FKGL) (Kincaid et al., 1975) and word frequency in a large corpus are used; and for saliency, a coverage model that uses the generated text to answer fill-in-the-blank questions about the input is used. (Laban et al., 2020). While this work can handle paragraphs as unit of text, we use it for sentence simplification, as legal sentences are much longer than typical sentences. Please refer to (Laban et al., 2021) 

## Supervised Text Simplification

We use BART, a denoising autoencoder for pretraining sequence-to-sequence models, for supervised TS (Lewis et al., 2020). It pre-trains a model combining bidirectional and auto-regressive Transformers, with pre-training tasks to corrupt text with noising functions and learning to reconstruct the original text. We fine-tune BART on three complexsimple datasets, one for sentence splitting, and two for split-and-rephrase task.6 MINIWIKISPLIT (MWS) is a sentence splitting corpus consisting of 203K complex-simple sentence pairs from Wikipedia edit histories (Niklaus et al., 2019b). It was created by running DISSIM (Niklaus et al., 2019a) over the complex input sentences from WIKISPLIT corpus (Botha et al., 2018) and filtering for grammatically incorrect sentences based on a set of dependency parse and part of speech tags.

For the task of split-and-rephrase, Zhang et al. (2020) proposed two benchmark datasets consisting of 500 complex-simple sentence pairs with significantly more diverse syntax in the Wikipedia and legal contracts domain. The data was collected by asking Amazon Mechanical Turk workers to split and rephrase the given complex sentences. We refer to them as SMALL-BUT-MIGHTY (SBM).

# Experiments

We train the KIS model on 67K legal text sentences selected randomly from LEDGAR dataset that do not occur in the test data (further implementation details in Appendix B). For evaluation, we use the LEDGAR dataset (Tuggener et al., 2020) consisting of Securities and Exchange Commission (SEC) contracts. We use 5K sentences randomly sampled from 100 most frequently occurring legal clauses in LEDGAR. Details on the types of clauses and sentence statistics are provided in Appendix A. Metrics. We evaluate the legal sentences and model outputs on meaning preservation, syntactic simplicity, fluency, hallucination, and readability measures. For readability, we use Flesch Kincaid (FK) (Kincaid et al., 1975), SMOG (Mc Laughlin, 1969), and Automatic Readability Index (ARI) (Senter and Smith, 1967) to estimate the minimum age required to understand the given text. We compute syntactic simplicity as the average depth of dependency parse trees of the sentences. For meaning preservation, we use BertScore (BS) (Zhang et al., 2019) which is a similarity score for each token in the input sentence with each token in the simplified sentence, Coverage (Cov) (Laban et al., 2020) which is the accuracy of filling-in the masked tokens in the masked input sentence using the simplified sentence, and BLANC (Vasilyev et al., 2020). We measure hallucination as: (1) % of outputs entailed by the input (Entail) computed using SoTA RoBERTa-based (Liu et al., 2019) textual entailment model trained on MNLI (Williams et al., 2018), and (2) % of entities (found using spaCy library) in the output not present in the input (%Unseen) (Nan et al., 2021). We compute Fluency (Ppl) using perplexity score from GPT-2.

# Results

Results are shown in Table 3. BERT-LS only replaces complex words (if they exist) in the legal sentences; thus it does not achieve much in terms of readability and syntactic simplicity. Since it retains rest other information, it has good meaning preservation and hallucination scores. However, the coverage scores drop, possibly due to

# Legal sentences

In the event that the Landlord shall deem it necessary or be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building in which the demised premises are located (unless the same result from Tenant's act, neglect, default or mode of operation in which event Tenant shall make all such repairs, alterations and improvements), then the same shall be made by the Landlord with reasonable dispatch, however, such obligation of Tenant shall not extend to maintenance, repairs or replacements necessitated by the intentional wrongdoing or gross negligence of Landlord.

# BERT-LS

In the event that the owner shall find it necessary or be required by any governmental authority to alter, repair, remove, rebuild or improve any part of the deceased premises or of the building in which the deceased premises are located (unless the same result from owner's act, neglect, default or mode of operation in which event owner shall make all such repairs, alterations and improvements), then the same shall be made by the owner with reasonable care, however, such obligation of owner shall not extend to maintenance, repairs or replacements caused by the intentional mistake or gross indifference of owner.

DISSIM This is however. This is in the event. The Landlord shall deem it necessary. The Landlord shall be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building in which the demised premises are located (unless the same result from Tenant's act, neglect, default or mode of operation in which event Tenant shall make all such repairs, alterations and improvements). This was with reasonable dispatch. The same shall be made by the Landlord. Such obligation of Tenant shall not extend to maintenance, repairs or replacements. Maintenance, repairs or replacements were necessitated by the intentional wrongdoing or gross negligence of Landlord. MWS Such obligation of Tenant shall not extend to maintenance, repairs or replacements necessitated by the intentional wrongdoing or gross negligence of Landlord. In the event that the Landlord shall deem it necessary or be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building. This was however. The same result from Tenant's act shall make all such repairs, alterations and improvements. Tenant's act is neglect, default or mode of operation in which event Tenant.

# KIS

The Landlord shall deem it necessary or be required by any governmental authority, to alter, repair, or improve, any part of the demised premises, in which he or she is to retain as a condition of his or her employment. This notice will be sent to Tenant via e-mail, to inform her of the changes that are to be made to the structure of the service. When the service is offered, the holder will advise him or her of his or her choice.

# SBM-WIKI

In the event that the Landlord shall deem it necessary or be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building in which the Demised premises are located (unless the same result from Tenant's act, neglect, default or mode of operation in which event Tenant shall make all such repairs, alterations and improvements), then the same shall be made by Landlord with reasonable dispatch. Such obligation of Tenant may not extend to maintenance, repairs or replacements necessitated by the intentional wrongdoing or gross negligence of Landlord.

# SBM-CONTRACTS

In the event that the Landlord shall deem it necessary or be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building in which the Demised premises are located. Such obligation of Tenant shall not extend to maintenance, repairs or replacements necessitated by the intentional wrongdoing or gross negligence of Landlord. This is unless the same result from Tenant's act, neglect, default or mode of operation. In this case, then the same shall be made by the Landlords with reasonable dispatch. This shall be in the event Tenant makes all such repairs, alterations and improvements. Both the unsupervised and supervised sentence splitting methods (DISSIM, MWS) result in significantly better readability scores and dependency depth, indicating splitting of longer legal sentences. Their meaning preservation and entailment scores are also high. However, they have very high perplexity scores, due to the abrupt sentence breaks. KIS achieves good readability and fluency scores; however, meaning preservation and entailment scores decrease significantly, also indicated by the generation of a few factually inconsistent phrases in the output (Table 4). This may be due to unsupervised nature of generation and the particularly complex nature of legal text as opposed to general news-like text. It is interesting that BART model trained on both SMB-Wiki (out-of-domain) and SMB-Contracts (in-domain) result in similar meaning preservation and entailment scores, where the out-of-domain effect is not seen. On closer examination, we note that in most cases, they just copy the sentences from input, with occasional sentence splitting or phrase deletions, that sometimes results in not very grammatical sentences and increased perplexity. A few qualitative examples are shown in Table 4. Human evaluation. Due to the domain-specific nature of legal texts, we conduct human studies with two legal experts (A1 and A2) on Upwork. Since legal experts will be able to better comprehend legal text, we choose them for our human evaluation as opposed to laypeople (who form our target group of readers for legal TS). We provide them 150 sentences randomly selected from the test data along with corresponding model outputs, and instruct to rate the legal sentences for simplicity, and model outputs for simplicity, meaning preservation, fluency, and hallucinations on a scale of 1 (very complex, low meaning preservation, least fluent, or less hallucinated) to 5 (simple, high meaning preservation, most fluent, or highly hallucinated). 7The task description and guideline are provided in Appendix C. Table 5 shows the ratings from the annotators. It is very interesting to note that the inter-rater agreement using Krippendorff's α (Krippendorff, 1970) between their ratings for simplicity is -0.06, indicating disagreement between the way they perceive simplicity of legal text. However, they have high agreement for meaning preservation and hallucinations, possibly due to their good understanding of legal text, and a moderate agreement for fluency. From a few simplifications (Table 7 in Appendix C) the annotators provided (as per their selection process), we note that A1 simplifies colloquially , and sometimes chooses to exclude some details that may not concern an average layperson. Whereas, A2's language is less colloquial, with most of the details included, a simpler language (with considerable paraphrasing, fewer nestings, and fewer legal jargons). We suspect this disagreement may be due to the legal experts' varying notions of simplicity in the manner in which they explain legal contract clauses to their clients;8 further studies are needed to examine the simplicity of model outputs from laypeople's perspective-simplification datasets need to be curated based on whether the target audience prefers all the details or the most important content, colloquial or more formal simplifications, to develop TS models for legal domain.

Overall, both the annotators rate the KIS model poorly in terms of meaning preservation and hallucination; in terms of simplicity, A2 rates KIS highest, while A1 rates it lowest, possibly due to the amount of hallucinations in the outputs. Correlation with automatic metrics. Table 3 (last row) shows the Pearson correlation coefficients of human ratings with automatic metrics for the 150 legal sentences and their model outputs. Since lower values are better for depth and readability metrics, we compute the correlation of inverse of human ratings with them. Tree depth and readability have weak (A1) to moderate (A2) correlations with annotators' simplicity ratings, indicating that these may not be appropriate metrics to measure simplicity of legal texts (Tanprasert and Kauchak, 2021). While splitting methods such as DISSIM and MWS are rated well for readability and depth using the automatic metrics, the annotators rate them lower for simplicity (Table 5), as these methods do not rephrase complex phrases into simpler ones. For meaning preservation, BertScore has good correlation with both the annotators' ratings; however, coverage and Blanc metrics have weak correlations, indicating that they may not fully capture the meaning preservation in legal texts. For hallucination, entailment score captures to a significant degree any factually inconsistent information (A2), though A1's ratings indicate no correlation. Similarly for fluency, A1's ratings are moderately correlated with the inverse of perplexity, while A2's ratings show no correlation. Further investigation is needed to concretely understand these metrics before using them for this task.

# Conclusions

While legal text is complex and domain-specific, thus making it a very interesting domain for TS, it is still in a nascent stage in NLP literature. We investigate and compare some of SoTA methods for lexical simplification, sentence splitting, and seq2seq sentence simplification, either unsupervised, or trained on closely related parallel datasets, using automatic metrics and human ratings. We conclude that lexical simplification methods will benefit from having a legal lexicon as they still sometimes generate replacements that do not fit the legal context. Seq2seq methods perform only surfacelevel transformations by either directly copying input sentences, or deleting a few phrases to make the sentences shorter, without much paraphrasing. While sentence splitting methods make the long nested sentences much shorter, they do so by sacrificing fluency. Reward-based generation method achieves transformations to an extent, but does so at the cost of meaning preservation. Legal TS can be particularly challenging, as even expert annotators have varied views of how to simplify legal sentences for laypeople. Understanding whether every detail is needed to be conveyed or providing a high-level overview suffices can aid in curating parallel datasets for furthering research in this space.

# Ethical statement

We are committed to ethical practices and protecting the anonymity and privacy of individuals who have contributed. We ensure that the privacy of the annotators is protected. For annotations, $15 -20/hr was paid per task.

# A Dataset Statistics

We use 5K sentences randomly sampled from 100 most frequently occurring legal clause types from SEC contracts from the LEDGAR dataset (Tuggener et al., 2020) for evaluation. Some of these clause types include amendments, base salary, benefits, duties, employment, entire agreements, expenses, governing laws, notices, positions, severability, terms, vacations, waivers, and so on.

# B Implementation Details

BERT-LS. BERT-LS requires identification of complex words in a sentence; we identify complex words in a given test sentence based on their frequency (< 10K) in normal Wikipedia (Coster and Kauchak, 2011) which is essentially the unsimplified text from Wikipedia. Its vocabulary is of size 594K tokens. In the test sentences, we consider a token potentially complex (or specific to legal domain) if it is less likely seen in normal Wikipedia

In this project, you are given a few sentences. For each sentence, there are at most 6 translations obtained using automatic AI models or human translations. Your task is to rate the sentence along with the translations on their simplicity. In addition, for each of the translations, you are required to rate the content preserved in them, their fluency, and any hallucinations that may have been introduced in them.

Simplicity: This refers to how simple of plain English-like the given sentence or translation is. When we say simplicity, we are referring to how plain English-like a given translation is looking. For pointers on plain English versions of SEC contracts, this resource gives very nice examples in Chapter 6: https://www.sec.gov/pdf/handbook.pdf. 1: very complex; 5: very simple and easily understandable for laypeople without much legal background.

Content preserved in a translation: This refers to the amount of information from the given sentence that is retained in the translation. 1: Almost every detail is missed; 5: Every detail is covered in the translation.

Fluency of a translation: Fluency refers to how natural and grammatical a sentence/translation is. Example of fluent sentence: In addition, it is impractical to make such a law. Example of non-fluent sentence: It is unfair to release a law only point to the genetic disorder. 1: Not fluent or unnatural or grammatically incorrect translation; 5: Very fluent, natural, and grammatically correct translation.

Hallucination in a translation: The refers to the degree of incorrect or redundant information included in the translation compared to given sentence. 1: No redundant or incorrect information is present in the translation, every detail in it is taken from the given sentence; 5: Lot of redundant or incorrect information present in the translation compared to given sentence. (frequency < 10K)9 . This results in a total of 2, 708 complex tokens, which include misconduct, acquisitions, and obligors.

DISSIM outputs a graph-like structure of the input. To get a sentence from the graph-like structure, we traverse it from left to right and construct an output using the leaf nodes. If DISSIM fails to generate any graphs, we copy the input as output without any transformations. It uses a set of hand-crafted transformation rules to recursively transform an input sentence into a two-layered hierarchical representation in the form of core sentences and accompanying contexts that are linked via rhetorical relations (such as list, elaboration). For further details on the specific rule, we refer the readers to Niklaus et al. (2019a). We train the KIS model on 67K legal text sentences selected randomly from LEDGAR dataset that do not occur in the test data. We train the KIS model using the same GPT-2 medium checkpoint and other hyperparameters as in (Laban et al., 2021). We use huggingface's transformers library (Wolf et al., 2019) to fine-tune BART models for 3 epochs using Adam optimizer with batch size of 8 and maximum sequence length of 256.

# C Human Evaluations

Table 6 shows the instructions used to guide the Upwork annotators for rating the legal sentences and model outputs for their simplicity, meaning preservation, hallucinations, and fluency. We conducted interviews by first giving a few legal sentences from SEC contracts and instructing them to explain the information conveyed in them in easy-to-understand language. Based on further discussions, we selected two annotators for this task. The two annotators are paid $15 and $20 per hour respectively. Table 7 shows a few simplifications that the annotators provided during the interviews.

# Legal sentence

Annotator-1

Annotator-2

In the event that the Landlord shall deem it necessary or be required by any governmental authority to alter, repair, remove, reconstruct or improve any part of the demised premises or of the building in which the demised premises are located (unless the same result from Tenant's act, neglect, default or mode of operation in which event Tenant shall make all such repairs, alterations and improvements), then the same shall be made by the Landlord with reasonable dispatch, however, such obligation of Tenant shall not extend to maintenance, repairs or replacements necessitated by the intentional wrongdoing or gross negligence of Landlord.

The landlord will repair, remove, reconstruct or improve the leased property if it is required by any governmental authority. However, the landlord is not entitled to do so if it is the tenant's fault. The Landlord will make the repairs or replacements as soon as possible. Also, it is not the tenant's duty to maintain or repair the property if the damages were caused by the Landlord's negligence.

Where the landlord feels necessary or where it is required by any government authority to repair, remove or reconstruct any part or the building which is used by a Tenant under lease agreement. The landlord will make reasonable efforts to repair or reconstruct such part or building leased. As an exception, where such damage to the leased part or building is the result from the Tenant's act, default or mode of operating the area in such case the Tenant will make all such repairs. This obligation of Tenant will not extend to repairs if such damage is the result of intention carelessness on the part of the landlord. Any termination of Executive's employment by the Company without Cause (and not due to Executive's death or Permanent Disability) shall be made by the provision of at least fourteen (14) days' prior written notice to Executive in accordance with Section 4.2 ; provided , however , that the Company may, in its sole discretion, elect to pay Executive for all or any part of the notice period in lieu of providing prior written notice, calculated based on the annualized rate of Executive's Effective Base Salary at the time of termination.

A written notice of fourteen days must be given by the company to the employee if the employee is terminated without any cause and not due to death. However, the company can pay an employee for the notice period as per their annual base salary.

As per Section 4.2, for terminating the Executive without cause (and not due to Executive's Death or Permanent Disability) the Company will provide a prior written notice of 14 days to the Executive. In this case, the Company at its own discretion can choose to pay to the Executive all or any part of the amount against such notice period. The calculation of such amount will be based on annual base salary of the Executive at the time of termination. The Stockholder hereby ratifies and confirms all that such irrevocable proxy may lawfully do or cause to be done by virtue hereof.

The company now agrees and agrees all that such a proxy may illegally do or cause to be done by virtue of.

## 0.18

There are no strikes, lockouts or other material labor disputes or grievances against the Borrower or any of its Subsidiaries, or, to the Borrower's knowledge, threatened against or affecting the Borrower or any of its Subsidiaries, and no significant unfair labor practice charges or grievances are pending against the Borrower or any of its Subsidiaries, or, to the Borrower's knowledge, threatened against any of them before any Governmental Authority.

There are no strikes, strikes or other material labor disputes or claims against the company or any of its branches, or, to the company's knowledge, threatened against or affecting the company or any of its branches, and no significant unfair labor practice charges or claims are pending against the company or any of its branches, or, to the company's knowledge, threatened against any of them before any Governmental Authority.

0.95 0.38 

