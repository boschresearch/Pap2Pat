{
    "id": "https://semopenalex.org/work/W3177640626",
    "authors": [
        "Yanzhi Wang",
        "Oliver J. Woodford",
        "Sergey Tulyakov",
        "Jinchang Ren",
        "Geng Yuan",
        "Jiazhuo Wang",
        "Qing Jin"
    ],
    "title": "Teachers Do More Than Teach: Compressing Image-to-Image Models",
    "date": "2021-06-01",
    "abstract": "Generative Adversarial Networks (GANs) have achieved huge success in generating high-fidelity images, however, they suffer from low efficiency due to tremendous computational cost and bulky memory usage. Recent efforts on compression GANs show noticeable progress in obtaining smaller generators by sacrificing image quality or involving a time-consuming searching process. In this work, we aim to address these issues by introducing a teacher network that provides a search space in which efficient network architectures can be found, in addition to performing knowledge distillation. First, we revisit the search space of generative models, introducing an inception-based residual block into generators. Second, to achieve target computation cost, we propose a one-step pruning algorithm that searches a student architecture from the teacher model and substantially reduces searching cost. It requires no \u2113 1 sparsity regularization and its associated hyper-parameters, simplifying the training procedure. Finally, we propose to distill knowledge through maximizing feature similarity between teacher and student via an index named Global Kernel Alignment (GKA). Our compressed networks achieve similar or even better image fidelity (FID, mIoU) than the original models with much-reduced computational cost, e.g., MACs. Code will be released at https://github.com/snap-research/CAT.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Generative adversarial networks (GANs), which synthesize images by adversarial training [21], have witnessed tremendous progress in generating high-quality, high-resolution, and photo-realistic images and videos [4,33,68]. In conditional setting [54], the generation process is controlled via additional input signals, such as segmentation information [7, 58,60,71,72], class labels [83], and sketches [29,85]. These techniques have seen applications in commercial image editing tools. However, due to their massive computation complexity and bulky size, applying generative models at scale is less practical, especially on resource-constrained platforms, where low memory foot-* Work done while at Snap Inc.  [11,20,36,64,70] on CycleGAN [85] for Horse Zebra dataset. Smaller MACs indicates more efficient models. Lower FID indicates models can generate more realistic images. Our method (red star) achieves the state-of-the-art performance-efficiency trade-off as it has the lowest FID with the smallest MACs. print, power consumption, and real-time execution are as, and often more, important than performance [36].",
                "To accelerate inference and save storage space for huge models without sacrificing performance, previous works propose to compress models with techniques including weight pruning [24], channel slimming [43,44], layer skipping [3,73], patterned or block pruning [17,35,40,42,49,50,51,52,56,57,82,84], and network quantization [12,18,30,31,32,38,75]. Specifically, these studies elaborate on compressing discriminative models for image classification, detection, or segmentation tasks. The problem of compressing generative models, on the other hand, is less investigated, despite that typical generators are bulky in memory usage and inefficient during inference. Up till now, only a handful of attempts exist [20,36,64,70], all of which degenerate the quality of synthetic images compared to the original model (Fig. 1).",
                "In this work, we focus on compressing image-to-image translation networks, such as CycleGAN [85] and Gau-GAN [58]. Existing compression method [36] obtains an efficient student model and employs two additional networks: teacher and supernet, where the former is for knowledge distillation and the latter for architecture search. However, we argue that the supernet is not necessary, as the teacher can play its role. Specifically, in our proposed framework, the teacher does more than teaching the student (i.e. knowledge distillation)-it plays a central role in all aspects of the framework through three key contributions:",
                "1. We introduce a new network design that can be applied to both encoder-decoder architectures such as Pix2pix [29], and decoder-style networks such as Gau-GAN [58]. It serves as both the teacher network design, and the architecture search space of the student. 2. We directly prune the trained teacher network using an efficient, one-step technique that removes certain channels in its generators to achieve a target computation budget, e.g., the number of Multiply-Accumulate Operations (MACs). This reduces architecture search costs by at least 10, 000\u00d7 than the state-of-the-art compression method for generative models. Furthermore, our pruning method only involves one hyperparameter, making its application straightforward."
            ],
            "subsections": []
        },
        {
            "title": "We introduce a knowledge distillation technique based",
            "paragraphs": [
                "on the similarity between teacher and student models' feature spaces, which we call knowledge distillation with kernel alignment (KDKA). KDKA directly forces feature representations from the two models to be similar, and avoids extra learnable layers [36] to match the different dimensions of teacher and student feature spaces, which could otherwise lead to information leakage.",
                "We name our method as CAT as we show teacher model can and should do Compression And Teaching (distillation) jointly, which we find is beneficial for finding generative networks with smaller MACs, using much lower computational resource than prior work. More importantly, our compressed networks can achieve similar or even better performance than their original counterparts (Tab. 1)."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "Due to their high computation cost, running GANs on resource-constrained devices in real-time remains a challenging problem. As a result, GAN compression has garnered attention recently. Existing methods [1,9,20,36,64,70] exploit network architecture search/pruning and knowledge distillation (discussed below). Although they can compress the original models (e.g., CycleGAN [85]) to a relatively small MACs, all these methods suffers from sacrifice on performance. In contrast, our method finds smaller networks than existing compressed GAN models, whilst improves performance over the original models, such as Pix2pix [29], CycleGAN [85], and GauGAN [58]. Network architecture search & pruning. To determine the structure of a pruned model, previous work employs neural architecture search (NAS) [6,10,37,40,39,41,42,46,47,53,61,66,74,80,86] and pruning techniques [3, 16,17,35,42,43,44,49,50,51,52,56,57,63,73,78,79,81,82,84], where the number of channels and/or operations can be optimized automatically. Applying these methods directly on generative models can lead to inferior performance of compressed models than their original counterparts. For example, Shu et al. [64] employ an evolutionary algorithm [59] and Fu et al. [20] engage differentiable network design [39], while Li et al. [36] train a supernet with random sampling technique [5, 23,79,80] to select the optimal architecture. The common key drawback of these methods is the slow searching process. In contrast, directly pruning on a pre-trained model is much faster. Following previous methods of network slimming [43,44], Wang et al. [70] apply 1 regularization to generative models for channel pruning. However, they report performance degradation compared to the original network. Besides, these pruning methods require tuning additional hyperparameters for 1 regularization to encourage channel-wise sparsity [43,44] and even more hyper-parameters to decide the number of channels to be pruned [53], making the process tedious. Additionally, GAN training involves optimizing multiple objective functions, and the associated hyperparameters make the training process even harder. Recently, lottery ticket hypothesis [19] is also investigated on GAN problem [11], while the performance is not satisfactory. Knowledge distillation [26] is a technique to transfer knowledge from a larger, teacher network to a smaller, student network, and has been used for model compression in various computer vision tasks [8,9,48,76,45]. A recent survey [22] categorizes knowledge distillation as responsebased, feature-based, or relation-based. Most GAN compression methods [1,9,20] use response-based distillation, enforcing the synthesized images from the teacher and student networks to be the same. Li et al. [36] apply featurebased distillation by introducing extra layers to match feature sizes between the teacher and student, and minimizing the differences of these embeddings using mean squared error (MSE) loss. However, this has the potential problem that some information can be stored in those extra layers, without being passed on to the student. Here, we propose to distill knowledge by directly maximizing the similarity between features from teacher and student models."
            ],
            "subsections": []
        },
        {
            "title": "Methods",
            "paragraphs": [
                "In this section, we show our method for searching a compressed student generator from a teacher generator. We revisit the network design of conditional image generation models and introduce inception-based residual blocks (Sec. 3.1). The teacher model is built upon the proposed block design and can serve two purposes. First, we show that the teacher model can be viewed as a large search space that enables one-shot neural architecture search without training an extra supernet. With the proposed onestep pruning method, a computationally efficient network that satisfies a given computational budget can be found instantly (Sec. 3.2). Second, we show the teacher model itself is sufficient for knowledge distillation, without necessity of introducing extra layers. By maximizing the similarity between intermediate features of teacher and student network directly, where features of the two networks contain different numbers of channels, we can effectively transfer knowledge from teacher to student (Sec. 3.3)."
            ],
            "subsections": [
                {
                    "title": "Design of Teacher Generator",
                    "paragraphs": [
                        "Existing efforts leverage supernet to introduce search space that contains more efficient networks [5, 23,36]. The optimization of supernet can lead to extra training costs. However, as we already have a teacher network in hand, searching efficient student from the teacher model should be more straightforward, as long as the teacher network contains a large searching space. In this way, the teacher network can perform both knowledge distillation and provide search space. Therefore, the goal of obtaining a good supernet can be changed to design a teacher generator that can synthesize high fidelity images; and itself contains a reasonable search space. Inception-based residual block. With the above goal bearing in mind, we design a new architecture for the image generation tasks so that a pre-trained teacher generator with such architecture can serve as a large search space. We aim to search for a smaller student network that can have different operations (e.g., convolution layers with various kernel size) and different numbers of channels than the teacher network through pruning. Towards this end, we adopt the widely used inception module on discriminative models [53,65,87] to the image generators and propose the inception-based residual block (IncResBlock). A conventional residual block in generators only contains convolution layers with one kernel size (e.g., 3 \u00d7 3), while in IncResBlock, as shown in Fig. 2, we introduce convolution layers with different kernel sizes, including 1 \u00d7 1, 3 \u00d7 3, and 5 \u00d7 5. Additionally, we incorporate depth-wise blocks [27] into IncResBlock as depth-wise convolution layers typically require less computation cost without sacrificing the performance, and are particularly suitable for models deployed on mobile devices [62]. Specifically, the IncResBlock includes six types of operations, with two types of convolution layers and three different kernel-sizes. To achieve similar total computation cost, we set the number of output channels for the first convolution layers of each operations to that of the original residual blocks divided by six, which is the number of different operations in the IncResBlock. We find the performance is maintained thanks to the architecture design.",
                        "To get our teacher networks, for Pix2pix and CycleGAN, we replace all residual blocks in original models with the IncResBlock. For GauGAN, we apply IncResBlock in both the SPADE modules and the residual blocks. More details are illustrated in the supplementary materials."
                    ],
                    "subsections": []
                },
                {
                    "title": "Search from Teacher Generator via Pruning",
                    "paragraphs": [
                        "With the teacher network introduced, we search a compressed student network from it. Our searching algorithm includes two parts. The first one is deciding a threshold based on the given computational budget, and the second one is pruning channels with a scale less than a threshold. Compared with existing iterative pruning methods [43,53], we only perform pruning once, and we name our searching algorithm as one-step pruning. Automatically threshold searching. Following existing efforts [43,44], we prune the channels through the magnitudes of scaling factors in normalization layers, such as Batch Normalization (BN) [28] and Instance Normalization (IN) [69]. To this end, a threshold is required to choose channels to prune. As we train the teacher model without regularization, there is no constraint to force the teacher model to be sparse. The magnitude of scaling factors from the normalization layers is not guaranteed to be small. Thus, the previous iterative pruning methods, which remove channels using a manually designed threshold, are not suitable for our network.",
                        "To solve this, we determine the threshold by a given computation budget, which can be MACs or latency. All channels with scale smaller than the threshold are pruned until the final model achieves the target computation budget. We find the scale threshold by binary search on the scaling factors of normalization layers from the pre-trained teacher model. Specifically, we temporarily prune all channels with a scaling factor magnitude smaller than the threshold and measure the computational cost of the pruned model. If it is smaller than the budget, the model is pruned too much and we search in the lower interval to get a smaller threshold; otherwise, we search in the upper interval to get a larger value. During this process, we also keep the number of output channels for convolution layers outside the IncResBlock larger than a pre-defined value to avoid an invalid model. Details of the algorithm are illustrated in Algorithm 1. Channel pruning. With the threshold decided, we perform network searching via pruning. Given an IncResBlock, it is possible to change both the number of channels in each layer and modify the operation, such that, e.g., one IncRes-Block may only include layers with kernel sizes 1 \u00d7 1 and 3 \u00d7 3. Similar to Mei et al. [53], we prune channels of the normalization layers together with the corresponding convolution layers. Specifically, we prune the first normalization layers for each operation in IncResBlock, namely the ones after the first k \u00d7 k convolution layers for conventional operations and the ones after the first 1 \u00d7 1 convolution layers for depth-wise operations. end if 12: end while Discussion. Our searching algorithm is different from previous works that focus on compressing generative models in the following three perspectives. First, we search an efficient network from a pre-trained teacher model without utilizing an extra supernet [36]. Second, we show the scales of the normalization layers in the pre-trained teacher network are sufficient for pruning, therefore, weight regularization for iterative pruning [53,70] might not be necessary for the generation tasks. Third, the teacher network can be compressed to several different architectures, and we can find the student network that satisfies an arbitrary type of computational cost, e.g., MACs, under any value of predefined budget during the searching directly. Such differences bring us three advantages. First, searching cost is significantly reduced without introducing extra network. Second, removing the weight regularization, e.g., 1 -norm, eases the searching process as a bunch of hyper-parameters are reduced, which we find are hard to tune in practice. Third, we have more flexibility to choose a student network with required computational cost."
                    ],
                    "subsections": []
                },
                {
                    "title": "Distillation from Teacher Generator",
                    "paragraphs": [
                        "After obtaining a student network architecture, we train it from scratch, leveraging the teacher model for knowledge distillation. In particular, we transfer knowledge between the two networks' feature spaces, since this has been shown [36] to achieve better performance than reconstructing images synthesized by the teacher [20]. With different numbers of channels between teacher and student layers, Li et al. [36] introduce auxiliary, learnable layers that project the student features into the same dimensional space as the teacher, as shown in Fig. 3. Whilst equalizing the number of channels between the two networks, these layers can also impact the efficacy of distillation, since some information can be stored in these extra layers. To avoid information loss, we propose to encourage similarity between the two feature spaces directly."
                    ],
                    "subsections": [
                        {
                            "title": "Similarity-based Knowledge Distillation",
                            "paragraphs": [
                                "We develop our distillation method based on centered kernel alignment (CKA) [14,15], a similarity index between two matrices, X \u2208 R n\u00d7p1 and Y \u2208 R n\u00d7p2 , where after centering the kernel alignment (KA) is calculated, which is defined as 1",
                                "It is invariant to an orthogonal transform and isotropic scaling of the rows, but is sensitive to an invertible linear transform. Importantly, p 1 and p 2 can differ. Kornblith et al.",
                                "[34] use this index to compute the similarity between different learned feature representations of varying lengths (p 1 = hwc 1 & p 2 = hwc 2 , where h, w and c \u2022 are the height, 1 The identity Y T X 2 F = vec(XX T ), vec(Y Y T ) is used to achieve computational complexity of O(n 2 hw max(c 1 , c 2 )) [34]. width and number of channels of the respective layer tensors; n is the batch size). KDKA. To compare similarity between teacher and student features, we adopt KA of the two tensors X and Y without centering. We find that centering is not necessary for our purpose of similarity-based knowledge distillation. To perform distillation, we maximize the similarity between features of teacher and student networks by maximizing KA."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "Distillation Loss",
                            "paragraphs": [
                                "We conduct distillation on the feature space. Let S KD denote the set of layers for performing knowledge distillation, whereas X (l) t and X (l) s denote feature tensors of layer l from the teacher and student networks, respectively. We minimize the distillation loss L dist as follows:",
                                "where the minus sign is introduced as we intend to maximize feature similarity between student and teacher models."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "Learning",
                    "paragraphs": [
                        "We train teacher networks using the original loss functions, which includes an adversarial loss L adv as follows:",
                        "(3) where x and y denote the input and real images, and D and G denote the discriminator and generator, respectively. Full objective for student. For the training of student generator for CycleGAN, we adopt the setting from [36] where we use the data generated from teacher network to form paired data and train the student the same way as Pix2pix with a reconstruction loss L recon . Therefore, for CycleGAN and Pix2pix, the overall loss function for student training is:",
                        "For the training of GauGAN, there is an additional feature matching loss L fm [72], and the overall loss function is as follows:",
                        "\u03bb adv , \u03bb recon , \u03bb dist and \u03bb fm in Eqn. 4 and Eqn. 5 indicate the hyper-parameters that balance the losses."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "In this section, we show the results of compressing image-to-image models. We introduce more details about network training and architectures, together with more qualitative results in the supplementary materials."
            ],
            "subsections": [
                {
                    "title": "Basic Setting",
                    "paragraphs": [
                        "Models. We conduct experiments on generation models, including Pix2pix [29], CycleGAN [85], and GauGAN [58]. Following [36], we inherit the teacher discriminator by using the same architecture and the pre-trained weights, and finetune it with the student generator for student training. Datasets. We examine our method on the following datasets. Horse Zebra and Zebra Horse are two datasets from CycleGAN [85], which converts horse images to zebra and vice versa. There are 1, 187 horse images and 1, 474 zebra images. Cityscapes [13] is a dataset for mapping semantic inputs to images of street scenes. There are 2, 975 training and 500 validation data, and we apply Pix2pix and GauGAN models on it. Map Aerial photo contains 2, 194 images [29], and we apply Pix2pix model on it. Evaluation metrics. We adopt two standard metrics for the evaluation of generative models. For the Cityspaces dataset, we follow existing works [29,58] to use a semantic segmentation metric to evaluate the quality of synthetic images. We run an image segmentation model, which is DRN-D-105 [77], on the generated images to calculate mean Intersection over Union (mIoU). A higher value of mIoU indicates better quality of generated images. For other datasets, we apply commonly used Fr\u00e9chet Inception Distance (FID) [25], as it estimates the distribution between real and generated images. We also adopt a recent proposed metric named Kernel Inception Distance (KID) [2] for more thorough comparison. A lower FID or KID value indicates better model performance."
                    ],
                    "subsections": []
                },
                {
                    "title": "Comparison Results",
                    "paragraphs": [
                        "Quantitative results. We compare our method with existing studies for image generation tasks on various datasets. The results are summarized in Tab. 1 and Tab. 2. We can see that for all datasets included, our models consume the smallest MACs while achieving comparable and mostly the best performance. Particularly, we achieve better performance than the original models for almost all datasets while  To further verify the effectiveness of our method for compressing generative models, we experiment on Gau-GAN with two target MACs: 30B and 5.6B. We choose 5.6B as it is similar to our compressed Pix2pix model on Cityscapes. We find that with 30B MACs, which is 9.4\u00d7 smaller than GauGAN, the mIoU of our model is better than the original, which is increased from 62.18 to 62.35.",
                        "We further compress the model to less than 5.6B with a compression ratio of 50.9\u00d7, and the mIoU is reduced to 54.71. However, it is still much better than that from the Pix2pix model. These demonstrate that our method is a sound technique for compressing image-to-image models, and provides the state-of-the-art trade-off between computation complexity and image generation performance.",
                        "Qualitative results. We further show qualitative results to illustrate the effectiveness of our method. Fig. 4 provides samples on Cityspaces, including input segmentation maps, ground-truth (GT), and generated images by different methods. Our compressed model (CAT-A) achieves better quality (higher mIoU and lower FID) than GauGAN. For example, for the leftmost image in Fig. 4, the back of the car synthesized by CAT-A is clearer than GauGAN, and CAT-A generates less blurry human images than GauGAN for the rightmost image. CAT-B, which has much-reduced MACs than GauGAN (50.9\u00d7), can also achieve better image fidelity (lower FID) than GauGAN. For Map Aerial photo with Pix2pix (Fig. 5), our method generates images with better quality for the river and buildings than the original Pix2pix model. For Horse Zebra on CycleGAN, our method can synthesize better zebra images for challenging input horse images, where the CycleGAN fails to generate. The examples shown in Fig. 4 & 5 demonstrate that our compression technique is an effective method for saving the computational cost of generative models. Besides, the compressed models can surpass the original models, even though they require much reduced computational cost and, thus, are more efficient during inference. These results indicate significant redundancy in the original large generators, and it is worth further studying the extreme of these generative models in terms of performance-efficiency trade-off. Analysis of searching cost. Here we show the analysis of searching costs for finding a student network. Our method can search the architecture under a pre-defined computational budget with a much reduced searching cost compared with previous state-of-the-art compressing method [36]. Tab. 3 provides the searching cost of the two methods on various datasets and models. As can be seen, our method is at least 10, 000\u00d7 times faster for searching. The searching time for the previous method [36] is estimated by only including the time for training a supernet, which is designed for architecture search. We estimate it as 20 hours with 1 GPU for the CycleGAN and Pix2pix models and 40 hours with 8 GPUs for the GauGAN model, both of which are much shorter than those required in practice and thus serves as a lower bound. Besides, we have ignored the time required for searching a student network from the supernet for [36], which is also non-negligible. For example, for Cityscapes with Pix2pix model, the supernet includes more than 5, 000 possible architectures, and each requires around 3 minutes with 1 GPU for evaluation, resulting in several days of architecture search. Despite, we do not take this process of [36] into account for time-estimation in Tab. 3."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "In this paper, we study the problem of compressing generative models, especially the generators for image-toimage tasks. We show the problem can be tackled by using a powerful teacher model, which is not restricted to teach a student through knowledge distillation, but can serve as a Compared with original networks (Pix2pix and Cycle-GAN), our models have much reduced MACs and can generate images with higher fidelity (lower FID) by synthesizing textures that are not well-handled by the original large models.",
                "supernet to search efficient architecture (for student) under pre-defined computational budgets.",
                "Specifically, our framework is built upon a newly designed teacher model, which incorporates the proposed In-cResBlock. We show such teacher model contains a large search space where efficient student architecture can be determined through network searching. The searching process is implemented with our proposed one-step pruning algorithm, which can be conducted with negligible efforts. We also introduce a similarity-based knowledge distillation technique to train student network, where feature similarity between student and teacher is measured directly by the proposed KA index. With our method, we can obtain networks that have similar or even better performance than original Pix2pix, CycleGAN, and GauGAN models on various datasets. More importantly, our networks have much reduced MACs than their original counterparts. Our work demonstrates that there remains redundancy in existing generative models, and we can achieve improved performance, e.g., synthesizing images with better fidelity, with much reduced computational cost. It is worth further investigating the ability of generative models to synthesize images with high quality under an extremely constrained computational budget, which we leave for future study. "
            ],
            "subsections": []
        },
        {
            "title": "Acknowledgement",
            "paragraphs": [
                "The Authors would like to appreciate Jieru Mei from John Hopkins University for invaluable technical discussion. Also this research is partially supported by National Science Foundation CNS-1909172."
            ],
            "subsections": []
        },
        {
            "title": "S1. Implementation Details",
            "paragraphs": [
                "In this section, we provide more implementation details in our work.",
                "Training details. For CycleGAN and Pix2pix models, we use batch size of 32 for teacher and batch size of 80 for student, while for GauGAN, the batch size is set to 16 for both. For each model and each dataset, we apply the same training epochs for teacher and student networks. The learning rate for both generators and discriminators are set as 0.0002 for all datasets and models. More detailed training hyperparameters are summarized in Table S1. For the layers used for knowledge distillation between teacher and student networks, we follow the same strategy as Li et al. [36]. Specifically, for Pix2pix and CycleGAN models, the 9 residual blocks are divided into 3 groups, each with three consecutive layers, and knowledge is distilled upon the four activations from each end layer of these three groups. For GauGAN models, knowledge distillation is applied on the output activations of 3 from the total 7 SPADE blocks, including the first, third and fifth ones.",
                "More details for normalization layers. We find that instance normalization [69] without tracking running statistics is critical for dataset Horse\u2192Zebra to achieve good performance on the student model, and for dataset Zebra\u2192Horse, synchronized batch normalization with tracked running statistics gives better performance. For the other datasets batch normalization [28] with tracked running statistics is better. Normalization layers without track running statistics introduce extra computation cost, and we take this into account for our calculation of MACs during pruning. Moreover, for GauGAN, we use synchronized batch normalization as suggested by previous work [58,67], and remove the spectral norm [55] as we find it does not have much impact on the model performance.",
                "Network details for GauGAN. For GauGAN, we find it is sufficient for each spade residual block to keep only the first SPADE module in the main body while replace the second one as well as the one in the shortcut by synchronized batch normalization layer. This saves computation cost by a large extent. Besides, we use learnable weights for the second synchronized block for the purpose of pruning. These weights do not introduce extra computation cost, as the running statistics are estimated from training data and not recalculated during inference, enabling fusing normalization layers into the convolution layers. Further, we replace the three convolution layers in the SPADE module by our proposed inception-based residual block (IncResBlock), with normalization layers included for pruning. The details for the architecture are illustrated in Figure S1. We name our SPADE module as IncSPADE and SPADE residual block as IncSPADE ResBlk.",
                "To prune the input channel for each model, we add an extra normalization layer (synchronized batch normalization) with learnable weights after the first fully-connected layer, and prune its channels together with other normalizations using our pruning algorithm described in the Section 3.2 of the main paper. During pruning, we keep the ratio of input channels between different layers as the original model, and the lower bound for the first layer (which has the largest number of channels) is determined by that for the last layer multiplied by the channel ratio, so that all channels are above the bound and the channel ratio is unchanged."
            ],
            "subsections": []
        },
        {
            "title": "S2. Ablation Analysis of Knowledge Distillation",
            "paragraphs": [
                "Here we show the ablation analysis for knowledge distillation methods. We use our searching method to find a student architecture on Pix2pix task using the Cityscapes dataset, and compare student training without knowledge distillation, with MSE distillation as in [36], and the similarity-based distillation we proposed. The results are summarized in Tab. S2, where w/o Distillation denotes training the student without distillation, and w/ MSE; Loss Weight 0.5 and w/ MSE; Loss Weight 1.0 denotes MSE distillation with weight 0.5 and 1.0, respectively. We find that distillation indeed improves performance, and our distillation method, which employs KA to maximize feature similarity, is better than MSE on transferring knowledge from teacher to student via intermediate features."
            ],
            "subsections": []
        },
        {
            "title": "S3. More Qualitative Results",
            "paragraphs": [
                "We show more qualitative results for CycleGAN on Horse Zebra, Pix2pix on Map Aerial photo, as well as GauGAN on Cityscapes in Figs. S2,S3, and S4, respectively.     "
            ],
            "subsections": []
        }
    ]
}