# DESCRIPTION

## TECHNICAL FIELD

- relate to machine-learning systems

## BACKGROUND

- limitations of existing solutions

## SUMMARY

- introduce BERT-based machine-learning tools

## DETAILED DESCRIPTION

- introduce BERT-based response prediction model
- describe response prediction engine
- explain encoding input text with BERT encoder
- combine encoded input text with encoded demographic data
- compute predicted emotional response score
- describe text-editing tool with BERT-based response prediction model
- provide example of text-editing tool
- describe classification module
- explain computing emotional response scores
- describe limitations of existing machine-learning models
- explain improvements of embodiments described herein
- describe computing environment for predicting emotional responses
- explain text processing system and training system
- describe BERT-based response prediction model training
- explain user interface engine
- describe interaction data and emotion prediction
- explain application to conversational artificial intelligence tools
- describe customizing text to different expected audiences
- outline process for using BERT-based machine-learning tools
- describe classification module
- illustrate architecture for BERT-based response prediction model
- explain text processing system
- detail BERT encoder
- describe feed-forward neural network
- explain concatenation module
- describe classification module
- illustrate classification heads
- explain dense layer sets and softmax layers
- describe output probability distributions
- detail training engine
- explain process for training BERT-based response prediction model
- describe accessing training dataset
- detail modifying BERT encoder parameters
- explain fine-tuning phase
- describe alternative and parallel training
- select parameter value sets
- define BERT-based response prediction model
- describe training engine
- compute loss values
- identify desirable parameter values
- output trained BERT-based response prediction model
- describe alternative training
- compute single-task loss
- compute multi-task loss
- describe graphical interfaces
- generate user interface
- detect input text
- detect input demographic profile
- provide input to BERT-based response prediction model
- update user interface
- describe architectures for BERT encoder
- implement BERT encoder
- describe encoder layer
- describe multi-head self-attention network
- describe scaled dot-product attention block
- describe computing system
- execute program code
- store program data
- establish data connection
- provide input and output

### Experimental Results

- conduct experiment on empathy prediction
- pre-train BERT on Blog Authorship Corpus
- fine-tune BERT-based response prediction model
- evaluate performance using five-fold cross validation
- compare BERT-based models with Random Forest and deep learning baselines
- report accuracies for gender-specific empathy prediction
- analyze results for age, income, and education
- evaluate performance of tBERT variants on full dataset
- discuss general considerations for claimed subject matter

