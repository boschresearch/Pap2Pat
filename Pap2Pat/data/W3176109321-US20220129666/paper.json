{
    "id": "https://semopenalex.org/work/W3176109321",
    "authors": [
        "Jue Wang",
        "Anoop Cherian"
    ],
    "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers",
    "date": "2022-10-01",
    "abstract": "One-class learning is the classic problem of fitting a model to the data for which annotations are available only for a single class. In this paper, we explore novel objectives for one-class learning, which we collectively refer to as Generalized One-class Discriminative Subspaces (GODS). Our key idea is to learn a pair of complementary classifiers to flexibly bound the one-class data distribution, where the data belongs to the positive half-space of one of the classifiers in the complementary pair and to the negative half-space of the other. To avoid redundancy while allowing non-linearity in the classifier decision surfaces, we propose to design each classifier as an orthonormal frame and seek to learn these frames via jointly optimizing for two conflicting objectives, namely: i) to minimize the distance between the two frames, and ii) to maximize the margin between the frames and the data. The learned orthonormal frames will thus characterize a piecewise linear decision surface that allows for efficient inference, while our objectives seek to bound the data within a minimal volume that maximizes the decision margin, thereby robustly capturing the data distribution. We explore several variants of our formulation under different constraints on the constituent classifiers, including kernelized feature maps. We demonstrate the empirical benefits of our approach via experiments on data from several applications in computer vision, such as anomaly detection in video sequences, human poses, and human activities. We also explore the generality and effectiveness of GODS for non-vision tasks via experiments on several UCI datasets, demonstrating state-of-the-art results.",
    "sections": [
        {
            "title": "INTRODUCTION",
            "paragraphs": [
                "T HERE are several real-world applications for which it may be straightforward to characterize the normal operating behavior and collect data to train learning systems, however may be difficult or sometimes even impossible to have data when abnormalities or rare events happen. Examples include but not limited to, an air conditioner making a spurious vibration, a network attacked by an intruder, sudden variations in a patient's vitals, or an accident captured in a video surveillance camera, among others [1]. In machine learning literature, such problems are usually called oneclass problems [2], [3], signifying the fact that we may have an unlimited supply of labeled training data for the one-class (corresponding to the normal operation of the system), but do not have any labels or training data for situations corresponding to abnormalities.",
                "Popular problems in computer vision, such as video novelty detection [4], [5], [6], surveillance anomaly detection [7], [8], [9], image denoising [10], and outlier detection [11], [12], are variants of the standard one-class setting. The main goal of such methods is usually to learn a model that fits to the normal set, such that abnormalities can be characterized as outliers of this learned model. Given that the distribution of abnormal samples are often unbounded or could even partially overlap with the normal data samples (e.g., in a video surveillance application, when both normal and abnormal features could be present in a time window), achieving one-class classification can be practically challenging.",
                "Classical solutions to one-class problems are mainly extensions to support vector machines (SVMs), such as the one-class SVM (OC-SVM), that maximizes the margin of the discriminative hyperplane from the origin [13]. There are extensions of this scheme, such as the least-squares one-class SVM (LS-OSVM) [14] or its online variants [15], that learn to find a tube of minimal diameter that includes all the labeled data. Another popular approach is the support-vector data description (SVDD) that finds a hypersphere of minimum radius that encapsulates the training data [16]. There have also been kernelized extensions of these schemes that use the kernel trick to embed the data points in a reproducible kernel Hilbert space, potentially enclosing the 'normal' data with arbitrarily-shaped boundaries.",
                "Apart from the classic one-class solutions, there is an increasing number of recent works that use deep neural networks for building the one-class model [6], [9], [17], [18], [19], [20]. In these approaches, typically a deep auto-encoder model is trained on the one-class data such that its reconstruction error is minimized. When such a model is provided with an out-ofdistribution data sample (anomaly), the reconstruction error can be large, which could be used as an anomaly cue [6], [20]. There are extensions of this general architecture using generative adversarial networks (GANs) to characterize the distribution of the in-class samples [21], [22]. For anomaly detection on time-varying inputs, there are also adaptations using predictive auto-encoders, such as [9], that attempts to generate the (latent) future samples, and flags anomalies if the predicted sample is significantly different from the observed one.",
                "While, these approaches have been widely adopted in several applications (see e.g., Chandala et al. [1]), they have drawbacks. For example, the OC-SVM uses only a single hyperplane, however using multiple hyperplanes may be beneficial [23]. The SVDD scheme makes a strong assumption on the spherical nature of the data distribution. Using kernel methods may impact scalability, while deep learning methods may need specialized hardware (such as GPUs) and large training sets. Thus, trading-off between the pros and cons of these diverse prior methods, we propose novel generalizations of these techniques, which we call generalized one-class discriminative subspaces (GODS). The key goal of GODS is to combine the linearity properties of OC-SVM, and the non-linear bounded characterization of SVDD in a single framework. However, our proposed one-class model is neither linear nor uses a spherical classifier, instead uses a pair of orthornormal frames 1 whose columns characterize linear classifiers. Specifically, these columns are optimized such that the one-class data belongs to the positive half-spaces of the columns in one of these classifiers and to the negative half-spaces of the columns in the other; thus these classifiers jointly form a complementary pair. These classifiers, with their respective half-spaces defined by their orthonormal columns, non-linearly bound the data from different directions. Our learning objective, to find these complementary classifiers, jointly optimizes two opposing criteria: i) to minimize the distance between the two classifiers, thus bounding the data within the smallest volume, and ii) to maximize the margin between the hyperplanes and the data, thereby avoiding overfitting, while improving classification robustness.",
                "1. Orthonormal frames are matrices with linearly independent unit norm columns.",
                "Our proposed GODS model offers several advantages against prior methods: (i) the piecewise linear decision boundaries approximate a non-linear classifier, while providing computationally cheap inference, and (ii) the use of the complementary classifier pair allows flexible bounding of the data distribution of arbitrary shapes, as illustrated in Figure 1. For example, our kernelized variant of GODS (called KODS), that we introduce in Section 4.4, bounds data from outside as well as inside (see Figure 1(d)), which is usually not possible in prior methods. Albeit these benefits, our objective is non-convex due to the orthogonality constraints. However, such non-convexity fortunately is not a significant practical concern as they naturally place the optimization objective on the Stiefel manifold [24]. This is a well-studied Riemannian manifold [25] for which there exist efficient nonlinear optimization methods at our disposal. We use one such optimization scheme, called Riemannian conjugate gradient [26], which is fast and efficient.",
                "To empirically evaluate the benefits of our GODS formulations, we apply them to one-class data arising from various anomaly detection problems in computer vision and machine learning. One novel task we consider in this paper is that of outof-pose (OOP) detection in cars [27], [28]. Specifically, in this task, our goal is to detect if the passengers or the driver are seated OOP as captured by an inward-looking dashboard camera. For this task, we showcase the effectiveness of our approaches on a new dataset, which we call Dash-Cam-Pose. Apart from this task, we also report experimental results on several standard and public anomaly detection benchmarks in computer vision, such as on the UCF-crime [29] and the UCSD Ped2 [30] datasets. We also provide experiments on the standard JHMDB action recognition dataset [31] re-purposed for the anomaly detection task. We further analyze the generalizability of our approach to non-computer vision applications by providing results on five UCI datasets. Our results demonstrate that GODS variants lead to significant performance improvements over the state of the art.",
                "We summarize below the main contributions of this paper:"
            ],
            "subsections": []
        },
        {
            "title": "\u2022",
            "paragraphs": [
                "We first introduce a basic one-class discriminative subspace (BODS) classifier that uses a pair of hyperplanes."
            ],
            "subsections": []
        },
        {
            "title": "\u2022",
            "paragraphs": [
                "We generalize BODS to use multiple hyperplanes, termed generalized one-class discriminative subspaces (GODS) and derive a kernelized variant, termed KODS. We also present several formulations of GODS under different assumptions on the classifiers."
            ],
            "subsections": []
        },
        {
            "title": "\u2022",
            "paragraphs": [
                "We explore Riemannian conjugate gradient algorithms for optimizing our objectives. Specifically, the BODS and GODS formulations use a Stiefel manifold, while KODS is modeled on the generalized Stiefel manifold."
            ],
            "subsections": []
        },
        {
            "title": "\u2022",
            "paragraphs": [
                "We present a novel task of out-of-pose detection, and provide a new dataset, termed Dash-Cam-Pose."
            ],
            "subsections": []
        },
        {
            "title": "\u2022",
            "paragraphs": [
                "We provide experiments on Dash-Cam-Pose, three standard vision datasets, and five UCI datasets, demonstrating substantial performance benefits of our methods. We note that this work is an extension of the ICCV conference paper [32] and extends it in the following ways: (i) we provide novel extensions to the GODS using different assumptions on the complementary classifiers (Section 4.3), (ii) we derive a kernelized variant of GODS (Section 4.4) and provide practical simplifications for optimization on the generalized Stiefel manifold (Section 6.2), (iii) we provide adaptive classifiation rules in Section 5, and (iv) provide additional experiments and ablative studies, including new results on UCSD Ped2 and UCI datasets."
            ],
            "subsections": []
        },
        {
            "title": "RELATED WORK",
            "paragraphs": [
                "As one-class problems arise in numerous practical settings, they have been explored to great depth in a variety of disciplines, including but not limited to remote sensing [33], network intruder detection [34], and fraud detection [35]. In computer vision, a few illustrative problems are novelty detection [4], [5], [6], video anomaly detection [7], [8], [36], diagnosis on medical images [37], and anomalous object attribute recognition in image collections [38]. For a comprehensive review of applications, we refer the interested reader to surveys, such as [1], [39], [40]. Classic methods for modelling such one-class problems are extensions of data density estimation techniques [41], [42]. These methods attempt to model the density of the given data in the input space by trading-off between maximizing their inclusivity within a (given) density quantile while minimizing its volume. The dependence on minimal density volume in the input space is discarded in Scholkopf et al. [13], [16] against smoothness of the decision function in a non-linear (kernelized) feature space. Working in the kernel space not only allows for more flexible characterizations of the distribution of the data samples, but also allows transferring the max-margin machinery (and the associated theory) developed for support vector machines to be directly used in the one-class setting. However, as alluded to earlier, working with kernel feature maps can be demanding in large data settings, and thus our main focus in this paper is on deriving one-class algorithms in the input (primal) space. That said, we also explore a kernelized dual variant of our scheme for problems that are impossible to be modelled using our primal variant.",
                "Modern efforts to one-class learning typically use either (i) good hand-crafted representations combined with effective statistical learning models, or (ii) implicit representation and learning via neural networks. Below, we review these efforts in detail. Explicit Modelling Approaches. Performance of any one-class approach inevitably depends on the effectiveness of the data representation. For example, visual representations such as histogram of oriented gradients [43] (HOG) and histogram of optical flows (HOF) [44] have been beneficial in developing several anomaly detection algorithms. A Markov random filed (MRF) on HOG and HOF descriptors is proposed in Zhang et al. [45] for modeling the normal patterns in a semi-supervised manner, where an abnormal sample model is iteratively derived from the normal one using Bayesian adaptation. In Xu and Caramanis [46], an outlier pursuit algorithm is proposed using convex optimization for the robust PCA problem. Similarly, Kim et al. [47] propose a spacetime MRF to detect abnormal activities in videos. This method uses a mixture of probabilistic principal component analysis to characterize the distribution of normal data characterized as densities on optical flow. Detecting out-of-context objects is explored in [48], [49] using support graph and generative models. Motion trajectory analysis [50], [51], [52] of objects in video sequences has been a common approach for modeling anomalies, under the strong assumption that deviant trajectories may correspond to abnormal data. Detecting salient regions in images has also been implemented by some researchers [53], [54]. In contrast to these approaches that propose problem-specific anomaly detection models, our solution is for a general setting.",
                "Sparse reconstruction analysis has been a powerful workhorse in the recent times in developing several one-class solutions [55], [56], [57], [58]. The assumption in these methods is that normal data can be encoded as sparse linear combinations of columns in a dictionary that is learned only on the normal data; however the reconstruction error of any out-of-distribution sample using this dictionary could be significant. In addition to the reconstruction loss, a study from Ren et al. [59] points out that the sparsity term should be taken into consideration for improving the anomaly detection accuracy. However, sparse reconstruction methods can be computationally expensive. To improve their efficiency, Bin et al. [57] proposes an online detection scheme using sparse reconstructibility of query signals from an atomically learned event dictionary. Yang et al. [55] improves efficiency via learning multiple small dictionaries to encode image patterns at multiple scales. While, our proposed GODS algorithm could also be treated as a dictionary of orthonormal columns, our inference is significantly cheaper in contrast to solving 1 -regularized problems in these works as GODS involves only evaluations of inner products of the data samples to the learned hyperplanes. Deep Learning Based Methods. The huge success of deep neural networks on several fundamental problems in computer vision [60], [61] has also casted its impact in devising schemes for anomaly detection [20]. Extending classical methods, a deep variant of SVDD is proposed in Ruff et al. [62], however assumes the one-class data is unimodal. Variants of OC-SVM are explored in [18], [63]. Parera and Patel [64] proposes a trade-off between compactness and descriptiveness using an external reference dataset to train a deep model on one-class data. Liang et al. [65] proposes to use statistical trends in the softmax predictions. Deep learning based feature representations have been used as replacements for hand-crafted features in several one-class problem settings. For example, Xu et al. [66] design a multi-layer autoencoder embracing data-driven feature learning. Similarly, Hasan et al. [20] propose a 3D convolutional auto-encoder to capture both spatial and temporal cues in video anomaly detection. Leveraging the success of convolutional neural networks (CNNs) to capture spatial cues, [67], [68], and [19] propose to embed recurrent networks, such as LSTMs, to model the appearance dynamics of normal data. In [69] and [70], frameworks to minimize the indistribution sample distances is proposed thereby maximizing the distance to out-of-distribution samples. In Sabokrou et al. [71], a pre-trained CNN is used for extracting region features, and a cascaded outlier detection scheme is applied. Multiple instance learning (MIL) in a deep learning setting is attempted in [29] for anomaly detection using weakly-labeled training videos via applying an MIL ranking loss with sparsity and smoothness constraints; however includes both normal and abnormal samples in the training set. Deep generative adversarial networks (GAN) have also been proposed to characterize the single class [6], [21], [22], [72]. These methods typically follow the same philosophy of training auto-encoders, however uses an adversarial discriminator to improve quality of the decoded data sample; the discriminator confidence is then used as an abnormality cue during inference.",
                "In contrast to these approaches, we focus on explicit modelling of one-class data distributions, allowing better and more controlled characterization of the single class. Deep learning approaches reviewed above are complimentary to our contributions; in fact we use deep-learned data representations in our experiments and simultaneously demonstrate our performances on non-deeplearned features as well. More recently, explicit characterization of the diversity of the normal data is explored in [73], self-supervised learning is proposed in [74], and normalizing flows for the task in [75]. We note that using our formulations within a deep neural network, while straightforward using methods described in [76], [77], is currently beyond the focus of this paper.  orthonormal frames (GODS) that maximize the margin with the data distribution while minimizing the volume of the one-class region captured (via minimizing the distance dist 2 between hyperplanes or frames). In Figure 2(c), we show a detailed depiction of the objectives in GODS using a single data point x. See Section 4.2 for more details."
            ],
            "subsections": []
        },
        {
            "title": "BACKGROUND",
            "paragraphs": [
                "Let D \u2282 R d denote the data distribution consisting of our one class-of-interest and everything outside it, denoted D, be the anomaly set. Suppose we are given n data instances",
                "The goal of one-class classifiers is to use D o to learn a functional f which is positive on D and negative on D. Typically, the label of D is assumed +1 and that of D is -1.",
                "We emphasize that we assume to have access only to the one-class data for training our models; i.e., the set D is not available for training, and samples from D are used only at test time.",
                "In One-Class Support Vector Machine (OC-SVM) [13], f is modeled as an extension of an SVM objective by learning a maxmargin hyperplane that separates the origin from the data points in D o . Mathematically, f has the form sgn(w T x + b), where (w, b) \u2208 R d \u00d7 R 1 and is learned by minimizing the following objective:",
                "where \u03be i 's are non-negative slacks, b is the hyperplane intercept, and C is the slack penalty. As a single hyperplane in the input space might be insufficient to capture non-linear data, kernel feature maps are proposed in [13].",
                "Another popular variant of one-class classifiers is the support vector data description (SVDD) [16] that instead of modeling data to belong to an open half-space of R d (as in the primal OC-SVM), assumes the labeled data inhabits a bounded set; specifically, the optimization seeks the centroid c \u2208 R d of a hypersphere of minimum radius R > 0 that can contain all points (or a given quantile of points) in D o . Mathematically, the objective reads:",
                "where, as in OC-SVM, the \u03be's model the slack. As is discussed in [13], OC-SVM and SVDD are equivalent when the underlying kernel is translation invariant. There have been extensions of this scheme, such as the mSVDD that uses a mixture of hyperspheres [78]. Other variants include (i) the density-induced SVDD [79], (ii) kernelized variants [80], and (iii) more recently, those that use subspaces for data description [81]. A major drawback of SVDD in general is the strong assumption it makes on the isotropic nature of the underlying data distribution. Such a demand is ameliorated by combining OC-SVM with the idea of SVDD in least-squares one-class SVM (LS-OSVM) [14] that learns a tube around the discriminative hyperplane that contains the input; however, this scheme also makes strong assumptions on the data distribution (such as being cylindrical).",
                "Unlike OC-SVM that learns a compact data model to enclose as many training samples as possible, a different approach is to use principal component analysis (PCA) (or its varaints, such as Robust PCA [82], [83], [84], [85], [86], and Kernel PCA) to summarize the data by using its principal subspaces. However, such an approach is usually unfavorable due to its high computational cost, especially when the dataset is large. Similar in motivation to the proposed technique, Bodesheim et al. [87] uses a null space transform for novelty detection, while Liu et al. [88] optimizes a kernel-based max-margin objective for outlier removal and soft label assignment. However, their problem setups are different from ours in that [87] requires multi-class labels in the training data and [88] is proposed for unsupervised learning.",
                "In contrast to these prior methods, we explore the one-class learning problem from a distinct perspective; specifically, to use orthonormal frames as in PCA, however instead of approximating the one-class data using these frames, our objective seeks a pair of complementary orthonormal frames that bounds the data discriminatively and in a piece-wise linear manner, such that the data subspace is sandwiched between these frames. We first present a simplified variant of this idea using two complementary hyperplanes, dubbed Basic One-class Discriminative Subspaces (BODS) (Fig. 2(a)); these hyperplanes are independently parameterized and bounds the data distribution. Note that there is a similar prior work, termed Slab-SVM [89], that learns two hyperplanes for one-class classification. However, their hyperplanes are constrained to have the same slope, which we do not impose in our BODS model; as a result, our model is more flexible than Slab-SVM. We extend BODS using multiple orthogonal hyperplanes, which we call Generalized One-class Discriminative Subspaces (GODS) (see Fig. 2(b)). The use of such discriminative subspaces has been recently explored in the context of representation learning on videos in Wang and Cherian [23] and Wang et al. [90], however requires a surrogate negative bag of features, that is found via adversarial learning. Notation: We use bold-face upper-case for matrices, bold-face lower-case for vectors, non-bold-face lower case for scalars. We use W to represent an orthonormal frame. We sometimes succinctly refer to a set of variables without their subscripts; e.g., W = {W 1 , W 2 }. The notation 1 p\u00d7q represents a matrix of ones with p rows and q columns."
            ],
            "subsections": []
        },
        {
            "title": "PROPOSED METHOD",
            "paragraphs": [
                "In this section, we formally introduce our schemes. First, we present BODS using a pair of hyperplanes, which we generalize to GODS using a pair of discriminative frames in Section 4.2. We explore variants of GODS in Section 4.3 and further generalize GODS using kernel feature maps, proposing KODS in Section 4.4. With a slight abuse of terminology, we call our entire suite of formulations as GODS."
            ],
            "subsections": [
                {
                    "title": "Basic One-class Discriminative Subspaces",
                    "paragraphs": [
                        "In this section, we introduce a basic variant of our objective, which we call Basic One-class Discriminative Subspaces (BODS). The key goal of BODS is to bound the one-class data distribution using a pair of hyperplanes. Similar to OC-SVM, we seek these hyperplanes to have a large margin from the data distribution, thus allowing robustness to minor differences in the test data distribution. Further, inspired by SVDD, we also demand the two hyperplanes to bound the data within a minimal volume. BODS combines these two conflicting objectives into a joint formulation. Mathematically, suppose (w 1 , b 1 ) and (w 2 , b 2 ) define the parameters of the pair of hyperplanes. Our goal in BODS is then to minimize an objective such that all data points x i be classified to the positive half-space of (w 1 , b 1 ) and to the negative half-space of (w 2 , b 2 ), while also minimizing a suitable distance between the two hyperplanes (see Figure 2(a)). To this end, we propose the following BODS objective: min (w1,b1),(w2,b2), \u03be1,\u03be2\u22650",
                        "where the first four terms of (1) seek large margins similar to a standard OC-SVM objective. The key element of BODS is the last term in (1) that aims to minimize a suitable distance dist between the two hyperplanes. In ( 2) and ( 3), we capture the complementary classification constraints noted above. We use the notation",
                        "for the squaredslack regularization2 and \u03b7 > 0 specifies a (given) classification margin. For BODS, we assume dist is the Euclidean distance, i.e.,"
                    ],
                    "subsections": [
                        {
                            "title": "BODS on the Unit Sphere",
                            "paragraphs": [
                                "It is often seen (especially for computer vision problems) that feature normalization, specifically unit normalizing of the inputs, demonstrate better performances [93], [94], [95]. While, such a step may be counter-intuitive as we lose the discriminativeness in the scale of the input features, such normalization often allows counteracting the effects of burstiness [93] -a statistical phenomenon due to a non-uniform distribution of data elements. Such normalization is even found to improve the performances of deep-learned features when used in max-margin frameworks [96], [97] and its importance to one-class tasks is ascertained in [98], in the context of SVDD. As the primary focus of this paper is in developing one-class solutions for vision problems, we decided to make this normalization as part of the pre-processing steps when generating our input features and thus in deriving our core formulations, (albeit we consider variants of our formulation without such assumptions in the next section). We empirically validate this assumption in Section 7.5.",
                                "Following this idea, we assume that our data is unit normalized, i.e., x i 2 = 1, and thus belongs to a unit hypersphere U d-1 , which is a sub-manifold of the Euclidean manifold R d . This assumption 3 on the data naturally places our hyperplanes also to belong to U d-1 ; i.e., w 1 2 = w 2 2 = 1. Using these manifold constraints, our BODS formulation can be rewritten as follows:",
                                ". We further simplify the BODS objective by substituting the constraints on the slacks \u03be's in ( 2) and (3) into \u2126 in (1) and include them in the objective as soft constraints using the hinge loss [ ] + . We use \u03bd to denote a penalty factor on these soft constraints. In Fig. 2(a), we illustrate the decision boundaries of the BODS model. While, BODS offers a simple and flexible model to capture the one-class distribution, the linearity of the classifiers may limit its applications to sophisticated data models. A natural idea is then to empower these classifiers to have non-linear decision boundaries. While, using a kernel method is perhaps a standard approach in this regard (which we present subsequently), we first propose to achieve non-linearity via piecewise linear decision boundaries. To this end, we equip each classifier in BODS with a set of hyperplanes; each set forming a complementary pair with the other. The use of piecewise linear decision boundaries makes inference computationally cheap as it requires only 2K inner products during inference, assuming K hyperplanes per set. Further, we also avoid the need for computing kernel matrices, allowing for scalability of our approach to larger datasets. However, using sets of hyperplanes brings in the challenge of how to effectively regularize them to avoid overfitting and redundancy. To this end, in the following subsections, we generalize BODS to use pairs of multiple hyperplanes, regularized as orthonormal frames, thus providing a richer and non-linear discriminative setup, and subsequently present other regularizations and kernel embeddings."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "One-class Discriminative Subspaces",
                    "paragraphs": [
                        "Let us continue to assume the input data is unit normalized, we will remove this assumption in the next section. Formally, suppose W 1 , W 2 \u2208 S K d be orthonormal frames -that is, matrices of dimensions d \u00d7 K, each with K columns where each column is 3. Such an SVM formulation is classically known as normalized margin SVMs, theoretically analyzed for its generalization performance in [99] and [100][Sec. 10.6.2]. orthonormal to the rest; i.e., W T 1 W 1 = W T 2 W 2 = I K , where I K is the K \u00d7 K identity matrix (see Fig. 2(b)). Such frames belong to the so-called Stiefel manifold, denoted S K d , with K d-dimensional directions. Note that the orthogonality assumption on the W i 's is to ensure they capture diverse discriminative directions, leading to better regularization, while also improving their characterization of the data distribution. A direct extension of P 1 then leads to:",
                        "where dist W is a suitable distance between orthonormal frames, and b \u2208 R K is a vector of biases, one for each hyperplane. Note that in ( 6) and ( 7), unlike BODS, W T x i + b is a K-dimensional vector. Thus, (6) says that the minimum value of this vector should be greater than \u03b7 and ( 7) says that the maximum value of it is less than -\u03b7. To simplify the notation, let us use",
                        ". Then, P 2 can be written as follows:",
                        "The formulation P 2 , due to the first term, enforces a tight coupling between W 1 and W 2 ; such a coupling might prevent the frames from freely aligning to the data distribution, resulting in sub-optimal performance. To circumvent this issue, we propose the following work around. Recall that the main motivation to define the distance between the frames is so that they sandwich the (one-class) data points compactly. Thus, rather than defining a distance between the frames directly, one could also use a measure that minimizes the Euclidean distance of each data point from both the hyperplanes; thereby achieving the same effect. Such a distance via the data points will also make the frames loosely coupled. More formally, we propose to redefine dist 2 W as:",
                        "where now we minimize the sum of the lengths of each x after projecting on to the respective frames; thereby pulling both the frames closer to the data point. Using this definition of dist 2 W , we formulate our generalized one-class discriminative subspace (GODS) classifier as:",
                        "See Figure 2(c) for a graphical illustration of this variant of our GODS objective."
                    ],
                    "subsections": []
                },
                {
                    "title": "Extensions to GODS Formulation",
                    "paragraphs": [
                        "The technical development of GODS in the previous section assumes the input data is unit normalized, as otherwise the orthornormal frames for discriminating them may not be generally fruitful. Our other important assumption in the previous section -that the hyperplanes in our discriminative decision parameters W are orthonormal -can be restrictive as well. In this section, we provide extensions of our GODS framework that relax or remove these restrictions. In the following variants, we will assume \u03bb > 0 to generically denote a penalty on the respective regularization."
                    ],
                    "subsections": [
                        {
                            "title": "Non-Compact Stiefel Manifold",
                            "paragraphs": [
                                "A matrix W \u2208 R d\u00d7K with its K columns being linearly independent, however not unit normalized, belongs to the socalled non-compact Stiefel manifold (Absil et al. [26][Chapter 3]), denoted R d\u00d7K * , which is an open subspace of the Euclidean space R d\u00d7K . One may represent such a manifold as a product manifold between a d\u00d7K Stiefel manifold and a K \u00d71 Euclidean vector; i.e., R d\u00d7K *",
                                ", and the ith dimension r i = W :,i 2 , the 2 norm of the i-th column in W. For non-unit-norm input data, we can extend the GODS formulation in (10) using a non-compact Stiefel manifold as:",
                                "where F is the objective in (10) and \u03bb > 0 is a penalty on the p -norm regularization over r. Note that in (11), for brevity we assume",
                                ", it is technically a product of two non-compact Stiefel manifolds corresponding to the two discriminative frames. There is an additional advantage with the proposed subspace representation -it can allow automatic selection of the number of subspace components one may need. For example, with the p regularization on r, some of the dimensions in r can go to zero (say using p = 1), thereby removing the respective subspace component from the final representation."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "Oblique Manifold",
                            "paragraphs": [
                                "We may also relax the orthogonality constraints on W, however maintain their unit normality. A set of matrices OB K d = W \u2208 R d\u00d7K : diag(W W) = I K forms a regular submanifold of the Euclidean manifold and is usually called an Oblique manifold [101] under the canonical inner product metric. This manifold is isometric to the product of K spheres \u00d7 K 1 S 1 d . We can rewrite a variant of GODS with the optimization on OB K d as:",
                                "the last term softly controls the correlations among columns in W."
                            ],
                            "subsections": []
                        },
                        {
                            "title": "Euclidean Manifold",
                            "paragraphs": [
                                "Removing both the orthogonality and the unit norm constraints on the classifiers and the input data results in our most general form of the GODS formulation, that assumes W belongs to the Euclidean manifold. We can write such a variant as:",
                                "Similar to (12), the last term in GODS E controls the correlations between the columns in W; a large \u03bb will promote W to be similar to the original GODS formulation using the Stiefel manifold in (10)."
                            ],
                            "subsections": []
                        }
                    ]
                },
                {
                    "title": "Kernelized One-class Discriminative Subspaces",
                    "paragraphs": [
                        "While, the classifier as described in our GODS formulation can offer computationally efficient, yet non-linear decision functions in the input space, it may fail in situations when input data cannot be bounded using rectilinear coordinates. For example, when the outliers form dense regions inside the one-class, or when the normal data forms islands, are xor-shaped, or ring-shaped. This lends a kernelized variant of GODS scheme inevitable.",
                        "To derive kernelized GODS, we use our formulation 4 in P 2 , however expand the min and max constraints in ( 6) and ( 7) via propagating the \u03b7 to each of the K hyperplanes. Such a simplification allows for a direct application of the Langrange multiplers to derive the dual. We also assume that there are no outliers in the one-class data provided. This allows us to simplify the expressions we derive. 5 With these simplifications, we rewrite our modified P 2 as:",
                        "Using the fact that W W = I K , and using non-negative dual variables Y, Z \u2208 R K\u00d7n + , we have the following Lagrangian formulation of ( 14):",
                        "A straightforward reduction provides the following dual:",
                        "where K = X X is a linear kernel, however could be replaced by any other positive definite kernel via the kernel trick. We call our formulation above as kernelized one-class discriminative subspaces (KODS). Recall that, for K \u2208 R n\u00d7n 0, the constraints in (17) pose the KODS objective on the generalized Stiefel manifold G K\u00d7n K , formally defined as G K\u00d7n K = \u039b \u2208 R K\u00d7n : \u039bK\u039b = I K , K 0 . However, there are two constraints in our objective that adds hurdle to directly using this manifold for optimization: (i) the null-space constraint in (16), and (ii) the requirement that the dual variables Y, Z are non-negative. Below, we present soft-constraints circumventing these challenges and derive an approximate KODS objective."
                    ],
                    "subsections": [
                        {
                            "title": "Approximate KODS Formulation",
                            "paragraphs": [
                                "We avoid the null-space constraint in KODS via incorporating (16) as a soft-constraint into the KODS objective using a regularization penalty, \u03bb > 0. To circumvent the non-negative constraints, we replace the dual variables Y, Z by their element-wise squares, 4. We attempted to use other GODS variants, however they resulted in objectives that seemed computational expensive.",
                                "5. We also note that the use of slacks brings in additional constraints, making our optimization setup computationally difficult. Note that the latter heuristic has been used before, such as in approximating quadratic assignment problems [102]. With these changes, we provide our approximate KODS formulation as:",
                                "where the last factor corresponds to (16). Note that we use the squared form only on the optimization variables, and not on the constraints, and thus our objective is still on the generalized Stiefel product manifold.",
                                "To derive the classification rules at test time (in the next section), we will need expressions for the primal variables in terms of the duals, which we provide below:",
                                "where rowmax and rowmin corresponds to the maximum and minimum values along the rows of the respective matrices."
                            ],
                            "subsections": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "ONE-CLASS CLASSIFICATION",
            "paragraphs": [
                "During inference, we use the decision functions with the learned parameters to classify a given data point as in-class or out-of-class. Specifically, for a new data point x, it is classified as in-class if the following criteria is met:",
                "where the variables W and b are either learned in the KODS formalism or the GODS. In case, we have access to a validation set consisting of in-class and out-of-class data (for which we know the class labels), then we may calibrate the threshold \u03b7 to improve our decision rules. Specifically, suppose we have access to m such validation data points, denoted D v . Then, to estimate an updated threshold \u03b7 , we propose to compute the decision scores",
                "Next, we apply K-Means (or spectral clustering) on v l and v u with K = 2 clusters. Suppose c lk and c uk (k = 1, 2) are the respective centroids for the two clustering problems; then we propose to update \u03b7 as the average of the smaller of the two centroids thresholded by \u03b7; i.e.,",
                "and use \u03b7 = \u03b7 + \u2206\u03b7 to form the new decision rules in (23)."
            ],
            "subsections": []
        },
        {
            "title": "GODS OPTIMIZATION",
            "paragraphs": [
                "In contrast to OC-SVM and SVDD, the GODS formulation in ( 10) is non-convex due to the orthogonality constraints on W 1 and W 2 . 6 However, these constraints naturally impose a geometry 6. Note that the function max(0, min(z)) for z in some convex set is also non-convex.",
                "on the solution space and in our case, puts optimization on the Stiefel manifold [103] -a Riemannian manifold characterizing the space of all orthogonal frames. There exist several schemes for geometric optimization over Riemannian manifolds (see [26] for a detailed survey) from which we use the Riemannian conjugate gradient (RCG) scheme in this paper, due to its stable and fast convergence. In the following, we review some essential components of the RCG scheme and provide the necessary formulae for using it to solve our objectives."
            ],
            "subsections": [
                {
                    "title": "Riemannian Conjugate Gradient",
                    "paragraphs": [
                        "Recall that the standard (Euclidean) conjugate gradient (CG) method [26][Sec.8.3] is a variant of the steepest descent method, however chooses its descent along directions conjugate to previous descent directions with respect to the parameters of the objective. Formally, suppose F (W) represents our objective. 7 Then, the CG method uses the following recurrence at the k-th iteration:",
                        "where \u03bb is a suitable step-size (found using line-search) and",
                        ", where grad F (W k-1 ) defines the gradient of F at W k-1 and \u03b1 k-1 is a direction built over the current residual, which is conjugate to previous descent directions (see [26][pp.182])).",
                        "When W belongs to a curved Riemannian manifold, we may use the same recurrence, however there are a few important differences from the Euclidean CG case, namely (i) we need to ensure that the updated point W k belongs to the manifold, (ii) there exists efficient vector transports 8 for computing \u03b1 k-1 , and (iii) the gradient grad is along tangent spaces to the manifold. For (i) and (ii), we may resort to computationally efficient retractions (using QR factorizations; see [26][Ex.4.1.2]) and vector transports [26][pp.182], respectively. For (iii), there exist standard ways that take as input a Euclidean gradient of the objective (i.e., assuming no manifold constraints exist), and maps them to the Riemannian gradients [26][Chap.3]. Specifically, for the Stiefel manifold, let \u2207 W F (W) define the Euclidean gradient of F (without the manifold constraints), then the Riemannian gradient is given by:",
                        "The direction grad F (W) corresponds to a curve along the manifold, descending along which ensures the optimization objective is decreased (atleast locally). Now, getting back to our one-class objective, all we need to derive to use the RCG, is compute the Euclidean gradients \u2207 W F (W) of our objective in GODS with regard to the variables W j s. The other variables, such as the biases and slacks, belong to the Euclidean space and their gradients are straightforward. The expression for the Euclidean gradient of our objective with respect to the W's is given by: 7. The other optimization variable -b, belongs to the Euclidean manifold, and thus (W, b) \u2208 S K d \u00d7 R K . However for brevity and focus, we omit these variables from our optimization discussion.",
                        "8. This is required for computing \u03b1 k-1 that involves the sum of two terms in potentially different tangent spaces, which would need vector transport for moving between them; see [26][pp.182]."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "\u2202F \u2202W",
            "paragraphs": [
                "where k * i \u2208 [K] denotes the hyperplane index for the respective subspaces; k * i = arg min k (W T 1 x i + b 1 ) for ( 27) and k * i = arg max k (W T 2 x i + b 2 ) for (28). The variable Z k * i is a d \u00d7 K matrix with all zeros, except k * i -th column, which is set to x i ."
            ],
            "subsections": [
                {
                    "title": "KODS Optimization",
                    "paragraphs": [
                        "In this section, we will derive the gradients for our approximate KODS formulation provided in (18). Similar to (26), the mapping from the Euclidean gradient to the Riemannian gradient for the generalized Stiefel manifold is provided in the following theorem.",
                        "Theorem 1. For the optimization problem min U K(U) s.t. UKU = I K , K 0, if \u2207 U K(U) denotes the Euclidean gradient of K(U), then the Riemannian gradient under the canonical metric is given by:",
                        "Proof. The proof follows directly from the results in [24][Section 4.5].",
                        "For the retraction of the iterates on to the manifold, we use the generalized polar decomposition [104] as suggested in [105]. As in the previous section, next we derive the expressions for the Euclidean gradients. ",
                        "Proof. If a i: , y i: , d :j represent the i-th row and j-th column of matrices A, Y, D respectively, then",
                        "Then, the gradient w.r.t. y ij , i.e., \u2207 yij f (Y) = a ij d ji , and we have the desired result.",
                        "Proof. To simplify the notation, let us use A = (Y Y), then using Proposition 1, and applying chain-rule:",
                        "where we used Lemma 1 to obtain (31). Using the symmetry of D, we have the result.  18) is:",
                        "where \u03b3 = 2 + 2\u03bb.",
                        "Proof. The result directly follows by applying Lemma 1 and Lemma 2 to the formulation in (18)."
                    ],
                    "subsections": []
                },
                {
                    "title": "Optimization Initialization",
                    "paragraphs": [
                        "Due to the non-convexity of our objective, there could be multiple local solutions. To this end, we resort to the following initialization of our optimization variables, which we found to be empirically beneficial. Specifically, for the GODS optimization, we first sort all the training points based on their Euclidean distances from the origin. Next, we randomly select a suitable number (3 \u00d7 # hyperplanes in our experiments) of such sorted points near and far from the origin, compute a compact singular value decomposition (thin-SVD) of these points, and initialize the GODS subspaces using these orthonormal frames from the SVD. The intercepts (b) are initialized to zero. For KODS, we initialize the dual variables as 1 nK , where K is the number of hyperplanes."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "EXPERIMENTS",
            "paragraphs": [
                "In this section, we provide experiments demonstrating the performance of our proposed schemes on several one-class tasks. We will introduce these tasks and the associated datasets briefly next along with detailing the data features used."
            ],
            "subsections": [
                {
                    "title": "Dash-Cam-Pose Dataset",
                    "paragraphs": [
                        "Out-of-position (OOP) human pose detection is an important problem with regard to the safety of passengers in a vehicle. While, there are public datasets for human pose estimation, they are usually annotated for generic pose estimation tasks, and neither do they contain any in-vehicle poses as captured by a dashboard camera, nor are they annotated for pose anomalies. To this end, we collected 104 videos, each 20-30 min long, from the Internet (including Youtube, ShutterStock, and Hollywood road movies). As these videos were originally recorded for diverse reasons, there are significant shifts in camera angles, perspectives, locations of the camera, scene changes, etc. We encourage the interested reader to refer to [32] for more details on this dataset and its collection. Next, we manually selected clips from these videos that are found interesting for our task. To extract as many clips as possible from these videos, we first segmented each video into three second clips at 30fps, which resulted in approximately 7000 clips. Next, we selected only those clips where the camera is approximately placed on the dashboard looking inwards, which amounted to 4,875 clips, totalling 4.06 hours. We annotated each clip with a weak binary label based on the poses of humans in the front seat. Specifically, if all the front-seat humans (passengers and the driver) are seated inposition, the clip was given a positive label, while if any human is seated OOP (based on [106], [107]) for the entire 3s, the clip was labeled as negative. Next, we used Open Pose [108] on each clip to extract a sequence of poses for every person. Our final Dash-Cam-Pose dataset consists of 4875 short videos, 1.06 million poses, of which 310,996 are OOP.",
                        "We explore two pose-sequence representations for this task: (i) a simple bag-of-words (BoW) model, and (ii) using a Temporal Convolutional Network (TCN) [109] consisting of residual units with 1D convolutional layers capturing both local and global information via convolutions for each joint across time. For BoW, we use 1024 pose centroids computed using K-Means clustering. For the TCN, we use the following procedure. The poses from each person in each frame are vectorized and stacked into the temporal dimension. For each pose thus passed through TCN, we extract features from the last pooling layer, using a model pretrained on the NTU-RGBD dataset [110] (for 3D skeleton action recognition) to produce 256-D features for every clip. As our pretrained TCN model takes 3D poses as input, we pad our Dash-Cam-Poses with zeros in the third dimension.",
                        "We use a four-fold cross-validation for evaluation on Dash-Cam-Pose. Specifically, we divide the entire dataset into four non-overlapping splits, each split consisting of approximately 1/4th the dataset, of which roughly 2/3rd's are labeled as positive (in-pose) and the rest as OOP. We use only the positive data in each split to train our one-class models. Once the models are trained, we evaluate on the held out split. For every embeddedpose feature, we use the binary classification accuracy against the ground truth. The evaluation is repeated on all the four splits and the performance averaged."
                    ],
                    "subsections": []
                },
                {
                    "title": "Public Datasets",
                    "paragraphs": [
                        "JHMDB dataset: is a video action recognition dataset [31] consisting of 968 clips with 21 classes (see Figure 4 for example frames). To adapt the dataset for a one-class evaluation, we use a one-versus-rest strategy by choosing sequences from an action class as \"normal\" while those from the rest 20 classes are treated as \"abnormal\". To evaluate the performance on the entire dataset, we cycle over the 21 classes, and the scores are averaged. For representing the frames, we use an ImageNet pre-trained VGG-16 model and extract features from the 'fc-6' layer (4096-D). UCF-Crime dataset: is the largest publicly available real-world anomaly detection dataset [29], consisting of 1900 surveillance videos and 13 crime categories such as fighting, robbery, etc. and several \"normal\" activities, such as the daily walking, running and driving. Illustrative video frames from this dataset and their class labels are shown in Figure 4. To encode the videos, we use the state-of-the-art Inflated-3D (I3D) neural network [111]. Specifically, video frames from non-overlapping sliding windows (8 frames each) are passed through the I3D network; features are extracted from the 'Mix 5c' network layer, that are then reshaped to 2048-D vectors. For anomaly detections on the test set, we first map back the features classified as anomalies by our scheme to the frame-level and apply the official evaluation metrics [29]. UCSD Ped2 dataset: contains 16 videos in the training and 12 videos in the test set. There are 12 abnormal events in the test videos, such as the Biker, Cart, Skater, etc. To encode the video data, we apply the deep autoencoder with causal 3D convolutions [112] trained to minimize the reconstruction loss. We extract features from the bottleneck layer of this model to be input to our algorithm. As the videos can be of arbitrary length, the pipeline is trained on clips from temporal sliding windows with a stride of one and consisting of 16 frames. As anomalous events are labeled frame-wise in this dataset, we use the averaged clip-level predictions within a window as the prediction for the center frame in that window. We use the evaluation metrics on these frame-level predictions similar to [9], [17], [19]."
                    ],
                    "subsections": []
                },
                {
                    "title": "Experimental Setup",
                    "paragraphs": [
                        "Before using the above features in our algorithms, we found that it is beneficial to unit-normalize them. However, we do report results without such normalization on other datasets in Section 7.7. These scaled features are then used in our GODS formulations, the optimization schemes for which are implemented using ManOpt [105] and PyManOpt [113]. We use the conjugate gradient scheme for optimization, which typically converges in about 200 iterations. We initialize the iterates using the approach described in Section 6.3. The hyper-parameters in our models are chosen via crossvalidation, and the sensitivities of these parameters are evaluated in the next section. We use regularization constants \u03bd = 1 and \u03bb = 1. We use the inference criteria described in Section 5 for classifying a test point as in-class or an anomaly."
                    ],
                    "subsections": []
                },
                {
                    "title": "Evaluation Metrics",
                    "paragraphs": [
                        "On the UCF-Crime dataset, we follow the official evaluation protocol, reporting AUC as well as the false alarm rate. For other datasets, we use the F 1 score to reflect the sensitivity and accuracy of our classification models. As the datasets we useespecially the Dash-Cam-Pose -are imbalanced across the two classes, having a single performance metric over the entire dataset may fail to characterize the quality of the discrimination for each class separately, which is of primary importance for the one-class task. To this end, we also report True Negative Rate T N R = T N N , Negative Predictive Value N P V = T N T N +F N , and F 1 = 2\u00d7T N R\u00d7N P V T N R+N P V , alongside standard F1 scores. We will use F 1 on the Dash-Cam-Pose dataset and F 1 score on other datasets. Informally, F 1 is the same as F 1 with the positive and negative categories switched."
                    ],
                    "subsections": []
                },
                {
                    "title": "Ablative Studies",
                    "paragraphs": [
                        "Synthetic Experiments: To gain insights into the inner workings of our schemes, we present results on several 2D synthetic toy datasets. In Figure 1(a)-1(c), we show three plots with 100 points distributed as (i) Gaussian and (ii) some arbitrary distribution 9 . We show the BODS hyperplanes in the Figure 1(a), and the GODS 2D subspaces in Figures 1(b), 1(c) with the hyperplanes belonging to each subspace shown in same color. As the plots show, our models are able to orient the subspaces such that they confine the data within a minimal volume. In Figure 1(d), we show 300 data points (black dots) distributed along a 2D ring, a situation when a rectilinear GODS may fail (as the inner circle does not contain the one-class). As seen in Figures 1(e) and 1(f), the two kernelized KODS hyperplanes capture the outer and inner decision regions separately, and their combined decision region is able to capture the ring structure of the input data, as seen in Figure 1(d). In Figure 1(g), we plot the decision surfaces for 3D data points. Choice of Manifold and Initialization. As described in Section 4.3, our GODS algorithm may assume several optimization manifolds based on the type of regularization used between the classifier hyperplanes. In the Figure 5, we evaluate three different manifold choices, namely (i) Stiefel manifold, (ii) oblique manifold, and (iii) Euclidean manifold. We also evaluate three different hyperplane initialization strategies: (i) random, (ii) SVD (as describved in Section 6.3), and (iii) mean of the data features.",
                        "From the Figure, it can be seen that the Stiefel manifold and SVD initialization works best compared to oblique and Euclidean manifolds and against random or mean initializations, consistently on the three datasets. Choice of Normalization and soft-orthogonality. In Table 1, we investigate the effectiveness of the L2 normalization and soft-orthogonality proposed in Equation ( 12) and (13). For fair comparison, we implement the GODS algorithm with Stifel, noncompact Stiefel, Euclidean and Oblique manifolds and evaluate on three different datasets (in Dash-Cam-Pose dataset, we use BOW and TCN features as input). From the experimental result, it is clear that the L2 norm helps to maintain a good performance in the GODS algorithm (verifying our assumptions in Section 4.1.1). Thus, we will apply L2 norm by default in our following experiments. Sensitivity of Margin \u03b7. The hyperparameter \u03b7 decides the support margin between the hyperplanes and the one-class data. In Figure 6, we analyze the performance sensitivity against changes in \u03b7 on the JHMDB, UCF-Crime, UCSD-Ped2, and the Dash-Cam-Pose datasets. To ensure the learning will not ignore the changes in \u03b7, we increased the regularization constants on the two terms involving \u03b7 in (4). On both datasets, the TPR (true positive rate) increases for increasing \u03b7, while the TNR decreases with a higher value of \u03b7. This is because the distances between the two orthonormal frames (W 1 , W 2 ) may become larger to satisfy the 9. The data follows the formula f (x) = \u221a x * (x + sign(randn) * rand), where randn and rand are standard MATLAB functions. TABLE 1: Comparisons of GODS variants on vision datasets. We compare: (i) GODS (Eq. ( 10)) using the Stiefel manifold, (ii) GODSN using the non-compact Stiefel (Eq. ( 11)), (iii) the Euclidean (Eq. ( 13)), and (iv) the oblique manifolds (Eq. ( 12)). We compare under (i) 2 unit-normalization of inputs and (ii) C, under softorthogonality (Eqs. ( 12), ( 13)). We report F 1, F 1 and AUC scores (in %) for JHMDB, Dash-Cam-Pose (DCP), and UCF-Crime datasets. new margin constraints imposed by \u03b7. Thus, more points will be included between the two frames and classified as positive. As F 1 relies on the classification sensitivity of the positive data, while the F 1 relies on the negative classifications, they show opposite trends. Observing these trends, we fix \u03b7 = 0.3 in our experiments. Number of Hyperplanes K. In Figure 7, we plot the influence of increasing number of hyperplanes on our four datasets. We find that after a certain number of hyperplanes, the performance saturates, which is expected, and suggests that more hyperplanes might lead to overfitting to the positive class. We also find that the TCN embedding is significantly better than the BoW model (by nearly 3%) on the Dash-Cam-Pose dataset when using our proposed methods. Surprisingly, S-SVDD is found to perform quite inferior against ours; note that this scheme learns a low- dimensional subspace to project the data to (as in PCA), and applies SVDD on this subspace. We believe, these subspaces perhaps are common to the negative points as well that it cannot be suitably discriminated, leading to poor performance. We make a similar observation on the other datasets as well. Kernel Choices and Number of Subspaces in K. In Figure 8, we demonstrate the performance of KODS on the datasets for various choices of the embedding kernels, and also when increasing the number of Hilbert space classifiers K for each choice of the kernel feature map on every dataset we use. Specifically, we experiment with linear, RBF, and polynomial kernels on JHMDB, UCF-Crime, UCSD Ped2, and Dash-Cam-Pose (with TCN) datasets, while use the Chi-square [114] and Histogram Intersection kernels [115] on the Dash-Cam-Pose dataset with the Bag-of-Words features. We set \u03c3 = 0.1 for the bandwidth in the RBF kernel, polynomial kernel degree is set to 3, and use 1024 words in the Bag-of-Words representation. In Figure 8, we see that the performance saturates with increasing number of hyperplanes.",
                        "The RBF kernel seems to work better on the JHMDB and UCF-Crime datasets, while the polynomial kernel demonstrates higher performances on the UCSD-Ped2 and the Dash-Cam-Pose datasets (with TCN). For the Bag-of-Words features on the Dash-Cam-Pose dataset, the Chi-square kernel shows better performance than the Historgram Intersection kernel. Empirical Convergence. In Figures 9(a) and 9(b), we show the empirical convergences of our GODS algorithm on the JHMDB dataset using the original GODS formulation in (10). We show the convergence in the objective value as well as the magnitude of the Riemannian gradients. As is clear, our algorithm convergences in about 200 iterations on this dataset. We repeat this experiment on the KODS formulation (18) using different kernel maps. As is seen from Figures 9(c) and 9(d), while the convergence is slower compared to that in GODS -perhaps due to our approximationsit does converge suitably for appropriate kernel choices. Running Time. In the Figure 9(e), we demonstrate the time taken for training our different models. For this analysis, we use an Intel i7-6800K 3.4GHz CPU with 6 cores. We implement the different algorithms in the Matlab, run it on the same data, and record the training time with an increasing number of training samples. For GODS, KODS, and S-SVDD, we use 3 hyperplanes in the subspaces. It can be seen that the GODS, BODS, and KODS are not substantially more computationally expensive against prior methods, while remaining empirically superior (Table 2)."
                    ],
                    "subsections": []
                },
                {
                    "title": "State-of-the-Art Comparisons",
                    "paragraphs": [
                        "In Table 2, we compare our variants to the state-of-the-art methods. As alluded to earlier, for our Dash-Cam-Pose dataset, as its positive and negative classes are imbalanced, we resort to reporting the F 1 score on the test set. From the table, our variants are seen to outperform prior methods by a significant margin; especially our GODS and KODS schemes demonstrate the best performances on different tasks. For example, using TCN, KODS outperforms other kernelized prior variants by over 20%. Similarly, on the JHMDB dataset, both GODS and KODS are better than the next best kernel-based method (K-OC-SVM) by about 20%, and improves the classification accuracy by over 30%. Overall, the experiments clearly substantiate the performance benefits afforded by our methods on the one-class task.",
                        "In Table 3-left, we present results against the state of the art on the UCF-Crime dataset using the AUC metric and false alarm rates; we use the standard threshold of 50%. While, our results are lower than [29] by 4% in AUC and 0.2 larger in false alarm rate, their problem setup is completely different from ours in that they use weakly labeled abnormal videos as well in their training, which we do not use and which as per definition is not a one-class problem. Thus, our results are incomparable to theirs. Against other methods on this dataset, our schemes are about 5-10% better. In the Table 3-right, we provide the performance (AUC) on the UCSD Ped2 dataset. Compared with the recent state-of-the-art methods, both GODS and KODS achieves similar performances using the 3D autoencoder features. Among these methods, Luo et al. [68] and Liu et al. [19] propose ConvLSTM-AE and S-RNN respectively, which rely on the recurrent neural networks, that might be hard to train. Abati et al. [17] also uses the 3D autoencoder features similar to ours, but also employs additional constraints for building the Conditional Probability Density (CPD), which is more expensive compared to our solution. We also note that Table 3-right lists prior methods that use deep learning models, such as the ConvLSTM autoencoder [68], Stacked-RNN [19], and TABLE 2: Average performances on the Dash-Cam-Pose and JHMDB datasets. Dash-Cam-Pose uses the F 1 score while JHMDB uses F 1 score as evaluation metric (classification accuracy is shown in the brackets). K-OC-SVM and K-SVDD are the RBF kernelized variants."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Method",
            "paragraphs": [
                "CarPose BOW CarPose TCN JHMDB OC-SVM [13] 0.167 (0.517) 0.279(0.527) 0.301 (0.568) SVDD [16] 0.448 (0.489) 0.477(0.482) 0.407 (0.566) K-OC-SVM [13] "
            ],
            "subsections": [
                {
                    "title": "Performance on UCI datasets",
                    "paragraphs": [
                        "As the reader might acknowledge, the algorithms proposed in this paper are not specialized to only computer vision datasets, but could be applied for the anomaly detection task on any data mining, machine learning, or robotics task. To this end, in Table 4, we evaluate GODS and KODS on five datasets downloaded from UCI datasets 10 and TU delft pattern recognition lab website 11 . These datasets are: (i) sonar, the task in which is to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock, (ii) the Delft pump dataset, the task in which is to detect abnormal condition of a submersible pump, (iii) the Scale dataset is to classify the balance scale tip to the right, tip to the left, or be balanced, (iv) the Haberman's survival dataset records the survival of patients who had undergone surgery for breast cancer, and (v) the Banknote dataset is to detect if the feature from an image passing the evaluation of an authentication procedure for banknotes. We follow the evaluation protocol in the recent paper [81] for all the datasets and compare the performances to those reported in that paper. Specifically, the evaluation uses a split of 70/30 for the positive class; the model training is performed on the 70%, and tested on the remaining 30% positive class data and the negative (anomalous) data (which is not used in training). We repeat the split in the positive class five times and report the average performance on the five trials. For datasets having more than two classes, we pick one class as positive, while the 10. http://archive.ics.uci.edu/ml/index.php 11. http://homepage.tudelft.nl/n9d04/occ/index.html     (10) using the Stiefel manifold, (ii) GODSN using the non-compact Stiefel (11), (iii) the Euclidean (13), and (iv) the oblique manifolds (12). We compare under (i) 2 unit-normalization of inputs and (ii) C, under soft-orthogonality ( ( 12), ( 13)). We report F1 scores (in %) and standard deviations over 5 trials. remaining as negative, and follow the same protocol as above. In Table 4, we report the performances of GODS and KODS against those reported in [81]. In the table, N represents the number of samples in the dataset, D denotes the feature dimension of each sample, and T is the target class picked as positive (same as in [81]). We use three hyperplanes in the GODS subspaces, and set the sensitivity margin \u03b7 = 0.3 in both GODS and KODS.",
                        "For KODS, we use polynomial kernel with degree as 3. As is clear from the table, we outperform the previous state-of-the-art results on all datasets. Specifically, GODS is substantially better than the previous best method S-SVDD by 2-8%, and the KODS is even better by 1-2%. This is because our GODS algorithm better characterizes the data distribution from positive classes and thus producing higher cost for anomalies during the inference. For KODS, the kernel embedding would further bring advantages in learning the decision regions better fitting the normal samples.",
                        "In the Table 5, we evaluate the various extensions of GODS as described in Section 4.3. Through these experiments, we evaluate the impact of unit-normalization on the data inputs and the orthogonality assumptions on the hyperplanes, and analyze the adequacy of each variant when such assumptions may not be relevant. In the Table 5, each column contains the results for one dataset while the four rows in one column are the result for one variant of GODS. From top to the bottom, we have 4 settings relaxing different constraints; they are: 1) unit normalization applied on the inputs, denoted 2 , 2) no unit norm is enforced, = 2 , 3) = 2 but with soft-orthogonality constraints (C) as described in (12), (13), and 4) 2 norm and C are used together. We also report the standard deviations associated with each experiment over the five trials.",
                        "From the experimental results, it is found that the orthogonal constraint is generally helpful, whenever applicable. For instance, the results in the third and fourth row in Euclidean and Oblique manifolds are better than the ones without orthogonal constraints by up to 2%. In terms of the 2 norm constraints, it depends on the nature of the data points. For example, in the Scale dataset, each dimension of the data captures the presence of some semantic attribute and thus 2 norm may not make much sense on them.",
                        "However, for the vision datasets or the other four UCI datasets, the feature normalization could bound the data allowing the oneclass model to better capture the distribution."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "CONCLUSIONS",
            "paragraphs": [
                "In this paper, we presented a novel one-class learning formulation -called GODS -using pairs of complementary classifiers; these classifiers are oriented so as to circumscribe the data density within a rectilinear space of minimal volume. We explored variants of GODS via relaxing the various constraints in our problem setup, as well as introducing kernel feature maps. Due to the orthonormality we impose on the classifiers, our objectives are non-convex, and solving for which we resorted to Riemannian optimization frameworks on the Stiefel manifold and its variants. We presented experiments on a diverse set of anomaly detection tasks, demonstrating state-of-the-art performances. We further analyzed the generalizability of our framework to non-vision data by presenting experiments on five UCI datasets; our results outperforming prior baselines by significant margins.",
                "An potential direction to extend this work is perhaps to use more than two classifiers in the GODS framework. While, we experimented with a variant of this idea using multiple orthonormal frames, its performance was poor. We presume this inferior performance is perhaps due to the lack of appropriate regularizations across the classifiers and the absence of suitable complementarity conditions between the classifiers and the data. We plan to pursue this research direction in a future paper.",
                "Further, there are several aspects of our scheme that needs rigorous treatment. For example, deriving generalization bounds on GODS is one such. Analysis of the representation complexity within a computational learning theory framework is yet another direction. An analysis of our optimization landscape is a direction that could help better initialize our schemes. Extending our framework as a module within an end-to-end neural network is an interesting direction as well."
            ],
            "subsections": []
        }
    ]
}