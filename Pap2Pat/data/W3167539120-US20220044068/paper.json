{
    "id": "https://semopenalex.org/work/W3167539120",
    "authors": [
        "Xiao Zhang",
        "Dragomir Anguelov",
        "Vijay Vasudevan",
        "Yuning Chai",
        "Pei Sun",
        "Weiyue Wang",
        "Benjamin Caine",
        "Jiquan Ngiam"
    ],
    "title": "To the Point: Efficient 3D Object Detection in the Range Image with Graph Convolution Kernels",
    "date": "2021-06-01",
    "abstract": "3D object detection is vital for many robotics applications. For tasks where a 2D perspective range image exists, we propose to learn a 3D representation directly from this range image view. To this end, we designed a 2D convolutional network architecture that carries the 3D spherical coordinates of each pixel throughout the network. Its layers can consume any arbitrary convolution kernel in place of the default inner product kernel and exploit the underlying local geometry around each pixel. We outline four such kernels: a dense kernel according to the bag-of-words paradigm, and three graph kernels inspired by recent graph neural network advances: the Transformer, the PointNet, and the Edge Convolution. We also explore cross-modality fusion with the camera image, facilitated by operating in the perspective range image view. Our method performs competitively on the Waymo Open Dataset and improves the state-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also efficient in that our smallest model, which still outperforms the popular PointPillars in quality, requires 180 times fewer FLOPS and model parameters.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Deep-learning-based point cloud understanding has increased in popularity in recent years. Numerous architectures [9,11,14,19,17,22,21,28,30,33] have been proposed to handle the sparse nature of point clouds, with successful applications ranging from 3D object recognition [4, 25,29], to indoor scene understanding [6,23] and autonomous driving [2,8,24].",
                "Point clouds may have different properties based on the way they are acquired. For example, point clouds for 3D object recognition are often generated by taking one or many depth images from multiple views around a single object. In other applications such as robotics and autonomous driving, a device such as a LiDAR continuously scans its surroundings in a rotating pattern, producing a 2D scan pattern called the range image. Each pixel in this image contains a range value and other features, such as each laser return's intensity.",
                "The operating range of these sensors has significantly improved over the past few years. As a result, state-of-the-art methods [11,21,30,33] that require projecting points into a dense 3D grid have become less efficient as their complexity scales quadratically with the range. In this work, we propose a new point cloud representation that directly operates on the perspective 2D range image without ever projecting the pixels to the 3D world coordinates. Therefore, it does not suffer from the efficiency scaling problem as mentioned earlier. We coin this new representation perspective point cloud, or PPC for short. We are not the first to attempt to do so. [12,14] have proposed a similar idea by applying a convolutional neural network to the range image. However, they showed that these models, despite being more efficient, are not as powerful as their 3D counterparts, i.e. 3D grid methods [9,11,21,30,33] and 3D graph methods [19,22]. We believe that this quality difference traces its root to the traditional 2D convolution layers that cannot easily exploit the range image's underlying 3D structure. To counter this deficiency, we propose four alternative kernels (Fig. 1: c,d) that can replace the scalar product kernel at the heart of the 2D convolution. These kernels inject much needed 3D information to the perspective model, and are inspired by recent advances in graph operations, including transformers [26], PointNet [18] and Edge Convolutions [28].",
                "We summarize the contributions of this paper as follows: 1) We propose a perspective range-image-based 3D model which allows the core of the 2D convolution operation to harness the underlying 3D structure; 2) We validate our model on the 3D detection problem and show that the resulting model sets a new state-of-the-art for pedestrians on the Waymo Open Dataset, while also matching the SOTA on vehicles; 3) We provide a detailed complexity/model-size-  [9,11,21,33] first voxelizes the 3D space, feeds the 3D dense structure to a 3D convolution network or a 2D top-down network, and make the final prediction based on 3D voxels. b) 3D graph models [19,22] builds a graph neural network on top of the sparse point cloud and makes predictions based on points. c) Our method, PPC, operates directly on the perspective range image view and predicts from pixels. d) It utilizes a set of specialized 2D convolution layers in the perspective 2D view. We propose four improved kernels in addition to the traditional inner product kernel (2D conv).",
                "vs.-accuracy analysis, and show that we can maintain the efficiency benefits from operating on the 2D range image. Our smallest model with only 24k parameters has higher accuracy than the popular PointPillars [11] model with over 4M parameters."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "We focus on 3D object detection tasks where a perspective range image view is available, such as a LiDAR scan for autonomous driving. We group most of existing works in this field into 3 categories (see Fig. 1 a,b,c): 3D Grid. The key component for these methods is the voxelization stage, where the projected sparse point cloud in 3D is voxelized into a 3D dense grid structure that is friendly to dense convolution operations in either 3D or 2D top-down. Popular works in this category include [11,30,33], all of which apply a PointNet-style [18] encoding for each voxel in the 3D grid. 3D grid methods have been performing the best in recent years and appear in some of the top entries on several academic and industrial leaderboards [2,8,24], thanks to its strong generalization and high efficiency due to the use of dense convolutions. There are three major drawbacks to 3D grid methods. 1) Needing a full dense 3D grid poses a limitation to handle long-range, since both the complexity and the memory consumption scale quadratically with the range. 2) The voxel representation has a limited resolution due to the scalability issue mentioned above. Therefore, the detection of thin objects such as the pedestrian or signs can be inaccurate. 3) There is no special handling in the model for treating occluded areas than true empty areas.",
                "3D Graph. This line of methods differs from the voxelized grid counterparts in that there is no voxelization stage after the 3D point cloud projection. Without voxelization, dense convolutions can no longer apply. Therefore, these methods resort to building a graph neural network (GNN) that preserves the points' spatial relationship. Popular methods include [16,19,28,22]. Although these methods can scale better with range, they lag behind the quality of voxelized grid methods. Moreover, the method requires a nearest neighbor search step to create the input graph for the GNN. Finally, like in the 3D grid case, these methods also cannot model occlusion either.",
                "Perspective 2D Grid. There has been minimal prior work that tries to solve the 3D point cloud representation problem with a 2D perspective range image alone. [12,14] applied a traditional 2D convolution network to the range image. Operating in 2D is more efficient than in 3D because compute is not wasted on empty cells as in the 3D grid case, nor do we need to perform a nearest-neighbor search for 3D points as in the 3D graph case. Additionally, occlusion is implicitly encoded in the range image, where for each pixel, the ray to its 3D position is indeed empty, and the area behind it is occluded. Unfortunately, perspective 2D grid methods often cannot match the quality of 3D methods. Our proposed method also belongs to this category, and the goal of this paper is to improve the perspective 2D models to match the accuracy of 3D methods.",
                "Finally, a few wildcard methods do not categorize into any of the three groups above. F-PointNet [17] generates proposals via the camera image and validates the proposals using a point-level rather than scene-level PointNet encoding [18]. StarNet [15] shares a similar mechanism for proposal validation, but the proposals generation use farthest-pointsampling instead of relying on the camera."
            ],
            "subsections": []
        },
        {
            "title": "Perspective Point Cloud Model",
            "paragraphs": [
                "In this section, we look at the proposed perspective point cloud (PPC) model. The heart of the model is a set of perspective 2D layers that can exploit the underlying 3D structure of the range-image pixels (Sec. 3.1). Because the range image can have missing returns, we need to handle down-and up-sampling differently than in a traditional CNN (Sec. 3.2). Finally, we outline the backbone network, a cross-modality fusion mechanism with the camera, and the detector head in Sec. 3.3, Sec. 3.4 and Sec. 3.5."
            ],
            "subsections": [
                {
                    "title": "Perspective Point-Set Aggregation Layers",
                    "paragraphs": [
                        "As shown in Fig. 1, we propose a generalization of the 2D convolution network that operates on a 2D LiDAR range or RGB-D image. Each layer takes inputs in the form of a feature map F i of shape [H, W, D], a per-pixel spherical polar coordinates map X i of shape [H, W, 3], and a binary mask M i of shape [H, W] that indicates the validity of each pixel, since returns may be missing. The three dimensions in the spherical polar coordinates {\u03b8, \u03c6, r} describe the azimuth, the inclination, and the depth of each pixel from the sensor's view. The layer outputs a new feature map",
                        "Each pixel in the output feature map F o [m, n] is a function of the corresponding input feature and its neighborhood",
                        ". k H and k W are neighborhood/kernel sizes along the height and width dimensions:",
                        "where f (.) is the Point-Set Aggregation kernel that reduces information from multiple pixels to a single one. A layer equivalent to the conventional 2D convolution can be constructed by applying the 2D convolution kernel f 2D :",
                        "where W are a set of trainable weights. Please note that we omit the depth dimension D and D' in the kernel definitions for writing simplicity. f 2D does not depend on the 3D coordinates X i . Therefore, it cannot reason about the underlying geometric pattern of the neighborhood. Next, we will present four kernels that can leverage this geometric pattern. Range-quantized (RQ) 2D convolution kernel. Inspired by the linearization idea in the bag-of-words approach, one of the simplest ways of adding the range information to the layer is to apply different sets of weights to the input feature depending on the relative depth difference of each neighboring pixel to the center pixel:",
                        "where we define K sets of weights W k , each with a predefined scalar range [\u03b1 k , \u03b2 k ]. These ranges differ from layer to layer and are computed using histograms over many input samples. Different weights are applied depending on the range difference \u2206r. R i denotes the range channel and is part of X i . 1 is the indicator function and has the value 1 if the expression is true and 0 otherwise. \u03b4 is an indicator function based on the validity of each the participating pixels, defined as:",
                        "\u03b4 also appears in subsequent kernels. While f 2D+ takes the range information into account, it is very inefficient in that the number of parameters increases by K-fold, which can be significant and cause overfitting. Moreover, the amount of computation also increases by Kfold. Self-attention kernel. Given the sparse nature of the range image data in the 3D space, graph operation are a more natural choice than projecting to a higher-dimensional space. The transformer [26] is one of the most popular graph operators. It has found success in both NLP [7] and computer vision [3]. In its core, the transformer generates weights depending on the input features and spatial locations of the features, and therefore does not require a set of weights in a dense form. A transformer-inspired kernel looks like follows:",
                        "where W q , W k , W v and W r are four sets of trainable weights. \u03b3(., .) is an asymmetric positional encoding between two points. It is defined as:",
                        "where x = {\u03b8, \u03c6, r}, x = {\u03b8 , \u03c6 , r } are the azimuth, inclination and depth of the points. \u03b3(., .) is also used in subsequent kernels. This asymmetric positional encoding has a geometric meaning. Namely, it is in an oblique Cartesian frame viewed from the sensor's location. For each pixel, after rotating the sphere by -\u03b8 and -\u03c6, x has the spherical polar coordinates {0, 0, r}, while x is at {\u2206\u03b8, \u2206\u03c6, r }. We project them to Cartesian, which yields {r, 0, 0} for x and {cos(\u2206\u03b8) \u2022 cos(\u2206\u03c6) \u2022 r , cos(\u2206\u03b8) \u2022 sin(\u2206\u03c6) \u2022 r , sin(\u2206\u03b8) \u2022 r } for x . The encoding is then their element-wise difference. Note that the oblique Cartesian frame is different from pixel to pixel, but does not depend on the weights of each layer, and therefore can be pre-computed once for all layers per sample. This positional encoding is also used for the subsequent kernels. PointNet kernel. While the transformer has seen great success in NLP and computer vision, PointNet [18] on the other hand has laid the groundwork to the majority of works for 3D point cloud understanding in the past years. It is widely used in robotics, thanks to VoxelNet [33], PointPillars [11] and PointRCNN [22]. The PointNet formulation is yet quite simple. It learns a multi-layer perceptron (MLP) that encodes the neighboring features and their relative coordinates to the center, and pools the encodings via max-pooling. Our PointNet-inspired kernel looks as follows:",
                        "where \u0398 are trainable weights for the MLP. EdgeConv kernel. The edge convolution proposed by [28] is very similar to PointNet. In PointNet, the input to the MLP is the feature itself and a relative positional encoding. The edge convolution adds one more feature that is the center feature to the input:",
                        "Although the last three, the Transformer, the PointNet, and EdgeConv kernels, are inspired by the 3D graph literature discussed in Sec. 2, they do not result in the inability to model occlusion and the inefficiency due to the need of the nearest neighbor search for each point. The perspective point-set aggregation layer can model occlusion just like any 2D range image-based method. Moreover, it does not require the nearest neighbor search, as it selects neighbors based on the distances in the 2D range image rather than in 3D. Finding neighbors in a dense 2D grid is trivial."
                    ],
                    "subsections": []
                },
                {
                    "title": "Smart Down-Sampling",
                    "paragraphs": [
                        "Unlike RGB images, the LiDAR range or RGBD images can have a noticeable amount of invalid range pixels in the image. It can be due to light-absorbing or less reflective surfaces. In the case of LiDAR, quantization and calibration artifacts can even result in missing returns that form a regular pattern, where the down-sampling with a fixed stride can inadvertently further emphasize the missing returns. Therefore, we define a smart down-sampling strategy to avoid missing returns as we sample: When we down-sample with a stride of, for example, 2\u00d72, we select 1 pixel from 4 neighboring pixels. But instead of always selecting the first or the last pixel, we select a valid pixel, if available, which is the closest to the centroid of all valid pixels among the four.",
                        "We define the down-sampling layer as follows (for brevity, we depict the math for the 1D space here):",
                        "where S contains valid s \u2208 m \u2022 \u03bb, . . . , (m + 1) \u2022 \u03bb -1 according to the mask M i , and \u03bb is the intended stride. R i is the range part of the spherical polar coordinates X i . During up-sampling, technically, we would need to generate new points X o from the input 3D coordinates X i , which is difficult to do. Luckily, an up-sampling usually mirrors a previous down-sampling and never exceeds the original input resolution. Therefore, we can remember the coordinates and the mask from the input of a corresponding down-sampling layer and reuse them for after up-sampling. We use the zeros vector as features for the new pixels generated from up-sampling. The up-sampling layer is the reverse operation of the down-sampling layer:",
                        "where S contains all valid s \u2208 m \u2022 \u03bb, . . . , ( m + 1) \u2022 \u03bb -1 according to the mask M i where \u03bb is the up-sampling stride.",
                        "X i and M i are values taken from a previous layer i whose down-sampling with stride S yields the input layer i. R o is the range part of the spherical polar coordinates X o ."
                    ],
                    "subsections": []
                },
                {
                    "title": "Backbone Architecture",
                    "paragraphs": [
                        "Now that we have both the perspective point-set aggregation layers and the sampling layers defined, we look at the backbone architecture to chain the layers together into a network. We performed a low-effort manual architecture search using the 2D convolution kernel and kept using the best architecture for the remaining experiments. Our network builds on top of the building blocks proposed in [14]: the feature extractor (FE) that extracts features with an optional down-sampling, and the feature aggregator (FA) that merges most features to lower-level features to create skip connections. Our pedestrian network consists of 4 FE and 1 FA blocks, and predictions are made on half of the input resolution. Since the vehicles appear wider in the range image, we extend the network to 8 FE and 5 FA blocks. Please find an illustration of the architecture in Supp. Sec. A."
                    ],
                    "subsections": []
                },
                {
                    "title": "Point-Cloud-Camera Sensor Fusion",
                    "paragraphs": [
                        "The perspective range image representation provides a natural way to fuse camera features to point cloud features since each location in the range image can be projected into the camera space. For each camera image, we first compute dense features using a modern convolution U-network [20] (please see Supp. Sec. B for details). We project it to a location in its corresponding camera image for each point in the range image. We then collect the feature vector computed at that pixel in the camera image and concatenate the feature vector to the range image features. A zeros feature vector is appended, should an area be not covered by any camera. This approach can apply to any layer since there is always a point associated with each location. We train our networks end-to-end, with the camera convolution networks randomly initialized."
                    ],
                    "subsections": []
                },
                {
                    "title": "CenterNet Detector",
                    "paragraphs": [
                        "We validate our point cloud representation via 3D object detection. We extend the CenterNet [31] to 3D: For each pixel in the backbone network's output feature map, we predict both a classification distribution and a regression vector. The classification distribution contains C + 1 targets, where C is the number of classes plus a background class.",
                        "During training, the classification target is controlled by a Gaussian ball around the center of each box (see Supp. Fig. 5): s i,j = N (||x i -b j || 2 , \u03c3), where x are points and b are boxes. \u03c3 is the Gaussian standard deviation, set to 0.25 meters for pedestrians and 0.5 meters for vehicles. For 2D detection in images, where CenterNet was originally pro-posed, the box center is always a valid pixel in the 2D image. In 3D, however, points are sparse, and the closest point to the center might be far away. Therefore, we normalize the target score by the highest score within each box to ensure that there is at least one point with a score 1.0 per box. We then take the maximum over all boxes and get the final training target score per point: y cls i = max j s i,j / max i\u2208Bj s i,j . The regression target y reg i for 3D detection contains 8 targets for 7 degrees-of-freedom boxes: a three-dimensional relative displacement vector from the point's 3D location to the center of the predicted box; another three dimensions that contain the absolute length, width, and height; and a single angle split into its sine and cosine forms in order to avoid discontinuity around 2\u03c0.",
                        "We used the penalty-reduced focal loss for the classification, as proposed by [31], and the 1 -loss for the regression. We then train with a batch size of 256 over 300 epochs with an Adam optimizer. The initial learning rate is set to 0.001, and it decays exponentially over the 300 epochs.",
                        "All of our experiments applies the CenterNet detector head to the final feature map of the backbone. We have observed no significant difference in quality between Cen-terNet and other single-shot detectors such as the SSD [13]. Two-stage methods, such as [22], usually outperform singlestage methods on vehicles by a significant margin. However, the impact on pedestrians is less prevalent."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [],
            "subsections": [
                {
                    "title": "Waymo Open Dataset",
                    "paragraphs": [
                        "We conducted experiments on the pedestrians and vehicles of the Waymo Open Dataset [24]. The dataset contains 1000 sequences, split into 780 training, 120 validation, and 100 test. Each sequence contains 200 frames, where each frame captures the full 360 degrees around the ego-vehicle that results in a range image of a dimension 64\u00d72650 pixels. The LiDAR has a maximum range of around 100 meters. Metrics. We use metrics defined by the Waymo Open Dataset. AP: Average precision at 0.5 IOU for vehicles and 0.7 IOU for pedestrians. APH: Same as AP, but also takes the heading into account when matching boxes. 3D vs. BEV: Whether IOU is measured on rotated 3D boxes or projected top-down rotated 2D boxes. L1 vs. L2: Level of difficulty, L2 include more difficult boxes.",
                        "Results for pedestrians and vehicles detections are highlighted in Tab. 1 and Tab. 2. Fig. 3 shows a few example results. Our method significantly outperforms all recent works on the pedestrian category, including the 3D grid or graph representations methods. This boost is not surprising for two reasons: a) pedestrians are tall so that the perspective view captures its full shape, b) they are also thin so that the voxels in the voxel-based methods end up too large and therefore cannot accurately make predictions. We perform very competitively on vehicles, outperforming recently published methods, including several published this year."
                    ],
                    "subsections": []
                },
                {
                    "title": "Detailed Kernel Analysis",
                    "paragraphs": [
                        "We take a closer look to compare the five kernels introduced in Sec. 3.1. Since different kernels have different computational complexity, it is unfair to compare them by their quality alone. Fig. 2a shows a complexity-vs.-accuracy analysis, while Fig. 2b shows the model-size-vs.-accuracy analysis. For each kernel, we train multiple models with each with a different depth multiplier, ranging from 0.  2.0. A depth multiplier is a factor that is applied to the number of channels in each layer in a network and is a simple way to yield multiple models at different complexity and accuracy.",
                        "Complexity vs. accuracy. The 2D kernel baseline is one of the least expensive methods, as expected. "
                    ],
                    "subsections": []
                },
                {
                    "title": "Mix-and-Match Kernels",
                    "paragraphs": [
                        "In the previous section, we noticed that a network consisting of all EdgeConv kernels delivers the strongest results. However, we also observed that the EdgeConv kernel with the same depth multiplier is not as efficient as the 2D kernel. In this ablation, we study if we can keep most of the 2D kernels while only applying the EdgeConv in a few layers. Tab. 3 shows the accuracy numbers. Conv2D and Edge-Conv are networks of only 2D or EdgeConv kernels and serve as a pair of pseudo upper and lower bounds. We then replace each of the blocks of the backbone from 2D to Edge-Conv. Interestingly, replacing either the first or the last block generates the most benefits. Since our backbone resembles a U-Net [20], it means that the EdgeConv kernel has the most impact with larger resolutions."
                    ],
                    "subsections": []
                },
                {
                    "title": "Additional Ablation",
                    "paragraphs": [
                        "Point-Cloud-Camera Sensor Fusion. Each frame in the Waymo Open Dataset comes with five calibrated camera images capturing views from the front and sides of the car. We downsize each camera image to 400 \u00d7 600 pixels and use a convolutional neural network to extract a 192-dimensional feature at each location. These features are concatenated to one of the layers in the perspective point cloud network. We experimented with fusing the features at different layers of the network and found that the model performed better regardless of the layer we chose to fuse. Our best result in Tab. 1 is obtained when the camera features fuses at the input to the first extractor layer. Sensor Polar vs. World Cartesian. In (7), the asymmetric positional encoding is defined in the spherical polar coordinate system around the sensor. It is possible to take the displacement vector between two points in the projected world Cartesian frame instead. If so, the method's overall concept becomes very similar to PointNet++ [19], with the main difference being that the neighbors of a point are taken from the perspective range image grid rather than through nearest neighbor search. Operating in the polar coordinate system is natural in the range image. However, the Cartesian system has a strong prior in the heading since most objects move perpendicular to the ego-vehicle. In Tab. 4 we show the ablation of polar vs. Cartesian for the PPC + EdgeConv model on pedestrians. It appears that the benefits of operating in the polar coordinate system significantly outweigh the drawbacks at 59.6% vs. 54.2% APH L2 .",
                        "Smart Down-Sampling. The smart down-and up-sampling strategy outlined in Sec. 3.2 allows the down-sampling to avoid missing returns at the cost that the down-sampling no longer follows a regular pattern. As shown in Tab. 4, the smart sampling technique yields 1.4% benefit in APH L2 ."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion and Limitations",
            "paragraphs": [
                "This paper presents a new 3D representation based on the range image, which leverages recent advances in graph convolutions. It is efficient and yet powerful, as demonstrated on pedestrians and vehicles on the Waymo Open Dataset.",
                "It is not without limitations. Most 3D detection tasks use a 7 degrees-of-freedom that only has a yaw rotation around the Z-axis in the world coordinate system. Suppose the sensor has a significant pitch or roll wrt. the world coordinate system, the boxes no longer appear only yaw-rotated in the range image. It is an issue for indoor scene datasets but less of a problem for autonomous driving configurations, where the rotating LiDAR usually sits upright to the world coordinate system. Another challenge is data augmentation. [5] in Tab. 1 and Tab. 2 shows a significant improvement by applying data augmentation to PointPillars [11] and StarNet [15]. In 3D, data augmentation can be diverse and effective. When points are in the dense range image form, we can no longer apply most of them without disturbing the dense structure. We also observed that the EdgeConv kernel network is not sensitive to strategies that are still reasonable in the range image, e.g., random flip and random points drop."
            ],
            "subsections": []
        },
        {
            "title": "A. Additional Details on the Backbone",
            "paragraphs": [
                "We use the basic building blocks proposed in [14]: the feature extractor (FE) and the feature aggregator (FA). Figure 4 in [14] shows a detailed diagram. In words, the FE block consists of ten 3\u00d73 filters. Every two layers are grouped and bypassed by a skip connection. Whenever a straightforward skip connection is not possible due to mismatched spatial resolution or depth, a 1\u00d71 filter with potential striding is applied. The FA block is used to up-sample lower resolution feature maps back to a high resolution for skip connections. It first applies a transposed convolution filter to up-sample the lower resolution feature, which is concatenated to a skip connection from a high-resolution feature from a previous layer before down-sampling. The combined feature then undergoes 4 additional 3\u00d73 filters.",
                "Fig. 4 shows backbone architectures for pedestrians and vehicles. Vehicles appear wider in the range image and require a larger receptive field. Therefore, the vehicle model is a lot deeper. The new feature extractors only have 4 instead of 10 convolutional layers each."
            ],
            "subsections": []
        },
        {
            "title": "B. Additional Details on the Camera Backbone and Fusion",
            "paragraphs": [
                "We use a U-Net [20] that resembles the 2D backbone network as depicted in Figure 4 of [33]. We make a small modification in that we skip the initial down-sampling by a stride of 2. The network has 16 convolutional layers with a kernel of 3\u00d73 each, split into 3 blocks at an increasingly smaller resolution. Features from each of the 3 blocks are concatenated to create the final feature map.    "
            ],
            "subsections": []
        }
    ]
}