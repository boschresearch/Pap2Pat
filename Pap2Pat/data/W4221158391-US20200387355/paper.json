{
    "id": "https://semopenalex.org/work/W4221158391",
    "authors": [
        "Radu B\u0103lan",
        "Maneesh Singh",
        "Naveed Haghani"
    ],
    "title": "Permutation Invariant Representations with Applications to Graph Deep  Learning",
    "date": "2022-03-14",
    "abstract": "This paper presents primarily two Euclidean embeddings of the quotient space generated by matrices that are identified modulo arbitrary row permutations. The original application is in deep learning on graphs where the learning task is invariant to node relabeling. Two embedding schemes are introduced, one based on sorting and the other based on algebras of multivariate polynomials. While both embeddings exhibit a computational complexity exponential in problem size, the sorting based embedding is globally bi-Lipschitz and admits a low dimensional target space. Additionally, an almost everywhere injective scheme can be implemented with minimal redundancy and low computational cost. In turn, this proves that almost any classifier can be implemented with an arbitrary small loss of performance. Numerical experiments are carried out on two data sets, a chemical compound data set (QM9) and a proteins data set (PROTEINS).",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "This paper is motivated by a class of problems in graph deep learning, where the primary task is either graph classification or graph regression. In either case, the result should be invariant to arbitrary permutations of graph nodes.",
                "As we explain below, the mathematical problem analyzed in this paper is a special case of the permutation invariance issue described above. To set the notations consider the vector space R n\u00d7d of n \u00d7 d matrices endowed with the Frobenius norm X = trace(XX T ) 1/2 and its associated Hilbert-Schmidt scalar product, X, Y = trace(XY T ). Let S n denote the symmetric group of n \u00d7 n permutation matrices. S n is a finite group of size |S n | = n!.",
                "On R n\u00d7d we consider the equivalence relation \u223c induced by the symmetric group of permutation matrices S n as follows. Let X, Y \u2208 R n\u00d7d . Then we say X \u223c Y if there is P \u2208 S n so that Y = P X. In other words, two matrices are equivalent if one is a row permutation of the other. The equivalence relation induces a natural distance on the quotient space",
                "This makes ( R n\u00d7d , d) a complete metric space.",
                "Our main problem can now be stated as follows:",
                "Problem 1.1. Given n, d \u2265 1 positive integers, find m and a bi-Lipschitz map \u03b1 : ( R n\u00d7d , d) \u2192 (R m , \u2022 2 ).",
                "Explicitly the problem can be restated as follows. One is asked to construct a map \u03b1 : R n\u00d7d \u2192 R m that satisfies the following conditions:",
                "(1) If X, Y \u2208 R n\u00d7d so that X \u223c Y then \u03b1(X) = \u03b1(Y ) (2) If X, Y \u2208 R n\u00d7d so that \u03b1(X) = \u03b1(Y ) then X \u223c Y",
                "(3) There are constants 0 < a 0 \u2264 b 0 so that for any X, Y \u2208 R n\u00d7d , (1.2)",
                "Condition (1) allows us to lift \u03b1 to the quotient space R n\u00d7d . Thus \u03b1( X) = \u03b1(X) is welldefined. Condition (2) says that \u03b1 is injective (or, that \u03b1 is faithful with respect to the equivalence relation \u223c). Condition (3) says that \u03b1 is bi-Lipschitz with constants a 0 , b 0 . By a slight abuse of notation, when \u03b1 satisfies (1) we shall use the same letter to denote the map \u03b1 : R n\u00d7d \u2192 R m as well as the induced map on the quotient space \u03b1 : R n\u00d7d \u2192 R m . For X, Y \u2208 R n\u00d7d , d(X, Y ) denotes the same quantity in (1.1) . In this case d is only a semi-distance on R n\u00d7d , i.e., it is symmetric, non-negative and satisfies the triangle inequality but fails the positivity condition.",
                "One approach to embedding R n\u00d7d is to consider the convex set of probability measures on R d , P(R d ), and the map",
                "where [x 1 , . . . , x n ] = X T , i.e., x k is the k th row of X reshaped as a vector, and \u03b4 denotes the Dirac measure. When P(R d ) is endowed with the Wasserstein-1 distance (the Earth Moving Distance), known also as the Kantorovich-Rubinstein metric,",
                "x -y d\u03c0(x, y)",
                "the distance between a \u221e (X) and a \u221e (Y ) becomes",
                "x k -(\u03a0Y ) k .",
                "By the Kantorovich-Rubinstein theorem ( [10]Theorem 1.14), d KR extends to a norm on the linear space of bounded signed Borel measures on R d , M b (R d ). It is easy to verify that",
                "which proves that a \u221e provides an embedding into a normed linear space. Yet this embedding does not solve the problem since the linear space M b (R d ) is infinite dimensional. Instead of the previous infinite dimensional embedding, we consider two different classes of embeddings. To illustrate these two constructions, consider the simplest case d = 1.",
                "(1) Algebraic Embedding. For x \u2208 R n , x = (x 1 , . . . , x n ) T , construct the polynomial P x (z) = (z -x 1 ) \u2022 \u2022 \u2022 (z -x n ) and then expand the product:",
                ". Using Vieta's formulas and Newton-Girard identities, an algebraically equivalent description of P x is given by the symmetric polynomials:",
                "(1.4) \u03b1 : R n \u2192 R n , \u03b1(x) = n k=1",
                "x k , n k=1",
                "x 2 k , . . . , n k=1",
                "x n k .",
                "It is not hard to see that this map satisfies Conditions (1) and (2) and therefore lifts to an injective continuous map \u03b1 on Rn . Yet it is not Lipschitz, let alone bi-Lipschitz. The approach in [20] can be used to modify \u03b1 to a Lipschitz continuous map, but, for the same reason as described in that paper, it cannot be \"fixed\" to a bi-Lipschitz embedding. In Section 2 we show how to construct an algebraic Lipschitz embedding in the case d > 1.",
                "(2) Sorting Embedding. For x \u2208 R n , consider the sorting map (1.5) \u2193: R n \u2192 R n , \u2193 (x) = (x \u03c0(1) , x \u03c0(2) , . . . , x \u03c0(n) ) T",
                "where the permutation \u03c0 is so that x \u03c0(1) \u2265 x \u03c0(2) \u2265 \u2022 \u2022 \u2022 \u2265 x \u03c0(n) . It is obvious that \u2193 satisfies Conditions (1) and (2) and therefore lifts to an injective map on R n\u00d7d . As we see in Section 3, the map \u2193 is bi-Lipschitz. In fact it is isometric, and hence produces an ideal embedding. Our work in Section 3 is to extend such construction to the more general case d > 1.",
                "The algebraic embedding is a special case of the more general kernel method that can be thought of as a projection of the measure a \u221e (X) onto a finite dimensional space, e.g., the space of polynomials spanned by {X, X 2 , \u2022 \u2022 \u2022 , X n }. In applications such kernel method is known as a \"Readout Map\" [40], based on \"Sum Pooling\".",
                "The sorting embedding has been used in applications under the name of \"Pooling Map\" [40], based on \"Max Pooling\". A na\u00efve extension of the unidimensional map (1.5) to the case d > 1 might employ the lexicographic order: order monotone decreasing the rows according to the first column, and break the tie by going to the next column. While this gives rise to an injective map, it is easy to see it is not even continuous, let alone Lipschitz. The main work in this paper is to extend the sorting embedding to the case d > 1 using a threestep procedure, first embed R n\u00d7d into a larger vector space R n\u00d7D , then apply \u2193 in each column independently, and then perform a dimension reduction by a linear map into R 2nd . Similar to the phase retrieval problem ([2, 9, 4]), the redundancy introduced in the first step counterbalances the loss of information (here, relative order of one column with respect to another) in the second step.",
                "A summary of main results presented in this paper is contained in the following result.",
                "Theorem 1.2. Consider the metric space ( R n\u00d7d , d).",
                "(1) (Polynomial Embedding) There exists a Lipschitz injective map",
                "Two explicit constructions of this map are given in (2.8) and",
                "(2.9). (2) (Sorting based Embedding) There exists a class of bi-Lipschitz maps",
                "with m = 2nd, where each map \u03b2A,B is the composition of two bi-Lipschitz maps: a full-rank linear operator B : R n\u00d7D \u2192 R m , with the nonlinear bi-Lipschitz map \u03b2A : R n\u00d7d \u2192 R n\u00d7D parametrized by a matrix A \u2208 R d\u00d7D called \"key\". Explicitly, \u03b2( X) =\u2193 (XA), where \u2193 acts column-wise. These maps are characterized by the following properties: (b) For any matrix (\"key\") A \u2208 R d\u00d7D such that the map \u03b2A is injective, then \u03b2A : ( R n\u00d7d , d) \u2192 (R n\u00d7D , \u2022 ) is bi-Lipschitz. Furthermore, an upper Lipschitz constant is given by s 1 (A), the largest singular value of A. (c) Assume A \u2208 R d\u00d7D is such that the map \u03b2A is injective (i.e., a \"universal key\").",
                "Then for almost any linear map B : R n\u00d7D \u2192 R 2nd the map \u03b2A,B = B \u2022 \u03b2A is bi-Lipschitz.",
                "An immediate consequence of this result is the following corollary whose proof is included in subsection 3.5:",
                "(1) For any continuous function f : R n\u00d7d \u2192 R invariant to row-permutation (i.e., f (P X) = f (X) for every X \u2208 R n\u00d7d and P \u2208 S n ) there exists a continuous function",
                "Conversely, for any g : R m \u2192 R continuous function, the function f = g \u2022 \u03b2 : R n\u00d7d \u2192 R is continuous and row-permutation invariant.",
                "(2) For any Lipschitz continuous function f : R n\u00d7d \u2192 R invariant to row-permutation (i.e., f (P X) = f (X) for every X \u2208 R n\u00d7d and P \u2208 S n ) there exists a Lipschitz continuous function g : R m \u2192 R such that f = g\u2022\u03b2. Conversely, for any g : R m \u2192 R Lipschitz continuous function, the function f = g \u2022 \u03b2 : R n\u00d7d \u2192 R is Lipschitz continuous and row-permutation invariant.",
                "The structure of the paper is as follows. Section 2 contains the algebraic embedding method and encoders \u03b1 described at part (1) of Theorem 1.2. Corollary 2.3 contains part (1) of the main result stated above. Section 3 introduces the sorting based embedding procedure and describes the key-based encoder \u03b2. Necessary and sufficient conditions for key universality are presented in Proposition 3.8; the injectivity of the encoder described at part (2.a) of Theorem 1.2 is proved in Theorem 3.9; the bi-Lipschitz property of any universal key described at part (2.b) of Theorem 1.2 is shown in Theorem 3.10; the dimension reduction statement (2.c) of Theorem 1.2 is included in Theorem 3.13. Proof of Corollary 1.3 is presented in subsection 3.5. Section 4 contains applications to graph deep learning. These application use Graph Convolution Networks and the numerical experiments are carried out on two graph data sets: a chemical compound data set (QM9) and a protein data set (PROTEINS FULL).",
                "While the motivation of this analysis is provided by graph deep learning applications, this is primarily a mathematical paper. Accordingly the formal theory is presented first, and then is followed by the machine learning application. Those interested in the application (or motivation) can skip directly to Section 4."
            ],
            "subsections": []
        },
        {
            "title": "Notations. For an integer",
            "paragraphs": [
                "1.1. Prior Works. Several methods for representing orbits of vector spaces under the action of permutation (sub)groups have been studied in literature. Here we describe some of these results, without claiming an exhaustive literature survey.",
                "A rich body of literature emanated from the early works on symmetric polynomials and group invariant representations of Hilbert, Noether, Klein and Frobenius. They are part of standard commutative algebra and finite group representation theory.",
                "Prior works on permutation invariant mappings have predominantly employed some form of summing procedure, though some have alternatively employed some form of sorting procedure.",
                "The idea of summing over the output nodes of an equivariant network has been well studied. The algebraic invariant theory goes back to Hilbert and Noether (for finite groups) and then continuing with the continuous invariant function theory of Weyl and Wigner (for compact groups), who posited that a generator function \u03c8 : X \u2192 R gives rise to a function E : X \u2192 R invariant to the action of a finite group G on X, (g, x) \u2192 g.x, via the averaging formula",
                "More recently, this approach provided the framework for universal approximation results of G-invariant functions. [27] showed that invariant or equivariant networks must satisfy a fixed point condition. The equivariant condition is naturally realized by GNNs. The invariance condition is realized by GNNs when followed by summation on the output layer, as was further shown in [21], [28] and [30]. Subsequently, [39] proved universal approximation results over compact sets for continuous functions invariant to the action of finite or continuous groups. In [16], the authors obtained bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests by tensorizing the input-output mapping. [35] studied approximations of equivariant maps, while [11] showed that if a GNN with sufficient expressivity is well trained, it can solve the graph isomorphism problem.",
                "The authors of [36] designed an algorithm for processing sets with no natural orderings. The algorithm applies an attention mechanism to achieve permutation invariance with the attention keys being generated by a Long-Short Term Memory (LSTM) network. Attention mechanisms amount to a weighted summing and therefore can be considered to fall within the domain of summing based procedures.",
                "In [24], the authors designed a permutation invariant mapping for graph embeddings. The mapping employs two separate neural networks, both applied over the feature set for each node. One neural network produces a set of new embeddings, the other serves as an attention mechanism to produce a weighed sum of those new embeddings.",
                "Sorting based procedures for producing permutation invariant mappings over single dimensional inputs have been addressed and used by [40], notably in their max pooling procedure.",
                "The authors of [31] developed a permutation invariant mapping pointnet for point sets that is based on a max function. The mapping takes in a set of vectors, processes each vector through a neural network followed by an scalar output function, and takes the maximum of the resultant set of scalars.",
                "The paper [41] introduced SortPooling. SortPooling orders the latent embeddings of a graph according to the values in a specific, predetermined column. All rows of the latent embeddings are sorted according to the values in that column. While this gives rise to an injective map, it is easy to see it is not even continuous, let alone Lipschitz. The same issue arises with any lexicographic ordering, including the well-known Weisfeiler-Leman embedding [37]. Our paper introduces a novel method that bypasses this issue.",
                "As shown in [28], the sum pooling-based GNNs provides universal approximations for of any permutation invariant continuous function but only on compacts. Our sorting based embedding removes the compactness restriction as well as it extends to all Lipschitz maps.",
                "While this paper is primarily mathematical in nature, methods developed here are applied to two graph data sets, QM9 and PROTEINS FULL. Researchers have applied various graph deep learning techniques to both data sets. In particular, [17] studied extensively the QM9 data set, and compared their method with many other algorithms proposed by that time."
            ],
            "subsections": []
        },
        {
            "title": "Algebraic Embeddings",
            "paragraphs": [
                "The algebraic embedding presented in this section can be thought of a special kernel to project equation (1.3) onto.",
                "2.1. Kernel Methods. The kernel method employs a family of continuous kernels (test) functions, {K(x; y)",
                "The embedding problem 1.1) can be restated as follows. One is asked to find a finite family of kernels {K(x; y)",
                "is injective, Lipschitz or bi-Lipschitz. Two natural choices for the kernel K are the Gaussian kernel and the complex exponential (or, the Fourier) kernel:",
                "where in both cases Y \u2282 R d . In this paper we analyze a different kernel, namely the polynomial kernel",
                "2.2. The Polynomial Embedding. Since the polynomial representation is intimately related to the Hilbert-Noether algebraic invariants theory [18] and the Hilbert-Weyl theorem, it is advantageous to start our construction from a different perspective. The linear space R n\u00d7d is isomorphic to R nd by stacking the columns one on top of each other. In this case, the action of the permutation group S n can be recast as the action of the subgroup I d \u2297 S n of the bigger group S nd on R nd . Specifically, let us denote by \u223c G the equivalence relation",
                "induced by a subgroup G of S nd . In the case G = I d \u2297 S n = {diag d (P ) , P \u2208 S n } of block diagonal permutation obtained by repeating d times the same P \u2208 S n permutation along the main diagonal, two vectors x, y \u2208 R nd are \u223c G equivalent iff there is a permutation matrix P \u2208 S n so that y(1 + (k -1)n : kn) = P x(1 + (k -1)n : kn) for each 1 \u2264 k \u2264 d. In other words, each disjoint n-subvectors in y and x are related by the same permutation. In this framework, the Hilbert-Weyl theorem (Theorem 4.2, Chapter XII, in [25]) states that the ring of invariant polynomials is finitely generated. The G\u00f6bel's algorithm (Section 3.10.2 in [18]) provides a recipe to find a complete set of invariant polynomials. In the following we provide a direct approach to construct a complete set of polynomial invariants.",
                "Let R[x 1 , x 2 , ..., x d ] denote the algebra of polynomials in d-variables with real coefficients. Let us denote X \u2208 R n\u00d7d a generic data matrix. Each row of this matrix defines a linear form over x 1 , ...",
                "the algebra of polynomials in variable t with coefficients in the ring R[x 1 , . . . ,",
                "by rearranging the terms according to degree in t.",
                "can be encoded as zeros of a polynomial P X of degree n in variable t with coefficients in R[x 1 , . . . , x d ]:",
                "(2.2)",
                "x d ] denote the vector space of homogeneous polynomials in d + 1 variables of degree n with real coefficients. Notice the real dimension of this vector space is",
                "By noting that P X is monic in t (the coefficient of t n is always 1) we obtain an injective embedding of (1.4). This is summarized in the following theorem:",
                "Specifically, for X \u2208 R n\u00d7d expand the polynomial (2.4)",
                "where the index set is given by",
                "The map \u03b10 : R n\u00d7d \u2192 R m-1 is the lifting of \u03b1 0 to the quotient space."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "Since for any permutation \u03c0 with associated permutation matrix \u03a0 \u2208 S n ,",
                "it follows that \u03b1 0 is invariant to the action of S n , \u03b1 0 (X) = \u03b1 0 (\u03a0X). Thus \u03b1 0 lifts to a map \u03b10 on R n\u00d7d . The coefficients of polynomial P X depend analytically on its roots (Vieta's formulas), hence on entries of matrix X.",
                "The only remaining claim is that if X, Y \u2208 R n\u00d7d so that \u03b1 0 (X) = \u03b1 0 (Y ) then there is \u03a0 \u2208 S n so that Y = \u03a0X. Assume P X = P Y . For each choice (x 1 , x 2 , . . . , x d ) = (f (1), . . . , f (d)) in R d , the n real zeros of the two polynomials in t, P X (t, f (1), . . . , f (d)) and",
                ")n! and choose F \u2208 R d\u00d7D so that each subset of d columns are linearly independent, in other words, the set F = {f 1 , f 2 , . . . , f D } formed by the D columns of F is a full spark frame in R d , see [1]. As proved in [1], almost every such set is a full spark frame. Then for each 1",
                "The set of invariants produced by map \u03b1 0 are proportional to those produced by the G\u00f6bel's algorithm in [18], \u00a73.10.2. Indeed, the nd primary invariants are given by",
                "corresponding to the elementary symmetric polynomials in entries of each column. The secondary invariants correspond to the remaining coefficients that have at least 2 nonzero indices among p 1 , . . . , p d .",
                "The embedding provided by \u03b1 0 is analytic and injective but is not globally Lipschitz because of the polynomial growth rate. Next we show how a simple modification of this map will make it Lipschitz. First, let us denote by L 0 the Lipschitz constant of \u03b1 0 when restricted to the closed unit ball",
                "be a Lipschitz monotone decreasing function with Lipschitz constant 1.",
                "Corollary 2.3. Consider the map:",
                "(2.8)",
                "The map \u03b1 1 lifts to an injective and globally Lipschitz map \u03b11 :",
                "is the nearest-point map to (or, the metric projection map onto) the convex closed set",
                "(ii) The nearest-point map to a convex closed subset of a Hilbert space is Lipschitz with constant 1, i.e. it shrinks distances, see [29].",
                "These two observations yield:",
                "This concludes the proof of this result.",
                "A simple modification of \u03c6 0 can produce a C \u221e map by smoothing it out around x = 1.",
                "On the other hand the lower Lipschitz constant of \u03b11 is 0 due to terms of the form X k i,j",
                "with k \u2265 2. In [20], the authors built a Lipschitz map by a retraction to the unit sphere instead of unit ball. Inspired by their construction, a modification of \u03b1 0 in their spirit reads:",
                "(2.9)",
                "It is easy to see that \u03b1 2 satisfies the non-parallel property in [20] and is Lipschitz with a slightly better constant than \u03b1 1 (the constant is determined by the tangential derivatives of \u03b1 0 ). But, for the same reasons as in [20] this map is not bi-Lipschitz. "
            ],
            "subsections": []
        },
        {
            "title": "2",
            "paragraphs": [
                "-1. On the other hand, consider the following approach. Each row of X defines a complex number",
                "that can be encoded by one polynomial of degree",
                "The coefficients of Q provide a 2n-dimensional real embedding \u03b6 0 ,",
                "), Im(q n-1 ), . . . , Re(q 0 ), Im(q 0 ))",
                "with properties similar to those of \u03b1 0 . One can similarly modify this embedding to obtain a globally Lipschitz embedding \u03b61 of R n,2 into R 2n+1 . It is instructive to recast this embedding in the framework of commutative algebras. Indeed, let x 1 -1, x 2  2 + 1 denote the ideal generated by polynomials x 1 -1 and",
                "]) denote the vector space projected through this quotient map. Then a basis for S is given by {1, t, . . . , t n , x 2 , x 2 t, . . . ,",
                "denote the set of polynomials realizable as in (2.4). Then the fact that \u03b60 : R n\u00d72 \u2192 R 2n is injective is equivalent to the fact that \u03c3| S : S \u2192 S is injective. On the other hand note",
                "where the last linear subspace is of dimension 2n.",
                "In the case d = 2 we obtain the identification R Remark 2.4. One may ask the question whether the quaternions can be utilized in the case d = 4. While the quaternions form an associative division algebra, unfortunately polynomials have in general an infinite number of factorization. This prevents an immediate extension of the previous construction to the case d = 4.",
                "Remark 2.5. Similar to the construction in [20], a linear dimension reduction technique may be applicable here (which, in fact, may answer the open problem above) which would reduce the embedding dimension to m = 2nd + 1 (twice the intrinsec dimension plus one for the homogenization variable). However we did not explore this approach since, even if possible, it would not produce a bi-Lipschitz embedding. Instead we analyze the linear dimension reduction technique in the next section in the context of sorting based embeddings."
            ],
            "subsections": []
        },
        {
            "title": "Sorting based Embedding",
            "paragraphs": [
                "In this section we present the extension of the sorting embedding (1.5) to the case d > 1. The embedding is performed by a linear-nonlinear transformation that resembles the phase retrieval problem. Consider a matrix A \u2208 R d\u00d7D and the induced nonlinear transformation:",
                "where \u2193 is the monotone decreasing sorting operator acting in each column independently. Specifically, let Y = XA \u2208 R n\u00d7D and note its column vectors Y = [y 1 , y 2 , . . . , y D ]. Then",
                "for some \u03a0 1 , \u03a0 2 , . . . , \u03a0 D \u2208 S n so that each column is sorted monotonically decreasing:",
                "Note the obvious invariance \u03b2 A (\u03a0X) = \u03b2 A (X) for any \u03a0 \u2208 S n and X \u2208 R n\u00d7d . Hence \u03b2 A lifts to a map \u03b2A on R n\u00d7d .",
                "Remark 3.1. Notice the similarity to the phase retrieval problem, e.g., [4], where the data is obtained via a linear transformation of the input signal followed by the nonlinear operation of taking the absolute value of the frame coefficients. Here the nonlinear transformation is implemented by sorting the coefficients. In both cases it represents the action of a particular subgroup of the unitary group.",
                "In this section we analyze necessary and sufficient conditions so that maps of type (3.1) are injective, or injective almost everywhere. First a few definitions.",
                "In general we refer to A as a key for encoder \u03b2 A .",
                "In other words, \u03b2A -1 ( \u03b2A ( X)) = { X}. We let A D (X), or simply A(X), denote the set of admissible keys for X.",
                "For a key A, we let S n (A), or simply S(A), denote the set of matrices separated by A. Thus a matrix X \u2208 S n (A) if and only if, for any matrix",
                "Thus a key A is universal if and only if S n (A) = R n\u00d7d . Our goal is to produce keys that are admissible for all matrices in R n\u00d7d , or at least for almost every data matrix. As we show in Proposition 3.6 below this requires that D \u2265 d and A is full rank. In particular this means that the columns of A form a frame for R d .",
                "3.1. Characterizations of A(X) and S(A). We start off with simple linear manipulations of sets of admissible keys and separated data matrices. Proposition 3.5. Fix A \u2208 R d\u00d7D and X \u2208 R n\u00d7d .",
                "(1) For an invertible",
                "In other words, if X is separated by A then XT -1 is separated by T A.",
                "(2) For any permutation matrix L \u2208 S D and diagonal invertible matrix \u039b \u2208 R D\u00d7D ,",
                "In other words, if X is separated by A then X is separated also by AL\u039b as well as by A\u039bL.",
                "In other words, if A is an admissible key for X then T -1 A is an admissible key for XT ."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "The proof is immediate, but we include it here for convenience of the reader.",
                "(",
                "The reverse include follows by replacing A with T A and T with T -1 . Together they prove (3.2).",
                "(",
                "where L 0 is the permutation matrix that has 1 on its main antidiagonal.",
                "Either way, \u2193 ((XA) j ) =\u2193 ((Y A) j ). Hence \u2193 (XA) =\u2193 (Y A). Therefore X \u223c Y and thus X \u2208 S n (AL\u039b). This shows S n (A) \u2282 S n (AL\u039b). the reverse inclusion follows by a similar argument. Finally, notice {L\u039b} forms a group since L -1 \u039bL is also a diagonal matrix. This shows S n (A\u039bL) = S(AL\u039b ) for some diagonal matrix \u039b , and the conclusion (3.3) then follows.",
                "( ",
                "(2) The set B is generic with respect to Zariski topology, i.e., open and dense. Specifically, its complement is the zero set of the polynomial",
                "(3) For an invertible matrix A \u2208 R d\u00d7d ,",
                ". Hence almost every matrix (w.r.t. Lebesgue measure) X \u2208 R n\u00d7d is not separated by A."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "(1) We need to show that any matrix X that on some columns k and l has distinct elements on same row positions is not separated by I d . Indeed if X is such a matrix, let Y denote a copy of X except on those 4 entries where we set",
                "Note X \u223c Y yet \u2193 (X) =\u2193 (Y ). Hence such matrices are not separated by I d .",
                "(2) By negation, the complement of B is given by",
                ")} This shows B c is the zero set of polynomial P as claimed. Thus B c is a closed Zariski set. Its complement is generic with respect to the Zariski topology since B c = R n\u00d7d .",
                "(3) The inclusion is immediate. Density claim follows from this inclusion.",
                "On the other hand, extending the identity matrix by only one column produces an almost universal key: Proposition 3.7. Assume d \u2265 2 and n \u2265 3.",
                "Let a \u2208 R d be a vector with non-zero entries, i.e.,",
                "be a key. Then S n (A) is generic with respect to the Zariski topology (i.e., open and dense), however S n (A) = R n\u00d7d . In particular, its complement S n (A) c := R n\u00d7d \\ S n (A) is non-empty but has Lebesgue measure zero. Thus almost every matrix X \u2208 R n\u00d7d is separated by A."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "First we show that S n (A) = R n\u00d7d . Consider the matrices X, Y \u2208 R n\u00d7d full of zeros except for the 3x2 top left corner where:",
                "the two left columns and the last column contain 1, 0 repeated n -2 times and -1) and yet X \u223c Y .",
                "Next we show that S n (A) c is included in a finite union of linear spaces each of positive codimension. This proves the clam.",
                "To simplify notation we introduce the following two operators. Let \u03a0, \u03a0 0 , \u03a0 1 , \u2022 \u2022 \u2022 , \u03a0 d \u2208 S n denote permutation matrices of size n. For X \u2208 R n\u00d7d denote by x 1 , . . . , x d its columns. Thus",
                "This is equivalent to say: ",
                "This show that ker L \u03a00,\u03a01,...,\u03a0 d = R n\u00d7d and hence it is a subspace of positive codimension. We obtain:",
                "This shows that S n (A) c is included in a finite union of proper subspaces of R n\u00d7d which in turn is a closed set with respect to the Zariski topology of empty interior. This ends the proof of this result.",
                "The next result provides a characterization of the set S n (A). To do so we need to introduce additional notation that extends the operators L \u03a00,...,\u03a0 d and M \u03a0,...,\u03a0 d defined in the proof of Proposition 3.7. For where \u03b4 k = (0, . . . , 0, 1, 0, . . . , 0) T has only one 1 on the k th position."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "The proof is a consequence of linear algebra analysis of map \u03b2 A .",
                "(1) Assume X is not separated by A. Then there is",
                "implies that there are permutation matrices \u03a0 1 , . . . , \u03a0 d , \u039e 1 , . . . , \u039e D-d \u2208 S n so that:",
                "Substituting the expressions for y 1 , . . . , y d provided by the first d equations into the latter D -d equations, we obtain part 1.(a).",
                "For same Y , the condition X \u223c Y implies that for every \u03a0 \u2208 S n , Y -\u03a0X = 0. Thus part 1(b) is proved.",
                "(3) Equation (3.9) is a transcription of part 1.",
                "(2) Equation (3.8) follows from (3.9) by taking the complement.",
                "3.2. Construction of universal keys. In this subsection we construct universal keys. Proposition 3.8 provides us with an algorithm to check whether a key A is universal. Unfortunately the algorithm has an exponential complexity in data size.",
                "If the key A \u2208 R d\u00d7D is universal then A must have full rank. Therefore there are permutation matrix L \u2208 S D and invertible T \u2208 GL(d, R) so that A = T I d \u00c3 L, with \u00c3 \u2208 R d\u00d7(D-d) . Proposition 3.5 shows that A is a universal key if and only if I d \u00c3 is a universal key. This observation allows us to prove the main result of this subsection stated earlier as part b of Theorem 2.1. Recall a set of vectors {f 1 , . . . , f m } in a linear space V of finite dimension n \u2264 m is called a full spark frame if any subset of n vectors is linearly independent. See [1,26] for more information and explicit constructions of full spark frames. Theorem 3.9. Consider the metric space ( R n\u00d7d , d). Set D = 1 + (d -1)n! and let A \u2208 R d\u00d7D be a matrix whose columns form a full spark frame, i.e., any subset of d columns is linearly independent. Then the key A is universal and the induced map \u03b2A : ",
                "where all norms are Frobenius norms."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "Let a 1 , . . . , a D denote the columns of",
                "Fix X, Y \u2208 R n\u00d7d two matrices. Then there are permutation matrices P 0 , \u03a0 1 , . . . , \u03a0 D , \u039e 1 , . . . , \u039e D \u2208 S n so that d( X, \u0176 ) = P 0 X -Y and",
                "Permutations \u03a0 k and \u039e k satisfy the optimality condition:",
                "where we obtain the upper bound in (3.10). The lower bound in (3.10) follows from the pigeonhole principle similar to the one employed in the proof of Theorem 2.1. In equation (3.11) there are D = 1 + (d -1)n! terms. Since only n! permutations are distinct, there is a permutation Q that repeats at least d times.",
                "The lower bound in (3.10) implies that \u03b2A : R n\u00d7d \u2192 R n\u00d7D is injective and hence A is a universal key. This ends the proof of Theorem 3.9."
            ],
            "subsections": []
        },
        {
            "title": "3.3.",
            "paragraphs": [
                "Bi-Lipschitz properties of universal keys. In this subsection we prove that any universal key defines a bi-Lipschitz encoding map, regardless of D.",
                "Theorem 3.10. Assume the key A \u2208 R d\u00d7D is universal, i.e., the induced map \u03b2A : R n\u00d7d \u2192 R n\u00d7D , X \u2192 \u03b2 A (X) =\u2193 (XA) is injective. Then \u03b2A is bi-Lipschitz, that is, there are constants a 0 > 0 and b 0 > 0 so that for all X, Y \u2208 R n\u00d7d ,",
                "where all are Frobenius norms. Furthermore, an estimate for b 0 is provided by the largest singular value of A, b 0 = s 1 (A)."
            ],
            "subsections": []
        },
        {
            "title": "Proof",
            "paragraphs": [
                "The upper bound in (3.13) follows as in the proof of Theorem 3.9, from equations (3.11) and (3.12). Notice that no property is assumed in order to obtain the upper Lipschitz bound.",
                "The lower bound in (3.13) is more difficult. It is shown by contradiction following the strategy utilized in the Complex Phase Retrieval problem [6]."
            ],
            "subsections": []
        },
        {
            "title": "Assume inf X \u223cY",
            "paragraphs": [
                "Step 1: Reduction to local analysis. Since d( t X, t Y ) = t d( X, \u0176 ) for all t > 0, the quotient",
                "is scale invariant. Therefore, there are sequences (X t ) t , (Y t ) t with Y t \u2264 X t = 1 and d( Xt , \u0176 t ) > 0 so that lim t\u2192\u221e",
                "= 0. By compactness of the closed unit ball, one can extract convergence subsequences. For easiness of notation, assume (X t ) t , (Y t ) t are these subsequences. Let X \u221e = lim t X t and Y \u221e = lim t Y t denote their limits. Notice lim",
                "This means that, if the lower Lipschitz bound vanishes, then this is achieved by vanishing of a local lower Lipschitz bound. To follow the terminology in [6], the type I local lower Lipschitz bound vanishes at some Z 0 \u2208 R n\u00d7d , with Z 0 = 1:",
                "Note that, in general, the infimum of the type I local lower Lipschitz bound over the unit sphere may be strictly larger than the global lower Lipschitz bound (see Theorems 2.1 and Theorem 2.2 in [6] and Theorem 4.3 in [5]). The compactness argument forces the local lower Lipschitz bound to vanish when the global lower bound vanishes.",
                ". This is immediate after squaring (b) and simplifying the terms.",
                "Consider now sequences ( Xt ) t , ( \u0176 t ) t that converge to \u1e900 and achieve lower bound 0 as in (3.14). Choose representatives X t and Y t in their equivalence classes that satisfy the hypothesis of Lemma 3.11 so that",
                "for some \u03a0 j,t \u2208 S n . In fact \u03a0 j,t \u2208 argmin \u03a0\u2208Hj U t -\u03a0V t )a j 2 . Pass to sub-sequences (that will be indexed by t for an easier notation) so that \u03a0 j,t = \u03a0 j for some \u03a0 j \u2208 S n . Thus",
                "Since the above sequence must converge to 0 as t \u2192 \u221e, while U t , V t \u2192 0, it follows that necessarily \u03a0 j \u2208 H j and the expressions simplify to",
                "Thus equation (3.14) implies that for every j \u2208 [D], (3.15) lim",
                "where \u03a0 j \u2208 H j , U t , V t \u2192 0, and U t , V t are aligned so that U t , V t \u2265 P U t , V t for every P \u2208 G. Equivalently, relation (3.14) can be restated as:",
                "for some permutations \u03a0 j \u2208 H j , j \u2208 [D]. By Lemma 3.11 the constraint in the optimization problem above implies U -V = min P \u2208G U -P V . Hence (3.16) implies:",
                "for same permutation matrices \u03a0 j 's. While the above optimization problem seems a relaxation of (3.16), in fact (3.17) implies (3.16) with a possibly change of permutation matrices \u03a0 j , but remaining still in H j .",
                "Step 3. Existence of a Minimizer. The optimization problem (3.16) is a Quadratically Constrained Ratio of Quadratics (QCRQ) optimization problem. A significant number of papers have been published on this topic [7,8]. In particular, [3] presents a formal setup for analysis of QCRQ problems. Our interest is to utilize some of these techniques in order to establish the existence of a minimizer for (3.16) or (3.17). Specifically we show: Lemma 3.12. Assume the key A has linearly independent rows (equivalently, the columns of A form a frame for R d ) and the lower Lipschitz bound of \u03b2A is 0. Then there are \u0168 , \u1e7c \u2208 R n\u00d7d so that:",
                "(1) \u0168 = P \u1e7c , for every P \u2208 G;",
                "(2) For every j \u2208 [D], ( \u0168 -\u03a0 j \u1e7c )a j = 0."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Lemma 3.12",
            "paragraphs": [
                "We start with the formulation (3.17). Therefore there are sequences (U t , V t ) t\u22651 so that U t = P V t for any P \u2208 G, t \u2265 1, and yet for any P \u2208 G,",
                "} denote the null space of the linear operator",
                "associated to the numerator of the above quotient. Let F P = {(U, V ) \u2208 R n\u00d7d \u00d7 R n\u00d7d , U -P V = 0} be the null space of the linear operator",
                "A consequence of (3.17) is that for every P \u2208 G, E\\F P = \u2205. In particular, F p \u2229E is a subspace of E of positive codimension. Using the Baire category theorem (or more elementary linear algebra arguments), we conclude that",
                "Let ( \u0168 , \u1e7c ) \u2208 E \\ (\u222a P \u2208G F P ). This pair satisfies the conclusions of Lemma 3.12.",
                "Step 4. Contradiction with the universality property of the key. So far we obtained that if the lower Lipschitz bound of \u03b2A vanishes than there are Z 0 , \u0168 , \u1e7c \u2208 R n\u00d7d with Z 0 = 0 and \u0168 = P \u1e7c , for all P \u2208 G that satisfy the conclusions of Lemma 3.12. Notice Z 0 , Z 0 = P Z 0 , Z 0 for all P \u2208 G and (Z 0 -\u03a0 j Z 0 )a j = 0 for all j \u2208 [D]. Choose s > 0 but small enough so that s \u0168 , s \u1e7c < 1 4 \u03b4 0 with \u03b4 0 = min P \u2208Sn\\G (I n -P )Z 0 . Let X = Z 0 + s \u0168 and Y = Z 0 + s \u1e7c . Then Lemma 3.11 implies d( X, \u0176 ) = min P \u2208G \u0168 -P \u1e7c > 0. Hence X = \u0176 . On the other hand, for every j \u2208 [D], Xa j = \u03a0 j Y a j . Thus \u03b2A ( X) = \u03b2A ( \u0176 ). Contradiction with the assumption that \u03b2A is injective.",
                "This ends the proof of Theorem 3.10.",
                "3.4. Dimension Reduction. Theorem 3.9 provides an Euclidean bi-Lipschitz embedding of very high dimension, D = 1 + (d -1)n!. On the other hand, Theorem 3.10 shows that any universal key A \u2208 R d\u00d7D for R n\u00d7d , and hence any injective map \u03b2A is bi-Lipschitz. In this subsection we show that any bi-Lipschitz Euclidean embedding \u03b2A : R n\u00d7d \u2192 R n\u00d7D with D > 2d can be further compressed to a smaller dimension space R m with m = 2nd thus yielding bi-Lipschitz Euclidean embeddings of redundancy 2. This is shown in the next result.",
                "Theorem 3.13. Assume A \u2208 R d\u00d7D is a universal key for R n\u00d7d with D \u2265 2d. Then, for m \u2265 2nd, a generic linear operator B : R n\u00d7D \u2192 R m with respect to Zariski topology on R n\u00d7D\u00d7m , the map",
                "is bi-Lipschitz. In particular, almost every full-rank linear operator B : R n\u00d7D \u2192 R 2nd produces such a bi-Lipschitz map.",
                "Remark 3.14. The proof shows that, in fact, the complement set of linear operators B that produce bi-Lipschitz embeddings is included in the zero-set of a polynomial.",
                "Remark 3.15. Putting together Theorems 3.9, 3.10, 3.13 we obtain that the metric space R n\u00d7d admits a global bi-Lipschitz embedding in the Euclidean space R 2nd . This result is compatible with a Whitney embedding theorem (see \u00a71.3 in [19]) with the important caveat that the Whitney embedding result applies to smooth manifolds, whereas here R n\u00d7d is merely a non-smooth algebraic variety.",
                "Remark 3.16. These three theorems are summarized in part two of the Theorem 2.1 presented in the first section.",
                "Remark 3.17. While the embedding dimension grows linearly in nd, in fact m = 2nd, the computational complexity of constructing \u03b2A,B is NP due to the 1 + (d -1)n! intermediary dimension.",
                "Remark 3.18. As the proofs show, for D \u2265 1 + (d -1)n!, a generic (A, B) with respect to Zariski topology, A \u2208 R d\u00d7D and linear map B : R n\u00d7D \u2192 R 2nd , produces a bi-Lipschitz embedding ( \u03b2A,B , d) of R n\u00d7d into (R 2nd , \u2022 2 )."
            ],
            "subsections": []
        },
        {
            "title": "Proof of Theorem 3.13",
            "paragraphs": [
                "The proof follows a similar approach as in Theorem 3 of [20]. See also [13].",
                "Without loss of generality, assume m < nD. Notice \u03b2 A : R n\u00d7d \u2192 R n\u00d7D is already homogeneous of degree 1 (with respect to positive scalars). Let \u2206 :",
                "for some P 1 , . . . , P D , Q 1 , . . . , Q D \u2208 S n , so that for each k \u2208 [D], P k , Q k are permutations that sort monotone decreasingly vectors Xa k and Y a k , respectively. In particular,",
                "where the (n!) 2D linear operators L \u03b3 : R n\u00d7d \u00d7 R n\u00d7d \u2192 R n\u00d7D , are defined by",
                "Claim: We claim that, for m \u2265 2nd and a generic linear operator B : R n\u00d7D \u2192 R m we have ker(B) \u2229 F = {0}. Such a generic linear operator has the kernel of dimension dim(ker(B)) = nD -m \u2264 n(D -2d). It is therefore sufficient to show that, for a generic subspace V \u2282 R n\u00d7D of dimension r \u2264 n(D -2d), for every \u03b3 \u2208 (S n ) 2D , V \u2229 F \u03b3 = {0}. This last claim follows from the observation dim(F \u03b3 ) \u2264 2nd.",
                "We now show how this claim proves the Theorem. Let B be such a linear map, and let",
                "Since \u03b2A is injective on R n\u00d7d it follows X = \u0176 . Thus \u03b2A,B is injective. On the other hand, for each \u03b3 = (P 1 , . . . ,",
                "n , the restriction of B to the linear space Ran(L \u03b3 ) is injective, and thus bounded below as a linear map: there is a \u03b3 > 0 so that for every",
                "bi-Lipschitz. By Theorem 3.10, the map \u03b2A is bi-Lipschitz. Therefore we get \u03b2A,B is bi-Lipschitz as well."
            ],
            "subsections": [
                {
                    "title": "Proof of Corollary 1.3. (1) It is clear that any continuous f induces a continuous",
                    "paragraphs": [
                        "Then a consequence of Tietze extension theorem (see problem 8 in \u00a712.1 of [33]) implies that \u03d5 admits a continuous extension g : R m \u2192 R. Thus g(\u03b2(X)) = f (X) for all X \u2208 R n\u00d7d . The converse is trivial.",
                        "(2) As at part (1), the Lipschitz continuous function f induces a Lipschitz continuous function \u03d5 : F \u2192 R. Since F \u2282 R m is a subset of a Hilbert space, by Kirszbraun extension theorem (see [38]), \u03d5 admits a Lipschitz continuous extension (even with the same Lipschitz constant!) g : R m \u2192 R so that g(\u03b2(X)) = f (X) for every X \u2208 R n\u00d7d . The converse is trivial."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Applications to Graph Deep Learning",
            "paragraphs": [
                "In this section we take an empirical look at the permutation invariant mappings presented in this paper. We focus on the problems of graph classification, for which we employ the PROTEINS FULL dataset [12], and graph regression, for which we employ the quantum chemistry QM9 dataset [32]. In both problems we want to estimate a function F : (A, Z) \u2192 p, where (A, Z) characterizes a graph where A \u2208 R n\u00d7n is an adjacency matrix and Z \u2208 R n\u00d7r is an associated feature matrix where the i th row encodes an array of r features associated with the i th node. p is a scalar output where we have p \u2208 {0, 1} for binary classification and p \u2208 R + for regression.",
                "We estimate F using a deep network that is trained in a supervised manor. The network is comprised of three successive components applied in series: \u0393, \u03c6, and \u03b6. \u0393 represents a graph deep network [23], which produces a set of embeddings X \u2208 R N \u00d7d across the nodes in the graph. Here N \u2265 n is chosen to accommodate the graph with the largest number of nodes. In this case, the last N -n rows of Y are filled with 0's. \u03c6 : R N \u00d7d \u2192 R m represents a permutation invariant mapping such as those proposed in this paper. \u03b6 : R m \u2192 R is a fully connected neural network. The entire end-to-end network is shown in Figure 1.",
                "In this paper, we model \u0393 using a Graph Convolutional Network (GCN) outlined in [23]. Let D \u2208 R n\u00d7n be the associated degree matrix for our graph G. Also let \u00c3 be the associated adjacency matrix of G with added self connection: \u00c3 = I + A, where I is the n \u00d7 n identity matrix, and D = D+I. Finally, we define the modified adjacency matrix \u00c2 = D-1/2 \u00c3 D-1/2 . A GCN layer is defined as H (l+1) = \u03c3( \u00c2H (l-1) W (l) ). Here H (l-1) represents the GCN state coming into the l th layer, \u03c3 represents a chosen nonlinear element-by-element operation such as ReLU, and W (l) represents a matrix of trainable weights assigned to the l th layer whose number of rows match the number of columns in H l and number of columns is set to the size of the embeddings at the (l+1)'th layer. The initial state H (0) of the network is set to the feature set of the nodes of the graph H (0) = Z.",
                "For \u03c6 we employ seven ( 7) different methods that are described next.",
                "(1) ordering: For the ordering method, we set D = d+1, \u03c6 ordering (X) = \u03b2 A (X) =\u2193 (XA) with A = [I 1] the identity matrix followed by a column of ones. The ordering and identity-based mappings have the notable disadvantage of not producing the same output embedding size for different sized graphs. To accommodate this and have consistently sized inputs for \u03b7, we choose to zero-pad \u03c6(X) for these methods to produce a vector in R m , where m = N D = N (d + 1) and N is the size of the largest graph in the dataset. (2) kernels: For the kernels method, (\u03c6 kernel (X)",
                ", where kernel vectors a 1 , . . . , a m \u2208 R d are generated randomly, each element of each vector is drawn from a standard normal distribution. Each resultant vector is then normalized to produce a kernel vector of magnitude one.",
                "When inputting the embedding X to the kernels mapping, we first normalized the embedding for each respective node. (3) identity: In this case \u03c6 id (X) = X, which is obviously not a permutation invariant map. (4) data augmentation: In this case \u03c6 data augment (X) = X but data augmentation is used. Our data augmentation scheme works as follows. We take the training set and create multiple permutations of the adjacency and associated feature matrix for each graph in the training set. We add each permuted graph to the training set to be included with the original graphs. In our experiments we use four added permutations for each graph when employing data augmentation. (5) sum pooling: The sum pooling method sums the feature values across the set of nodes: \u03c6 sum pooling (X) = 1 T n\u00d71 X. (6) sort pooling: The sort pooling method flips entire rows of X so that the last column is ordered descendingly, \u03c6 sort pool (X) = \u03a0X where \u03a0 \u2208 S n so that \u03a0 X(:, d) =\u2193 (X(: , d)). (7) set-2-set: This method employs a recurrent neural network that achieves permutation invariance through attention-based weighted summations. It has been introduced in [36].",
                "For our deep neural network \u03b7 we use a simple multilayer perceptron of size described below.",
                "where B 0 = B t=1 1 pt=0 and B 1 = B t=1 1 pt=1 = B -B 0 . These four statistics predict Precision P (\u03c4 ), Recall R(\u03c4 ) (also known as sensitivity or true positive rate), and Specificity S(\u03c4 ) (also known as true negative rate) (4.5) P (\u03c4 ) = T P (\u03c4 ) T P (\u03c4 ) + F P (\u03c4 )",
                ", R(\u03c4 ) = T P (\u03c4 ) T P (\u03c4 ) + F N (\u03c4 )",
                ", S(\u03c4 ) = T N (\u03c4 ) T N (\u03c4 ) + F P (\u03c4 ) Accuracy (ACC) is defined as the fraction of correct classification for default threshold \u03c4 = 1  2 over the set of batch samples:",
                "Area under the receiver operating characteristic curve (AUC) is computed from prediction scores as the area under true positive rate (TPR) vs. false positive rate (FPR) curve, i.e. the recall vs. 1-specificity curve",
                "where K is the number of thresholds. Average precision (AP) summarizes a precision-recall curve as the weighted mean of precision achieved at each threshold, with the increase in recall from the previous thresholds used as the weight:",
                "We track the binary cross entropy (BCE) through training and we compute it on the holdout set and a random node permutation of the holdout set (see Figures 2 and3). The lower the value the better.",
                "We look at the three performance metrics on the training set, the holdout set, and a random node permutation of the holdout set: see Figures 4, and5 for accuracy (ACC); see Figures 6, and7 for area under the receiver operating characteristic curve (AUC); and see Figures 8, and9 for average precision (AP). For all these performance metrics, the higher the score the better.  The authors of [22] utilized a Support Vector Machine (1-layer perceptron) for classification and obtained an accuracy (ACC) of 77% on the entire data set using 52 features, and an accuracy of 80% on a smaller set of 36 features. By comparison, our data augmentation method for d = 100 achieved an accuracy of 97.5% on training data set, but dropped dramatically to 73% on holdout data, and 72% on holdout data set with randomly permuted nodes. On the other hand, both the kernels method and the sum-pooling method with d = 50 achieved an accuracy of around 79% on training data set, while dropping accuracy performance by only 2% to around 77% on holdout data (as well as holdout data with nodes permuted).",
                "For d = 1, data augmentation performed the best on the training set with an area under the receiver operating characteristic (AUC) of 0.896, followed closely by the identity method with an AUC of 0.886. On the permuted holdout set however, sort-pooling performed the best with an AUC of 0.803. For d = 10, sum-pooling, ordering, and kernels performed well on the permuted holdout set with AUC's of 0.821, 0.820, and 0.818 respectively. The high performance of the identity method, data augmentation, and sort-pooling on the training set did not translate to the permuted holdout set at d = 10. By d = 100, sum-pooling still performed the best on the permuted holdout set with an AUC of 0.817. This was followed by the kernels method which achieved an AUC of 0.801 on the permuted holdout set.",
                "For experiments where d > 1, the identity method and data augmentation show a notable drop in performance from the training set to the holdout set. This trend is also, to a lesser extent, visible in the sort pooling and ordering methods. In the holdout permuted set we see significant oscillations in the performance of both the identity and data augmentation methods."
            ],
            "subsections": [
                {
                    "title": "Graph Regression.",
                    "paragraphs": [
                        "4.2.1. Methodology. For our experiments in graph regression we consider the qm9 dataset [32]. This dataset consists of 134 thousand molecules represented as graphs, where the nodes represent atoms and edges represent the bonds between them.",
                        "Each graph has between 3 and 29 nodes, 3 \u2264 n \u2264 29. Each node has 11 features, r = 11. We hold out 20 thousand of these molecules for evaluation purposes. The dataset includes 19 quantitative features for each molecule.",
                        "For the purposes of our study, we focus on electron energy gap (units eV ), which is \u2206\u03b5 in [14] whose chemical accuracy is 0.043eV and whose prediction performance of any machine learning technique is worse than any other feature. The best existing estimator for this feature is enn-s2s-ens5 from [17] and has a mean absolute error (MAE) of 0.0529eV which is 1.23 larger than the chemical accuracy. We run the end to end model with three GCN layers in \u0393, each with 50 hidden units. \u03b7 consists of three multi-layer perceptron layers, each with 150 hidden units. We use rectified linear units as our nonlinear activation function. Finally, we vary d, the size of the node embeddings that are outputted by \u0393. We set d equal to 1, 10, 50 and 100.",
                        "For each method and embedding size we train for 300 epochs. Note though that the data augmentation method will have experienced five times as many training steps due to the increased size of its training set. We use a batch size of 128 graphs. The loss function minimized during training is the mean square error (MSE) between the ground truth and the network output (see Figures 10,11)",
                        "where B = 128 is the batch size of 128 graphs and \u2206\u03b5 t is the electron energy gap of the t th graph (molecule). The performance metric is Mean Absolute Error (MAE)",
                        "We track the mean absolute error through the course of training. We look at this performance metric on the training set, the holdout set, and a random node permutation of the holdout set (see Figures 12,and 13).",
                        "4.2.2. Discussion. Numerical results at the end of training (after 300 epochs) are included in Tables 13, 14, 15 and 16. From the results we see that the ordering method performed best for d = 100 followed closely by the data augmentation method, while both the ordering method and the kernels method performed well for d = 10, though both fell slightly short of data augmentation which performed marginally better on both the training data and the holdout data, though with significantly more training iterations. For d = 1, the kernels method failed to train adequately. The identity mapping performed relatively well on training data (for d = 100 it achieved the smallest MAE among all methods and all parameters) and even the holdout data, however it lost its performance on the permuted holdout data. The identity mapping's failure to generalize across permutations of the holdout set is likely exacerbated by the fact that the QM9 data as presented to the network comes ordered in its node positions from heaviest atom to lightest. Data augmentation notably kept its performance despite this due to training on many permutations of the data. For d = 100, our ordering method achieved a MAE of 0.155eV on training data set and 0.187eV on holdout data set, which are 3.6 and 4.35 times larger than the chemical accuracy (0.043eV ), respectively. This is worse than the enn-s2s-ens5 method in [17] (current best method) that achieved a MAE 0.0529 (eV), 1.23 larger than the chemical accuracy, but better than the Coulomb Matrix (CM) representation in [34] "
                    ],
                    "subsections": []
                }
            ]
        }
    ]
}