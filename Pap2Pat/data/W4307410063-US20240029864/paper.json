{
    "id": "https://semopenalex.org/work/W4307410063",
    "authors": [
        "Benjamin L. Odry",
        "Arijit Sehanobish",
        "Anasuya Das",
        "Kawshik Kannan",
        "Nabila Abraham"
    ],
    "title": "Meta-learning Pathologies from Radiology Reports using Variance Aware  Prototypical Networks",
    "date": "2022-10-22",
    "abstract": "Large pretrained Transformer-based language models like BERT and GPT have changed the landscape of Natural Language Processing (NLP). However, fine tuning such models still requires a large number of training examples for each target task, thus annotating multiple datasets and training these models on various downstream tasks becomes time consuming and expensive. In this work, we propose a simple extension of the Prototypical Networks for few-shot text classification. Our main idea is to replace the class prototypes by Gaussians and introduce a regularization term that encourages the examples to be clustered near the appropriate class centroids. Experimental results show that our method outperforms various strong baselines on 13 public and 4 internal datasets. Furthermore, we use the class distributions as a tool for detecting potential out-of-distribution (OOD) data points during deployment.",
    "sections": [
        {
            "title": "Introduction",
            "paragraphs": [
                "Pretrained Transformer-based language models (PLMs) have achieved great success on many NLP tasks (Devlin et al., 2019;Brown et al., 2020), but still need a large number of in-domain labeled examples for finetuning (Yogatama et al., 2019). Learning to learn (Lake et al., 2015a;Schmidhuber, 1987;Bengio et al., 1997) from limited supervision is an important problem with widespread application in areas where obtaining labeled data can be difficult or expensive. To that end, metalearning methods have been proposed as effective solutions for few-shot learning (Hospedales et al., 2020). Current applications of such meta-learning methods have shown improved performance in fewshot learning for vision tasks such as learning to classify new image classes within a similar dataset. Namely, on classical few-shot image classification * Equal Contribution benchmarks, the training tasks are sampled from a \"single\" larger dataset (for ex: Omniglot (Lake et al., 2015b) and miniImageNet (Vinyals et al., 2016)), and the label space contains the same task structure for all tasks. There has been a similar trend of such classical methods in NLP as well (Geng et al., 2019). In contrast, in text classification tasks, the set of source tasks available during training and target tasks during evaluation can range from sentiment analysis to grammatical acceptability judgment (Bansal et al., 2020a,b). In recent works (Wang et al., 2021), the authors use a range of different source tasks (different not only in terms of input domain, but also their task structure i.e. label semantics, and number of labels) for meta-training and show successful performance on a wide range of downstream tasks. In spite of this success, meta-training on various source tasks is quite challenging as it requires resistance to overfitting to certain source tasks due to its few-shot nature and more task-specific adaptation due to the distinct nature among tasks (Roelofs et al., 2019).",
                "However, in medical NLP, collecting large number of diverse labeled datasets is difficult. In our institution, we collect high quality labeled radiology reports (which are always used as held out test data) and use it to train our internal annotators who then annotate our unlabeled data. This training process is expensive and time consuming. Our annotation process is described in section A. Thus a natural question is: if we have a large labeled dataset consisting of a lot of classes, can we use it to meta-train a model that can be used on a large number of downstream datasets where we have little to no training examples? This is a challenging problem as the reports can be structured differently based on the report type and there can be a substantial variation in writing style across radiologists from different institutions. Our main goal is to build out a set of extensible pipelines that can generalize to new pathologies typically in new sub-specialties while arXiv:2210.13979v2 [cs.LG] 10 Nov 2022 also generalizing across different health systems. In addition, the exact definition of the pathologies and their severity change can change depending on the clinical use case. This makes fully supervised approaches that rely on large labeled datasets expensive. Having few-shot capabilities allows us to annotate a handful of cases and rapidly expand the list of pathologies we can detect and classify. In addition, we can use our approach to generate pseudo labels for rare pathologies and enrich our validation and test sets for annotation by an in-house clinical team. Lastly our approach can be extended to support patient search and define custom cohorts of patients.",
                "Our contributions in this work are the following: (1) We develop a novel loss function that extends the vanilla prototypical networks and introduce a regularization term that encourages tight clustering of examples near the class prototypes. (2) We meta-train our models on a large labeled dataset on shoulder MRI reports (single domain) and show good performance on 4 diverse downstream classification tasks on radiology reports on knee, cervical spine and chest. In addition to our internal datasets, we show superior performance of our method on 13 public benchmarks over well-known methods like Leopard. Our model is very simple to train, easy to deploy unlike gradient based methods and just requires a few additional lines of codes to a vanilla prototypical network trainer. (3) We deploy our system and use the dataset statistics to inform out-of-distribution (OOD) cases."
            ],
            "subsections": []
        },
        {
            "title": "Related Work",
            "paragraphs": [
                "There are three common approaches to metalearning:",
                "metric-based, model-based, and optimization-based. Model agnostic meta-learning (MAML) (Finn et al., 2017) is an optimizationbased approach to meta-learning which is agnostic to the model architecture and task specification. Over the years, several variants of the method have shown that it is an ideal candidate for learning to learn from diverse tasks (Nichol et al., 2018;Raghu et al., 2019;Bansal et al., 2020b). However, to solve a new task, MAML type methods would require training a new classification layer for the task. In contrast, metric-based approaches, such as prototypical networks (Vinyals et al., 2016;Snell et al., 2017), being non-parametric in nature can handle varied number of classes and thus can be easily deployed. Given the simple nature of prototypical networks, a lot of work has been done to improve them (Allen et al., 2019;Zhang et al., 2019;Ding et al., 2022;Wang et al., 2021). Prototypical networks usually construct a class prototype (mean) using the support vectors to describe the class and, given a query example, assigns the class whose class prototype is closest to the query vector. In (Allen et al., 2019), the authors use a mixture of Gaussians to describe the class conditional distribution and in (Zhang et al., 2019); the authors try to model an unknown general class distribution. In (Ding et al., 2022), the authors use spherical Gaussians and a KL-divergence type function between the Gaussians to compute the function d in equation 2. However, the function used by the above authors is not a true metric, i.e. does not satisfy the triangle inequality. Triangle inequality is implicitly important since we use this metric as a form of distance which we optimize, so it makes sense to use a true metric. In this work we replace it by the Wasserstein distance which is a true metric and add in a regularization term that encourages the L 2 norm of the covariance matrices to be small, encouraging the class examples to be clustered close to the centroid. One of our main reasons to work with Gaussians is due to the closed form formula of the Wasserstein distance.",
                "Few shot learning (FSL) in the medical domain has been mostly focused in computer vision (Singh et al., 2021). There are only a few works that have applied FSL in medical NLP (Ge et al., 2022) but most of those works have only focused on different tasks on MIMIC-III (Johnson et al., 2016) which is a single domain dataset (patients from ICU and one hospital system). To the best of our knowledge, ours is the first study to successfully apply FSL on a diverse set of medical datasets (diverse in terms of tasks and patient populations)."
            ],
            "subsections": []
        },
        {
            "title": "Datasets",
            "paragraphs": [
                "All our internal datasets are MRI radiology reports detailing various pathologies in different body parts. Our models are meta-trained on a dataset of shoulder pathologies which is collected from 74 unique and de-identified institutions in the United States. 60 labels are chosen for training and 20 novel labels are chosen for validation. The number of training labels is similar to some well-known image datasets (Lake et al., 2015b;Vinyals et al., 2016;Wah et al., 2011). This diverse dataset has a rich label space detailing multiple structures in shoulder, granular pathologies and their severity levels in each structure. The relationship between the granularity/severity of these pathologies at different structures can be leveraged for other pathologies in different body parts and may lead to successful transfer to various downstream tasks. The labels are split such that all pathologies in a given structure appear at either training or validation but not both. More details about the label space can be found in section B. The figure 1 and the table 1 shows the distribution of labels and an example of this dataset can be found in figure 4. Our met- alearner is applied to 4 downstream binary classification tasks spanning different sub-specialities (cancer screening, musculoskeletal radiology, and neuro-radiology) that are both common as well as clinically important. The statistics for each task are given in table 2 : (1) High risk cancer screening for lung nodules (according to Fleischner guidelines (Nair et al., 2018) which bucket patients at high-risk of lung cancer and requiring follow up imaging immediately or within 3 months as belonging to Category High Risk ; we consider patients not at high-risk as Low Risk), (2) Complete Anterior Cruciate Ligament (ACL) tear (Grade 3) vs not Complete ACL tear, (3) Acute ACL tears (MRI examination was performed within 6 weeks of injury) and typified by the presence of diffuse or focal increased signal within the ligament vs not Acute ACL tear (Dimond et al., 1998), (4) Severe vs not severe neural foraminal stenosis in the cervical spine as severe foraminal stenosis may indicate nerve impingement, which is clinically significant. Acute tear in ACL refers to the age of the tear/injury whereas the complete tear refers to the integrity of the ligament. Our testing datasets are diverse and sampled from different institutions: the knee  "
            ],
            "subsections": []
        },
        {
            "title": "Workflow",
            "paragraphs": [
                "Our workflow consists of the following parts: A report is first de-identified according to HIPAA regulations and passed through a sentence parser (ex. Spacy (Honnibal et al., 2020)) that splits the report into sentences. In the shoulder dataset, each of these sentences is labeled with the appropriate structure and severity label and we filter out sentences that do not have such a label. We first train a meta-learner in an episodic fashion on this dataset and choose the best model based on metavalidation accuracy.",
                "For our downstream tasks, we use a body-part specific custom data processor to collect sentences related to a given structure (ACL in knee, different vertebrae in the cervical spine, the entire impression section for lung reports) and concatenate them together to create a paragraph describing all the pathologies in the structure of interest. Detailed description of preprocessing for different body parts, is presented in Appendix C. The concatenated text from the validation sets of each task is passed to our trained meta-learner to generate the relevant class statistics (mean and variance). We then perform pathology classification on the test set by using our trained meta-learner and the saved class statistics. The downstream tasks are similar to the shoulder task in the sense that the pathology classification is performed on a sequence of sentences that all pertain to the same anatomical structure. Thus our approach needs to learn the language that describes the severity of the pathology for a specific anatomical structure.",
                "We would like to shed some light on the complexity of the language we encounter. Since our dataset is sourced from multiple health systems, and not all reports follow a standard structure, there is a large amount of variation in the language describing the same diagnosis. For example: a severe tear can be referred to as a rupture, or only the size of the nodule is mentioned without specifying that it is low risk (see Appendix C for more examples). Furthermore, most of our pipelines attempt to classify the different severities for a given pathology and the language describing severity can vary. While it might be possible to construct a rule based system to extract the diagnoses and severities we are interested in, it will be difficult to generalize as we expand to more diagnoses as well as to new health systems."
            ],
            "subsections": []
        },
        {
            "title": "Prototypical Networks",
            "paragraphs": [
                "Prototypical Networks or ProtoNets (Snell et al., 2017) use an embedding function f \u03b8 to encode each input into a M -dimensional feature vector. A prototype is defined for every class c \u2208 L, as the mean of the set of embedded support data samples (S c ) for the given class, i.e.",
                "(1)",
                "The distribution over classes for a given test input x is a softmax over the inverse of distances between the test data embedding and prototype vectors.",
                "where d can be any (differentiable) distance function. The loss function is negative log-likelihood:",
                "ProtoNets are simple and easy to train and deploy.",
                "The mean is used to capture the entire conditional distribution P (y = c|x), thus losing a lot of information about the underlying distribution. A lot of work (Ding et al., 2022;Allen et al., 2019;Zhang et al., 2019) has focused on improving ProtoNets by taking into account the above observation. We extend ProtoNets by incorporating the variance (2nd moment) of the distribution and use distributional distance, i.e. 2-Wasserstein metric, directly generalizing the vanilla ProtoNets."
            ],
            "subsections": [
                {
                    "title": "Variance Aware ProtoNets",
                    "paragraphs": [
                        "In this work, we model each conditional distribution as a Gaussian. Now the main question is: how do we match a query example with a distribution?",
                        "The simplest thing here is to treat the query example as a Dirac distribution. With that formulation in mind, recall: the Wasserstein-Bures metric between Gaussians (m i , \u03a3 i ) is given by:",
                        ", where S c is the support set of examples belonging to class c, we compute the mean m c and covariance matrix \u03a3 c ; the computation of Wasserstein distance between a Gaussian and a query vector q (i.e. a Dirac) boils down to",
                        "The above formula shows that we can simplify our conditional distribution to be a Gaussian with a diagonal covariance matrix. This brings down our space complexity to store this covariance matrix from O(n 2 ) to O(n). Note, this is a direct generalization of the vanilla prototypical networks as the vanilla prototypical networks can be interpreted as computing the Wasserstein distance (aka simple L 2 distance) between two Dirac distributions (mean of the conditional distribution and the query sample). We also propose another variant of the above  called Isotropic Gaussian variant where we average over the diagonal entries of \u03a3 c , i.e. \u03b1 = 1 n (\u03a3 c ) ii and redefine \u03a3 c = \u03b1I, where I is the identity matrix, allowing us to just store the scalar \u03b1, further reducing the space complexity. Furthermore, we regularize the negative log likelihood loss to prevent the variance term from blowing up. Our new loss function reads:",
                        "where ways are the number of classes in the minibatch and || \u2022 || F is the Frobenius norm and we average the norm of the variance matrix over all the classes in a given meta-batch. The extra regularization term is designed to encourage the examples to be close to the appropriate cluster centroid. This term can also be seen as an entropic regularization term, i.e. up to a factor as the exponential of KL(p||q), where p = N (m c , \u03a3 c ) and q = N (m c , I). This type of entropy regularized Wasserstein distances is widely studied (Cuturi and Doucet, 2014;Altschuler et al., 2021).",
                        "A PyTorch style pseudocode is described in Algorithm 1, where the teal color refers to the changes to a vanilla prototypical networks trainer. We provide detailed motivation for using Wasserstein distance instead of KL divergence in section E.2. This also explains why we compute the Wasserstein distance between the query and the estimated class distribution instead of a simple likelihood."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Experiments",
            "paragraphs": [
                "All our experiments are run on 4 V100 16 GB GPU using PyTorch (Paszke et al., 2019) and Hugging-Face libraries (Wolf et al., 2020). Bert-base (Devlin et al., 2019), Clinical BERT (Alsentzer et al., 2019) and PubMedBERT (Gu et al., 2021) are used as our backbone models. Adapters (Pfeiffer et al., 2020) are applied to each of these backbone models. While training adapter based models, the BERT weights are frozen and only the adapter weights are updated, thus requiring less resources to train. This idea is similar to (Raghu et al., 2019) in the sense that we are reusing the features from these deep pretrained models. We compare our methods to Leopard (Bansal et al., 2020a), vanilla ProtoNets and big ProtoNets (Ding et al., 2022). Additional results with BERT-base and Clinical BERT backbones can be found in table 6 andtable 7. Meta-training is done in an episodic manner using 4-way 8-shot and 16-examples as support. For meta-training on the shoulder dataset, we set the variance regularizer hyperparameter to be .1. It is an important hyperparameter and detailed ablation study is conducted in section E.1. Other hyperparameters and design choices are described in section E.",
                "To prevent overfitting on the test set, we choose the best model from each of these experiments based on the meta-validation accuracy and apply it to our downstream classification tasks. We note that these downstream tasks are significantly different from the few shot regime these models are trained in. Moreover for these downstream tasks, we train BERT models on each task and a multitasking model to provide additional baselines.",
                "In all our experiments, PubMedBERT consistently outperforms BERT-base and Clinical BERT by an average of 5 points and 3 points respectively. We believe the reason behind the improved per-formance is the domain specific vocabulary. Even though Clinical BERT is pre-trained on MIMIC-III (Johnson et al., 2016), it still shares the same vocabulary as BERT-base.",
                "ProtoNet-BERT shows better performance and faster convergence rates during training and validation (Table 4), but it is outperformed by ProtoNet-AdapterBERT which has fewer orders of magnitude of parameters to learn (Table 3). Like (Wang et al., 2021) we believe that ProtoNet-BERT is more vulnerable to overfitting on the meta-training tasks than the ProtoNet-AdapterBERT. Finally, we note that even though Big ProtoNets work well on meta-validation, they fail on our downstream tasks. We hypothesize that it is due to the fact that big protonets are encouraged to have large radii which has the potential to become a bottleneck where the data distribution is highly imbalanced causing the spherical Gaussians to overlap. In fact, we have found that doing the exact opposite (i.e. constricting the norms of the covariance matrix), tends to produce better results. Finally instead of using the entire validation set to compute the class distribution, we also experiment with choosing a k shots from the validation set to compute the class distribution (figure 12 in section G).",
                "Our regularized Variance Aware ProtoNets with BERT-base + Adapter is also validated on 13 public datasets. For the models and datasets marked with * in table 5, we use the results reported in (Bansal et al., 2020a) and for those datasets, we use the code from (Wang et al., 2021) to generate the results for ProtoNet with Bottleneck Adapters while the rest of the results are taken from (Wang et al., 2021). The variance regularization hyper-parameter is set to .01 for these experiments. Our method beats Leopard by 5, 3 and 2 points on 4, 8 and 16 shots, respectively. The training details for these experiments can be found in section F."
            ],
            "subsections": []
        },
        {
            "title": "Deployment",
            "paragraphs": [
                "Based on the results described in table 3, we choose to deploy our regularized Variance Aware ProtoNet with Adapter-PubMedBERT. Our pipeline is deployed on AWS using a single p3.2x instance housed with one NVIDIA V100 GPU. The main pipeline components include (1) body-part specific report segmenter, (2) PubMedBERT backbone with adapters and (3) a dictionary of class prototypes and class variances, for all classes in the datasets. On inference, requests sent to the pipeline include a body part which the pipeline utilizes to load up the relevant report segmenter, class prototypes and variances. A report is then ingested by the pipeline, parsed by a sentencizer, grouped into segments according to its body part specific segmentation, and then passed to the model. Class probabilities and labels are inferred after computing the Wasserstein distance between the text embedding and the appropriate class distributions. These outputs and pipeline metadata are written out to an AWS Redshift database cluster. The entire pipeline is orchestrated in batch mode with a large enough batch size to maximize GPU capacity resulting in an average latency of 68ms/report. "
            ],
            "subsections": [
                {
                    "title": "Monitoring",
                    "paragraphs": [
                        "It is well-known that the BERT embeddings are highly anisotropic (Ethayarajh, 2019). We observe the same phenomenon in our meta-learned models as well (figure 3) which we use to our advantage to monitor OOD cases. For each class in a dataset, we  pick top k-dimensions (a hyperparameter) of maximum variance. We then take the union of these indices that we call the set of dataset indices i.e. the indices that explain the variance among all classes in the dataset. For any given query example, we compute the absolute difference ( d j ) between its embedding vector ( q) and class centroids ( v j ), i.e. the i-th coordinate d j :",
                        "We then select top k dimensions of the each of these d j . We propose an OOD metric called Average Variance Indices (AVI_k) by the overlap between the topk difference vector indices and the top-k dataset indices, i.e. AV I_k :=",
                        "dataset indices , where c is the number of classes. For ex: in case of the lung dataset: The text \"The heart is normal in size. There is no pericardial effusion. The pulmonary artery is enlarged.\" shows an AVI_10 score .79, whereas \"L1L2: There is no disc herniation in lumbar spine.\" gives a score of .31. As part of our monitoring, we threshold reports with an AVI_10 < .5 to further investigate if the report is OOD."
                    ],
                    "subsections": []
                }
            ]
        },
        {
            "title": "Conclusion",
            "paragraphs": [
                "We extend Prototypical Networks by using Wasserstein distances instead of Euclidean distances and introduce a regularization term to encourage the class examples to be clustered close to the class prototype. By training our models on a label rich dataset (shoulder MRI reports), we show successful performance on a variety of tasks. Since the model weights are reused for all tasks, a single model is deployed enabling us to cut inference costs. Moreover, adapters are used allowing us to tune smaller number of parameters (\u223c 10 million) resulting in huge training cost savings. Our model is also benchmarked on 13 public datasets and outperforms strong baselines like Leopard. Current work is underway to make our training dataset more diverse so that our models are more generalizable."
            ],
            "subsections": []
        },
        {
            "title": "Ethical Considerations",
            "paragraphs": [
                "Due to various legal and institutional concerns arising from the sensitivity of clinical data, it is difficult for researchers to gain access to relevant data except for MIMIC (Johnson et al., 2016). Despite its large size (covering over 58k hospital admissions), it is only representative of patients from a specific clinical domain (the intensive care unit) and geographic location (a single hospital in the United States). We can not expect such a sample to be representative of either the larger population of patient admissions or other geographical regions/hospital systems. We have tried to address this partially by collecting radiology data for various body parts across multiple practices in the US. However we are always mindful that our work may not generalize to new body parts/pathologies and radiology practices (see Section H). Even though we introduce a simple OOD metric, we realize it is far from perfect. We understand the need to minimize ethical risks of AI implementation like threats to privacy and confidentiality, informed consent, and patient autonomy. And thus we strongly believe that stakeholders should be flexible in incorporating AI technology as a complementary tool and not a replacement for a physician. Thus, we develop our workflows, annotation guidelines and generate actionable insights by working in conjunction with a varied group of radiologists and medical professionals to minimize these above risks. Finally our pipeline as deployed is meant as a pseudo-labeling tool which we expect would cut down on expensive annotation costs but can potentially introduce some bias in our pseudo-labels."
            ],
            "subsections": []
        },
        {
            "title": "A Annotation",
            "paragraphs": [
                "First we collect data from various sources and a part of the data are annotated by our team of in-house expert annotators with deep clinical expertise, which we use as test and development sets for our model training. We then use this annotated data to train a larger pool of other annotators who are generally medical students. They are provided clear guidelines on the task and performance is measured periodically on a benchmark set and feedback is provided. As of the writing of the manuscript, the validation and the test sets as described in section 3 are being used to train the annotators. After the completion of their training, the annotators will annotate the remaining unlabeled data that will be used as a training data for our models. The entire process is slow but is designed to generate high quality annotated data. We believe that our few shot models can be used as a source of pseudolabels and will greatly simplify and quicken our annotation process."
            ],
            "subsections": []
        },
        {
            "title": "B Shoulder Dataset",
            "paragraphs": [
                "In this section we will briefly describe our label rich shoulder dataset that is used as meta-training and meta-validation sets. There are 80 labels for the shoulder dataset. They range from Clinical history, metadata, Impressions, Finding to various granular pathologies at different structures in the shoulder like AC joint, Rotator Cuff, Muscles, Bursal Fluid, Supraspinatus, Infraspinatus, Subscapularis, Labrum, Glenohumeral Joint, Humeral Head, Acromial Morphology, Impingement: AC Joint. The labels are split such that all pathologies in a given structure appear at either training or validation but not both. We believe that such a split would help a model to learn the key words that may describe the granularity of a pathology in a given structure of interest. The dataset level statistics can be found in figure 1 andtable 1. An example of the shoulder data is shown in figure 4."
            ],
            "subsections": []
        },
        {
            "title": "C Detailed Workflow",
            "paragraphs": [
                "We now present a detailed description of various body part specific workflows. All reports, irrespective of body part, are first de-identified according"
            ],
            "subsections": []
        },
        {
            "title": "Text Labels",
            "paragraphs": [
                "The AC joint and anterior acromion show evidence of prior subacromial decompression and there may have been a distal clavicle excision as well with widening of the AC fluid in the joint glenohumeral joint/ labrum."
            ],
            "subsections": []
        },
        {
            "title": "AC Joint: Mild Arthritis with Edema",
            "paragraphs": [
                "Type Il acromion with hypertrophic changes causing impingement and partial rotator cuff tear of the infraspinatus and supraspinatus myotendinous junction."
            ],
            "subsections": []
        },
        {
            "title": "Impingement: Acromion",
            "paragraphs": [
                "Mild subacromial-subdeltoid bursitis. Findings are age-indeterminate unless otherwise specified.",
                "Bursal Fluid: Small Acromioclavicular joint: Anatomic alignment. No substantial degenerative change.",
                "AC Joint: Normal",
                "There is fraying of the anterior labrum above the level of the equator. Labrum: Normal or mild degeneration to HIPAA regulations. We then pass the report through a sentence parser to parse the report in sentences."
            ],
            "subsections": []
        },
        {
            "title": "C.1 Lung Dataset",
            "paragraphs": [
                "For the lung dataset, we use a report segmenter which is a rule-based regex to extract the \"Impression\" section from the entire report. This section can be thought as the summary of the report and contains all the critical information like number of lung nodules and their sizes and potential for malignancy. This section text is used for final classification task as shown in figure 5. Figure 6 shows examples of the labels in the dataset. Several pulmonary nodules are present bilaterally with the largest in the left upper lobe medially measuring 1.9 x 1 .7 cm (image 17 series 6). A nodule more superiorly in the medial left upper lobe measures 1.4 x 1 .2 cm (image 38). A right middle lobe nodule measures 1.2 x 1.0 cm. The remaining nodules measure less than 1 cm. A tiny calcified granuloma is visible in the right upper lobe. No acute infiltrate is identified. The central airways are patent. No pleural or pericardial effusion is present. The heart is normal in size. Thoracic aorta is normal in caliber. No mediastinal mass or adenopathy is evident on this unenhanced exam. Included images of the upper abdomen show no mass or acute abnormality. No aggressive osseous lesions are evident. There is disc degeneration in the thoracic spine. A subcentimeter sclerotic focus in the T3 vertebral body is compatible with a bone island. IMPRESSION: Several pulmonary nodules bilaterally measuring up to 1.9 cm, suspicious for metastatic disease. WARNING : this patient with history of malignancy."
            ],
            "subsections": []
        },
        {
            "title": "IMPRESSION:",
            "paragraphs": [
                "Several pulmonary nodules bilaterally measuring up to 1.9 cm, suspicious for metastatic disease. WARNING : this patient with history of malignancy. "
            ],
            "subsections": []
        },
        {
            "title": "C.2 Cervical Dataset",
            "paragraphs": [
                "Our task in the dataset is to predict the severity of a neural foraminal stenosis for each motion segment -the smallest physiological motion unit of the spinal cord (Swartz et al., 2005).",
                "Breaking information down at the motion segment level in this way enables pathological findings to be correlated with clinical exam findings, and can inform future treatment interventions. A BERT based NER model is used to identify the motion segment(s) referenced in each sentence, and all the sentences containing a particular motion segment are concatenated together. We also use additional rule-based logic to assign motion segments to relevant sentences that may not mention a motion segment in it. We then predict the disease severity using this concatenated text at each motion segment. This data pre-processing mostly follows the ideas and the steps outlined in (Sehanobish et al., 2022). Figure 7 shows our preprocessing steps and figure 8 shows examples in the datasets. "
            ],
            "subsections": []
        },
        {
            "title": "C.3 Knee Dataset",
            "paragraphs": [
                "The data processing steps for the knee dataset is similar to the cervical dataset. A BERT based NER model is used to tag sentences that mention the structure of importance, i.e. the anterior cruciate ligament (ACL). We group all the sentences together that mention ACL and we use these grouped sentences to predict our pathology severity as shown in the workflow (figure 9). An example of the labels in the knee dataset can be found in figure 10. Text Acute Tear"
            ],
            "subsections": []
        },
        {
            "title": "Complete Tear",
            "paragraphs": [
                "The anterior cruciate ligament is intact and there is a partial tear of the posterior cruciate ligament with a thin residual component of the distal half of the PCL still intact. 0 0",
                "While there is some edema in the ACL, there appear to be intact fibers. This may indicate ACL sprain or partial tear. A complete tear is not identified. Secondary signs of ACL insufficiency are not identified. While there is likely an ACL sprain or mild partial tear, there are intact ACL fibers.The pattern of bone bruises raises some concern for an ACL tear, but the femoral bone bruises slightly more lateral than commonly seen.",
                "1 0",
                "Proximal ACL tear and PCL intact. Left knee MRI demonstrates: Complete ACL tear with bone bruises in the medial tibial plateau, medial femoral condyle and lateral femoral condyle.",
                "1 1",
                "There is no normal anterior cruciate ligament identified. There is diffuse intermediate signal within the posterior cruciate ligament on the proton density images and to a lesser extent on the T2-weighted images compatible with chronic PCL degeneration. While this could be the sequela of old surgery, degenerative tearing of the meniscus including involvement of the root attachment cannot be excluded. Nonvisualization of the ACL compatible with an old ACL tear. "
            ],
            "subsections": []
        },
        {
            "title": "D Additional Experiments",
            "paragraphs": [
                "We also experiment with BERT-base and Clinical BERT as additional backbones. We add adapters to these backbones as well. Finally, we choose the best model based on meta-validation accuracy and use it for our downstream tasks. In all our experiments, PubMedBERT-based backbones outperform the BERT-base and the Clinical BERT backbones."
            ],
            "subsections": []
        },
        {
            "title": "E Hyperparameters and Additional Experimental Details",
            "paragraphs": [
                "In this section, we will describe the hyperparameters used for experiments on our internal and  public datasets and explain some of the design choices. Table 8 shows the best hyperparameters used for our experiments. For our internal dataset, we use the Pfeiffer configuration in the adapter implementation from (Pfeiffer et al., 2020), whereas for the public datasets we use the exact implementation and configuration as in (Wang et al., 2021) for a fair comparison to the results reported there. For all vanilla ProtoNet experiments, we use the Euclidean distance as it outperforms the cosine distance. All BERT models without adapters are trained with 8 shots and 8 support due to memory considerations. We choose learning rate and the variance regularizer for each model from {1e -5, 2e -5, 5e -5, 1e -4} and {1e -4, 1e -3, .01, .1, .5} based on the validation performance. For all the experiments, a dropout layer is added after the final BERT layer.",
                "For our internal dataset, we also experiment with {2, 3, 4} ways and {4, 6, 8} shots and {4, 6, 8, 12, 16} support. The experiments with 2way and 3-way produce poor results on our downstream tasks irrespective of the number of shots and support. During training with 4-way, the metavalidation results for lower support show worse performance than the numbers reported in table 4. We believe that it is caused by the high variability between the various groups of samples of a given   With these assumptions,",
                "Note that Wasserstein distance does not change if the variance changes (w can be arbitrarily large) whereas the KL divergence does. In fact, this is pointed out in (Ding et al., 2022) where their goal is to create spherical Gaussians with large radii. However, we found that having large variance produces worse results in our downstream tasks. Finally similar dependence "
            ],
            "subsections": []
        },
        {
            "title": "F Public Benchmarks",
            "paragraphs": [
                "In this section, we describe the training procedure for the public benchmark datasets. The baseline results are taken from (Wang et al., 2021;Bansal et al., 2020a). We have followed the same meta-training procedure as described in (Wang et al., 2021). Specifically, for meta-training, WNLI (m/mm), SST-2, QQP, RTE, MRPC, QNLI, and the SNLI datasets (Bowman et al., 2015) are used. The validation set of each dataset is used for hyperparameter searching and model selection. The models are trained by sampling episodes from the metatraining tasks. The sampling process first selects a dataset and then randomly selects m examples for each class as the support set and another k-shots as the query set and the probability of a selected task is proportional to the square root of its dataset size (Bansal et al., 2020b). For meta-testing, we use 13 datasets ranging from NLI, text classification and sentiment analysis. For the models and datasets marked with * , we use the results reported in (Bansal et al., 2020a) and for those datasets, we use the code from (Wang et al., 2021) to generate the results for ProtoNet with Bottleneck Adapters while the rest of the results are taken from (Wang et al., 2021). We reuse their implementation and configuration of their adapters but modify the loss function with the Wasserstein distance along with our variance regularization term. Table 5 shows the superior performance of our method beating all the baselines. For detailed hyperparameters, please see section E. Our method without the variance regularization term shows similar performance to that of the Leopard baselines. For the isotropic variant method, it shows similar performance to Leopard with the variance regularization term and worse without."
            ],
            "subsections": []
        },
        {
            "title": "G Stability of the Prototypes",
            "paragraphs": [
                "For simplicity, we use our entire validation sets to compute prototypes. In this section we show how our results vary if we choose a subset of our validation set to create the prototypes. The figure 12 shows the F1 scores when a subset of the data is used to compute the prototypes and the variances for a given class. "
            ],
            "subsections": []
        },
        {
            "title": "H Failure Cases",
            "paragraphs": [
                "We also test our models on few additional tasks like (i) predicting the severity of disc herniation in our cervical dataset and (ii) predict the presence of cord compression at various motion segments in our internal dataset on the lumbar spine. Our models achieve an F1 score of .51 and .39 respectively. The figure 13 shows how the classes are distributed. We attribute the failures to the poor separability between classes and the high variance in the data distribution.",
                "It is an ongoing project to understand what makes our model work for these downstream tasks and why our model works on some tasks and not others. We hope that by simply increasing the diversity of our training data or applying newer adapter architectures like Mix-and-Match Adapter (He et al., 2022) and Compacter (Karimi Mahabadi et al., 2021), our current methods will work on a wide range of downstream pathologies."
            ],
            "subsections": []
        },
        {
            "title": "E.1 Effect of Regularization on means and variances",
            "paragraphs": [
                "Table 9 illustrates the benefits of adding the regularization term. The regularization term not only aids in lowering the variances but also manages to push the centroids away further which we believe sheds some light on our method's success in downstream classification tasks. We also carry out various ablation studies by changing the regularization hyperparameter."
            ],
            "subsections": []
        },
        {
            "title": "E.2 Metric and other modeling choices",
            "paragraphs": [
                "In (Snell et al., 2017), only the sample means (i.e. means of the support vectors) are used to estimate the true population mean. In fact, by the Central limit theorem, we can use the sample variance (after normalization) to get an unbiased estimate of the population variance. Unlike the original work, we sought to use this extra information to better understand the class distribution. The Gaussian assumption is strong but it is motivated by the fact that it allows us to compute Wasserstein distances in a computationally tractable manner. Finally to motivate the choice of using the Wasserstein distance instead of a Bergman divergence like KL divergence, consider the following motivating example, N 1 (\u00b5 1 , \u03a3 1 ), N 2 (\u00b5 2 , \u03a3 2 ) be 2 Gaussians and for simplicity assume: \u03a3 1 = \u03a3 2 = wI and \u00b5 1 = \u00b5 2 ."
            ],
            "subsections": []
        }
    ]
}